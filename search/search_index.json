{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solutions to Introduction to Algorithms Third Edition Getting Started This website contains nearly complete solutions to the bible textbook - Introduction to Algorithms Third Edition published by Thomas H. Cormen , Charles E. Leiserson , Ronald L. Rivest and Clifford Stein . Hope to reorganize solutions to help more people and myself study algorithms. By using Markdown (.md) files, it's much more readable on portable devices now. \"Many a little makes a mickle.\" Contributors Thanks to: the Instructor's Manual by Thomas H. Cormen , @skanev , @CyberZHG , @yinyanghu , @ajl213 , etc. Special thanks to @JeffreyCA , who fixed math rendering on iOS Safari in #26 . If I miss your name here, please tell me! Feel free to give me your feedback if any adjustment is needed with the sorted solutions. You can press the \"pencil icon\" in the upper right corner to edit the contents or simply open an issue in my repository . How I generate this website I use the static site generator MkDocs and the beautiful theme Material for MkDocs to build this website! All mathematical equations are rendered by KaTeX , and I added overflow-x: auto to prevent overflow issue on small screen devices, so you can scroll horizontally in some math display equations. More Informations I recently rebased my repository for clearer commit histories. Therefore, if you have forked the repository before, consider reforking it again. For more informations, you can visit my GitHub: walkccc (Jay Chen) . Updated to this new page on April 13, 2018 at 04:48 (GMT+8) . License Licensed under the MIT License. document.body.dataset.mdColorPrimary = \"indigo\"; document.body.dataset.mdColorAccent = \"deep-orange\";","title":"\u5f15\u8a00"},{"location":"#solutions-to-introduction-to-algorithms-third-edition","text":"","title":"Solutions to Introduction to Algorithms Third Edition"},{"location":"#getting-started","text":"This website contains nearly complete solutions to the bible textbook - Introduction to Algorithms Third Edition published by Thomas H. Cormen , Charles E. Leiserson , Ronald L. Rivest and Clifford Stein . Hope to reorganize solutions to help more people and myself study algorithms. By using Markdown (.md) files, it's much more readable on portable devices now. \"Many a little makes a mickle.\"","title":"Getting Started"},{"location":"#contributors","text":"Thanks to: the Instructor's Manual by Thomas H. Cormen , @skanev , @CyberZHG , @yinyanghu , @ajl213 , etc. Special thanks to @JeffreyCA , who fixed math rendering on iOS Safari in #26 . If I miss your name here, please tell me! Feel free to give me your feedback if any adjustment is needed with the sorted solutions. You can press the \"pencil icon\" in the upper right corner to edit the contents or simply open an issue in my repository .","title":"Contributors"},{"location":"#how-i-generate-this-website","text":"I use the static site generator MkDocs and the beautiful theme Material for MkDocs to build this website! All mathematical equations are rendered by KaTeX , and I added overflow-x: auto to prevent overflow issue on small screen devices, so you can scroll horizontally in some math display equations.","title":"How I generate this website"},{"location":"#more-informations","text":"I recently rebased my repository for clearer commit histories. Therefore, if you have forked the repository before, consider reforking it again. For more informations, you can visit my GitHub: walkccc (Jay Chen) . Updated to this new page on April 13, 2018 at 04:48 (GMT+8) .","title":"More Informations"},{"location":"#license","text":"Licensed under the MIT License. document.body.dataset.mdColorPrimary = \"indigo\"; document.body.dataset.mdColorAccent = \"deep-orange\";","title":"License"},{"location":"color/","text":"Primary colors Default: Indigo Click on a tile to change the primary color of the theme: Red Pink Purple Deep Purple Indigo Blue Light Blue Cyan Teal Green Light Green Lime Yellow Amber Orange Deep Orange Brown Grey Blue Grey White var buttons = document.querySelectorAll(\"button[data-md-color-primary]\"); Array.prototype.forEach.call(buttons, function(button) { button.addEventListener(\"click\", function() { document.body.dataset.mdColorPrimary = this.dataset.mdColorPrimary; }) }) Accent colors Default: Deep Orange Click on a tile to change the accent color of the theme: Red Pink Purple Deep Purple Indigo Blue Light Blue Cyan Teal Green Light Green Lime Yellow Amber Orange Deep Orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\"); Array.prototype.forEach.call(buttons, function(button) { button.addEventListener(\"click\", function() { document.body.dataset.mdColorAccent = this.dataset.mdColorAccent; console.log(this.dataset.mdColorAccent); }) })","title":"Color"},{"location":"color/#primary-colors","text":"Default: Indigo Click on a tile to change the primary color of the theme: Red Pink Purple Deep Purple Indigo Blue Light Blue Cyan Teal Green Light Green Lime Yellow Amber Orange Deep Orange Brown Grey Blue Grey White var buttons = document.querySelectorAll(\"button[data-md-color-primary]\"); Array.prototype.forEach.call(buttons, function(button) { button.addEventListener(\"click\", function() { document.body.dataset.mdColorPrimary = this.dataset.mdColorPrimary; }) })","title":"Primary colors"},{"location":"color/#accent-colors","text":"Default: Deep Orange Click on a tile to change the accent color of the theme: Red Pink Purple Deep Purple Indigo Blue Light Blue Cyan Teal Green Light Green Lime Yellow Amber Orange Deep Orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\"); Array.prototype.forEach.call(buttons, function(button) { button.addEventListener(\"click\", function() { document.body.dataset.mdColorAccent = this.dataset.mdColorAccent; console.log(this.dataset.mdColorAccent); }) })","title":"Accent colors"},{"location":"Chap01/1.1/","text":"1.1-1 Give a real-world example that requires sorting or a real-world example that requires computing a convex hull. Sorting: browse the price of the restaurants with ascending prices on NTU street. Convex hull: computing the diameter of set of points. 1.1-2 Other than speed, what other measures of efficiency might one use in a real-world setting? Memory efficiency and coding efficiency. 1.1-3 Select a data structure that you have seen previously, and discuss its strengths and limitations. Linked-list: Strengths: insertion and deletion. Limitations: random access. 1.1-4 How are the shortest-path and traveling-salesman problems given above similar? How are they different? Similar: finding path with shortest distance. Different: traveling-salesman has more constrains. 1.1-5 Come up with a real-world problem in which only the best solution will do. Then come up with one in which a solution that is \"approximately\" the best is good enough. Best: find the GCD of two positive integer numbers. Approximately: find the solution of differential equations.","title":"1.1 Algorithms"},{"location":"Chap01/1.1/#11-1","text":"Give a real-world example that requires sorting or a real-world example that requires computing a convex hull. Sorting: browse the price of the restaurants with ascending prices on NTU street. Convex hull: computing the diameter of set of points.","title":"1.1-1"},{"location":"Chap01/1.1/#11-2","text":"Other than speed, what other measures of efficiency might one use in a real-world setting? Memory efficiency and coding efficiency.","title":"1.1-2"},{"location":"Chap01/1.1/#11-3","text":"Select a data structure that you have seen previously, and discuss its strengths and limitations. Linked-list: Strengths: insertion and deletion. Limitations: random access.","title":"1.1-3"},{"location":"Chap01/1.1/#11-4","text":"How are the shortest-path and traveling-salesman problems given above similar? How are they different? Similar: finding path with shortest distance. Different: traveling-salesman has more constrains.","title":"1.1-4"},{"location":"Chap01/1.1/#11-5","text":"Come up with a real-world problem in which only the best solution will do. Then come up with one in which a solution that is \"approximately\" the best is good enough. Best: find the GCD of two positive integer numbers. Approximately: find the solution of differential equations.","title":"1.1-5"},{"location":"Chap01/1.2/","text":"1.2-1 Give an example of an application that requires algorithmic content at the application level, and discuss the function of the algorithms involved. Drive navigation. 1.2-2 Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$ , insertion sort runs in $8n^2$ steps, while merge sort runs in $64n\\lg n$ steps. For which values of $n$ does insertion sort beat merge sort? $$ \\begin{aligned} 8n^2 & < 64n\\lg n \\\\ 2^n & < n^8 \\\\ n & \\le 43. \\end{aligned} $$ 1.2-3 What is the smallest value of $n$ such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine? $$ \\begin{aligned} 100n^2 & < 2^n \\\\ n & \\ge 15. \\end{aligned} $$","title":"1.2 Algorithms as a technology"},{"location":"Chap01/1.2/#12-1","text":"Give an example of an application that requires algorithmic content at the application level, and discuss the function of the algorithms involved. Drive navigation.","title":"1.2-1"},{"location":"Chap01/1.2/#12-2","text":"Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$ , insertion sort runs in $8n^2$ steps, while merge sort runs in $64n\\lg n$ steps. For which values of $n$ does insertion sort beat merge sort? $$ \\begin{aligned} 8n^2 & < 64n\\lg n \\\\ 2^n & < n^8 \\\\ n & \\le 43. \\end{aligned} $$","title":"1.2-2"},{"location":"Chap01/1.2/#12-3","text":"What is the smallest value of $n$ such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine? $$ \\begin{aligned} 100n^2 & < 2^n \\\\ n & \\ge 15. \\end{aligned} $$","title":"1.2-3"},{"location":"Chap01/Problems/1-1/","text":"For each function $f(n)$ and time $t$ in the following table, determine the largest size $n$ of a problem that can be solved in time $t$, assuming that the algorithm to solve the problem takes $f(n)$ microseconds. $$ \\begin{array}{cccccccc} & \\text{1 second} & \\text{1 minute} & \\text{1 hour} & \\text{1 day} & \\text{1 month} & \\text{1 year} & \\text{1 century} \\\\ \\hline \\lg n & 2^{10^6} & 2^{6 \\times 10^7} & 2^{3.6 \\times 10^9} & 2^{8.64 \\times 10^{10}} & 2^{2.59 \\times 10^{12}} & 2^{3.15 \\times 10^{13}} & 2^{3.15 \\times 10^{15}} \\\\ \\sqrt n & 10^{12} & 3.6 \\times 10^{15} & 1.3 \\times 10^{19} & 7.46 \\times 10^{21} & 6.72 \\times 10^{24} & 9.95 \\times 10^{26} & 9.95 \\times 10^{30} \\\\ n & 10^6 & 6 \\times 10^7 & 3.6 \\times 10^9 & 8.64 \\times 10^{10} & 2.59 \\times 10^{12} & 3.15 \\times 10^{13} & 3.15 \\times 10^{15} \\\\ n\\lg n & 6.24 \\times 10^4 & 2.8 \\times 10^6 & 1.33 \\times 10^8 & 2.76 \\times 10^9 & 7.19 \\times 10^{10} & 7.98 \\times 10^{11} & 6.86 \\times 10^{13} \\\\ n^2 & 1000 & 7745 & 60000 & 293938 & 1609968 & 5615692 & 56156922 \\\\ n^3 & 100 & 391 & 1532 & 4420 & 13736 & 31593 & 146645 \\\\ 2^n & 19 & 25 & 31 & 36 & 41 & 44 & 51 \\\\ n! & 9 & 11 & 12 & 13 & 15 & 16 & 17 \\end{array} $$","title":"Problem 1-1"},{"location":"Chap02/2.1/","text":"2.1-1 Using Figure 2.2 as a model, illustrate the operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$. $$ \\begin{aligned} A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 59, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 59, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 58, 59 \\rangle \\end{aligned} $$ 2.1-2 Rewrite the $\\text{INSERTION-SORT}$ procedure to sort into nonincreasing instead of nondecreasing order. 1 2 3 4 5 6 7 8 INSERTION - SORT ( A ) for j = 2 to A . length key = A [ j ] i = j - 1 while i > 0 and A [ i ] < key A [ i + 1 ] = A [ i ] i = i - 1 A [ i + 1 ] = key 2.1-3 Consider the searching problem : Input : A sequence of $n$ numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and a value $v$. Output: An index $i$ such that $v = A[i]$ or the special value $\\text{NIL}$ if $v$ does not appear in $A$. Write pseudocode for linear search , which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties. 1 2 3 4 5 LINEAR - SEARCH ( A , v ) for i = 1 to A . length if A [ i ] == v return i return NIL Loop invariant: At the start of each iteration of the for loop, the subarray $A[1..i - 1]$ consists of elements that are different than $v$. Initialization: Initially the subarray is the empty array, so the prove is trivial. Maintenance: On each step, we know that $A[1..i - 1]$ does not contain $v$. We compare it with $A[i]$. If they are the same, we return $i$, which is a correct result. Otherwise, we continue to the next step. We have already insured that $A[1..i - 1]$ does not contain $v$ and that $A[i]$ is different from $v$, so this step preserves the invariant. Termination: The loop terminates when $i > A.length$. Since $i$ increases by $1$ and $i > A.length$, we know that all the elements in $A$ have been checked and it has been found that $v$ is not among them. Thus, we return $\\text{NIL}$. 2.1-4 Consider the problem of adding two $n$-bit binary integers, stored in two $n$-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an $(n + 1)$-element array $C$. State the problem formally and write pseudocode for adding the two integers. Input: An array of booleans $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and an array of booleans $B = \\langle b_1, b_2, \\ldots, b_n \\rangle$, each representing an integer stored in binary format (each digit is a number, either $0$ or $1$, least-significant digit first) and each of length $n$. Output: An array $C = \\langle c_1, c_2, \\ldots, c_{n + 1} \\rangle$ such that $C' = A' + B'$ where $A'$, $B'$ and $C'$ are the integers, represented by $A$, $B$ and $C$. 1 2 3 4 5 6 7 8 ADD - BINARY ( A , B ) C = new integer [ A . length + 1 ] carry = 0 for i = 1 to A . length C [ i ] = ( A [ i ] + B [ i ] + carry ) % 2 // remainder carry = ( A [ i ] + B [ i ] + carry ) / 2 // quotient C [ i ] = carry return C","title":"2.1 Insertion sort"},{"location":"Chap02/2.1/#21-1","text":"Using Figure 2.2 as a model, illustrate the operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$. $$ \\begin{aligned} A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 59, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 59, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 58, 59 \\rangle \\end{aligned} $$","title":"2.1-1"},{"location":"Chap02/2.1/#21-2","text":"Rewrite the $\\text{INSERTION-SORT}$ procedure to sort into nonincreasing instead of nondecreasing order. 1 2 3 4 5 6 7 8 INSERTION - SORT ( A ) for j = 2 to A . length key = A [ j ] i = j - 1 while i > 0 and A [ i ] < key A [ i + 1 ] = A [ i ] i = i - 1 A [ i + 1 ] = key","title":"2.1-2"},{"location":"Chap02/2.1/#21-3","text":"Consider the searching problem : Input : A sequence of $n$ numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and a value $v$. Output: An index $i$ such that $v = A[i]$ or the special value $\\text{NIL}$ if $v$ does not appear in $A$. Write pseudocode for linear search , which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties. 1 2 3 4 5 LINEAR - SEARCH ( A , v ) for i = 1 to A . length if A [ i ] == v return i return NIL Loop invariant: At the start of each iteration of the for loop, the subarray $A[1..i - 1]$ consists of elements that are different than $v$. Initialization: Initially the subarray is the empty array, so the prove is trivial. Maintenance: On each step, we know that $A[1..i - 1]$ does not contain $v$. We compare it with $A[i]$. If they are the same, we return $i$, which is a correct result. Otherwise, we continue to the next step. We have already insured that $A[1..i - 1]$ does not contain $v$ and that $A[i]$ is different from $v$, so this step preserves the invariant. Termination: The loop terminates when $i > A.length$. Since $i$ increases by $1$ and $i > A.length$, we know that all the elements in $A$ have been checked and it has been found that $v$ is not among them. Thus, we return $\\text{NIL}$.","title":"2.1-3"},{"location":"Chap02/2.1/#21-4","text":"Consider the problem of adding two $n$-bit binary integers, stored in two $n$-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an $(n + 1)$-element array $C$. State the problem formally and write pseudocode for adding the two integers. Input: An array of booleans $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and an array of booleans $B = \\langle b_1, b_2, \\ldots, b_n \\rangle$, each representing an integer stored in binary format (each digit is a number, either $0$ or $1$, least-significant digit first) and each of length $n$. Output: An array $C = \\langle c_1, c_2, \\ldots, c_{n + 1} \\rangle$ such that $C' = A' + B'$ where $A'$, $B'$ and $C'$ are the integers, represented by $A$, $B$ and $C$. 1 2 3 4 5 6 7 8 ADD - BINARY ( A , B ) C = new integer [ A . length + 1 ] carry = 0 for i = 1 to A . length C [ i ] = ( A [ i ] + B [ i ] + carry ) % 2 // remainder carry = ( A [ i ] + B [ i ] + carry ) / 2 // quotient C [ i ] = carry return C","title":"2.1-4"},{"location":"Chap02/2.2/","text":"2.2-1 Express the function $n^3 / 1000 - 100n^2 - 100n + 3n$ in terms of $\\Theta$-notation. $\\Theta(n^3)$. 2.2-2 Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n - 1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort . What loop invariant does this algorithm maintain? Why does it need to run for only the first $n - 1$ elements, rather than for all $n$ elements? Give the best-case and worst-case running times of selection sort in $\\Theta$-notation. 1 2 3 4 5 6 7 8 SELECTION - SORT ( A ) n = A . length for j = 1 to n - 1 smallest = j for i = j + 1 to n if A [ i ] < A [ smallest ] smallest = i exchange A [ j ] with A [ smallest ] The algorithm maintains the loop invariant that at the start of each iteration of the outer for loop, the subarray $A[1..j - 1]$ consists of the $j - 1$ smallest elements in the array $A[1..n]$, and this subarray is in sorted order. After the first $n - 1$ elements, the subarray $A[1..n]$ contains the smallest $n - 1$ elements, sorted, and therefore element $A[n]$ must be the largest element. The running time of the algorithm is $\\Theta(n^2)$ for all cases. 2.2-3 Consider linear search again (see Exercise 2.1-3). How many elements of the in- put sequence need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? What are the average-case and worst-case running times of linear search in $\\Theta$-notation? Justify your answers. If the element is present in the sequence, half of the elements are likely to be checked before it is found in the average case. In the worst case, all of them will be checked. That is, $n / 2$ checks for the average case and $n$ for the worst case. Both of them are $\\Theta(n)$. 2.2-4 How can we modify almost any algorithm to have a good best-case running time? Modify the algorithm so it tests whether the input satisfies some special-case condition and, if it does, output a pre-computed answer. The best-case running time is generally not a good measure of an algorithm.","title":"2.2 Analyzing algorithms"},{"location":"Chap02/2.2/#22-1","text":"Express the function $n^3 / 1000 - 100n^2 - 100n + 3n$ in terms of $\\Theta$-notation. $\\Theta(n^3)$.","title":"2.2-1"},{"location":"Chap02/2.2/#22-2","text":"Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n - 1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort . What loop invariant does this algorithm maintain? Why does it need to run for only the first $n - 1$ elements, rather than for all $n$ elements? Give the best-case and worst-case running times of selection sort in $\\Theta$-notation. 1 2 3 4 5 6 7 8 SELECTION - SORT ( A ) n = A . length for j = 1 to n - 1 smallest = j for i = j + 1 to n if A [ i ] < A [ smallest ] smallest = i exchange A [ j ] with A [ smallest ] The algorithm maintains the loop invariant that at the start of each iteration of the outer for loop, the subarray $A[1..j - 1]$ consists of the $j - 1$ smallest elements in the array $A[1..n]$, and this subarray is in sorted order. After the first $n - 1$ elements, the subarray $A[1..n]$ contains the smallest $n - 1$ elements, sorted, and therefore element $A[n]$ must be the largest element. The running time of the algorithm is $\\Theta(n^2)$ for all cases.","title":"2.2-2"},{"location":"Chap02/2.2/#22-3","text":"Consider linear search again (see Exercise 2.1-3). How many elements of the in- put sequence need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? What are the average-case and worst-case running times of linear search in $\\Theta$-notation? Justify your answers. If the element is present in the sequence, half of the elements are likely to be checked before it is found in the average case. In the worst case, all of them will be checked. That is, $n / 2$ checks for the average case and $n$ for the worst case. Both of them are $\\Theta(n)$.","title":"2.2-3"},{"location":"Chap02/2.2/#22-4","text":"How can we modify almost any algorithm to have a good best-case running time? Modify the algorithm so it tests whether the input satisfies some special-case condition and, if it does, output a pre-computed answer. The best-case running time is generally not a good measure of an algorithm.","title":"2.2-4"},{"location":"Chap02/2.3/","text":"2.3-1 Using Figure 2.4 as a model, illustrate the operation of merge sort on the array $A = \\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$. $$[3] \\quad [41] \\quad [52] \\quad [26] \\quad [38] \\quad [57] \\quad [9] \\quad [49]$$ $$\\downarrow$$ $$[3|41] \\quad [26|52] \\quad [38|57] \\quad [9|49]$$ $$\\downarrow$$ $$[3|26|41|52] \\quad [9|38|49|57]$$ $$\\downarrow$$ $$[3|9|26|38|41|49|52|57]$$ 2.3-2 Rewrite the $\\text{MERGE}$ procedure so that it does not use sentinels, instead stopping once either array $L$ or $R$ has had all its elements copied back to $A$ and then copying the remainder of the other array back into $A$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 MERGE ( A , p , q , r ) n [ 1 ] = q - p + 1 n [ 2 ] = r - q let L [ 1. . n [ 1 ]] and R [ 1. . n [ 2 ]] be new arrays for i = 1 to n [ 1 ] L [ i ] = A [ p + i - 1 ] for j = 1 to n [ 2 ] R [ j ] = A [ q + j ] i = 1 j = 1 for k = p to r if i > n [ 1 ] A [ k ] = R [ j ] j = j + 1 else if j > n [ 2 ] A [ k ] = L [ i ] i = i + 1 else if L [ i ] \u2264 R [ j ] A [ k ] = L [ i ] i = i + 1 else A [ k ] = R [ j ] j = j + 1 2.3-3 Use mathematical induction to show that when $n$ is an exact power of $2$, the solution of the recurrence $$ T(n) = \\begin{cases} 2 & \\text{if } n = 2, \\\\ 2T(n / 2) & \\text{if } n = 2^k, \\text{for } k > 1 \\end{cases} $$ is $T(n) = n\\lg n$. The base case is when $n = 2$, and we have $n\\lg n = 2\\lg 2 = 2 \\cdot 1 = 2$. For the inductive step, our inductive hypothesis is that $T(n / 2) = (n / 2)\\lg(n / 2)$. Then $$ \\begin{aligned} T(n) & = 2T(n / 2) + n \\\\ & = 2(n / 2) \\lg(n / 2) + n \\\\ & = n(\\lg n - 1) + n \\\\ & = n\\lg n - n + n \\\\ & = n\\lg n, \\end{aligned} $$ which completes the inductive proof for exact powers of $2$. 2.3-4 We can express insertion sort as a recursive procedure as follows. In order to sort $A[1..n]$, we recursively sort $A[1..n - 1]$ and then insert $A[n]$ into the sorted array $A[1..n - 1]$. Write a recurrence for the running time of this recursive version of insertion sort. Since it takes $\\Theta(n)$ time in the worst case to insert $A[n]$ into the sorted array $A[1..n - 1]$, we get the recurrence $$ T(n) = \\begin{cases} \\Theta(1) & \\text{if } n = 1, \\\\ T(n - 1) + \\Theta(n) & \\text{if } n > 1. \\end{cases} $$ Although the exercise does not ask you to solve this recurrence, its solution is $T(n) = \\Theta(n^2)$. 2.3-5 Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\\Theta(\\lg n)$. Procedure $\\text{BINARY-SEARCH}$ takes a sorted array $A$, a value $v$, and a range $[low..high]$ of the array, in which we search for the value $v$. The procedure compares to the array entry at the midpoint of the range and decides to eliminate half the range from further consideration. We give both iterative and recursive versions, each of which returns either an index $i$ such that $A[i] = v$, or $\\text{NIL}$ if no entry of $A[low..high]$ contains the value $v$. The initial call to either version should have the parameters $A$, $v$, $1$, $n$. 1 2 3 4 5 6 7 8 9 ITERATIVE - BINARY - SEARCH ( A , v , low , high ) while low \u2264 high mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] low = mid + 1 else high = mid - 1 return NIL 1 2 3 4 5 6 7 8 9 RECURSIVE - BINARY - SEARCH ( A , v , low , high ) if low > high return NIL mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , v , mid + 1 , high ) else return RECURSIVE - BINARY - SEARCH ( A , v , low , mid - 1 ) Both procedures terminate the search unsuccessfully when the range is empty (i.e., $low > high$) and terminate it successfully if the value $v$ has been found. Based on the comparison of $v$ to the middle element in the searched range, the search continues with the range halved. The recurrence for these procedures is therefore $T(n) = T(n / 2) + \\Theta(1)$, whose solution is $T(n) = \\Theta(\\lg n)$. 2.3-6 Observe that the while loop of lines 5\u20137 of the $\\text{INSERTION-SORT}$ procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[i..j - 1]$. Can we use a binary search (see Exercise 2.3-5) instead to improve the overall worst-case running time of insertion sort to $\\Theta(n\\lg n)$? The while loop of lines 5\u20137 of procedure $\\text{INSERTION-SORT}$ scans backward through the sorted array $A[1..j - 1]$ to find the appropriate place for $A[j]$. The hitch is that the loop not only searches for the proper place for $A[j]$, but that it also moves each of the array elements that are bigger than $A[j]$ one position to the right (line 6). These movements can take as much as $\\Theta(j)$ time, which occurs when all the $j - 1$ elements preceding $A[j]$ are larger than $A[j]$. We can use binary search to improve the running time of the search to $\\Theta(\\lg j)$, but binary search will have no effect on the running time of moving the elements. Therefore, binary search alone cannot improve the worst-case running time of $\\text{INSERTION-SORT}$ to $\\Theta(n\\lg n)$. 2.3-7 $\\star$ Describe a $\\Theta(n\\lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$. The following algorithm solves the problem: Sort the elements in $S$. Form the set $S' = \\{z: z = x - y$ for some $y \\in S\\}$. Sort the elements in $S'$. Merge the two sorted sets $S$ and $S'$. There exist two elements in $S$ whose sum is exactly $x$ if and only if the same value appears in consecutive positions in the merged output. To justify the claim in step 4, first observe that if any value appears twice in the merged output, it must appear in consecutive positions. Thus, we can restate the condition in step 5 as there exist two elements in $S$ whose sum is exactly $x$ if and only if the same value appears twice in the merged output. Suppose that some value $w$ appears twice. Then $w$ appeared once in $S$ and once in $S'$. Because $w$ appeared in $S'$, there exists some $y \\in S$ such that $w = x - y$, or $x = w + y$. Since $w \\in S$, the elements $w$ and $y$ are in $S$ and sum to $x$. Conversely, suppose that there are values $w, y \\in S$ such that $w + y = x$. Then, since $x - y = w$, the value $w$ appears in $S'$. Thus, $w$ is in both $S$ and $S'$, and so it will appear twice in the merged output. Steps 1 and 3 require $\\Theta(n\\lg n)$ steps. Steps 2, 4, 5, and 6 require $O(n)$ steps. Thus the overall running time is $O(n\\lg n)$. A reader submitted a simpler solution that also runs in $\\Theta(n\\lg n)$ time. First, sort the elements in $S$, taking $\\Theta(n\\lg n)$ time. Then, for each element $y$ in $S$, perform a binary search in $S$ for $x - y$. Each binary search takes $O(\\lg n)$ time, and there are are most $n$ of them, and so the time for all the binary searches is $O(n\\lg n)$. The overall running time is $\\Theta(n\\lg n)$. Another reader pointed out that since $S$ is a set, if the value $x / 2$ appears in $S$, it appears in $S$ just once, and so $x / 2$ cannot be a solution.","title":"2.3 Designing algorithms"},{"location":"Chap02/2.3/#23-1","text":"Using Figure 2.4 as a model, illustrate the operation of merge sort on the array $A = \\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$. $$[3] \\quad [41] \\quad [52] \\quad [26] \\quad [38] \\quad [57] \\quad [9] \\quad [49]$$ $$\\downarrow$$ $$[3|41] \\quad [26|52] \\quad [38|57] \\quad [9|49]$$ $$\\downarrow$$ $$[3|26|41|52] \\quad [9|38|49|57]$$ $$\\downarrow$$ $$[3|9|26|38|41|49|52|57]$$","title":"2.3-1"},{"location":"Chap02/2.3/#23-2","text":"Rewrite the $\\text{MERGE}$ procedure so that it does not use sentinels, instead stopping once either array $L$ or $R$ has had all its elements copied back to $A$ and then copying the remainder of the other array back into $A$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 MERGE ( A , p , q , r ) n [ 1 ] = q - p + 1 n [ 2 ] = r - q let L [ 1. . n [ 1 ]] and R [ 1. . n [ 2 ]] be new arrays for i = 1 to n [ 1 ] L [ i ] = A [ p + i - 1 ] for j = 1 to n [ 2 ] R [ j ] = A [ q + j ] i = 1 j = 1 for k = p to r if i > n [ 1 ] A [ k ] = R [ j ] j = j + 1 else if j > n [ 2 ] A [ k ] = L [ i ] i = i + 1 else if L [ i ] \u2264 R [ j ] A [ k ] = L [ i ] i = i + 1 else A [ k ] = R [ j ] j = j + 1","title":"2.3-2"},{"location":"Chap02/2.3/#23-3","text":"Use mathematical induction to show that when $n$ is an exact power of $2$, the solution of the recurrence $$ T(n) = \\begin{cases} 2 & \\text{if } n = 2, \\\\ 2T(n / 2) & \\text{if } n = 2^k, \\text{for } k > 1 \\end{cases} $$ is $T(n) = n\\lg n$. The base case is when $n = 2$, and we have $n\\lg n = 2\\lg 2 = 2 \\cdot 1 = 2$. For the inductive step, our inductive hypothesis is that $T(n / 2) = (n / 2)\\lg(n / 2)$. Then $$ \\begin{aligned} T(n) & = 2T(n / 2) + n \\\\ & = 2(n / 2) \\lg(n / 2) + n \\\\ & = n(\\lg n - 1) + n \\\\ & = n\\lg n - n + n \\\\ & = n\\lg n, \\end{aligned} $$ which completes the inductive proof for exact powers of $2$.","title":"2.3-3"},{"location":"Chap02/2.3/#23-4","text":"We can express insertion sort as a recursive procedure as follows. In order to sort $A[1..n]$, we recursively sort $A[1..n - 1]$ and then insert $A[n]$ into the sorted array $A[1..n - 1]$. Write a recurrence for the running time of this recursive version of insertion sort. Since it takes $\\Theta(n)$ time in the worst case to insert $A[n]$ into the sorted array $A[1..n - 1]$, we get the recurrence $$ T(n) = \\begin{cases} \\Theta(1) & \\text{if } n = 1, \\\\ T(n - 1) + \\Theta(n) & \\text{if } n > 1. \\end{cases} $$ Although the exercise does not ask you to solve this recurrence, its solution is $T(n) = \\Theta(n^2)$.","title":"2.3-4"},{"location":"Chap02/2.3/#23-5","text":"Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\\Theta(\\lg n)$. Procedure $\\text{BINARY-SEARCH}$ takes a sorted array $A$, a value $v$, and a range $[low..high]$ of the array, in which we search for the value $v$. The procedure compares to the array entry at the midpoint of the range and decides to eliminate half the range from further consideration. We give both iterative and recursive versions, each of which returns either an index $i$ such that $A[i] = v$, or $\\text{NIL}$ if no entry of $A[low..high]$ contains the value $v$. The initial call to either version should have the parameters $A$, $v$, $1$, $n$. 1 2 3 4 5 6 7 8 9 ITERATIVE - BINARY - SEARCH ( A , v , low , high ) while low \u2264 high mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] low = mid + 1 else high = mid - 1 return NIL 1 2 3 4 5 6 7 8 9 RECURSIVE - BINARY - SEARCH ( A , v , low , high ) if low > high return NIL mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , v , mid + 1 , high ) else return RECURSIVE - BINARY - SEARCH ( A , v , low , mid - 1 ) Both procedures terminate the search unsuccessfully when the range is empty (i.e., $low > high$) and terminate it successfully if the value $v$ has been found. Based on the comparison of $v$ to the middle element in the searched range, the search continues with the range halved. The recurrence for these procedures is therefore $T(n) = T(n / 2) + \\Theta(1)$, whose solution is $T(n) = \\Theta(\\lg n)$.","title":"2.3-5"},{"location":"Chap02/2.3/#23-6","text":"Observe that the while loop of lines 5\u20137 of the $\\text{INSERTION-SORT}$ procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[i..j - 1]$. Can we use a binary search (see Exercise 2.3-5) instead to improve the overall worst-case running time of insertion sort to $\\Theta(n\\lg n)$? The while loop of lines 5\u20137 of procedure $\\text{INSERTION-SORT}$ scans backward through the sorted array $A[1..j - 1]$ to find the appropriate place for $A[j]$. The hitch is that the loop not only searches for the proper place for $A[j]$, but that it also moves each of the array elements that are bigger than $A[j]$ one position to the right (line 6). These movements can take as much as $\\Theta(j)$ time, which occurs when all the $j - 1$ elements preceding $A[j]$ are larger than $A[j]$. We can use binary search to improve the running time of the search to $\\Theta(\\lg j)$, but binary search will have no effect on the running time of moving the elements. Therefore, binary search alone cannot improve the worst-case running time of $\\text{INSERTION-SORT}$ to $\\Theta(n\\lg n)$.","title":"2.3-6"},{"location":"Chap02/2.3/#23-7-star","text":"Describe a $\\Theta(n\\lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$. The following algorithm solves the problem: Sort the elements in $S$. Form the set $S' = \\{z: z = x - y$ for some $y \\in S\\}$. Sort the elements in $S'$. Merge the two sorted sets $S$ and $S'$. There exist two elements in $S$ whose sum is exactly $x$ if and only if the same value appears in consecutive positions in the merged output. To justify the claim in step 4, first observe that if any value appears twice in the merged output, it must appear in consecutive positions. Thus, we can restate the condition in step 5 as there exist two elements in $S$ whose sum is exactly $x$ if and only if the same value appears twice in the merged output. Suppose that some value $w$ appears twice. Then $w$ appeared once in $S$ and once in $S'$. Because $w$ appeared in $S'$, there exists some $y \\in S$ such that $w = x - y$, or $x = w + y$. Since $w \\in S$, the elements $w$ and $y$ are in $S$ and sum to $x$. Conversely, suppose that there are values $w, y \\in S$ such that $w + y = x$. Then, since $x - y = w$, the value $w$ appears in $S'$. Thus, $w$ is in both $S$ and $S'$, and so it will appear twice in the merged output. Steps 1 and 3 require $\\Theta(n\\lg n)$ steps. Steps 2, 4, 5, and 6 require $O(n)$ steps. Thus the overall running time is $O(n\\lg n)$. A reader submitted a simpler solution that also runs in $\\Theta(n\\lg n)$ time. First, sort the elements in $S$, taking $\\Theta(n\\lg n)$ time. Then, for each element $y$ in $S$, perform a binary search in $S$ for $x - y$. Each binary search takes $O(\\lg n)$ time, and there are are most $n$ of them, and so the time for all the binary searches is $O(n\\lg n)$. The overall running time is $\\Theta(n\\lg n)$. Another reader pointed out that since $S$ is a set, if the value $x / 2$ appears in $S$, it appears in $S$ just once, and so $x / 2$ cannot be a solution.","title":"2.3-7 $\\star$"},{"location":"Chap02/Problems/2-1/","text":"Although merge sort runs in $\\Theta(n\\lg n)$ worst-case time and insertion sort runs in $\\Theta(n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus, it makes sense to coarsen the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which $n / k$ sublists of length $k$ are sorted using insertion sort and then merged using the standard merging mechanism, where $k$ is a value to be determined. a. Show that insertion sort can sort the $n / k$ sublists, each of length $k$, in $\\Theta(nk)$ worst-case time. b. Show how to merge the sublists in $\\Theta(n\\lg(n / k))$ worst-case time. c. Given that the modified algorithm runs in $\\Theta(nk + n\\lg(n / k))$ worst-case time, what is the largest value of $k$ as a function of $n$ for which the modified algorithm has the same running time as standard merge sort, in terms of $\\Theta$-notation? d. How should we chosse $k$ in practice? a. Insertion sort takes $\\Theta(k^2)$ time per $k$-element list in the worst case. Therefore, sorting $n / k$ lists of $k$ elements each takes $\\Theta(k^2n / k) = \\Theta(nk)$ worst-case time. b. Just extending the $2$-list merge to merge all the lists at once would take $\\Theta(n \\cdot(n / k)) = \\Theta(n^2/k)$ time ($n$ from copying each element once into the result list, $n / k$ from examining $n / k$ lists at each step to select next item for result list). To achieve $\\Theta(n\\lg(n / k))$-time merging, we merge the lists pairwise, then merge the resulting lists pairwise, and so on, until there's just one list. The pairwise merging requires $\\Theta(n)$ work at each level, since we are still working on $n$ elements, even if they are partitioned among sublists. The number of levels, starting with $n / k$ lists (with $k$ elements each) and finishing with 1 list (with $n$ elements), is $\\lceil \\lg(n / k) \\rceil$. Therefore, the total running time for the merging is $\\Theta(n\\lg(n / k))$. c. The modified algorithm has the same asymptotic running time as standard merge sort when $\\Theta(nk + n\\lg(n / k)) = \\Theta(n\\lg n)$. The largest asymptotic value of $k$ as a function of $n$ that satisfies this condition is $k = \\Theta(\\lg n)$. To see why, first observe that $k$ cannot be more than $\\Theta(\\lg n)$ (i.e., it can't have a higher-order term than $\\lg n$), for otherwise the left-hand expression wouldn't be $\\Theta(n\\lg n)$ (because it would have a higher-order term than $n\\lg n$). So all we need to do is verify that $k = \\Theta(\\lg n)$ works, which we can do by plugging $k = \\lg n$ into $$\\Theta(nk + n\\lg(n / k)) = \\Theta(nk + n\\lg n - n\\lg k)$$ to get $$\\Theta(n\\lg n + n\\lg n - n\\lg\\lg n) = \\Theta(2n\\lg n - n\\lg\\lg n),$$ which by taking just the high-order term and ignorin the constant coefficient, equals $\\Theta(n\\lg n)$. d. In practice, $k$ should be the largest list length on which insertion sort is faster than merge sort.","title":"2-1 Insertion sort on small arrays in merge sort"},{"location":"Chap02/Problems/2-2/","text":"Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order. 1 2 3 4 5 BUBBLESORT ( A ) for i = 1 to A . length - 1 for j = A . length downto i + 1 if A [ j ] < A [ j - 1 ] exchange A [ j ] with A [ j - 1 ] a. Let $A'$ denote the output of $\\text{BUBBLESORT}(A)$ To prove that $\\text{BUBBLESORT}$ is correct, we need to prove that it terminates and that $$A'[1] \\le A'[2] \\le \\cdots \\le A'[n], \\tag{2.3}$$ where $n = A.length$. In order to show that BUBBLESORT actually sorts, what else do we need to prove? The next two parts will prove inequality $\\text{(2.3)}$. b. State precisely a loop invariant for the for loop in lines 2\u20134, and prove that this loop invariant holds. Your proof should use the structure of the loop invariant proof presented in this chapter. c. Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the for loop in lines 1\u20134 that will allow you to prove inequality $\\text{(2.3)}$. Your proof should use the structure of the loop invariant proof presented in this chapter. d. What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort? a. We need to show that the elements of $A'$ form a permutation of the elements of $A$. b. Loop invariant: At the start of each iteration of the for loop of lines 2\u20134, $A[j] = \\min\\{A[k]: j \\le k \\le n\\}$ and the subarray $A[j..n]$ is a permutation of the values that were in $A[j..n]$ at the time that the loop started. Initialization: Initially, $j = n$, and the subarray $A[j..n]$ consists of single element $A[n]$. The loop invariant trivially holds. Maintenance: Consider an iteration for a given value of $j$. By the loop invariant, $A[j]$ is the smallest value in $A[j..n]$. Lines 3\u20134 exchange $A[j]$ and $A[j - 1]$ if $A[j]$ is less than $A[j - 1]$, and so $A[j - 1]$ will be the smallest value in $A[j - 1..n]$ afterward. Since the only change to the subarray $A[j - 1..n]$ is this possible exchange, and the subarray $A[j..n]$ is a permutation of the values that were in $A[j..n]$ at the time that the loop started, we see that $A[j - 1..n]$ is a permutation of the values that were in $A[j - 1..n]$ at the time that the loop started. Decrementing $j$ for the next iteration maintains the invariant. Termination: The loop terminates when $j$ reaches $i$. By the statement of the loop invariant, $A[i] = \\min\\{A[k]: i \\le k \\le n\\}$ and $A[i..n]$ is a permutation of the values that were in $A[i..n]$ at the time that the loop started. c. Loop invariant: At the start of each iteration of the for loop of lines 1\u20134, the subarray $A[1..i - 1]$ consists of the $i - 1$ smallest values originally in $A[1..n]$, in sorted order, and $A[i..n]$ consists of the $n - i + 1$ remaining values originally in $A[1..n]$. Initialization: Before the first iteration of the loop, $i = 1$. The subarray $A[1..i - 1]$ is empty, and so the loop invariant vacuously holds. Maintenance: Consider an iteration for a given value of $i$. By the loop invariant, $A[1..i - 1]$ consists of the $i$ smallest values in $A[1..n]$, in sorted order. Part (b) showed that after executing the for loop of lines 2\u20134, $A[i]$ is the smallest value in $A[i..n]$, and so $A[1..i]$ is now the $i$ smallest values originally in $A[1..n]$, in sorted order. Moreover, since the for loop of lines 2\u20134 permutes $A[i..n]$, the subarray $A[i + 1..n]$ consists of the $n - i$ remaining values originally in $A[1..n]$. Termination: The for loop of lines 1\u20134 terminates when $i = n$, so that $i - 1 = n - 1$. By the statement of the loop invariant, $A[1..i - 1]$ is the subarray $A[1..n - 1]$, and it consists of the $n - 1$ smallest values originally in $A[1..n]$, in sorted order. The remaining element must be the largest value in $A[1..n]$, and it is in $A[n]$. Therefore, the entire array $A[1..n]$ is sorted. Note: Tn the second edition, the for loop of lines 1\u20134 had an upper bound of $A.length$. The last iteration of the outer for loop would then result in no iterations of the inner for loop of lines 1\u20134, but the termination argument would simplify: $A[1..i - 1]$ would be the entire array $A[1..n]$, which, by the loop invariant, is sorted. d. The running time depends on the number of iterations of the for loop of lines 2\u20134. For a given value of $i$, this loop makes $n - i$ iterations, and $i$ takes on the values $1, 2, \\ldots, n - 1$. The total number of iterations, therefore, is $$ \\begin{aligned} \\sum_{i = 1}^{n - 1} (n - i) & = \\sum_{i = 1}^{n - 1} n - \\sum_{i = 1}^{n - 1} i \\\\ & = n(n - 1) - \\frac{n(n - 1)}{2} \\\\ & = \\frac{n(n - 1)}{2} \\\\ & = \\frac{n^2}{2} - \\frac{n}{2}. \\end{aligned} $$ Thus, the running time of bubblesort is $\\Theta(n^2)$ in all cases. The worst-case running time is the same as that of insertion sort.","title":"2-2 Correctness of bubblesort"},{"location":"Chap02/Problems/2-3/","text":"The following code fragment implements Horner's rule for evaluating a polynomial $$ \\begin{aligned} P(x) & = \\sum_{k = 0}^n a_k x^k \\\\ & = a_0 + x(a_1 + x (a_2 + \\cdots + x(a_{n - 1} + x a_n) \\cdots)), \\end{aligned} $$ given the coefficients $a_0, a_1, \\ldots, a_n$ and a value of $x$: 1 2 3 y = 0 for i = n downto 0 y = a [ i ] + x * y a. In terms of $\\Theta$-notation, what is the running time of this code fragment for Horner's rule? b. Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare to Horner's rule c. Consider the following loop invariant: At the start of each iteration of the for loop of lines 2-3, $$y = \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1} x^k.$$ Interpret a summation with no terms as equaling $0$. Following the structure of the loop invariant proof presented in this chapter, use this loop invariant to show that, at termination, $y = \\sum_{k = 0}^n a_k x^k$. d. Conclude by arguing that the given code fragment correctly evaluates a polynomial characterized by the coefficients $a_0, a_1, \\ldots, a_n$. a. $\\Theta(n)$. b. 1 2 3 4 5 6 7 NAIVE - HORNER () y = 0 for k = 0 to n temp = 1 for i = 0 to k temp = temp * x y = y + a [ k ] * temp The running time is $\\Theta(n^2)$, because of the nested loop. It is obviously slower. c. Initialization: It is pretty trivial, since the summation has no terms which implies $y = 0$. Maintenance: By using the loop invariant, in the end of the $i$-the iteration, we have $$ \\begin{aligned} y & = a_i + x \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1} x^k \\\\ & = a_i x^0 + \\sum_{k = 0}^{n - i - 1} a_{k + i + 1} x^{k + 1} \\\\ & = a_i x^0 \\sum_{k = 1}^{n - i} a_{k + i} x^k \\\\ & = \\sum_{k = 0}^{n - i} a_{k + i} x^k. \\end{aligned} $$ Termination: The loop terminates at $i = -1$. If we substitute, $$y = \\sum_{k = 0}^{n - i - 1} a_{k + i + 1} x^k = \\sum_{k = 0}^n a_k x^k.$$ d. The invariant of the loop is a sum that equals a polynomial with the given coefficients.","title":"2-3 Correctness of Horner's rule"},{"location":"Chap02/Problems/2-4/","text":"Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i, j)$ is called an inversion of $A$. a. List the five inversions in the array $\\langle 2, 3, 8, 6, 1 \\rangle$. b. What array with elements from the set $\\{1, 2, \\ldots, n\\}$ has the most inversions? How many does it have? c. What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer. d. Give an algorithm that determines the number of inversions in any permutation of $n$ elements in $\\Theta(n\\lg n)$ worst-case time. ($\\textit{Hint:}$ Modify merge sort). a. The inversions are $(1,5)$, $(2,5)$, $(3,4)$, $(3,5)$, $(4,5)$. (Remember that inversions are specified by indices rather than by the values in the array.) b. The array with elements from $\\{1, 2, \\ldots, n\\}$ with the most inversions is $\\langle n, n - 1, n - 2, \\ldots, 2, 1 \\rangle$. For all $1 \\le i < j \\le n$, there is an inversion $(i, j)$. The number of such inversions is $\\binom{n}{2} = n(n - 1) / 2$. c. Suppose that the array $A$ starts out with an inversion $(k, j)$. Then $k < j$ and $A[k] > A[j]$. At the time that the outer for loop of lines 1\u20138 sets $key = A[j]$\u008d, the value that started in $A[k]$\u008d is still somewhere to the left of $A[j]$\u008d. That is, it's in $A[i]$\u008d, where $1 \\le i < j$, and so the inversion has become $(i, j)$. Some iteration of the while loop of lines 5\u20137 moves $A[i]$\u008d one position to the right. Line 8 will eventually drop $key$ to the left of this element, thus eliminating the inversion. Because line 5 moves only elements that are greater than $key$, it moves only elements that correspond to inversions. In other words, each iteration of the while loop of lines 5\u20137 corresponds to the elimination of one inversion. d. We follow the hint and modify merge sort to count the number of inversions in $\\Theta(n\\lg n)$ time. To start, let us define a merge-inversion as a situation within the execution of merge sort in which the $\\text{MERGE}$ procedure, after copying $A[p..q]$\u008d to $L$ and $A[q + 1..r]$ to $R$, has values $x$ in $L$ and $y$ in $R$ such that $x > y$. Consider an inversion $(i, j)$, and let $x = A[i]$\u008d and $y = A[j]$ \u008d, so that $i < j$ and $x > y$. We claim that if we were to run merge sort, there would be exactly one mergeinversion involving $x$ and $y$. To see why, observe that the only way in which array elements change their positions is within the $\\text{MERGE}$ procedure. Moreover, since $\\text{MERGE}$ keeps elements within $L$ in the same relative order to each other, and correspondingly for $R$, the only way in which two elements can change their ordering relative to each other is for the greater one to appear in $L$ and the lesser one to appear in $R$. Thus, there is at least one merge-inversion involving $x$ and $y$. To see that there is exactly one such merge-inversion, observe that after any call of $\\text{MERGE}$ that involves both $x$ and $y$, they are in the same sorted subarray and will therefore both appear in $L$ or both appear in $R$ in any given call thereafter. Thus, we have proven the claim. We have shown that every inversion implies one merge-inversion. In fact, the correspondence between inversions and merge-inversions is one-to-one. Suppose we have a merge-inversion involving values $x$ and $y$, where $x$ originally was $A[i]$\u008d and $y$ was originally $A[j]$\u008d. Since we have a merge-inversion, $x > y$. And since $x$ is in $L$ and $y$ is in $R$, $x$ must be within a subarray preceding the subarray containing $y$. Therefore $x$ started out in a position $i$ preceding $y$'s original position $j$, and so $(i, j)$ is an inversion. Having shown a one-to-one correspondence between inversions and mergeinversions, it suffices for us to count merge-inversions. Consider a merge-inversion involving $y$ in $R$. Let $z$ be the smallest value in $L$ that is greater than $y$. At some point during the merging process, $z$ and $y$ will be the \"exposed\" values in $L$ and $R$, i.e., we will have $z = L[i]$\u008d and $y = R[j]$\u008d in line 13 of $\\text{MERGE}$. At that time, there will be merge-inversions involving $y$ and $L[i], L[i + 1], L[i + 2], \\ldots, L[n_1]$\u008d, and these $n_1 - i + 1$ merge-inversions will be the only ones involving $y$. Therefore, we need to detect the first time that $z$ and $y$ become exposed during the $\\text{MERGE}$ procedure and add the value of $n_1 - i + 1$ at that time to our total count of merge-inversions. The following pseudocode, modeled on merge sort, works as we have just described. It also sorts the array $A$. 1 2 3 4 5 6 7 8 COUNT - INVERSIONS ( A , p , r ) inversions = 0 if p < r q = floor (( p + r ) / 2 ) inversions = inversions + COUNT - INVERSIONS ( A , p , q ) inversions = inversions + COUNT - INVERSIONS ( A , q + 1 , r ) inversions = inversions + MERGE - INVERSIONS ( A , p , q , r ) return inversions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 MERGE - INVERSIONS ( A , p , q , r ) n [ 1 ] = q - p + 1 n [ 2 ] = r - q let L [ 1. . n [ 1 ] + 1 ] and R [ 1. . n [ 2 ] + 1 ] be new arrays for i = 1 to n [ 1 ] L [ i ] = A [ p + i - 1 ] for j = 1 to n [ 2 ] R [ j ] = A [ q + j ] L [ n [ 1 ] + 1 ] = \u221e L [ n [ 2 ] + 1 ] = \u221e i = 1 j = 1 inversions = 0 for k = p to r if R [ j ] < L [ i ] inversions = inversions + n [ 1 ] - i + 1 A [ k ] = R [ j ] j = j + 1 else A [ k ] = L [ i ] i = i + 1 return inversions The initial call is $\\text{COUNT-INVERSIONS}(A, 1, n)$. In $\\text{MERGE-INVERSIONS}$, whenever $R[j]$\u008d is exposed and a value greater than $R[j]$\u008d becomes exposed in the $L$ array, we increase inersions by the number of remaining elements in $L$. Then because $R[j + 1]$\u008d becomes exposed, $R[j]$ \u008dcan never be exposed again. We don't have to worry about merge-inversions involving the sentinel $\\infty$ in $R$, since no value in $L$ will be greater than $\\infty$. Since we have added only a constant amount of additional work to each procedure call and to each iteration of the last for loop of the merging procedure, the total running time of the above pseudocode is the same as for merge sort: $\\Theta(n\\lg n)$.","title":"2-4 Inversions"},{"location":"Chap03/3.1/","text":"3.1-1 Let $f(n) + g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that $\\max(f(n), g(n)) = \\Theta(f(n) + g(n))$. First, let's clarify what the function $\\max(f(n), g(n))$ is. Let's define the function $h(n) = \\max(f(n), g(n))$. Then $$ h(n) = \\begin{cases} f(n) & \\text{ if } f(n) \\ge g(n), \\\\ g(n) & \\text{ if } f(n) < g(n). \\end{cases} $$ Since $f(n)$ and $g(n)$ are asymptotically nonnegative, there exists $n_0$ such that $f(n) \\ge 0$ and $g(n) \\ge 0$ for all $n \\ge n_0$. Thus for $n \\ge n_0$ , $f(n) + g(n) \\ge f(n) \\ge 0$ and $f(n) + g(n) \\ge g(n) \\ge 0$. Since for any particular $n$, $h(n)$ is either $f(n)$ or $g(n)$, we have $f(n) + g(n) \\ge h(n) \\ge 0$, which shows that $$h(n) = \\max(f(n), g(n)) \\le c_2(f(n) + g(n))$$ for all $n \\ge n_0$ (with $c_2 = 1$ in the definition of $\\Theta$). Similarly, since for any particular $n$, $h(n)$ is the larger of $f(n)$ and $g(n)$, we have for all $n \\ge n_0$, $0 \\le f(n) \\le h(n)$ and $0 \\le g(n) \\le h(n)$. Adding these two inequalities yields $0 \\le f(n) + g(n) \\le 2h(n)$, or equivalently $0 \\le (f(n) + g(n)) / 2 \\le h(n)$, which shows that $$h(n) = \\max(f(n), g(n)) \\ge c_1(f(n) + g(n))$$ for all $n \\ge n_0$ (with $c_1 = 1 / 2$ in the definition of $\\Theta$). 3.1-2 Show that for any real constants $a$ and $b$, where $b > 0$, $$(n + a)^b = \\Theta(n^b). \\tag{3.2}$$ To show that $(n + a)^b = \\Theta(n^b)$, we want to find constants $c_1, c_2, n_0 > 0$ such that $0 \\le c_1 n^b \\le (n + a)^b \\le c_2 n^b$ for all $n \\ge n_0$. Note that $$ \\begin{aligned} n + a & \\le n + |a| & \\\\ & \\le 2n & \\text{ when } |a| \\le n, \\end{aligned} $$ and $$ \\begin{aligned} n + a & \\ge n - |a| & \\\\ & \\ge \\frac{1}{2}n & \\text{ when } |a| \\le \\frac{1}{2}n. \\end{aligned} $$ Thus, when $n \\ge 2|a|$, $$0 \\le \\frac{1}{2}n \\le n + a \\le 2n.$$ Since $b > 0$, the inequality still holds when all parts are raised to the power $b$: $$ \\begin{aligned} 0 \\le \\Big(\\frac{1}{2}n\\Big)^b & \\le (n + a)^b \\le (2n)^b, \\\\ 0 \\le \\Big(\\frac{1}{2}\\Big)^b n^b & \\le (n + a)^b \\le 2^b n^b. \\end{aligned} $$ Thus, $c_1 = (1 / 2)^b$, $c_2 = 2^b$, and $n_0 = 2|a|$ satisfy the definition. 3.1-3 Explain why the statement, \"The running time of algorithm $A$ is at least $O(n^2)$,\" is meaningless. Let the running time be $T(n)$. $T(n) \\ge O(n^2)$ means that $T(n) \\ge f(n)$ for some function $f(n)$ in the set $O(n^2)$. This statement holds for any running time $T(n)$, since the function $g(n) = 0$ for all $n$ is in $O(n^2)$, and running times are always nonnegative. Thus, the statement tells us nothing about the running time. 3.1-4 Is $2^{n + 1} = O(2^n)$? Is $2^{2n} = O(2^n)$? $2^{n + 1} = O(2^n)$, but $2^{2n} \\ne O(2^n)$. To show that $2^{n + 1} = O(2^n)$, we must find constants $c$; $n_0 > 0$ such that $$0 \\le 2^{n + 1} \\le c \\cdot 2^n \\text{ for all } n \\ge n_0.$$ Since $2^{n + 1} = 2 \\cdot 2^n$ for all $n$, we can satisfy the definition with $c = 2$ and $n_0 = 1$. To show that $2^{2n} \\ne O(2^n)$, assume there exist constants $c, n_0 > 0$ such that $$0 \\le 2^{2n} \\le c \\cdot 2^n \\text{ for all } n \\ge n_0.$$ Then $2^{2n} = 2^n \\cdot 2^n \\le c \\cdot 2^n \\Rightarrow 2^n \\le c$. But no constant is greater than all $2^n$, and so the assumption leads to a contradiction. 3.1-5 Prove Theorem 3.1. The theorem states: For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. From $f = \\Theta(g(n))$, we have that $$0 \\le c_1 g(n) \\le f(n) \\le c_2g(n) \\text{ for } n > n_0.$$ We can pick the constants from here and use them in the definitions of $O$ and $\\Omega$ to show that both hold. From $f(n) = \\Omega(g(n))$ and $f(n) = O(g(n))$, we have that $$ \\begin{aligned} & 0 \\le c_3g(n) \\le f(n) & \\text{ for all } n \\ge n_1 \\\\ \\text{and } & 0 \\le f(n) \\le c_4g(n) & \\text{ for all } n \\ge n_2. \\end{aligned} $$ If we let $n_3 = \\max(n_1, n_2)$ and merge the inequalities, we get $$0 \\le c_3g(n) \\le f(n) \\le c_4g(n) \\text{ for all } n > n_3.$$ Which is the definition of $\\Theta$. 3.1-6 Prove that the running time of an algorithm is $\\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\\Omega(g(n))$. If $T_w$ is the worst-case running time and $T_b$ is the best-case running time, we know that $$ \\begin{aligned} & 0 \\le c_1g(n) \\le T_b(n) & \\text{ for } n > n_b \\\\ \\text{and } & 0 \\le T_w(n) \\le c_2g(n) & \\text{ for } n > n_w. \\end{aligned} $$ Combining them we get $$0 \\le c_1g(n) \\le T_b(n) \\le T_w(n) \\le c_2g(n) \\text{ for } n > \\max(n_b, n_w).$$ Since the running time is bound between $T_b$ and $T_w$ and the above is the definition of the $\\Theta$-notation, proved. 3.1-7 Prove $o(g(n)) \\cap w(g(n))$ is the empty set. We know that for any $c > 0$, $$ \\begin{aligned} & \\exists n_1 > 0: 0 \\le f(n) < cg(n) \\\\ \\text{and } & \\exists n_2 > 0: 0 \\le cg(n) < f(n). \\end{aligned} $$ If we pick $n_0 = \\max(n_1, n_2)$, from the problem definition we get $$f(n) < cg(n) < f(n).$$ There is no solutions, which means that the intersection is the empty set. 3.1-8 We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n, m)$ we denote $O(g(n, m))$ the set of functions: $$ \\begin{aligned} O(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le f(n, m) \\le cg(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$ Give corresponding definitions for $\\Omega(g(n, m))$ and $\\Theta(g(n, m))$. $$ \\begin{aligned} \\Omega(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le cg(n, m) \\le f(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$ $$ \\begin{aligned} \\Theta(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c_1, c_2, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le c_1 g(n, m) \\le f(n, m) \\le c_2 g(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$","title":"3.1 Asymptotic notation"},{"location":"Chap03/3.1/#31-1","text":"Let $f(n) + g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that $\\max(f(n), g(n)) = \\Theta(f(n) + g(n))$. First, let's clarify what the function $\\max(f(n), g(n))$ is. Let's define the function $h(n) = \\max(f(n), g(n))$. Then $$ h(n) = \\begin{cases} f(n) & \\text{ if } f(n) \\ge g(n), \\\\ g(n) & \\text{ if } f(n) < g(n). \\end{cases} $$ Since $f(n)$ and $g(n)$ are asymptotically nonnegative, there exists $n_0$ such that $f(n) \\ge 0$ and $g(n) \\ge 0$ for all $n \\ge n_0$. Thus for $n \\ge n_0$ , $f(n) + g(n) \\ge f(n) \\ge 0$ and $f(n) + g(n) \\ge g(n) \\ge 0$. Since for any particular $n$, $h(n)$ is either $f(n)$ or $g(n)$, we have $f(n) + g(n) \\ge h(n) \\ge 0$, which shows that $$h(n) = \\max(f(n), g(n)) \\le c_2(f(n) + g(n))$$ for all $n \\ge n_0$ (with $c_2 = 1$ in the definition of $\\Theta$). Similarly, since for any particular $n$, $h(n)$ is the larger of $f(n)$ and $g(n)$, we have for all $n \\ge n_0$, $0 \\le f(n) \\le h(n)$ and $0 \\le g(n) \\le h(n)$. Adding these two inequalities yields $0 \\le f(n) + g(n) \\le 2h(n)$, or equivalently $0 \\le (f(n) + g(n)) / 2 \\le h(n)$, which shows that $$h(n) = \\max(f(n), g(n)) \\ge c_1(f(n) + g(n))$$ for all $n \\ge n_0$ (with $c_1 = 1 / 2$ in the definition of $\\Theta$).","title":"3.1-1"},{"location":"Chap03/3.1/#31-2","text":"Show that for any real constants $a$ and $b$, where $b > 0$, $$(n + a)^b = \\Theta(n^b). \\tag{3.2}$$ To show that $(n + a)^b = \\Theta(n^b)$, we want to find constants $c_1, c_2, n_0 > 0$ such that $0 \\le c_1 n^b \\le (n + a)^b \\le c_2 n^b$ for all $n \\ge n_0$. Note that $$ \\begin{aligned} n + a & \\le n + |a| & \\\\ & \\le 2n & \\text{ when } |a| \\le n, \\end{aligned} $$ and $$ \\begin{aligned} n + a & \\ge n - |a| & \\\\ & \\ge \\frac{1}{2}n & \\text{ when } |a| \\le \\frac{1}{2}n. \\end{aligned} $$ Thus, when $n \\ge 2|a|$, $$0 \\le \\frac{1}{2}n \\le n + a \\le 2n.$$ Since $b > 0$, the inequality still holds when all parts are raised to the power $b$: $$ \\begin{aligned} 0 \\le \\Big(\\frac{1}{2}n\\Big)^b & \\le (n + a)^b \\le (2n)^b, \\\\ 0 \\le \\Big(\\frac{1}{2}\\Big)^b n^b & \\le (n + a)^b \\le 2^b n^b. \\end{aligned} $$ Thus, $c_1 = (1 / 2)^b$, $c_2 = 2^b$, and $n_0 = 2|a|$ satisfy the definition.","title":"3.1-2"},{"location":"Chap03/3.1/#31-3","text":"Explain why the statement, \"The running time of algorithm $A$ is at least $O(n^2)$,\" is meaningless. Let the running time be $T(n)$. $T(n) \\ge O(n^2)$ means that $T(n) \\ge f(n)$ for some function $f(n)$ in the set $O(n^2)$. This statement holds for any running time $T(n)$, since the function $g(n) = 0$ for all $n$ is in $O(n^2)$, and running times are always nonnegative. Thus, the statement tells us nothing about the running time.","title":"3.1-3"},{"location":"Chap03/3.1/#31-4","text":"Is $2^{n + 1} = O(2^n)$? Is $2^{2n} = O(2^n)$? $2^{n + 1} = O(2^n)$, but $2^{2n} \\ne O(2^n)$. To show that $2^{n + 1} = O(2^n)$, we must find constants $c$; $n_0 > 0$ such that $$0 \\le 2^{n + 1} \\le c \\cdot 2^n \\text{ for all } n \\ge n_0.$$ Since $2^{n + 1} = 2 \\cdot 2^n$ for all $n$, we can satisfy the definition with $c = 2$ and $n_0 = 1$. To show that $2^{2n} \\ne O(2^n)$, assume there exist constants $c, n_0 > 0$ such that $$0 \\le 2^{2n} \\le c \\cdot 2^n \\text{ for all } n \\ge n_0.$$ Then $2^{2n} = 2^n \\cdot 2^n \\le c \\cdot 2^n \\Rightarrow 2^n \\le c$. But no constant is greater than all $2^n$, and so the assumption leads to a contradiction.","title":"3.1-4"},{"location":"Chap03/3.1/#31-5","text":"Prove Theorem 3.1. The theorem states: For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. From $f = \\Theta(g(n))$, we have that $$0 \\le c_1 g(n) \\le f(n) \\le c_2g(n) \\text{ for } n > n_0.$$ We can pick the constants from here and use them in the definitions of $O$ and $\\Omega$ to show that both hold. From $f(n) = \\Omega(g(n))$ and $f(n) = O(g(n))$, we have that $$ \\begin{aligned} & 0 \\le c_3g(n) \\le f(n) & \\text{ for all } n \\ge n_1 \\\\ \\text{and } & 0 \\le f(n) \\le c_4g(n) & \\text{ for all } n \\ge n_2. \\end{aligned} $$ If we let $n_3 = \\max(n_1, n_2)$ and merge the inequalities, we get $$0 \\le c_3g(n) \\le f(n) \\le c_4g(n) \\text{ for all } n > n_3.$$ Which is the definition of $\\Theta$.","title":"3.1-5"},{"location":"Chap03/3.1/#31-6","text":"Prove that the running time of an algorithm is $\\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\\Omega(g(n))$. If $T_w$ is the worst-case running time and $T_b$ is the best-case running time, we know that $$ \\begin{aligned} & 0 \\le c_1g(n) \\le T_b(n) & \\text{ for } n > n_b \\\\ \\text{and } & 0 \\le T_w(n) \\le c_2g(n) & \\text{ for } n > n_w. \\end{aligned} $$ Combining them we get $$0 \\le c_1g(n) \\le T_b(n) \\le T_w(n) \\le c_2g(n) \\text{ for } n > \\max(n_b, n_w).$$ Since the running time is bound between $T_b$ and $T_w$ and the above is the definition of the $\\Theta$-notation, proved.","title":"3.1-6"},{"location":"Chap03/3.1/#31-7","text":"Prove $o(g(n)) \\cap w(g(n))$ is the empty set. We know that for any $c > 0$, $$ \\begin{aligned} & \\exists n_1 > 0: 0 \\le f(n) < cg(n) \\\\ \\text{and } & \\exists n_2 > 0: 0 \\le cg(n) < f(n). \\end{aligned} $$ If we pick $n_0 = \\max(n_1, n_2)$, from the problem definition we get $$f(n) < cg(n) < f(n).$$ There is no solutions, which means that the intersection is the empty set.","title":"3.1-7"},{"location":"Chap03/3.1/#31-8","text":"We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n, m)$ we denote $O(g(n, m))$ the set of functions: $$ \\begin{aligned} O(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le f(n, m) \\le cg(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$ Give corresponding definitions for $\\Omega(g(n, m))$ and $\\Theta(g(n, m))$. $$ \\begin{aligned} \\Omega(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le cg(n, m) \\le f(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$ $$ \\begin{aligned} \\Theta(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c_1, c_2, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le c_1 g(n, m) \\le f(n, m) \\le c_2 g(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$","title":"3.1-8"},{"location":"Chap03/3.2/","text":"3.2-1 Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n) + g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n) \\cdot g(n)$ is monotonically increasing. $$ \\begin{aligned} f(m) & \\le f(n) \\quad \\text{ for } m \\le n \\\\ g(m) & \\le g(n) \\quad \\text{ for } m \\le n, \\\\ \\to f(m) + g(m) & \\le f(n) + g(n), \\end{aligned} $$ which proves the first function. Then $$f(g(m)) \\le f(g(n)) \\text{ for } m \\le n.$$ This is true, since $g(m) \\le g(n)$ and $f(n)$ is monotonically increasing. If both functions are nonnegative, then we can multiply the two equalities and we get $$f(m) \\cdot g(m) \\le f(n) \\cdot g(n).$$ 3.2-2 Prove equation $\\text{(3.16)}$. $$ \\begin{aligned} a^{\\log_b c} = a^\\frac{\\log_a c}{\\log_a b} = (a^{\\log_a c})^{\\frac{1}{\\log_a b}} = c^{\\log_b a} \\end{aligned} $$ 3.2-3 Prove equation $\\text{(3.19)}$. Also prove that $n \\ne \\omega(2^n)$ and $n \\ne o(n^n)$. $$\\lg(n!) = \\Theta(n\\lg n) \\tag{3.19}$$ We use Stirling's approximation: $$ \\begin{aligned} \\lg(n!) & = \\lg\\Bigg(\\sqrt{2\\pi n}\\Big(\\frac{n}{e}\\Big)^n\\Big(1 + \\Theta(\\frac{1}{n})\\Big)\\Bigg) \\\\ & = \\lg\\sqrt{2\\pi n } + \\lg\\Big(\\frac{n}{e}\\Big)^n + \\lg\\Big(1+\\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + n\\lg{\\frac{n}{e}} + \\lg\\Big(\\Theta(1) + \\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + \\Theta(n\\lg n) + \\Theta(\\frac{1}{n}) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ The other two are $$\\forall n > 3: 2^n = \\underbrace{2 \\cdot 2 \\cdot \\cdots \\cdot 2}_\\text{n times} < 1 \\cdot 2 \\cdot \\cdots \\cdot n = n! \\quad \\Rightarrow \\quad n! = \\omega(2^n).$$ and $$\\forall n > 1 : n! = 1 \\cdot 2 \\cdot \\cdots n < \\underbrace{n \\cdot n \\cdot \\cdots \\cdot n}_\\text{n times} = n^n \\quad \\Rightarrow \\quad n! = o(n^n).$$ 3.2-4 $\\star$ Is the function $\\lceil \\lg n \\rceil!$ polynomially bounded? Is the function $\\lceil \\lg\\lg n \\rceil!$ polynomially bounded? $\\lceil \\lg n \\rceil!$ is not polynomially bounded, but $\\lceil \\lg\\lg n \\rceil!$ is. Proving that a function $f(n)$ is polynomially bounded is equivalent to proving that $\\lg(f(n)) = O(\\lg n)$ for the following reasons. If $f$ is polynomially bounded, then there exist constants $c$, $k$, $n_0$ such that for all $n \\ge n_0$, $f(n) \\le cn^k$. Hence, $\\lg(f(n)) \\le kc\\lg n$, which, since $c$ and $k$ are constants, means that $\\lg(f(n)) = O(\\lg n)$. Similarly, if $\\lg(f(n)) = O(\\lg n)$, then $f$ is polynomially bounded. In the following proofs, we will make use of the following two facts: $\\lg(n!) = \\Theta(n\\lg n)$ (by equation $\\text{(3.19)}$). $\\lceil \\lg n \\rceil = \\Theta(\\lg n)$, because $\\lceil \\lg n \\rceil \\ge \\lg n$ $\\lceil \\lg n \\rceil < \\lg n + 1 \\le 2\\lg n \\text{ for all } n \\ge 2$ $$ \\begin{aligned} \\lg(\\lceil \\lg n \\rceil!) & = \\Theta(\\lceil \\lg n \\rceil \\lg \\lceil \\lg n \\rceil) \\\\ & = \\Theta(\\lg n\\lg\\lg n) \\\\ & = \\omega(\\lg n). \\end{aligned} $$ Therefore, $\\lg(\\lceil \\lg n \\rceil!) \\ne O(\\lg n)$, and so $\\lceil \\lg n \\rceil!$ is not polynomially bounded. $$ \\begin{aligned} \\lg(\\lceil \\lg\\lg n \\rceil!) & = \\Theta(\\lceil \\lg\\lg n \\rceil \\lg \\lceil \\lg\\lg n \\rceil) \\\\ & = \\Theta(\\lg\\lg n\\lg\\lg\\lg n) \\\\ & = o((\\lg\\lg n)^2) \\\\ & = o(\\lg^2(\\lg n)) \\\\ & = o(\\lg n). \\end{aligned} $$ The last step above follows from the property that any polylogarithmic function grows more slowly than any positive polynomial function, i.e., that for constants $a, b > 0$, we have $\\lg^b = o(n^a)$. Substitute $\\lg n$ for $n$, $2$ for $b$, and $1$ for $a$, giving $\\lg^2(\\lg n) = o(\\lg n)$. Therefore, $\\lg(\\lceil \\lg\\lg n \\rceil!) = O(\\lg n)$, and so $\\lceil \\lg\\lg n \\rceil!$ is polynomially bounded. 3.2-5 $\\star$ Which is asymptotically larger: $\\lg(\\lg^*n)$ or $\\lg^*(\\lg n)$? $\\lg^*(\\lg n)$ is asymptotically larger because $\\lg^*(\\lg n) = \\lg^*n - 1$. 3.2-6 Show that the golden ratio $\\phi$ and its conjugate $\\hat \\phi$ both satisfy the equation $x^2 = x + 1$. $$ \\begin{aligned} \\phi^2 - \\phi - 1 & = \\big(\\frac{1 + \\sqrt 5}{2}\\big)^2 - \\frac{1 + \\sqrt 5}{2} - 1 \\\\ & = \\frac{1 + 2\\sqrt 5 + 5 - 2 - 2\\sqrt 5 - 4}{4} \\\\ & = 0. \\end{aligned} $$ $$ \\begin{aligned} \\hat\\phi^2 - \\hat\\phi - 1 & = \\big(\\frac{1 - \\sqrt 5}{2}\\big)^2 - \\frac{1 - \\sqrt 5}{2} - 1 \\\\ & = \\frac{1 - 2\\sqrt 5 + 5 - 2 + 2\\sqrt 5 - 4}{4} \\\\ & = 0. \\end{aligned} $$ 3.2-7 Prove by induction that the $i$th Fibonacci number satisfies the equality $$F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5},$$ where $\\phi$ is the golden ratio and $\\hat\\phi$ is its conjugate. We have two base cases: $i = 0$ and $i = 1$. For $i = 0$, we have $$ \\begin{aligned} \\frac{\\phi^0 - \\hat\\phi^0}{\\sqrt 5} & = \\frac{1 - 1}{\\sqrt 5} \\\\ & = 0 \\\\ & = F_0, \\end{aligned} $$ and for $i = 1$, we have $$ \\begin{aligned} \\frac{\\phi^1 - \\hat\\phi^1}{\\sqrt 5} & = \\frac{(1 + \\sqrt 5) - (1 - \\sqrt 5)}{2\\sqrt 5} \\\\ & = \\frac{2\\sqrt 5}{2\\sqrt 5} \\\\ & = 1 \\\\ & = F_1. \\end{aligned} $$ For the inductive case, the inductive hypothesis is that $F_{i - 1} = (\\phi^{i - 1} - \\hat\\phi^{i - 1}) / \\sqrt 5$ and $F_{i - 2} = (\\phi^{i - 2} - \\hat\\phi^{i - 2}) / \\sqrt 5$. We have $$ \\begin{aligned} F_i & = F_{i - 1} + F_{i - 2} & \\text{(equation (3.22)} \\\\ & = \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt 5} + \\frac{\\phi^{i - 2} - \\hat\\phi^{i - 2}}{\\sqrt 5} & \\text{(inductive hypothesis)} \\\\ & = \\frac{\\phi^{i - 2}(\\phi + 1) - \\hat\\phi^{i - 2}(\\hat\\phi + 1)}{\\sqrt 5} \\\\ & = \\frac{\\phi^{i - 2}\\phi^2 - \\hat\\phi^{i - 2}\\hat\\phi^2}{\\sqrt 5} & \\text{(Exercise 3.2-6)} \\\\ & = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5}. \\end{aligned} $$ 3.2-8 Show that $k\\ln k = \\Theta(n)$ implies $k = \\Theta(n / \\ln n)$. From the symmetry of $\\Theta$, $$k\\ln k = \\Theta(n) \\Rightarrow n = \\Theta(k\\ln k).$$ Let's find $\\ln n$, $$\\ln n = \\Theta(\\ln(k\\ln k)) = \\Theta(\\ln k + \\ln\\ln k) = \\Theta(\\ln k).$$ Let's divide the two, $$\\frac{n}{\\ln n} = \\frac{\\Theta(k\\ln k)}{\\Theta(\\ln k)} = \\Theta\\Big({\\frac{k\\ln k}{\\ln k}}\\Big) = \\Theta(k).$$","title":"3.2 Standard notations and common functions"},{"location":"Chap03/3.2/#32-1","text":"Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n) + g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n) \\cdot g(n)$ is monotonically increasing. $$ \\begin{aligned} f(m) & \\le f(n) \\quad \\text{ for } m \\le n \\\\ g(m) & \\le g(n) \\quad \\text{ for } m \\le n, \\\\ \\to f(m) + g(m) & \\le f(n) + g(n), \\end{aligned} $$ which proves the first function. Then $$f(g(m)) \\le f(g(n)) \\text{ for } m \\le n.$$ This is true, since $g(m) \\le g(n)$ and $f(n)$ is monotonically increasing. If both functions are nonnegative, then we can multiply the two equalities and we get $$f(m) \\cdot g(m) \\le f(n) \\cdot g(n).$$","title":"3.2-1"},{"location":"Chap03/3.2/#32-2","text":"Prove equation $\\text{(3.16)}$. $$ \\begin{aligned} a^{\\log_b c} = a^\\frac{\\log_a c}{\\log_a b} = (a^{\\log_a c})^{\\frac{1}{\\log_a b}} = c^{\\log_b a} \\end{aligned} $$","title":"3.2-2"},{"location":"Chap03/3.2/#32-3","text":"Prove equation $\\text{(3.19)}$. Also prove that $n \\ne \\omega(2^n)$ and $n \\ne o(n^n)$. $$\\lg(n!) = \\Theta(n\\lg n) \\tag{3.19}$$ We use Stirling's approximation: $$ \\begin{aligned} \\lg(n!) & = \\lg\\Bigg(\\sqrt{2\\pi n}\\Big(\\frac{n}{e}\\Big)^n\\Big(1 + \\Theta(\\frac{1}{n})\\Big)\\Bigg) \\\\ & = \\lg\\sqrt{2\\pi n } + \\lg\\Big(\\frac{n}{e}\\Big)^n + \\lg\\Big(1+\\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + n\\lg{\\frac{n}{e}} + \\lg\\Big(\\Theta(1) + \\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + \\Theta(n\\lg n) + \\Theta(\\frac{1}{n}) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ The other two are $$\\forall n > 3: 2^n = \\underbrace{2 \\cdot 2 \\cdot \\cdots \\cdot 2}_\\text{n times} < 1 \\cdot 2 \\cdot \\cdots \\cdot n = n! \\quad \\Rightarrow \\quad n! = \\omega(2^n).$$ and $$\\forall n > 1 : n! = 1 \\cdot 2 \\cdot \\cdots n < \\underbrace{n \\cdot n \\cdot \\cdots \\cdot n}_\\text{n times} = n^n \\quad \\Rightarrow \\quad n! = o(n^n).$$","title":"3.2-3"},{"location":"Chap03/3.2/#32-4-star","text":"Is the function $\\lceil \\lg n \\rceil!$ polynomially bounded? Is the function $\\lceil \\lg\\lg n \\rceil!$ polynomially bounded? $\\lceil \\lg n \\rceil!$ is not polynomially bounded, but $\\lceil \\lg\\lg n \\rceil!$ is. Proving that a function $f(n)$ is polynomially bounded is equivalent to proving that $\\lg(f(n)) = O(\\lg n)$ for the following reasons. If $f$ is polynomially bounded, then there exist constants $c$, $k$, $n_0$ such that for all $n \\ge n_0$, $f(n) \\le cn^k$. Hence, $\\lg(f(n)) \\le kc\\lg n$, which, since $c$ and $k$ are constants, means that $\\lg(f(n)) = O(\\lg n)$. Similarly, if $\\lg(f(n)) = O(\\lg n)$, then $f$ is polynomially bounded. In the following proofs, we will make use of the following two facts: $\\lg(n!) = \\Theta(n\\lg n)$ (by equation $\\text{(3.19)}$). $\\lceil \\lg n \\rceil = \\Theta(\\lg n)$, because $\\lceil \\lg n \\rceil \\ge \\lg n$ $\\lceil \\lg n \\rceil < \\lg n + 1 \\le 2\\lg n \\text{ for all } n \\ge 2$ $$ \\begin{aligned} \\lg(\\lceil \\lg n \\rceil!) & = \\Theta(\\lceil \\lg n \\rceil \\lg \\lceil \\lg n \\rceil) \\\\ & = \\Theta(\\lg n\\lg\\lg n) \\\\ & = \\omega(\\lg n). \\end{aligned} $$ Therefore, $\\lg(\\lceil \\lg n \\rceil!) \\ne O(\\lg n)$, and so $\\lceil \\lg n \\rceil!$ is not polynomially bounded. $$ \\begin{aligned} \\lg(\\lceil \\lg\\lg n \\rceil!) & = \\Theta(\\lceil \\lg\\lg n \\rceil \\lg \\lceil \\lg\\lg n \\rceil) \\\\ & = \\Theta(\\lg\\lg n\\lg\\lg\\lg n) \\\\ & = o((\\lg\\lg n)^2) \\\\ & = o(\\lg^2(\\lg n)) \\\\ & = o(\\lg n). \\end{aligned} $$ The last step above follows from the property that any polylogarithmic function grows more slowly than any positive polynomial function, i.e., that for constants $a, b > 0$, we have $\\lg^b = o(n^a)$. Substitute $\\lg n$ for $n$, $2$ for $b$, and $1$ for $a$, giving $\\lg^2(\\lg n) = o(\\lg n)$. Therefore, $\\lg(\\lceil \\lg\\lg n \\rceil!) = O(\\lg n)$, and so $\\lceil \\lg\\lg n \\rceil!$ is polynomially bounded.","title":"3.2-4 $\\star$"},{"location":"Chap03/3.2/#32-5-star","text":"Which is asymptotically larger: $\\lg(\\lg^*n)$ or $\\lg^*(\\lg n)$? $\\lg^*(\\lg n)$ is asymptotically larger because $\\lg^*(\\lg n) = \\lg^*n - 1$.","title":"3.2-5 $\\star$"},{"location":"Chap03/3.2/#32-6","text":"Show that the golden ratio $\\phi$ and its conjugate $\\hat \\phi$ both satisfy the equation $x^2 = x + 1$. $$ \\begin{aligned} \\phi^2 - \\phi - 1 & = \\big(\\frac{1 + \\sqrt 5}{2}\\big)^2 - \\frac{1 + \\sqrt 5}{2} - 1 \\\\ & = \\frac{1 + 2\\sqrt 5 + 5 - 2 - 2\\sqrt 5 - 4}{4} \\\\ & = 0. \\end{aligned} $$ $$ \\begin{aligned} \\hat\\phi^2 - \\hat\\phi - 1 & = \\big(\\frac{1 - \\sqrt 5}{2}\\big)^2 - \\frac{1 - \\sqrt 5}{2} - 1 \\\\ & = \\frac{1 - 2\\sqrt 5 + 5 - 2 + 2\\sqrt 5 - 4}{4} \\\\ & = 0. \\end{aligned} $$","title":"3.2-6"},{"location":"Chap03/3.2/#32-7","text":"Prove by induction that the $i$th Fibonacci number satisfies the equality $$F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5},$$ where $\\phi$ is the golden ratio and $\\hat\\phi$ is its conjugate. We have two base cases: $i = 0$ and $i = 1$. For $i = 0$, we have $$ \\begin{aligned} \\frac{\\phi^0 - \\hat\\phi^0}{\\sqrt 5} & = \\frac{1 - 1}{\\sqrt 5} \\\\ & = 0 \\\\ & = F_0, \\end{aligned} $$ and for $i = 1$, we have $$ \\begin{aligned} \\frac{\\phi^1 - \\hat\\phi^1}{\\sqrt 5} & = \\frac{(1 + \\sqrt 5) - (1 - \\sqrt 5)}{2\\sqrt 5} \\\\ & = \\frac{2\\sqrt 5}{2\\sqrt 5} \\\\ & = 1 \\\\ & = F_1. \\end{aligned} $$ For the inductive case, the inductive hypothesis is that $F_{i - 1} = (\\phi^{i - 1} - \\hat\\phi^{i - 1}) / \\sqrt 5$ and $F_{i - 2} = (\\phi^{i - 2} - \\hat\\phi^{i - 2}) / \\sqrt 5$. We have $$ \\begin{aligned} F_i & = F_{i - 1} + F_{i - 2} & \\text{(equation (3.22)} \\\\ & = \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt 5} + \\frac{\\phi^{i - 2} - \\hat\\phi^{i - 2}}{\\sqrt 5} & \\text{(inductive hypothesis)} \\\\ & = \\frac{\\phi^{i - 2}(\\phi + 1) - \\hat\\phi^{i - 2}(\\hat\\phi + 1)}{\\sqrt 5} \\\\ & = \\frac{\\phi^{i - 2}\\phi^2 - \\hat\\phi^{i - 2}\\hat\\phi^2}{\\sqrt 5} & \\text{(Exercise 3.2-6)} \\\\ & = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5}. \\end{aligned} $$","title":"3.2-7"},{"location":"Chap03/3.2/#32-8","text":"Show that $k\\ln k = \\Theta(n)$ implies $k = \\Theta(n / \\ln n)$. From the symmetry of $\\Theta$, $$k\\ln k = \\Theta(n) \\Rightarrow n = \\Theta(k\\ln k).$$ Let's find $\\ln n$, $$\\ln n = \\Theta(\\ln(k\\ln k)) = \\Theta(\\ln k + \\ln\\ln k) = \\Theta(\\ln k).$$ Let's divide the two, $$\\frac{n}{\\ln n} = \\frac{\\Theta(k\\ln k)}{\\Theta(\\ln k)} = \\Theta\\Big({\\frac{k\\ln k}{\\ln k}}\\Big) = \\Theta(k).$$","title":"3.2-8"},{"location":"Chap03/Problems/3-1/","text":"Let $$p(n) = \\sum_{i = 0}^d a_i n^i,$$ where $a_d > 0$, be a degree-$d$ polynomial in $n$, and let $k$ be a constant. Use the definitions of the asymptotic notations to prove the following properties. a. If $k \\ge d$, then $p(n) = O(n^k)$. b. If $k \\le d$, then $p(n) = \\Omega(n^k)$. c. If $k = d$, then $p(n) = \\Theta(n^k)$. d. If $k > d$, then $p(n) = o(n^k)$. e. If $k < d$, then $p(n) = \\omega(n^k)$. Let's see that $p(n) = O(n^d)$. We need do pick $c = a_d + b$, such that $$\\sum\\limits_{i = 0}^d = a_d n^d + a_{d - 1}n^{d - 1} + \\cdots + a_1n + a_0 \\le cn^d.$$ When we divide by $n^d$, we get $$c = a_d + b \\ge a_d + \\frac{a_{d - 1}}n + \\frac{a_{d - 2}}{n^2} + \\cdots + \\frac{a_0}{n^d}.$$ and $$b \\ge \\frac{a_{d - 1}}n + \\frac{a_{d - 2}}{n^2} + \\cdots + \\frac{a_0}{n^d}.$$ If we choose $b = 1$, then we can choose $n_0$, $$n_0 = \\max(da_{d - 1}, d\\sqrt{a_{d - 2}}, \\ldots, d\\sqrt[d]{a_0}).$$ Now we have $n_0$ and $c$, such that $$p(n) \\le cn^d \\quad \\text{for } n \\ge n_0,$$ which is the definition of $O(n^d)$. By chosing $b = -1$ we can prove the $\\Omega(n^d)$ inequality and thus the $\\Theta(n^d)$ inequality. It is very similar to prove the other inequalities.","title":"3-1 Asymptotic behavior of polynomials"},{"location":"Chap03/Problems/3-2/","text":"Indicate for each pair of expressions $(A, B)$ in the table below, whether $A$ is $O$, $o$, $\\Omega$, $\\omega$, or $\\Theta$ of $B$. Assume that $k \\ge 1$, $\\epsilon > 0$, and $c > 1$ are constants. Your answer should be in the form of the table with \"yes\" or \"no\" written in each box. $$ \\begin{array}{ccccccc} A & B & O & o & \\Omega & \\omega & \\Theta \\\\ \\hline \\lg^k n & n^\\epsilon & yes & yes & no & no & no \\\\ n^k & c^n & yes & yes & no & no & no \\\\ \\sqrt n & n^{\\sin n} & no & no & no & no & no \\\\ 2^n & 2^{n / 2} & no & no & yes & yes & no \\\\ n^{\\lg c} & c^{\\lg n} & yes & no & yes & no & yes \\\\ \\lg(n!) & \\lg(n^n) & yes & no & yes & no & yes \\end{array} $$","title":"3-2 Relative asymptotic growths"},{"location":"Chap03/Problems/3-3/","text":"a. Rank the following functions by order of growth; that is, find an arrangement $g_1, g_2, \\ldots , g_{30}$ of the functions $g_1 = \\Omega(g_2), g_2 = \\Omega(g_3), \\ldots, g_{29} = \\Omega(g_{30})$. Partition your list into equivalence classes such that functions $f(n)$ and $g(n)$ are in the same class if and only if $f(n) = \\Theta(g(n))$. $$ \\begin{array}{cccccc} \\lg(\\lg^{^*}n) \\quad & \\quad 2^{\\lg^*n} \\quad & \\quad (\\sqrt 2)^{\\lg n} \\quad & \\quad n^2 \\quad & \\quad n! \\quad & \\quad (\\lg n)! \\\\ (\\frac{3}{2})^n \\quad & \\quad n^3 \\quad & \\quad \\lg^2 n \\quad & \\quad \\lg(n!) \\quad & \\quad 2^{2^n} \\quad & \\quad n^{1/\\lg n} \\\\ \\lg\\lg n \\quad & \\quad \\lg^* n \\quad & \\quad n\\cdot 2^n \\quad & \\quad n^{\\lg\\lg n} \\quad & \\quad \\lg n \\quad & \\quad 1 \\\\ 2^{\\lg n} \\quad & \\quad (\\lg n)^{\\lg n} \\quad & \\quad e^n \\quad & \\quad 4^{\\lg n} \\quad & \\quad (n + 1)! \\quad & \\quad \\sqrt{\\lg n} \\\\ \\lg^*(\\lg n) \\quad & \\quad 2^{\\sqrt{2\\lg n}} \\quad & \\quad n \\quad & \\quad 2^n \\quad & \\quad n\\lg n \\quad & \\quad 2^{2^{n + 1}} \\end{array} $$ b. Give an example of a single nonnegative function $f(n)$ such that for all functions $g_i(n)$ in part (1), $f(n)$ is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$. a. Here is the ordering, where functions on the same line are in the same equivalence class, and those higher on the page are $\\Omega$ of those below them: $$ \\begin{array}{ll} 2^{2^{n + 1}} & \\\\ 2^{2^n} & \\\\ (n + 1)! & \\\\ n! & \\text{see justification 7} \\\\ e^n & \\text{see justification 1} \\\\ n\\cdot 2^n & \\\\ 2^n & \\\\ (3 / 2)^n & \\\\ (\\lg n)^{\\lg n} = n^{\\lg\\lg n} & \\text{see identity 1} \\\\ (\\lg n)! & \\text{see justifications 2, 8} \\\\ n^3 & \\\\ n^2 = 4^{\\lg n} & \\text{see identity 2} \\\\ n\\lg n \\text{ and } \\lg(n!) & \\text{see justification 6} \\\\ n = 2^{\\lg n} & \\text{see identity 3} \\\\ (\\sqrt 2)^{\\lg n}(= \\sqrt n) & \\text{see identity 6, justification 3} \\\\ 2^{\\sqrt{2\\lg n}} & \\text{see identity 5, justification 4}\\\\ \\lg^2 n & \\\\ \\ln n & \\\\ \\sqrt{\\lg n} & \\\\ \\ln\\ln n & \\text{see justification 5} \\\\ 2^{\\lg^*n} & \\\\ \\lg^*n \\text{ and } \\lg^*(\\lg n) & \\text{see identity 7} \\\\ \\lg(\\lg^*)n & \\\\ n^{1 / \\lg n}(= 2) \\text{ and } 1 & \\text{see identity 4} \\end{array} $$ Much of the ranking is based on the following properties: Exponential functions grow faster than polynomial functions, which grow faster than polylogarithmic functions. The base of a logarithm doen't matter asymptotically, but the base of an exponential and the degree of a polynomial do matter. We have the following $\\textit{identities}$: $(\\lg n)^{\\lg n} = n^{\\lg\\lg n}$ because $a^{\\log_b c} = c^{\\log_b a}$. $4^{\\lg n} = n^2$ because $a^{\\log_b c} = c^{\\log_b a}$. $2^{\\lg n} = n$. $2 = n^{1 / \\lg n}$ by raising identity 3 to the power $1 / \\lg n$. $2^{\\sqrt{2\\lg n}} = n^{\\sqrt{2 / \\lg n}}$ by raising identity 4 to the power $\\sqrt{2\\lg n}$. $(\\sqrt 2)^{\\lg n} = \\sqrt n$ because $(\\sqrt 2)^{\\lg n} = 2^{(1 / 2)\\lg n} = 2^{\\lg\\sqrt n} = \\sqrt n$. $\\lg^*(\\lg n) = (\\lg^*n) - 1$. The following $\\textit{justifications}$ explain some of the rankings: $e^n = 2^n(e / 2)^n = \\omega(n2^n)$, since $(e/2)^n = \\omega(n)$. $(\\lg n) \\ne \\omega(n^3)$ by taking logs: $\\lg(\\lg n) \\ne \\Theta(\\lg n\\lg\\lg n)$ by Stirling's approximation, $\\lg(n^3) = 3\\lg n$. $\\lg\\lg n = \\omega(3)$. $(\\sqrt 2)^{\\lg n} = \\omega(2^{\\sqrt{2\\lg n}})$ by taking logs: $\\lg(\\sqrt 2)^{\\lg n} = (1 / 2)\\lg n,\\lg 2^{\\sqrt{2\\lg n}} = \\sqrt{2\\lg n}$. $(1 / 2)\\lg n = \\omega(\\sqrt{2\\lg n})$. $2^{\\sqrt{2\\lg n}} = \\omega(\\lg^2 n)$ by taking logs: $\\lg 2^{\\sqrt{2\\lg n}} = \\sqrt{2\\lg n},\\lg\\lg^2n = 2\\lg\\lg n$. $\\sqrt{2\\lg n} = \\omega(2\\lg\\lg n)$. $\\ln\\ln n = \\omega(2^{\\lg^*n})$ by taking logs: $\\lg 2^{\\lg^* n} = \\lg^*n$. $\\lg\\ln\\ln n = \\omega(\\lg^*n)$. $\\lg(n!) = \\Theta(n\\lg n)$ (equation $\\text{(3.18)}$). $n \\ne \\Theta(n^{n + 1}e^{-n})$ by dropping constants and low-order terms in equation $\\text{(3.17)}$. $(\\lg n) \\ne \\Theta((\\lg n)^{\\lg n + 1 / 2} e^{-\\lg n}$ by substituting $\\lg n$ for $n$ in the previous justification. $(\\lg n) \\ne \\Theta((\\lg n)^{\\lg n + 1 / 2}n^{-\\lg e})$ because $a^{\\log_b c} = c^{\\log_b a}$. b. The following $f(n)$ is nonnegative, and for all functions $g_i(n)$ in part (1), $f(n)$ is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$. $$ f(n) = \\begin{cases} 2^{2^{n + 2}} & \\text{if $n$ is even}, \\\\ 0 & \\text{if $n$ is odd}. \\end{cases} $$","title":"3-3 Ordering by asymptotic growth rates"},{"location":"Chap03/Problems/3-4/","text":"Let $f(n)$ and $g(n)$ by asymptotically positive functions. Prove or disprove each of the following conjectures. a. $f(n) = O(g(n))$ implies $g(n) = O(f(n))$. b. $f(n) + g(n) = \\Theta(\\min(f(n), g(n)))$. c. $f(n) = O(g(n))$ implies $\\lg(f(n)) = O(\\lg(g(n)))$, where $\\lg(g(n)) \\ge 1$ and $f(n) \\ge 1$ for all sufficiently large $n$. d. $f(n) = O(g(n))$ implies $2^{f(n)} = O(2^{g(n)})$. e. $f(n) = O((f(n))^2)$. f. $f(n) = O(g(n))$ implies $g(n) = \\Omega(f(n))$. g. $f(n) = \\Theta(f(n / 2))$. h. $f(n) + o(f(n)) = \\Theta(f(n))$. a. Disprove, $n = O(n^2)$, but $n^2 \\ne O(n)$. b. Disprove, $n^2 + n \\ne \\Theta(\\min(n^2, n)) = \\Theta(n)$. c. Prove, because $f(n) \\ge 1$ after a certain $n \\ge n_0$. $$ \\begin{aligned} \\exists c, n_0: \\forall n \\ge n_0, 0 \\le f(n) \\le cg(n) \\\\ \\Rightarrow 0 \\le \\lg f(n) \\le \\lg (cg(n)) = \\lg c + \\lg g(n). \\end{aligned} $$ We need to prove that $$\\lg f(n) \\le d\\lg g(n).$$ We can find $d$, $$d = \\frac{\\lg c + \\lg g(n)}{\\lg g(n)} = \\frac{\\lg c}{\\lg g(n)} + 1 \\le \\lg c + 1,$$ where the last step is valid, because $\\lg g(n) \\ge 1$. d. Disprove, because $2n = O(n)$, but $2^{2n} = 4^n \\ne O(2^n)$. e. Prove, $0 \\le f(n) \\le cf^2(n)$ is trivial when $f(n) \\ge 1$, but if $f(n) < 1$ for all $n$, it's not correct. However, we don't care this case. f. Prove, from the first, we know that $0 \\le f(n) \\le cg(n)$ and we need to prove that $0 \\le df(n) \\le g(n)$, which is straightforward with $d = 1 / c$. g. Disprove, let's pick $f(n) = 2^n$. We will need to prove that $$\\exists c_1, c_2, n_0: \\forall n \\ge n_0, 0 \\le c_1 \\cdot 2^{n / 2} \\le 2^n \\le c_2 \\cdot 2^{n / 2},$$ which is obviously untrue. h. Prove, let $g(n) = o(f(n))$. Then $$\\exists c, n_0: \\forall n \\ge n_0, 0 \\le g(n) < cf(n).$$ We need to prove that $$\\exists c_1, c_2, n_0: \\forall n \\ge n_0, 0 \\le c_1f(n) \\le f(n) + g(n) \\le c_2f(n).$$ Thus, if we pick $c_1 = 1$ and $c_2 = c + 1$, it holds.","title":"3-4 Asymptotic notation properties"},{"location":"Chap03/Problems/3-5/","text":"Some authors define $\\Omega$ in a slightly different way than we do; let's use ${\\Omega}^{\\infty}$ (read \"omega infinity\") for this alternative definition. We say that $f(n) = {\\Omega}^{\\infty}(g(n))$ if there exists a positive constant $c$ such that $f(n) \\ge cg(n) \\ge 0$ for infinitely many integers $n$. a. Show that for any two functions $f(n)$ and $g(n)$ that are asymptotically nonnegative, either $f(n) = O(g(n))$ or $f(n) = {\\Omega}^{\\infty}(g(n))$ or both, whereas this is not true if we use $\\Omega$ in place of ${\\Omega}^{\\infty}$. b. Describe the potential advantages and disadvantages of using ${\\Omega}^{\\infty}$ instead of $\\Omega$ to characterize the running times of programs. Some authors also define $O$ in a slightly different manner; let's use $O'$ for the alternative definition. We say that $f(n) = O'(g(n))$ if and only if $|f(n)| = O(g(n))$. c. What happens to each direction of the \"if and only if\" in Theorem 3.1 if we substitute $O'$ for $O$ but we still use $\\Omega$? Some authors define $\\tilde O$ (read \"soft-oh\") to mean $O$ with logarithmic factors ignored: $$ \\begin{aligned} \\tilde{O}(g(n)) = \\{f(n): & \\text{ there exist positive constants $c$, $k$, and $n_0$ such that } \\\\ & \\text{ $0 \\le f(n) \\le cg(n) \\lg^k(n)$ for all $n \\ge n_0$ }.\\} \\end{aligned} $$ d. Define $\\tilde\\Omega$ and $\\tilde\\Theta$ in a similar manner. Prove the corresponding analog to Theorem 3.1. a. We have $$ f(n) = \\begin{cases} O(g(n)) \\text{ and } {\\Omega}^{\\infty}(g(n)) & \\text{if $f(n) = \\Theta(g(n))$}, \\\\ O(g(n)) & \\text{if $0 \\le f(n) \\le cg(n)$}, \\\\ {\\Omega}^{\\infty}(g(n)) & \\text{if $0 \\le cg(n) \\le f(n)$, for infinitely many integers $n$}. \\end{cases} $$ If there are only finite $n$ such that $f(n) \\ge cg(n) \\ge 0$. When $n \\to \\infty$, $0 \\le f(n) \\le cg(n)$, i.e., $f(n) = O(g(n))$. Obviously, it's not hold when we use $\\Omega$ in place of ${\\Omega}^{\\infty}$. b. Advantages: We can characterize all the relationships between all functions. Disadvantages: We cannot characterize precisely. c. For any two functions $f(n)$ and $g(n)$, we have if $f(n) = \\Theta(g(n))$ then $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$ and $f(n) = \\Omega(g(n))$. But the conversion is not true. d. We have $$ \\begin{aligned} \\tilde\\Omega(g(n)) = \\{f(n): & \\text{ there exist positive constants $c$, $k$, and $n_0$ such that } \\\\ & \\text{ $0 \\le cg(n)\\lg^k(n) \\le f(n)$ for all $n \\ge n_0$}.\\} \\end{aligned} $$ $$ \\begin{aligned} \\tilde{\\Theta}(g(n)) = \\{f(n): & \\text{ there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, and $n_0$ such that } \\\\ & \\text{ $0\\le c_1 g(n) \\lg^{k_1}(n) \\le f(n)\\le c_2g (n) \\lg^{k_2}(n)$ for all $n\\ge n_0$.}\\} \\end{aligned} $$ For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\tilde\\Theta(g(n))$ if and only if $f(n) = \\tilde O(g(n))$ and $f(n) = \\tilde\\Omega(g(n))$.","title":"3-5 Variations on $O$ and $\\Omega$"},{"location":"Chap03/Problems/3-6/","text":"We can apply the iteration operator $^*$ used in the $\\lg^*$ function to any monotonically increasing function $f(n)$ over the reals. For a given constant $c \\in \\mathbb R$, we define the iterated function ${f_c}^*$ by ${f_c}^*(n) = \\min \\{i \\ge 0 : f^{(i)}(n) \\le c \\}$ which need not be well defined in all cases. In other words, the quantity ${f_c}^*(n)$ is the number of iterated applications of the function $f$ required to reduce its argument down to $c$ or less. For each of the following functions $f(n)$ and constants $c$, give as tight a bound as possible on ${f_c}^*(n)$. $$ \\begin{array}{ccl} f(n) & c & {f_c}^* \\\\ \\hline n - 1 & 0 & \\Theta(n) \\\\ \\lg n & 1 & \\Theta(\\lg^*{n}) \\\\ n / 2 & 1 & \\Theta(\\lg n) \\\\ n / 2 & 2 & \\Theta(\\lg n) \\\\ \\sqrt 2 & 2 & \\Theta(\\lg\\lg n) \\\\ \\sqrt 2 & 1 & \\text{does not converge} \\\\ n^{1 / 2} & 2 & \\Theta(\\log_3{\\lg n}) \\\\ n / \\lg n & 2 & \\omega(\\lg\\lg n), o(\\lg n) \\end{array} $$","title":"3-6 Iterated functions"},{"location":"Chap04/4.1/","text":"4.1-1 What does $\\text{FIND-MAXIMUM-SUBARRAY}$ return when all elements of $A$ are negative? If the index of the greatest element of $A$ is $i$, it returns $(i, i, A[i])$. 4.1-2 Write pseudocode for the brute-force method of solving the maximum-subarray problem. Your procedure should run in $\\Theta(n^2)$ time. 1 2 3 4 5 6 7 8 9 10 11 12 MAX - SUBARRAY - BRUTE - FORCE ( A ) n = A . length max - so - far = - \u221e for l = 1 to n sum = 0 for h = l to n sum = sum + A [ h ] if sum > max - so - far max - so - far = sum low = l high = h return ( low , high ) 4.1-3 Implement both the brute-force and recursive algorithms for the maximum-subarray problem on your own computer. What problem size $n_0$ gives the crossover point at which the recursive algorithm beats the brute-force algorithm? Then, change the base case of the recursive algorithm to use the brute-force algorithm whenever the problem size is less than $n_0$. Does that change the crossover point? On my computer, $n_0$ is $37$. If the algorithm is modified to used divide and conquer when $n \\ge 37$ and the brute-force approach when $n$ is less, the performance at the crossover point almost doubles. The performance at $n_0 - 1$ stays the same, though (or even gets worse, because of the added overhead). What I find interesting is that if we set $n_0 = 20$ and used the mixed approach to sort $40$ elements, it is still faster than both. 4.1-4 Suppose we change the definition of the maximum-subarray problem to allow the result to be an empty subarray, where the sum of the values of an empty subarray is $0$. How would you change any of the algorithms that do not allow empty subarrays to permit an empty subarray to be the result? If the algorithm returns a negative sum, toss out the answer and use an empty subarray instead. 4.1-5 Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray $A[1..j]$, extend the answer to find a maximum subarray ending at index $j + 1$ by using the following observation: a maximum subarray $A[i..j + 1]$, for some $1 \\le i \\le j + 1$. Determine a maximum subarray of the form $A[i..j + 1]$ in constant time based on knowing a maximum subarray ending at index $j$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 MAX - SUBARRAY - LINEAR ( A ) n = A . length max - sum = - \u221e ending - here - sum = - \u221e for j = 1 to n ending - here - high = j if ending - here - sum > 0 ending - here - sum = ending - here - sum + A [ j ] else ending - here - low = j ending - here - sum = A [ j ] if ending - here - sum > max - sum max - sum = ending - here - sum low = ending - here - low high = ending - here - high return ( low , high , max - sum ) The variables are intended as follows: $low$ and $high$ demarcate a maximum subarray found so far. $max\\text-sum$ gives the sum of the values in a maximum subarray found so far. $ending\\text-here\\text-low$ and $ending\\text-here\\text-high$ demarcate a maximum subarray ending at index $j$ . Since the high end of any subarray ending at index $j$ must be $j$, every iteration of the for loop automatically sets $ending\\text-here\\text-high = j$. $ending\\text-here\\text-sum$ gives the sum of the values in a maximum subarray ending at index $j$. The first test within the for loop determines whether a maximum subarray ending at index $j$ contains just $A[j]$. As we enter an iteration of the loop, $ending\\text-here\\text-sum$ has the sum of the values in a maximum subarray ending at $j - 1$. If $ending\\text-here\\text-sum + A[j] > A[j]$, then we extend the maximum subarray ending at index $j - 1$ to include index $j$. (The test in the if statement just subtracts out $A[j]$ from both sides.) Otherwise, we start a new subarray at index $j$, so both its low and high ends have the value $j$ and its sum is $A[j]$. Once we know the maximum subarray ending at index $j$, we test to see whether it has a greater sum than the maximum subarray found so far, ending at any position less than or equal to $j$. If it does, then we update $low$, $high$, and $max\\text-sum$ appropriately. Since each iteration of the for loop takes constant time, and the loop makes $n$ iterations, the running time of $\\text{MAX-SUBARRAY-LINEAR}$ is $O(n)$.","title":"4.1 The maximum-subarray problem"},{"location":"Chap04/4.1/#41-1","text":"What does $\\text{FIND-MAXIMUM-SUBARRAY}$ return when all elements of $A$ are negative? If the index of the greatest element of $A$ is $i$, it returns $(i, i, A[i])$.","title":"4.1-1"},{"location":"Chap04/4.1/#41-2","text":"Write pseudocode for the brute-force method of solving the maximum-subarray problem. Your procedure should run in $\\Theta(n^2)$ time. 1 2 3 4 5 6 7 8 9 10 11 12 MAX - SUBARRAY - BRUTE - FORCE ( A ) n = A . length max - so - far = - \u221e for l = 1 to n sum = 0 for h = l to n sum = sum + A [ h ] if sum > max - so - far max - so - far = sum low = l high = h return ( low , high )","title":"4.1-2"},{"location":"Chap04/4.1/#41-3","text":"Implement both the brute-force and recursive algorithms for the maximum-subarray problem on your own computer. What problem size $n_0$ gives the crossover point at which the recursive algorithm beats the brute-force algorithm? Then, change the base case of the recursive algorithm to use the brute-force algorithm whenever the problem size is less than $n_0$. Does that change the crossover point? On my computer, $n_0$ is $37$. If the algorithm is modified to used divide and conquer when $n \\ge 37$ and the brute-force approach when $n$ is less, the performance at the crossover point almost doubles. The performance at $n_0 - 1$ stays the same, though (or even gets worse, because of the added overhead). What I find interesting is that if we set $n_0 = 20$ and used the mixed approach to sort $40$ elements, it is still faster than both.","title":"4.1-3"},{"location":"Chap04/4.1/#41-4","text":"Suppose we change the definition of the maximum-subarray problem to allow the result to be an empty subarray, where the sum of the values of an empty subarray is $0$. How would you change any of the algorithms that do not allow empty subarrays to permit an empty subarray to be the result? If the algorithm returns a negative sum, toss out the answer and use an empty subarray instead.","title":"4.1-4"},{"location":"Chap04/4.1/#41-5","text":"Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray $A[1..j]$, extend the answer to find a maximum subarray ending at index $j + 1$ by using the following observation: a maximum subarray $A[i..j + 1]$, for some $1 \\le i \\le j + 1$. Determine a maximum subarray of the form $A[i..j + 1]$ in constant time based on knowing a maximum subarray ending at index $j$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 MAX - SUBARRAY - LINEAR ( A ) n = A . length max - sum = - \u221e ending - here - sum = - \u221e for j = 1 to n ending - here - high = j if ending - here - sum > 0 ending - here - sum = ending - here - sum + A [ j ] else ending - here - low = j ending - here - sum = A [ j ] if ending - here - sum > max - sum max - sum = ending - here - sum low = ending - here - low high = ending - here - high return ( low , high , max - sum ) The variables are intended as follows: $low$ and $high$ demarcate a maximum subarray found so far. $max\\text-sum$ gives the sum of the values in a maximum subarray found so far. $ending\\text-here\\text-low$ and $ending\\text-here\\text-high$ demarcate a maximum subarray ending at index $j$ . Since the high end of any subarray ending at index $j$ must be $j$, every iteration of the for loop automatically sets $ending\\text-here\\text-high = j$. $ending\\text-here\\text-sum$ gives the sum of the values in a maximum subarray ending at index $j$. The first test within the for loop determines whether a maximum subarray ending at index $j$ contains just $A[j]$. As we enter an iteration of the loop, $ending\\text-here\\text-sum$ has the sum of the values in a maximum subarray ending at $j - 1$. If $ending\\text-here\\text-sum + A[j] > A[j]$, then we extend the maximum subarray ending at index $j - 1$ to include index $j$. (The test in the if statement just subtracts out $A[j]$ from both sides.) Otherwise, we start a new subarray at index $j$, so both its low and high ends have the value $j$ and its sum is $A[j]$. Once we know the maximum subarray ending at index $j$, we test to see whether it has a greater sum than the maximum subarray found so far, ending at any position less than or equal to $j$. If it does, then we update $low$, $high$, and $max\\text-sum$ appropriately. Since each iteration of the for loop takes constant time, and the loop makes $n$ iterations, the running time of $\\text{MAX-SUBARRAY-LINEAR}$ is $O(n)$.","title":"4.1-5"},{"location":"Chap04/4.2/","text":"4.2-1 Use Strassen's algorithm to compute the matrix product $$ \\begin{pmatrix} 1 & 3 \\\\ 7 & 5 \\end{pmatrix} \\begin{pmatrix} 6 & 8 \\\\ 4 & 2 \\end{pmatrix} . $$ Show your work. The first matrices are $$ \\begin{array}{ll} S_1 = 6 & S_6 = 8 \\\\ S_2 = 4 & S_7 = -2 \\\\ S_3 = 12 & S_8 = 6 \\\\ S_4 = -2 & S_9 = -6 \\\\ S_5 = 6 & S_{10} = 14. \\end{array} $$ The products are $$ \\begin{aligned} P_1 & = 1 \\cdot 6 = 6 \\\\ P_2 & = 4 \\cdot 2 = 8 \\\\ P_3 & = 6 \\cdot 12 = 72 \\\\ P_4 & = -2 \\cdot 5 = -10 \\\\ P_5 & = 6 \\cdot 8 = 48 \\\\ P_6 & = -2 \\cdot 6 = -12 \\\\ P_7 & = -6 \\cdot 14 = -84. \\end{aligned} $$ The four matrices are $$ \\begin{aligned} C_{11} & = 48 + (-10) - 8 + (-12) = 18 \\\\ C_{12} & = 6 + 8 = 14 \\\\ C_{21} & = 72 + (-10) = 62 \\\\ C_{22} & = 48 + 6 - 72 - (-84) = 66. \\end{aligned} $$ The result is $$ \\begin{pmatrix} 18 & 14 \\\\ 62 & 66 \\end{pmatrix} . $$ 4.2-2 Write pseudocode for Strassen's algorithm. $$ A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix} , B = \\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{pmatrix} , C = \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{pmatrix} . \\tag{4.9} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 STRASSEN ( A , B ) n = A . rows let C be a new n \u00d7 n matrix if n == 1 c [ 1 , 1 ] = a [ 1 , 1 ] * b [ 1 , 1 ] else partition A and B in equations ( 4.9 ) let C [ 1 , 1 ], C [ 1 , 2 ], C [ 2 , 1 ], and C [ 2 , 2 ] be n / 2 \u00d7 n / 2 matrices create n / 2 \u00d7 n / 2 matrices S [ 1 ], S [ 2 ], ..., S [ 10 ] and P [ 1 ], P [ 2 ], ..., P [ 7 ] S [ 1 ] = B [ 1 , 2 ] - B [ 2 , 2 ] S [ 2 ] = A [ 1 , 1 ] + A [ 1 , 2 ] S [ 3 ] = A [ 1 , 2 ] + A [ 2 , 2 ] S [ 4 ] = B [ 2 , 1 ] - B [ 1 , 1 ] S [ 5 ] = A [ 1 , 1 ] + A [ 2 , 2 ] S [ 6 ] = B [ 1 , 1 ] + B [ 2 , 2 ] S [ 7 ] = A [ 1 , 2 ] - A [ 2 , 2 ] S [ 8 ] = B [ 2 , 1 ] + B [ 2 , 2 ] S [ 9 ] = A [ 1 , 1 ] - A [ 2 , 1 ] S [ 10 ] = B [ 1 , 1 ] + B [ 1 , 2 ] P [ 1 ] = STRASSEN ( A [ 1 , 1 ], S [ 1 ]) P [ 2 ] = STRASSEN ( S [ 2 ], B [ 2 , 2 ]) P [ 3 ] = STRASSEN ( S [ 3 ], B [ 1 , 1 ]) P [ 4 ] = STRASSEN ( A [ 2 , 2 ], S [ 4 ]) P [ 5 ] = STRASSEN ( S [ 5 ], S [ 6 ]) P [ 6 ] = STRASSEN ( S [ 7 ], S [ 8 ]) P [ 7 ] = STRASSEN ( S [ 9 ], S [ 10 ]) C [ 1 , 1 ] = P [ 5 ] + P [ 4 ] - P [ 2 ] + P [ 6 ] C [ 1 , 2 ] = P [ 1 ] + P [ 2 ] C [ 2 , 1 ] = P [ 3 ] + P [ 4 ] C [ 2 , 2 ] = P [ 5 ] + P [ 1 ] - P [ 3 ] - P [ 7 ] combine C [ 1 , 1 ], C [ 1 , 2 ], C [ 2 , 1 ], and C [ 2 , 2 ] into C return C 4.2-3 How would you modify Strassen's algorithm to multiply $n \\times n$ matrices in which $n$ is not an exact power of $2$? Show that the resulting algorithm runs in time $\\Theta(n^{\\lg7})$. We can just extend it to an $n \\times n$ matrix and pad it with zeroes. It's obviously $\\Theta(n^{\\lg7})$. 4.2-4 What is the largest $k$ such that if you can multiply $3 \\times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n \\times n$ matrices is time $o(n^{\\lg 7})$? What would the running time of this algorithm be? If you can multiply $3 \\times 3$ matrices using $k$ multiplications, then you can multiply $n \\times n$ matrices by recursively multiplying $n / 3 \\times n /3$ matrices, in time $T(n) = kT(n / 3) + \\Theta(n^2)$. Using the master method to solve this recurrence, consider the ratio of $n^{\\log_3 k}$ and $n^2$: If $\\log_3 k = 2$, case 2 applies and $T(n) = \\Theta(n^2\\lg n)$. In this case, $k = 9$ and $T(n) = o(n^{\\lg 7})$. If $\\log_3 k < 2$, case 3 applies and $T(n) = \\Theta(n^2)$. In this case, $k < 9$ and $T(n) = o(n^{\\lg 7})$. If $\\log_3 k > 2$, case 1 applies and $T(n) = \\Theta(n^{\\log_3 k})$. In this case, $k > 9$. $T(n) = o(n^{\\lg 7})$ when $\\log_3 k < \\lg 7$, i.e., when $k < 3^{\\lg 7} \\approx 21.85$. The largest such integer $k$ is $21$. Thus, $k = 21$ and the running time is $\\Theta(n^{\\log_3 k}) = \\Theta(n^{\\log_3 21} = O(n^{2.80})$ (since $\\log_3 21 \\approx 2.77$). 4.2-5 V. Pan has discovered a way of multiplying $68 \\times 68$ matrices using $132464$ multiplications, a way of multiplying $70 \\times 70$ matrices using $143640$ multiplications, and a way of multiplying $72 \\times 72$ matrices using $155424$ multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare to Strassen's algorithm? Using what we know from the last exercise, we need to pick the smallest of the following $$ \\begin{aligned} \\log_{68} 132464 & \\approx 2.795128 \\\\ \\log_{70} 143640 & \\approx 2.795122 \\\\ \\log_{72} 155424 & \\approx 2.795147. \\end{aligned} $$ The fastest one asymptotically is $70 \\times 70$ using $143640$. 4.2-6 How quickly can you multiply a $kn \\times n$ matrix by an $n \\times kn$ matrix, using Strassen's algorithm as a subroutine? Answer the same question with the order of the input matrices reversed. $(kn \\times n)(n \\times kn)$ produces a $kn \\times kn$ matrix. This produces $k^2$ multiplications of $n \\times n$ matrices. $(n \\times kn)(kn \\times n)$ produces an $n \\times n$ matrix. This produces $k$ multiplications and $k - 1$ additions. 4.2-7 Show how to multiply the complex numbers $a + bi$ and $c + di$ using only three multiplications of real numbers. The algorithm should take $a$, $b$, $c$ and $d$ as input and produce the real component $ac - bd$ and the imaginary component $ad + bc$ separately. The three matrices are $$ \\begin{aligned} A & = (a + b)(c + d) = ac + ad + bc + bd \\\\ B & = ac \\\\ C & = bd. \\end{aligned} $$ The result is $$(B - C) + (A - B - C)i.$$","title":"4.2 Strassen's algorithm for matrix multiplication"},{"location":"Chap04/4.2/#42-1","text":"Use Strassen's algorithm to compute the matrix product $$ \\begin{pmatrix} 1 & 3 \\\\ 7 & 5 \\end{pmatrix} \\begin{pmatrix} 6 & 8 \\\\ 4 & 2 \\end{pmatrix} . $$ Show your work. The first matrices are $$ \\begin{array}{ll} S_1 = 6 & S_6 = 8 \\\\ S_2 = 4 & S_7 = -2 \\\\ S_3 = 12 & S_8 = 6 \\\\ S_4 = -2 & S_9 = -6 \\\\ S_5 = 6 & S_{10} = 14. \\end{array} $$ The products are $$ \\begin{aligned} P_1 & = 1 \\cdot 6 = 6 \\\\ P_2 & = 4 \\cdot 2 = 8 \\\\ P_3 & = 6 \\cdot 12 = 72 \\\\ P_4 & = -2 \\cdot 5 = -10 \\\\ P_5 & = 6 \\cdot 8 = 48 \\\\ P_6 & = -2 \\cdot 6 = -12 \\\\ P_7 & = -6 \\cdot 14 = -84. \\end{aligned} $$ The four matrices are $$ \\begin{aligned} C_{11} & = 48 + (-10) - 8 + (-12) = 18 \\\\ C_{12} & = 6 + 8 = 14 \\\\ C_{21} & = 72 + (-10) = 62 \\\\ C_{22} & = 48 + 6 - 72 - (-84) = 66. \\end{aligned} $$ The result is $$ \\begin{pmatrix} 18 & 14 \\\\ 62 & 66 \\end{pmatrix} . $$","title":"4.2-1"},{"location":"Chap04/4.2/#42-2","text":"Write pseudocode for Strassen's algorithm. $$ A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix} , B = \\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{pmatrix} , C = \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{pmatrix} . \\tag{4.9} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 STRASSEN ( A , B ) n = A . rows let C be a new n \u00d7 n matrix if n == 1 c [ 1 , 1 ] = a [ 1 , 1 ] * b [ 1 , 1 ] else partition A and B in equations ( 4.9 ) let C [ 1 , 1 ], C [ 1 , 2 ], C [ 2 , 1 ], and C [ 2 , 2 ] be n / 2 \u00d7 n / 2 matrices create n / 2 \u00d7 n / 2 matrices S [ 1 ], S [ 2 ], ..., S [ 10 ] and P [ 1 ], P [ 2 ], ..., P [ 7 ] S [ 1 ] = B [ 1 , 2 ] - B [ 2 , 2 ] S [ 2 ] = A [ 1 , 1 ] + A [ 1 , 2 ] S [ 3 ] = A [ 1 , 2 ] + A [ 2 , 2 ] S [ 4 ] = B [ 2 , 1 ] - B [ 1 , 1 ] S [ 5 ] = A [ 1 , 1 ] + A [ 2 , 2 ] S [ 6 ] = B [ 1 , 1 ] + B [ 2 , 2 ] S [ 7 ] = A [ 1 , 2 ] - A [ 2 , 2 ] S [ 8 ] = B [ 2 , 1 ] + B [ 2 , 2 ] S [ 9 ] = A [ 1 , 1 ] - A [ 2 , 1 ] S [ 10 ] = B [ 1 , 1 ] + B [ 1 , 2 ] P [ 1 ] = STRASSEN ( A [ 1 , 1 ], S [ 1 ]) P [ 2 ] = STRASSEN ( S [ 2 ], B [ 2 , 2 ]) P [ 3 ] = STRASSEN ( S [ 3 ], B [ 1 , 1 ]) P [ 4 ] = STRASSEN ( A [ 2 , 2 ], S [ 4 ]) P [ 5 ] = STRASSEN ( S [ 5 ], S [ 6 ]) P [ 6 ] = STRASSEN ( S [ 7 ], S [ 8 ]) P [ 7 ] = STRASSEN ( S [ 9 ], S [ 10 ]) C [ 1 , 1 ] = P [ 5 ] + P [ 4 ] - P [ 2 ] + P [ 6 ] C [ 1 , 2 ] = P [ 1 ] + P [ 2 ] C [ 2 , 1 ] = P [ 3 ] + P [ 4 ] C [ 2 , 2 ] = P [ 5 ] + P [ 1 ] - P [ 3 ] - P [ 7 ] combine C [ 1 , 1 ], C [ 1 , 2 ], C [ 2 , 1 ], and C [ 2 , 2 ] into C return C","title":"4.2-2"},{"location":"Chap04/4.2/#42-3","text":"How would you modify Strassen's algorithm to multiply $n \\times n$ matrices in which $n$ is not an exact power of $2$? Show that the resulting algorithm runs in time $\\Theta(n^{\\lg7})$. We can just extend it to an $n \\times n$ matrix and pad it with zeroes. It's obviously $\\Theta(n^{\\lg7})$.","title":"4.2-3"},{"location":"Chap04/4.2/#42-4","text":"What is the largest $k$ such that if you can multiply $3 \\times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n \\times n$ matrices is time $o(n^{\\lg 7})$? What would the running time of this algorithm be? If you can multiply $3 \\times 3$ matrices using $k$ multiplications, then you can multiply $n \\times n$ matrices by recursively multiplying $n / 3 \\times n /3$ matrices, in time $T(n) = kT(n / 3) + \\Theta(n^2)$. Using the master method to solve this recurrence, consider the ratio of $n^{\\log_3 k}$ and $n^2$: If $\\log_3 k = 2$, case 2 applies and $T(n) = \\Theta(n^2\\lg n)$. In this case, $k = 9$ and $T(n) = o(n^{\\lg 7})$. If $\\log_3 k < 2$, case 3 applies and $T(n) = \\Theta(n^2)$. In this case, $k < 9$ and $T(n) = o(n^{\\lg 7})$. If $\\log_3 k > 2$, case 1 applies and $T(n) = \\Theta(n^{\\log_3 k})$. In this case, $k > 9$. $T(n) = o(n^{\\lg 7})$ when $\\log_3 k < \\lg 7$, i.e., when $k < 3^{\\lg 7} \\approx 21.85$. The largest such integer $k$ is $21$. Thus, $k = 21$ and the running time is $\\Theta(n^{\\log_3 k}) = \\Theta(n^{\\log_3 21} = O(n^{2.80})$ (since $\\log_3 21 \\approx 2.77$).","title":"4.2-4"},{"location":"Chap04/4.2/#42-5","text":"V. Pan has discovered a way of multiplying $68 \\times 68$ matrices using $132464$ multiplications, a way of multiplying $70 \\times 70$ matrices using $143640$ multiplications, and a way of multiplying $72 \\times 72$ matrices using $155424$ multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare to Strassen's algorithm? Using what we know from the last exercise, we need to pick the smallest of the following $$ \\begin{aligned} \\log_{68} 132464 & \\approx 2.795128 \\\\ \\log_{70} 143640 & \\approx 2.795122 \\\\ \\log_{72} 155424 & \\approx 2.795147. \\end{aligned} $$ The fastest one asymptotically is $70 \\times 70$ using $143640$.","title":"4.2-5"},{"location":"Chap04/4.2/#42-6","text":"How quickly can you multiply a $kn \\times n$ matrix by an $n \\times kn$ matrix, using Strassen's algorithm as a subroutine? Answer the same question with the order of the input matrices reversed. $(kn \\times n)(n \\times kn)$ produces a $kn \\times kn$ matrix. This produces $k^2$ multiplications of $n \\times n$ matrices. $(n \\times kn)(kn \\times n)$ produces an $n \\times n$ matrix. This produces $k$ multiplications and $k - 1$ additions.","title":"4.2-6"},{"location":"Chap04/4.2/#42-7","text":"Show how to multiply the complex numbers $a + bi$ and $c + di$ using only three multiplications of real numbers. The algorithm should take $a$, $b$, $c$ and $d$ as input and produce the real component $ac - bd$ and the imaginary component $ad + bc$ separately. The three matrices are $$ \\begin{aligned} A & = (a + b)(c + d) = ac + ad + bc + bd \\\\ B & = ac \\\\ C & = bd. \\end{aligned} $$ The result is $$(B - C) + (A - B - C)i.$$","title":"4.2-7"},{"location":"Chap04/4.3/","text":"4.3-1 Show that the solution of $T(n) = T(n - 1) + n$ is $O(n^2)$. We guess $T(n) \\le cn^2$ for some constant $c > 0$. We have $$ \\begin{aligned} T(n) & = T(n - 1) + n \\\\ & \\le c(n - 1)^2 + n \\\\ & = cn^2 - 2cn + c + n \\\\ & = cn^2 + c(1 - 2n) + n. \\end{aligned} $$ The last quantity is less than or equal to $cn^2$ if $c(1 - 2n) + n \\le 0$ or, equivalently, $c \\ge n / (2n - 1)$. This last condition holds for all $n \\ge 1$ and $c \\ge 1$. For the boundary condition, we set $T(1) = 1$, and so $T(1) = 1 \\le c \\cdot 1^2$. Thus, we can choose $n_0 = 1$ and $c = 1$. 4.3-2 Show that the solution of $T(n) = T(\\lceil n / 2 \\rceil) + 1$ is $O(\\lg n)$. We guess $T(n) \\le c\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c\\lg(\\lceil n / 2 \\rceil - 2) + 1 \\\\ & \\le c\\lg(n / 2 + 1 - 2) + 1 \\\\ & = c\\lg((n - 2) / 2) + 1 \\\\ & = c\\lg(n - 2) - c\\lg2 + 1 \\\\ & \\le c\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c \\ge 1$. 4.3-3 We saw that the solution of $T(n) = 2T(\\lfloor n / 2 \\rfloor) + n$ is $O(n\\lg n)$. Show that the solution of this recurrence is also $\\Omega(n\\lg n)$. Conclude that the solution is $\\Theta(n\\lg n)$. First we guess $T(n) \\le cn\\lg n$, $$ \\begin{aligned} T(n) & \\le 2c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + n \\\\ & \\le cn\\lg(n / 2) + n \\\\ & = cn\\lg n - cn\\lg 2 + n \\\\ & = cn\\lg n + (1 - c)n \\\\ & \\le cn\\lg n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. Next we guess $T(n) \\ge c(n + 2)\\lg(n + 2)$, $$ \\begin{aligned} T(n) & \\ge 2c(\\lfloor n / 2 \\rfloor + 2)(\\lg(\\lfloor n / 2 \\rfloor + 2) + n \\\\ & \\ge 2c(n / 2 - 1 + 2)(\\lg(n / 2 - 1 + 2) + n \\\\ & = 2c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + n \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2)\\lg 2 + n \\\\ & = c(n + 2)\\lg(n + 2) + (1 - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $n \\ge \\frac{2c}{1 - c}$, $0 \\le c < 1$. 4.3-4 Show that by making a different inductive hyptohesis, we can overcome the difficulty with the boundary condition $T(1) = 1$ for recurrence $\\text{(4.19)}$ without adjusting the boundary conditions for the inductive proof. We guess $T(n) \\le n\\lg n + n$, $$ \\begin{aligned} T(n) & \\le 2(c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + \\lfloor n / 2 \\rfloor) + n \\\\ & \\le 2c(n / 2)\\lg(n / 2) + 2(n / 2) + n \\\\ & = cn\\lg(n / 2) + 2n \\\\ & = cn\\lg n - cn\\lg{2} + 2n \\\\ & = cn\\lg n + (2 - c)n \\\\ & \\le cn\\lg n + n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. This time, the boundary condition is $$T(1) = 1 \\le cn\\lg n + n = 0 + 1 = 1.$$ 4.3-5 Show that $\\Theta(n\\lg n)$ is the solution to the \"exact\" recurrence $\\text{(4.3)}$ for merge sort. The recurrence is $$T(n) = T(\\lceil n / 2 \\rceil) + T(\\lfloor n / 2 \\rfloor) + \\Theta(n) \\tag{4.3}$$ To show $\\Theta$ bound, separately show $O$ and $\\Omega$ bounds. For $O(n\\lg n)$, we guess $T(n) \\le c(n - 2)\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c(\\lceil n / 2 \\rceil -2 )\\lg(\\lceil n / 2 \\rceil - 2) + c(\\lfloor n / 2 \\rfloor - 2)\\lg(\\lfloor n / 2 \\rfloor - 2) + dn \\\\ & \\le c(n / 2 + 1 -2 )\\lg(n / 2 + 1 - 2) + c(n / 2 - 2)\\lg(n / 2 - 2) + dn \\\\ & \\le c(n / 2 - 1 )\\lg(n / 2 - 1) + c(n / 2 - 1)\\lg(n / 2 - 1) + dn \\\\ & = c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg(n - 2) - c(n - 2) + dn \\\\ & = c(n - 2)\\lg(n - 2) + (d - c)n + 2c \\\\ & \\le c(n - 2)\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c > d$. For $\\Omega(n\\lg n)$, we guess $T(n) \\ge c(n + 2)\\lg (n + 2)$, $$ \\begin{aligned} T(n) & \\ge c(\\lceil n / 2 \\rceil +2 )\\lg(\\lceil n / 2 \\rceil + 2) + c(\\lfloor n / 2 \\rfloor + 2)\\lg(\\lfloor n / 2 \\rfloor + 2) + dn \\\\ & \\ge c(n / 2 + 2)\\lg(n / 2 + 2) + c(n / 2-1+2)\\lg(n / 2-1+2) + dn \\\\ & \\ge c(n / 2 + 1 )\\lg(n / 2 + 1) + c(n / 2 + 1)\\lg(n / 2 + 1) + dn \\\\ & \\ge c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2) + dn \\\\ & = c(n + 2)\\lg(n + 2) + (d - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $d > c$. 4.3-6 Show that the solution to $T(n) = 2T(\\lfloor n / 2 \\rfloor + 17) + n$ is $O(n\\lg n)$. We guess $T(n) \\le c(n - a)\\lg(n - a)$, $$ \\begin{aligned} T(n) & \\le 2c(\\lfloor n / 2 \\rfloor + 17 - a)\\lg(\\lfloor n / 2 \\rfloor + 17 - a) + n \\\\ & \\le 2c(n / 2 + 1 + 17 - a)\\lg(n / 2 + 1 + 17 - a) + n \\\\ & = c(n + 36 - 2a)\\lg\\frac{n + 36 - 2a}{2} + n \\\\ & = c(n + 36 - 2a)\\lg(n + 36 - 2a) - c(n + 36 - 2a) + n & (c > 1, n > n_0 = f(a))\\\\ & \\le c(n + 36 - 2a)\\lg(n + 36 - 2a) & (a \\ge 36) \\\\ & \\le c(n - a)\\lg(n - a). \\end{aligned} $$ 4.3-7 Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 3) + n$ is $T(n) = \\Theta(n^{\\log_3 4})$. Show that a substitution proof with the assumption $T(n) \\le cn^{\\log_3 4}$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. If we were to try a straight substitution proof, assuming that $T(n) \\le cn^{\\log_3 4}$, we would get stuck: $$ \\begin{aligned} T(n) & \\le 4(c(n / 3)^{\\log_3 4}) + n \\\\ & = 4c \\Big(\\frac{n^{\\log_3 4}}{4}\\Big) + n \\\\ & = cn^{\\log_3 4} + n, \\end{aligned} $$ which is greater than $cn^{\\log_3 4}$. Instead, we subtract off a lower-order term and assume that $T(n) \\le cn^{\\log_3 4} - dn$. Now we have $$ \\begin{aligned} T(n) & \\le 4(c(n / 3)^{\\log_3 4} - dn / 3) + n \\\\ & = 4c \\Big(\\frac{cn^{\\log_3 4}}{4} - \\frac{dn}{3}\\Big) + n \\\\ & = cn^{\\log_3 4} - \\frac{4}{3}dn + n, \\end{aligned} $$ which is less than or equal to $cn^{\\log_3 4} - dn$ if $d \\ge 3$. 4.3-8 Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 2) + n^2$ is $T(n) = \\Theta(n^2)$. Show that a substitution proof with the assumption $T(n) \\le cn^2$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. I think this question is wrong. According to the master theorem, the solution to the recurrence $T(n) = 4T(n / 2) + n^2$ should be $T(n) = \\Theta(n^2\\log n)$. 4.3-9 Solve the recurrence $T(n) = 3T(\\sqrt n) + \\log n$ by making a change of variables. Your solution should be asymptotically tight. Do not worry about whether values are integral. First, $$ \\begin{aligned} T(n) & = 3T(\\sqrt n) + \\lg n & \\text{ let } m = \\lg n \\\\ T(2^m) & = 3T(2^{m / 2}) + m \\\\ S(m) & = 3S(m / 2) + m. \\end{aligned} $$ Now we guess $S(m) \\le cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\le 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\le cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\le -2) \\\\ & \\le cm^{\\lg 3} + dm. \\end{aligned} $$ Then we guess $S(m) \\ge cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\ge 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\ge cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\ge -2) \\\\ & \\ge cm^{\\lg 3} + dm. \\end{aligned} $$ Thus, $$ \\begin{aligned} S(m) & = \\Theta(m^{\\lg 3}) \\\\ T(n) & = \\Theta(\\lg^{\\lg 3}{n}). \\end{aligned} $$","title":"4.3 The substitution method for solving recurrences"},{"location":"Chap04/4.3/#43-1","text":"Show that the solution of $T(n) = T(n - 1) + n$ is $O(n^2)$. We guess $T(n) \\le cn^2$ for some constant $c > 0$. We have $$ \\begin{aligned} T(n) & = T(n - 1) + n \\\\ & \\le c(n - 1)^2 + n \\\\ & = cn^2 - 2cn + c + n \\\\ & = cn^2 + c(1 - 2n) + n. \\end{aligned} $$ The last quantity is less than or equal to $cn^2$ if $c(1 - 2n) + n \\le 0$ or, equivalently, $c \\ge n / (2n - 1)$. This last condition holds for all $n \\ge 1$ and $c \\ge 1$. For the boundary condition, we set $T(1) = 1$, and so $T(1) = 1 \\le c \\cdot 1^2$. Thus, we can choose $n_0 = 1$ and $c = 1$.","title":"4.3-1"},{"location":"Chap04/4.3/#43-2","text":"Show that the solution of $T(n) = T(\\lceil n / 2 \\rceil) + 1$ is $O(\\lg n)$. We guess $T(n) \\le c\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c\\lg(\\lceil n / 2 \\rceil - 2) + 1 \\\\ & \\le c\\lg(n / 2 + 1 - 2) + 1 \\\\ & = c\\lg((n - 2) / 2) + 1 \\\\ & = c\\lg(n - 2) - c\\lg2 + 1 \\\\ & \\le c\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c \\ge 1$.","title":"4.3-2"},{"location":"Chap04/4.3/#43-3","text":"We saw that the solution of $T(n) = 2T(\\lfloor n / 2 \\rfloor) + n$ is $O(n\\lg n)$. Show that the solution of this recurrence is also $\\Omega(n\\lg n)$. Conclude that the solution is $\\Theta(n\\lg n)$. First we guess $T(n) \\le cn\\lg n$, $$ \\begin{aligned} T(n) & \\le 2c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + n \\\\ & \\le cn\\lg(n / 2) + n \\\\ & = cn\\lg n - cn\\lg 2 + n \\\\ & = cn\\lg n + (1 - c)n \\\\ & \\le cn\\lg n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. Next we guess $T(n) \\ge c(n + 2)\\lg(n + 2)$, $$ \\begin{aligned} T(n) & \\ge 2c(\\lfloor n / 2 \\rfloor + 2)(\\lg(\\lfloor n / 2 \\rfloor + 2) + n \\\\ & \\ge 2c(n / 2 - 1 + 2)(\\lg(n / 2 - 1 + 2) + n \\\\ & = 2c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + n \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2)\\lg 2 + n \\\\ & = c(n + 2)\\lg(n + 2) + (1 - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $n \\ge \\frac{2c}{1 - c}$, $0 \\le c < 1$.","title":"4.3-3"},{"location":"Chap04/4.3/#43-4","text":"Show that by making a different inductive hyptohesis, we can overcome the difficulty with the boundary condition $T(1) = 1$ for recurrence $\\text{(4.19)}$ without adjusting the boundary conditions for the inductive proof. We guess $T(n) \\le n\\lg n + n$, $$ \\begin{aligned} T(n) & \\le 2(c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + \\lfloor n / 2 \\rfloor) + n \\\\ & \\le 2c(n / 2)\\lg(n / 2) + 2(n / 2) + n \\\\ & = cn\\lg(n / 2) + 2n \\\\ & = cn\\lg n - cn\\lg{2} + 2n \\\\ & = cn\\lg n + (2 - c)n \\\\ & \\le cn\\lg n + n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. This time, the boundary condition is $$T(1) = 1 \\le cn\\lg n + n = 0 + 1 = 1.$$","title":"4.3-4"},{"location":"Chap04/4.3/#43-5","text":"Show that $\\Theta(n\\lg n)$ is the solution to the \"exact\" recurrence $\\text{(4.3)}$ for merge sort. The recurrence is $$T(n) = T(\\lceil n / 2 \\rceil) + T(\\lfloor n / 2 \\rfloor) + \\Theta(n) \\tag{4.3}$$ To show $\\Theta$ bound, separately show $O$ and $\\Omega$ bounds. For $O(n\\lg n)$, we guess $T(n) \\le c(n - 2)\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c(\\lceil n / 2 \\rceil -2 )\\lg(\\lceil n / 2 \\rceil - 2) + c(\\lfloor n / 2 \\rfloor - 2)\\lg(\\lfloor n / 2 \\rfloor - 2) + dn \\\\ & \\le c(n / 2 + 1 -2 )\\lg(n / 2 + 1 - 2) + c(n / 2 - 2)\\lg(n / 2 - 2) + dn \\\\ & \\le c(n / 2 - 1 )\\lg(n / 2 - 1) + c(n / 2 - 1)\\lg(n / 2 - 1) + dn \\\\ & = c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg(n - 2) - c(n - 2) + dn \\\\ & = c(n - 2)\\lg(n - 2) + (d - c)n + 2c \\\\ & \\le c(n - 2)\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c > d$. For $\\Omega(n\\lg n)$, we guess $T(n) \\ge c(n + 2)\\lg (n + 2)$, $$ \\begin{aligned} T(n) & \\ge c(\\lceil n / 2 \\rceil +2 )\\lg(\\lceil n / 2 \\rceil + 2) + c(\\lfloor n / 2 \\rfloor + 2)\\lg(\\lfloor n / 2 \\rfloor + 2) + dn \\\\ & \\ge c(n / 2 + 2)\\lg(n / 2 + 2) + c(n / 2-1+2)\\lg(n / 2-1+2) + dn \\\\ & \\ge c(n / 2 + 1 )\\lg(n / 2 + 1) + c(n / 2 + 1)\\lg(n / 2 + 1) + dn \\\\ & \\ge c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2) + dn \\\\ & = c(n + 2)\\lg(n + 2) + (d - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $d > c$.","title":"4.3-5"},{"location":"Chap04/4.3/#43-6","text":"Show that the solution to $T(n) = 2T(\\lfloor n / 2 \\rfloor + 17) + n$ is $O(n\\lg n)$. We guess $T(n) \\le c(n - a)\\lg(n - a)$, $$ \\begin{aligned} T(n) & \\le 2c(\\lfloor n / 2 \\rfloor + 17 - a)\\lg(\\lfloor n / 2 \\rfloor + 17 - a) + n \\\\ & \\le 2c(n / 2 + 1 + 17 - a)\\lg(n / 2 + 1 + 17 - a) + n \\\\ & = c(n + 36 - 2a)\\lg\\frac{n + 36 - 2a}{2} + n \\\\ & = c(n + 36 - 2a)\\lg(n + 36 - 2a) - c(n + 36 - 2a) + n & (c > 1, n > n_0 = f(a))\\\\ & \\le c(n + 36 - 2a)\\lg(n + 36 - 2a) & (a \\ge 36) \\\\ & \\le c(n - a)\\lg(n - a). \\end{aligned} $$","title":"4.3-6"},{"location":"Chap04/4.3/#43-7","text":"Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 3) + n$ is $T(n) = \\Theta(n^{\\log_3 4})$. Show that a substitution proof with the assumption $T(n) \\le cn^{\\log_3 4}$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. If we were to try a straight substitution proof, assuming that $T(n) \\le cn^{\\log_3 4}$, we would get stuck: $$ \\begin{aligned} T(n) & \\le 4(c(n / 3)^{\\log_3 4}) + n \\\\ & = 4c \\Big(\\frac{n^{\\log_3 4}}{4}\\Big) + n \\\\ & = cn^{\\log_3 4} + n, \\end{aligned} $$ which is greater than $cn^{\\log_3 4}$. Instead, we subtract off a lower-order term and assume that $T(n) \\le cn^{\\log_3 4} - dn$. Now we have $$ \\begin{aligned} T(n) & \\le 4(c(n / 3)^{\\log_3 4} - dn / 3) + n \\\\ & = 4c \\Big(\\frac{cn^{\\log_3 4}}{4} - \\frac{dn}{3}\\Big) + n \\\\ & = cn^{\\log_3 4} - \\frac{4}{3}dn + n, \\end{aligned} $$ which is less than or equal to $cn^{\\log_3 4} - dn$ if $d \\ge 3$.","title":"4.3-7"},{"location":"Chap04/4.3/#43-8","text":"Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 2) + n^2$ is $T(n) = \\Theta(n^2)$. Show that a substitution proof with the assumption $T(n) \\le cn^2$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. I think this question is wrong. According to the master theorem, the solution to the recurrence $T(n) = 4T(n / 2) + n^2$ should be $T(n) = \\Theta(n^2\\log n)$.","title":"4.3-8"},{"location":"Chap04/4.3/#43-9","text":"Solve the recurrence $T(n) = 3T(\\sqrt n) + \\log n$ by making a change of variables. Your solution should be asymptotically tight. Do not worry about whether values are integral. First, $$ \\begin{aligned} T(n) & = 3T(\\sqrt n) + \\lg n & \\text{ let } m = \\lg n \\\\ T(2^m) & = 3T(2^{m / 2}) + m \\\\ S(m) & = 3S(m / 2) + m. \\end{aligned} $$ Now we guess $S(m) \\le cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\le 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\le cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\le -2) \\\\ & \\le cm^{\\lg 3} + dm. \\end{aligned} $$ Then we guess $S(m) \\ge cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\ge 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\ge cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\ge -2) \\\\ & \\ge cm^{\\lg 3} + dm. \\end{aligned} $$ Thus, $$ \\begin{aligned} S(m) & = \\Theta(m^{\\lg 3}) \\\\ T(n) & = \\Theta(\\lg^{\\lg 3}{n}). \\end{aligned} $$","title":"4.3-9"},{"location":"Chap04/4.4/","text":"4.4-1 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 3T(\\lfloor n / 2 \\rfloor) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $3^{\\lg n} = n^{\\lg 3}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $3^i(n / 2^i) = (3 / 2)^i n$. $$ \\begin{aligned} T(n) & = n + \\frac{3}{2}n + \\Big(\\frac{3}{2}\\Big)^2 n + \\cdots + \\Big(\\frac{3}{2}\\Big)^{\\lg n - 1} n + \\Theta(n^{\\lg 3}) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{3}{2}\\Big)^i n + \\Theta(n^{\\lg 3}) \\\\ & = \\frac{(3 / 2)^{\\lg n} - 1}{(3 / 2) - 1}n + \\Theta(n^{\\lg 3}) \\\\ & = 2[(3 / 2)^{\\lg n} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg(3 / 2)} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - \\lg 2} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - 1 + 1} - n] + \\Theta(n^{\\lg 3}) \\\\ & = O(n^{\\lg 3}). \\end{aligned} $$ We guess $T(n) \\le cn^{\\lg 3} - dn$, $$ \\begin{aligned} T(n) & = 3T(\\lfloor n / 2 \\rfloor) + n \\\\ & \\le 3 \\cdot (c(n / 2)^{\\lg 3} - d(n / 2)) + n \\\\ & = (3 / 2^{\\lg 3})cn^{\\lg 3} - (3d / 2)n + n \\\\ & = cn^{\\lg 3} + (1 - 3d / 2)n, \\end{aligned} $$ where the last step holds for $d \\ge 2$. 4.4-2 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n / 2) + n^2$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $1^{\\lg n} = 1$ leaf. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg{n - 1}$, is $1^i (n / 2^i)^2 = (1 / 4)^i n^2$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & < \\sum_{i = 0}^\\infty \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & = \\frac{1}{1 - (1 / 4)} n^2 + \\Theta(1) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n / 2)^2 + n^2 \\\\ & = cn^2 / 4 + n^2 \\\\ & = (c / 4 + 1)n^2 \\\\ & \\le cn^2, \\end{aligned} $$ where the last step holds for $c \\ge 4 / 3$. 4.4-3 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 4T(n / 2 + 2) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(n / 2^i + 2) = 2^i n + 2 \\cdot 4^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} (2^i n + 2 \\cdot 4^i) + \\Theta(n^2) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} 2^i n + \\sum_{i = 0}^{\\lg n - 1} 2 \\cdot 4^i + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}n + 2 \\cdot \\frac{4^{\\lg n} - 1}{4 - 1} + \\Theta(n^2) \\\\ & = (2^{\\lg n} - 1)n + \\frac{2}{3} (4^{\\lg n} - 1) + \\Theta(n^2) \\\\ & = (n - 1)n + \\frac{2}{3}(n^2 - 1) + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le c(n^2 - dn)$, $$ \\begin{aligned} T(n) & = 4T(n / 2 + 2) + n \\\\ & \\le 4c[(n / 2 + 2)^2 - d(n / 2 + 2)] + n \\\\ & = 4c(n^2 / 4 + 2n + 4 - dn / 2 - 2d) + n \\\\ & = cn^2 + 8cn + 16c - 2cdn - 8cd + n \\\\ & = cn^2 - cdn + 8cn + 16c - cdn - 8cd + n \\\\ & = c(n^2 - dn) - (cd - 8c - 1)n - (d - 2) \\cdot 8c \\\\ & \\le c(n^2 - dn), \\end{aligned} $$ where the last step holds for $cd - 8c - 1 \\ge 0$. 4.4-4 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 2T(n - 1) + 1$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n - i$. Thus, the tree has $n$ levels and $2^{n - 1}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n - 1$, is $2^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n - 1} 2^i \\\\ & = \\frac{2^{n - 1} - 1}{2 - 1} \\\\ & = 2^{n - 1} - 1 \\\\ & = \\Theta(2^n). \\end{aligned} $$ We guess $T(n) \\le c2^n + n$, $$ \\begin{aligned} T(n) & \\le 2 \\cdot c2^{n - 1} + (n - 1) + 1 \\\\ & = c2^n + n \\\\ & = O(2^n). \\end{aligned} $$ 4.4-5 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n - 1) + T(n / 2) + n$. Use the substitution method to verify your answer. This is a curious one. The tree makes it look like it is exponential in the worst case. The tree is not full (not a complete binary tree of height $n$), but it is not polynomial either. It's easy to show $O(2^n)$ and $\\Omega(n^2)$. To justify that this is a pretty tight upper bound, we'll show that we can't have any other choice. If we have that $T(n) \\le cn^k$, when we substitue into the recurrence, the new coefficient for $n^k$ can be as high as $c(1 + \\frac{1}{2^k})$ which is bigger than $c$ regardless of how we choose the value $c$. We guess $T(n) \\le c2^n - 4n$, $$ \\begin{aligned} T(n) & \\le c2^{n - 1} - 4(n - 1) + c2^{n / 2} - 4n / 2 + n \\\\ & = c(2^{n - 1} + 2^{n / 2}) - 5n + 4 & (n \\ge 1 / 4) \\\\ & \\le c(2^{n - 1} + 2^{n / 2}) - 4n & (n \\ge 2)\\\\ & = c(2^{n - 1} + 2^{n - 1}) - 4n \\\\ & \\le c2^n - 4n \\\\ & = O(2^n). \\end{aligned} $$ We guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - 1)^2 + c(n / 2)^2 + n \\\\ & = cn^2 - 2cn + c + cn^2 / 4 + n \\\\ & = (5 / 4)cn^2 + (1 - 2c)n + c \\\\ & \\ge cn^2 + (1 - 2c)n + c & (c \\le 1 / 2) \\\\ & \\ge cn^2 \\\\ & = O(n^2). \\end{aligned} $$ 4.4-6 Argue that the solution to the recurrence $T(n) = T(n / 3) + T(2n / 3) + cn$, where $c$ is a constant, is $\\Omega(n\\lg n)$ by appealing to the recursion tree. The shortest path from the root to a leaf in the recursion tree is $n \\to (1 / 3)n \\to (1 / 3)^2 n \\to \\cdots \\to 1$. Since $(1 / 3)^k n = 1$ when $k = \\log_3 n$, the height of the part of the tree in which every node has two children is $log_3 n$. Since the values at each of these levels of the tree add up to $n$, the solution to the recurrence is at least $n \\log_3 n$ = $\\Omega(n\\lg n)$. 4.4-7 Draw the recursion tree for $T(n) = 4T(\\lfloor n / 2 \\rfloor) + cn$, where $c$ is a constant, and provide a tight asymptotic bound on its solution. Verify your answer with the substitution method. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^{\\lg 4} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(cn / 2^i) = 2^icn$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} 2^icn + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}cn + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le cn^2 + 2cn$, $$ \\begin{aligned} T(n) & \\le 4c(n / 2)^2 + 2c(n / 2) + cn \\\\ & = cn^2 + 2cn. \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge cn^2 + 2cn$, $$ \\begin{aligned} T(n) & \\ge 4c(n / 2)^2 + 2c(n / 2) + cn \\\\ & = cn^2 + 2cn. \\end{aligned} $$ 4.4-8 Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(n - a) + T(a) + cn$, where $a \\ge 1$ and $c > 0$ are constants. The tree has $n / a + 1$ levels. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n / a - 1$, is $c(n - ia)$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n / a} c(n - ia) + (n / a)ca \\\\ & = \\sum_{i = 0}^{n / a} cn - \\sum_{i = 0}^{n / a} cia + (n / a)ca \\\\ & = cn^2/a - \\Theta(n) + \\Theta(n) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n - a)^2 + ca + cn \\\\ & \\le cn^2 - 2can + ca + cn \\\\ & \\le cn^2 - c(2an - a - n) & (a > 1 / 2, n > 2a) \\\\ & \\le cn^2 - cn \\\\ & \\le cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - a)^2 + ca + cn \\\\ & \\ge cn^2 - 2acn + ca + cn \\\\ & \\ge cn^2 - c(2an - a - n) & (a < 1 / 2, n > 2a) \\\\ & \\ge cn^2 + cn \\\\ & \\ge cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$ 4.4-9 Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(\\alpha n) + T((1 - \\alpha)n) + cn$, where $\\alpha$ is a constant in the range $0 < \\alpha < 1$, and $c > 0$ is also a constant. $$T(n) = T(\\alpha n) + T((1 - \\alpha)n) + cn$$ We saw the solution to the recurrence $T(n) = T(n / 3) + T(2n / 3) + cn$ in the text. This recurrence can be similarly solved. Without loss of generality, let $\\alpha \\ge 1 - \\alpha$, so that $0 < 1 - \\alpha \\le 1 / 2$ and $1 / 2 \\le \\alpha < 1$. The recursion tree is full for $\\log_{1 / (1 - \\alpha)} n$ levels, each contributing $cn$, so we guess $$\\Omega(n\\log_{1 / (1 - \\alpha)} n) = \\Omega(n\\lg n).$$ It has $\\log_{1 / \\alpha} n$ levels, each contributing $\\le cn$, so we guess $$O(n\\log_{1 / \\alpha} n) = O(n\\lg n).$$ Now we show that $T(n) = \\Theta(n\\lg n)$ by substitution. To prove the upper bound, we need to show that $T(n) \\le dn\\lg n$ for a suitable constant $d > 0$. $$ \\begin{aligned} T(n) & = T(\\alpha n) + T((1 - \\alpha)n) + cn \\\\ & \\le d\\alpha n\\lg(\\alpha n) + d(1 - \\alpha)n\\lg((1 - \\alpha)n) + cn \\\\ & = d\\alpha n\\lg\\alpha + d\\alpha n\\lg n + d(1 - \\alpha)n\\lg(1 - \\alpha) + d(1 - \\alpha)n\\lg n + cn \\\\ & = dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\\\ & \\le dn\\lg n, \\end{aligned} $$ if $dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\le 0$. This condition is equivalent $$d(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) \\le -c.$$ Since $1 / 2 \\le \\alpha < 1$ and $0 < 1 - \\alpha \\le 1 / 2$, we have that $\\lg\\alpha < 0$ and $\\lg(1 - \\alpha ) < 0$. Thus, $\\alpha \\lg\\alpha + (1 - \\alpha ) \\lg(1 - \\alpha ) < 0$, so that when we multiply both sides of the inequality by this factor, we need to reverse the inequality: $$d \\ge \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}$$ or $$d \\ge \\frac{c}{-\\alpha\\lg\\alpha + -(1 - \\alpha)\\lg(1 - \\alpha)}.$$ The fraction on the right-hand side is a positive constant, and so it suffices to pick any value of $d$ that is greater than or equal to this fraction. To prove the lower bound, we need to show that $T(n) \\ge dn\\lg n$ for a suitable constant $d > 0$. We can use the same proof as for the upper bound, substituting $\\ge$ for $\\le$, and we get the requirement that $$0 < d \\le \\frac{c}{-\\alpha \\lg\\alpha - (1 - \\alpha ) \\lg(1 - \\alpha )}.$$ Therefore, $T(n) = \\Theta(n\\lg n)$.","title":"4.4 The recursion-tree method for solving recurrences"},{"location":"Chap04/4.4/#44-1","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 3T(\\lfloor n / 2 \\rfloor) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $3^{\\lg n} = n^{\\lg 3}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $3^i(n / 2^i) = (3 / 2)^i n$. $$ \\begin{aligned} T(n) & = n + \\frac{3}{2}n + \\Big(\\frac{3}{2}\\Big)^2 n + \\cdots + \\Big(\\frac{3}{2}\\Big)^{\\lg n - 1} n + \\Theta(n^{\\lg 3}) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{3}{2}\\Big)^i n + \\Theta(n^{\\lg 3}) \\\\ & = \\frac{(3 / 2)^{\\lg n} - 1}{(3 / 2) - 1}n + \\Theta(n^{\\lg 3}) \\\\ & = 2[(3 / 2)^{\\lg n} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg(3 / 2)} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - \\lg 2} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - 1 + 1} - n] + \\Theta(n^{\\lg 3}) \\\\ & = O(n^{\\lg 3}). \\end{aligned} $$ We guess $T(n) \\le cn^{\\lg 3} - dn$, $$ \\begin{aligned} T(n) & = 3T(\\lfloor n / 2 \\rfloor) + n \\\\ & \\le 3 \\cdot (c(n / 2)^{\\lg 3} - d(n / 2)) + n \\\\ & = (3 / 2^{\\lg 3})cn^{\\lg 3} - (3d / 2)n + n \\\\ & = cn^{\\lg 3} + (1 - 3d / 2)n, \\end{aligned} $$ where the last step holds for $d \\ge 2$.","title":"4.4-1"},{"location":"Chap04/4.4/#44-2","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n / 2) + n^2$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $1^{\\lg n} = 1$ leaf. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg{n - 1}$, is $1^i (n / 2^i)^2 = (1 / 4)^i n^2$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & < \\sum_{i = 0}^\\infty \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & = \\frac{1}{1 - (1 / 4)} n^2 + \\Theta(1) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n / 2)^2 + n^2 \\\\ & = cn^2 / 4 + n^2 \\\\ & = (c / 4 + 1)n^2 \\\\ & \\le cn^2, \\end{aligned} $$ where the last step holds for $c \\ge 4 / 3$.","title":"4.4-2"},{"location":"Chap04/4.4/#44-3","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 4T(n / 2 + 2) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(n / 2^i + 2) = 2^i n + 2 \\cdot 4^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} (2^i n + 2 \\cdot 4^i) + \\Theta(n^2) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} 2^i n + \\sum_{i = 0}^{\\lg n - 1} 2 \\cdot 4^i + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}n + 2 \\cdot \\frac{4^{\\lg n} - 1}{4 - 1} + \\Theta(n^2) \\\\ & = (2^{\\lg n} - 1)n + \\frac{2}{3} (4^{\\lg n} - 1) + \\Theta(n^2) \\\\ & = (n - 1)n + \\frac{2}{3}(n^2 - 1) + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le c(n^2 - dn)$, $$ \\begin{aligned} T(n) & = 4T(n / 2 + 2) + n \\\\ & \\le 4c[(n / 2 + 2)^2 - d(n / 2 + 2)] + n \\\\ & = 4c(n^2 / 4 + 2n + 4 - dn / 2 - 2d) + n \\\\ & = cn^2 + 8cn + 16c - 2cdn - 8cd + n \\\\ & = cn^2 - cdn + 8cn + 16c - cdn - 8cd + n \\\\ & = c(n^2 - dn) - (cd - 8c - 1)n - (d - 2) \\cdot 8c \\\\ & \\le c(n^2 - dn), \\end{aligned} $$ where the last step holds for $cd - 8c - 1 \\ge 0$.","title":"4.4-3"},{"location":"Chap04/4.4/#44-4","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 2T(n - 1) + 1$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n - i$. Thus, the tree has $n$ levels and $2^{n - 1}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n - 1$, is $2^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n - 1} 2^i \\\\ & = \\frac{2^{n - 1} - 1}{2 - 1} \\\\ & = 2^{n - 1} - 1 \\\\ & = \\Theta(2^n). \\end{aligned} $$ We guess $T(n) \\le c2^n + n$, $$ \\begin{aligned} T(n) & \\le 2 \\cdot c2^{n - 1} + (n - 1) + 1 \\\\ & = c2^n + n \\\\ & = O(2^n). \\end{aligned} $$","title":"4.4-4"},{"location":"Chap04/4.4/#44-5","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n - 1) + T(n / 2) + n$. Use the substitution method to verify your answer. This is a curious one. The tree makes it look like it is exponential in the worst case. The tree is not full (not a complete binary tree of height $n$), but it is not polynomial either. It's easy to show $O(2^n)$ and $\\Omega(n^2)$. To justify that this is a pretty tight upper bound, we'll show that we can't have any other choice. If we have that $T(n) \\le cn^k$, when we substitue into the recurrence, the new coefficient for $n^k$ can be as high as $c(1 + \\frac{1}{2^k})$ which is bigger than $c$ regardless of how we choose the value $c$. We guess $T(n) \\le c2^n - 4n$, $$ \\begin{aligned} T(n) & \\le c2^{n - 1} - 4(n - 1) + c2^{n / 2} - 4n / 2 + n \\\\ & = c(2^{n - 1} + 2^{n / 2}) - 5n + 4 & (n \\ge 1 / 4) \\\\ & \\le c(2^{n - 1} + 2^{n / 2}) - 4n & (n \\ge 2)\\\\ & = c(2^{n - 1} + 2^{n - 1}) - 4n \\\\ & \\le c2^n - 4n \\\\ & = O(2^n). \\end{aligned} $$ We guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - 1)^2 + c(n / 2)^2 + n \\\\ & = cn^2 - 2cn + c + cn^2 / 4 + n \\\\ & = (5 / 4)cn^2 + (1 - 2c)n + c \\\\ & \\ge cn^2 + (1 - 2c)n + c & (c \\le 1 / 2) \\\\ & \\ge cn^2 \\\\ & = O(n^2). \\end{aligned} $$","title":"4.4-5"},{"location":"Chap04/4.4/#44-6","text":"Argue that the solution to the recurrence $T(n) = T(n / 3) + T(2n / 3) + cn$, where $c$ is a constant, is $\\Omega(n\\lg n)$ by appealing to the recursion tree. The shortest path from the root to a leaf in the recursion tree is $n \\to (1 / 3)n \\to (1 / 3)^2 n \\to \\cdots \\to 1$. Since $(1 / 3)^k n = 1$ when $k = \\log_3 n$, the height of the part of the tree in which every node has two children is $log_3 n$. Since the values at each of these levels of the tree add up to $n$, the solution to the recurrence is at least $n \\log_3 n$ = $\\Omega(n\\lg n)$.","title":"4.4-6"},{"location":"Chap04/4.4/#44-7","text":"Draw the recursion tree for $T(n) = 4T(\\lfloor n / 2 \\rfloor) + cn$, where $c$ is a constant, and provide a tight asymptotic bound on its solution. Verify your answer with the substitution method. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^{\\lg 4} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(cn / 2^i) = 2^icn$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} 2^icn + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}cn + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le cn^2 + 2cn$, $$ \\begin{aligned} T(n) & \\le 4c(n / 2)^2 + 2c(n / 2) + cn \\\\ & = cn^2 + 2cn. \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge cn^2 + 2cn$, $$ \\begin{aligned} T(n) & \\ge 4c(n / 2)^2 + 2c(n / 2) + cn \\\\ & = cn^2 + 2cn. \\end{aligned} $$","title":"4.4-7"},{"location":"Chap04/4.4/#44-8","text":"Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(n - a) + T(a) + cn$, where $a \\ge 1$ and $c > 0$ are constants. The tree has $n / a + 1$ levels. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n / a - 1$, is $c(n - ia)$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n / a} c(n - ia) + (n / a)ca \\\\ & = \\sum_{i = 0}^{n / a} cn - \\sum_{i = 0}^{n / a} cia + (n / a)ca \\\\ & = cn^2/a - \\Theta(n) + \\Theta(n) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n - a)^2 + ca + cn \\\\ & \\le cn^2 - 2can + ca + cn \\\\ & \\le cn^2 - c(2an - a - n) & (a > 1 / 2, n > 2a) \\\\ & \\le cn^2 - cn \\\\ & \\le cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - a)^2 + ca + cn \\\\ & \\ge cn^2 - 2acn + ca + cn \\\\ & \\ge cn^2 - c(2an - a - n) & (a < 1 / 2, n > 2a) \\\\ & \\ge cn^2 + cn \\\\ & \\ge cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$","title":"4.4-8"},{"location":"Chap04/4.4/#44-9","text":"Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(\\alpha n) + T((1 - \\alpha)n) + cn$, where $\\alpha$ is a constant in the range $0 < \\alpha < 1$, and $c > 0$ is also a constant. $$T(n) = T(\\alpha n) + T((1 - \\alpha)n) + cn$$ We saw the solution to the recurrence $T(n) = T(n / 3) + T(2n / 3) + cn$ in the text. This recurrence can be similarly solved. Without loss of generality, let $\\alpha \\ge 1 - \\alpha$, so that $0 < 1 - \\alpha \\le 1 / 2$ and $1 / 2 \\le \\alpha < 1$. The recursion tree is full for $\\log_{1 / (1 - \\alpha)} n$ levels, each contributing $cn$, so we guess $$\\Omega(n\\log_{1 / (1 - \\alpha)} n) = \\Omega(n\\lg n).$$ It has $\\log_{1 / \\alpha} n$ levels, each contributing $\\le cn$, so we guess $$O(n\\log_{1 / \\alpha} n) = O(n\\lg n).$$ Now we show that $T(n) = \\Theta(n\\lg n)$ by substitution. To prove the upper bound, we need to show that $T(n) \\le dn\\lg n$ for a suitable constant $d > 0$. $$ \\begin{aligned} T(n) & = T(\\alpha n) + T((1 - \\alpha)n) + cn \\\\ & \\le d\\alpha n\\lg(\\alpha n) + d(1 - \\alpha)n\\lg((1 - \\alpha)n) + cn \\\\ & = d\\alpha n\\lg\\alpha + d\\alpha n\\lg n + d(1 - \\alpha)n\\lg(1 - \\alpha) + d(1 - \\alpha)n\\lg n + cn \\\\ & = dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\\\ & \\le dn\\lg n, \\end{aligned} $$ if $dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\le 0$. This condition is equivalent $$d(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) \\le -c.$$ Since $1 / 2 \\le \\alpha < 1$ and $0 < 1 - \\alpha \\le 1 / 2$, we have that $\\lg\\alpha < 0$ and $\\lg(1 - \\alpha ) < 0$. Thus, $\\alpha \\lg\\alpha + (1 - \\alpha ) \\lg(1 - \\alpha ) < 0$, so that when we multiply both sides of the inequality by this factor, we need to reverse the inequality: $$d \\ge \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}$$ or $$d \\ge \\frac{c}{-\\alpha\\lg\\alpha + -(1 - \\alpha)\\lg(1 - \\alpha)}.$$ The fraction on the right-hand side is a positive constant, and so it suffices to pick any value of $d$ that is greater than or equal to this fraction. To prove the lower bound, we need to show that $T(n) \\ge dn\\lg n$ for a suitable constant $d > 0$. We can use the same proof as for the upper bound, substituting $\\ge$ for $\\le$, and we get the requirement that $$0 < d \\le \\frac{c}{-\\alpha \\lg\\alpha - (1 - \\alpha ) \\lg(1 - \\alpha )}.$$ Therefore, $T(n) = \\Theta(n\\lg n)$.","title":"4.4-9"},{"location":"Chap04/4.5/","text":"4.5-1 Use the master method to give tight asymptotic bounds for the following recurrences: a. $T(n) = 2T(n / 4) + 1$. b. $T(n) = 2T(n / 4) + \\sqrt n$. c. $T(n) = 2T(n / 4) + n$. d. $T(n) = 2T(n / 4) + n^2$. a. $\\Theta(n^{\\log_4 2}) = \\Theta(\\sqrt n)$. b. $\\Theta(n^{\\log_4 2}\\lg n) = \\Theta(\\sqrt n\\lg n)$. c. $\\Theta(n)$. d. $\\Theta(n^2)$. 4.5-2 Professor Caesar wishes to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen's algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into pieces of size $n / 4 \\times n / 4$, and the divide and combine steps together will take $\\Theta(n^2)$ time. He needs to determine how many subproblems his algorithm has to create in order to beat Strassen's algorithm. If his algorithm creates $a$ subproblems, then the recurrence for the running time $T(n)$ becomes $T(n) = aT(n / 4) + \\Theta(n^2)$. What is the largest integer value of $a$ for which Professor Caesar's algorithm would be asymptotically faster than Strassen's algorithm? We need to find the largest integer $a$ such that $\\log_4 a < \\lg 7$. The answer is $a = 48$. 4.5.3 Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n / 2) + \\Theta(1)$ is $T(n) = \\Theta(\\lg n)$. (See exercise 2.3-5 for a description of binary search.) $$ \\begin{aligned} a & = 1, b = 2, \\\\ f(n) & = \\Theta(n^{\\lg 1}) = \\Theta(1), \\\\ T(n) & = \\Theta(\\lg n). \\end{aligned} $$ 4.5-4 Can the master method be applied to the recurrence $T(n) = 4T(n / 2) + n^2\\lg n$? Why or why not? Give an asymptotic upper bound for this recurrence. With $a = 4$, $b = 2$, we have $f(n) = n^2\\lg n \\ne O(n^{2 - \\epsilon}) \\ne \\Omega(n^{2 - \\epsilon})$, so we cannot apply the master method. We guess $T(n) \\le cn^2\\lg^2 n$, $$ \\begin{aligned} T(n) & \\le 4T(n / 2) + n^2\\lg n \\\\ & = 4c(n / 2)^2\\lg^2(n / 2) + n^2\\lg n \\\\ & = cn^2\\lg(n / 2)\\lg n - cn^2\\lg(n / 2)\\lg 2 + n^2\\lg n \\\\ & = cn^2\\lg^2 n - cn^2\\lg n\\lg 2 - cn^2\\lg(n / 2) + n^2\\lg n \\\\ & = cn^2\\lg^2 n + (1 - c)n^2\\lg n - cn^2\\lg(n / 2) & (c > 1) \\\\ & \\le cn^2\\lg^2 n - cn^2\\lg(n / 2) \\\\ & \\le cn^2\\lg^2 n. \\end{aligned} $$ Exercise 4.6-2 is the general case for this. 4.5-5 $\\star$ Consider the regularity condition $af(n / b) \\ge cf(n)$ for some constant $c < 1$, which is part of case 3 of the master theorem. Give an example of constants $a \\ge 1$ and $b > 1$ and a function $f(n)$ that satisfies all the conditions in case 3 of the master theorem, except the regularity condition. $a = 1$, $b = 2$ and $f(n) = n(2 - \\cos n)$. If we try to prove it, $$ \\begin{aligned} \\frac{n}{2}(2 - \\cos\\frac{n}{2}) & < cn \\\\ \\frac{1 - cos(n / 2)}{2} & < c \\\\ 1 - \\frac{cos(n / 2)}{2} & \\le c. \\end{aligned} $$ Since $\\min\\cos(n / 2) = -1$, this implies that $c \\ge 3 / 2$. But $c < 1$.","title":"4.5 The master method for solving recurrences"},{"location":"Chap04/4.5/#45-1","text":"Use the master method to give tight asymptotic bounds for the following recurrences: a. $T(n) = 2T(n / 4) + 1$. b. $T(n) = 2T(n / 4) + \\sqrt n$. c. $T(n) = 2T(n / 4) + n$. d. $T(n) = 2T(n / 4) + n^2$. a. $\\Theta(n^{\\log_4 2}) = \\Theta(\\sqrt n)$. b. $\\Theta(n^{\\log_4 2}\\lg n) = \\Theta(\\sqrt n\\lg n)$. c. $\\Theta(n)$. d. $\\Theta(n^2)$.","title":"4.5-1"},{"location":"Chap04/4.5/#45-2","text":"Professor Caesar wishes to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen's algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into pieces of size $n / 4 \\times n / 4$, and the divide and combine steps together will take $\\Theta(n^2)$ time. He needs to determine how many subproblems his algorithm has to create in order to beat Strassen's algorithm. If his algorithm creates $a$ subproblems, then the recurrence for the running time $T(n)$ becomes $T(n) = aT(n / 4) + \\Theta(n^2)$. What is the largest integer value of $a$ for which Professor Caesar's algorithm would be asymptotically faster than Strassen's algorithm? We need to find the largest integer $a$ such that $\\log_4 a < \\lg 7$. The answer is $a = 48$.","title":"4.5-2"},{"location":"Chap04/4.5/#453","text":"Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n / 2) + \\Theta(1)$ is $T(n) = \\Theta(\\lg n)$. (See exercise 2.3-5 for a description of binary search.) $$ \\begin{aligned} a & = 1, b = 2, \\\\ f(n) & = \\Theta(n^{\\lg 1}) = \\Theta(1), \\\\ T(n) & = \\Theta(\\lg n). \\end{aligned} $$","title":"4.5.3"},{"location":"Chap04/4.5/#45-4","text":"Can the master method be applied to the recurrence $T(n) = 4T(n / 2) + n^2\\lg n$? Why or why not? Give an asymptotic upper bound for this recurrence. With $a = 4$, $b = 2$, we have $f(n) = n^2\\lg n \\ne O(n^{2 - \\epsilon}) \\ne \\Omega(n^{2 - \\epsilon})$, so we cannot apply the master method. We guess $T(n) \\le cn^2\\lg^2 n$, $$ \\begin{aligned} T(n) & \\le 4T(n / 2) + n^2\\lg n \\\\ & = 4c(n / 2)^2\\lg^2(n / 2) + n^2\\lg n \\\\ & = cn^2\\lg(n / 2)\\lg n - cn^2\\lg(n / 2)\\lg 2 + n^2\\lg n \\\\ & = cn^2\\lg^2 n - cn^2\\lg n\\lg 2 - cn^2\\lg(n / 2) + n^2\\lg n \\\\ & = cn^2\\lg^2 n + (1 - c)n^2\\lg n - cn^2\\lg(n / 2) & (c > 1) \\\\ & \\le cn^2\\lg^2 n - cn^2\\lg(n / 2) \\\\ & \\le cn^2\\lg^2 n. \\end{aligned} $$ Exercise 4.6-2 is the general case for this.","title":"4.5-4"},{"location":"Chap04/4.5/#45-5-star","text":"Consider the regularity condition $af(n / b) \\ge cf(n)$ for some constant $c < 1$, which is part of case 3 of the master theorem. Give an example of constants $a \\ge 1$ and $b > 1$ and a function $f(n)$ that satisfies all the conditions in case 3 of the master theorem, except the regularity condition. $a = 1$, $b = 2$ and $f(n) = n(2 - \\cos n)$. If we try to prove it, $$ \\begin{aligned} \\frac{n}{2}(2 - \\cos\\frac{n}{2}) & < cn \\\\ \\frac{1 - cos(n / 2)}{2} & < c \\\\ 1 - \\frac{cos(n / 2)}{2} & \\le c. \\end{aligned} $$ Since $\\min\\cos(n / 2) = -1$, this implies that $c \\ge 3 / 2$. But $c < 1$.","title":"4.5-5 $\\star$"},{"location":"Chap04/4.6/","text":"4.6-1 $\\star$ Give a simple and exact expression for $n_j$ in equation $\\text{(4.27)}$ for the case in which $b$ is a positive integer instead of an arbitrary real number. $n_j$ is obtained by shifting the base $b$ representation $j$ positions to the right, and adding $1$ if any of the $j$ least significant positions are non-zero. 4.6-2 $\\star$ Show that if $f(n) = \\Theta(n^{\\log_b a}\\lg^k{n})$, where $k \\ge 0$, then the master recurrence has solution $T(n) = \\Theta(n^{\\log_b a}\\lg^{k + 1}n)$. For simplicity, confine your analysis to exact powers of $b$. $$ \\begin{aligned} g(n) & = \\sum_{j = 0}^{\\log_b n - 1} a^j f(n / b^j) \\\\ f(n / b^j) & = \\Theta\\Big((n / b^j)^{\\log_b a} \\lg^k(n / b^j) \\Big) \\\\ g(n) & = \\Theta\\Big(\\sum_{j = 0}^{\\log_b n - 1}a^j\\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\big(\\frac{n}{b^j}\\big)\\Big) \\\\ & = \\Theta(A) \\\\ A & = \\sum_{j = 0}^{\\log_b n - 1} a^j \\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\frac{a}{b^{\\log_b a}}\\Big)^j\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a}\\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} B \\\\ \\lg^k\\frac{n}{d} & = (\\lg n - \\lg d)^k = \\lg^k{n} + o(\\lg^k{n}) \\\\ B & = \\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\lg^k{n} - o(\\lg^k{n})\\Big) \\\\ & = \\log_b n\\lg^k{n} + \\log_b n \\cdot o(\\lg^k{n}) \\\\ & = \\Theta(\\log_b n\\lg^k{n}) \\\\ & = \\Theta(\\lg^{k + 1}{n}) \\\\ g(n) & = \\Theta(A) \\\\ & = \\Theta(n^{\\log_b a}B) \\\\ & = \\Theta(n^{\\log_b a}\\lg^{k + 1}{n}). \\end{aligned} $$ 4.6-3 $\\star$ Show that case 3 of the master method is overstated, in the sense that the regularity condition $af(n / b) \\le cf(n)$ for some constant $c < 1$ implies that there exists a constant $\\epsilon > 0$ such that $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$. $$ \\begin{aligned} af(n / b) & \\le cf(n) \\\\ \\alpha f(n / b) & \\le f(n), \\alpha = a / c \\\\ \\alpha f(n) & \\le f(nb) \\\\ \\alpha^i f(1) & \\le f(b^i) \\\\ \\end{aligned} $$ $$ \\begin{aligned} n = b^i & \\Rightarrow i = \\log_b n \\Rightarrow f(n) \\ge \\alpha^{\\log_b n}f(1) = n^{\\log_b \\alpha} \\\\ \\alpha > a & \\Rightarrow \\alpha = a + d \\quad (c < 1, d > 0) \\\\ & \\Rightarrow f(n) = n^{\\log_b a + \\log_b d} = n^{\\log_b a+\\epsilon}. \\quad (\\epsilon = \\log_b d) \\end{aligned} $$","title":"4.6 Proof of the master theorem"},{"location":"Chap04/4.6/#46-1-star","text":"Give a simple and exact expression for $n_j$ in equation $\\text{(4.27)}$ for the case in which $b$ is a positive integer instead of an arbitrary real number. $n_j$ is obtained by shifting the base $b$ representation $j$ positions to the right, and adding $1$ if any of the $j$ least significant positions are non-zero.","title":"4.6-1 $\\star$"},{"location":"Chap04/4.6/#46-2-star","text":"Show that if $f(n) = \\Theta(n^{\\log_b a}\\lg^k{n})$, where $k \\ge 0$, then the master recurrence has solution $T(n) = \\Theta(n^{\\log_b a}\\lg^{k + 1}n)$. For simplicity, confine your analysis to exact powers of $b$. $$ \\begin{aligned} g(n) & = \\sum_{j = 0}^{\\log_b n - 1} a^j f(n / b^j) \\\\ f(n / b^j) & = \\Theta\\Big((n / b^j)^{\\log_b a} \\lg^k(n / b^j) \\Big) \\\\ g(n) & = \\Theta\\Big(\\sum_{j = 0}^{\\log_b n - 1}a^j\\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\big(\\frac{n}{b^j}\\big)\\Big) \\\\ & = \\Theta(A) \\\\ A & = \\sum_{j = 0}^{\\log_b n - 1} a^j \\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\frac{a}{b^{\\log_b a}}\\Big)^j\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a}\\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} B \\\\ \\lg^k\\frac{n}{d} & = (\\lg n - \\lg d)^k = \\lg^k{n} + o(\\lg^k{n}) \\\\ B & = \\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\lg^k{n} - o(\\lg^k{n})\\Big) \\\\ & = \\log_b n\\lg^k{n} + \\log_b n \\cdot o(\\lg^k{n}) \\\\ & = \\Theta(\\log_b n\\lg^k{n}) \\\\ & = \\Theta(\\lg^{k + 1}{n}) \\\\ g(n) & = \\Theta(A) \\\\ & = \\Theta(n^{\\log_b a}B) \\\\ & = \\Theta(n^{\\log_b a}\\lg^{k + 1}{n}). \\end{aligned} $$","title":"4.6-2 $\\star$"},{"location":"Chap04/4.6/#46-3-star","text":"Show that case 3 of the master method is overstated, in the sense that the regularity condition $af(n / b) \\le cf(n)$ for some constant $c < 1$ implies that there exists a constant $\\epsilon > 0$ such that $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$. $$ \\begin{aligned} af(n / b) & \\le cf(n) \\\\ \\alpha f(n / b) & \\le f(n), \\alpha = a / c \\\\ \\alpha f(n) & \\le f(nb) \\\\ \\alpha^i f(1) & \\le f(b^i) \\\\ \\end{aligned} $$ $$ \\begin{aligned} n = b^i & \\Rightarrow i = \\log_b n \\Rightarrow f(n) \\ge \\alpha^{\\log_b n}f(1) = n^{\\log_b \\alpha} \\\\ \\alpha > a & \\Rightarrow \\alpha = a + d \\quad (c < 1, d > 0) \\\\ & \\Rightarrow f(n) = n^{\\log_b a + \\log_b d} = n^{\\log_b a+\\epsilon}. \\quad (\\epsilon = \\log_b d) \\end{aligned} $$","title":"4.6-3 $\\star$"},{"location":"Chap04/Problems/4-1/","text":"Give asymptotic upper and lower bound for $T(n)$ in each of the following recurrences. Assume that $T(n)$ is constant for $n \\le 2$. Make your bounds as tight as possible, and justify your answers. a. $T(n) = 2T(n / 2) + n^4$. b. $T(n) = T(7n / 10) + n$. c. $T(n) = 16T(n / 4) + n^2$. d. $T(n) = 7T(n / 3) + n^2$. e. $T(n) = 7T(n / 2) + n^2$. f. $T(n) = 2T(n / 4) + \\sqrt n$. g. $T(n) = T(n - 2) + n^2$. Note: In parts (a), (b), and (d) below, we are applying case 3 or the master theorem, which requires the regularity condition that $af(n / b) \\le cf(n)$ for some constant $c < 1$. In each of these parts, $f(n)$ has the form $n^k$. The regularity condition is satisfied because $$af(n / b) = an^k / b^k = (a / b^k) n^k = (a / b^k)f(n),$$ and in each of the cases below, $a / b^k$ is a constant strctly less than $1$. a. $T(n) = 2T(n / 2) + n^4 = \\Theta(n^4)$. This is a divide-and-conquer recurrence with $a = 2$, $b = 2$, $f(n) = n^4$, and $n^{\\log_b a} = n^{\\lg 2} = n$. Since $n^4 = \\Omega(n^{\\lg 2 + 3})$ and $a / b^k = 2 / 2^4 = 1 / 8 < 1$, case 3 of the master theorem applies, and $T(n) = \\Theta(n^4)$. b. $T(n) = T(7n / 10) + n = \\Theta(n)$. This is a divide-and-conquer recurrence with $a = 1$, $b = 10 / 7$, $f(n) = n$, and $n^{\\log_b a} = n^{\\log_{10 / 7} 1} = n^0 = 1$. Since $n = \\Omega(n^{\\log_{10 / 7} 1 + 1})$ and $a / b^k = 1 / (10 / 7)^1 = 7 / 10 < 1$, case 3 of the master theorem applies, and $T(n) = \\Theta(n)$. c. $T(n) = 16T(n / 4) + n^2 = \\Theta(n^2 \\lg n)$. This is another divide-and-conquer recurrence with $a = 16$, $b = 4$, $f(n) = n^2$, and $n^{\\log_b a} = n^{\\log_4 16} = n^2$. Since $n^2 = \\Theta(n^{\\log_4 16})$, case 2 of the master theorem applies, and $T(n) = \\Theta(n^2 \\lg n)$. d. $T(n) = 7T(n / 3) + n^2 = \\Theta(n^2)$. This is a divide-and-conquer recurrence with $a = 7$, $b = 3$, $f(n) = n^2$, and $n^{\\log_b a} = n^{\\log_3 7}$. Since $1 < \\log_3 7 < 2$, we have that $n^2 = \\Omega(n^{\\log_3 7 + \\epsilon})$ for some constant $\\epsilon > 0$. We also have $a / b^k = 7 / 3^2 = 7 / 9 < 1$, so that case 3 of the master theorem applies, and $T(n) = \\Theta(n^2)$. e. $T(n) = 7T(n / 2) + n^2 = O(n^{\\lg 7})$. This is a divide-and-conquer recurrence with $a = 7$, $b = 2$, $f(n) = n^2$, and $n^{\\log_b a} = n^{\\lg 7}$. Since $2 < \\lg 7 < 3$, we have that $n^2 = O(n^{\\lg 7 - \\epsilon})$ for some constant $\\epsilon > 0$. Thus case 1 of the master theorem applies, and $T(n) = \\Theta(n^{\\lg 7})$. f. $T(n) = 2T(n / 4) + \\sqrt n = \\Theta(\\sqrt n\\lg n)$. This is another divide-and-conquer recurrence with $a = 2$, $b = 4$, $f(n) = \\sqrt n$, and $n^{\\log_b a} = n^{\\log_4 2} = \\sqrt n$. Since $\\sqrt n = \\Theta(n^{\\log_4 2})$, case 2 of the master theorem applies, and $T(n) = \\Theta(\\sqrt n\\lg n)$. g. $T(n) = T(n - 1) + n$ Using the recursion tree shown below, we get a guess of $T(n) = \\Theta(n^2)$. First, we prove the $T(n) = \\Omega(n^2)$ part by induction. The inductive hypothesis is $T(n) \\ge cn^2$ for some constant $c > 0$. $$ \\begin{aligned} T(n) & = T(n - 1) + n \\\\ & \\ge c(n - 1)^2 + n \\\\ & = cn^2 - 2cn + c + n \\\\ & \\ge cn^2, \\end{aligned} $$ if $-2cn + n + c \\ge 0$ or, equivalently, $n(1 - 2c) + c \\ge 0$. This condition holds when $n \\ge 0$ and $0 < c \\le 1 / 2$. For the upper bound, $T(n) = O(n^2)$, we use the inductive hypothesis that $T(n) \\le cn^2$ for some constant $c > 0$. By a similar derivation, we get that $T(n) \\le cn^2$ if $-2cn + n + c \\le 0$ or, equivalently, $n(1 - 2c) + c \\le 0$. This condition holds for $c = 1$ and $n \\ge 1$. Thus, $T(n) = \\Omega(n^2)$ and $T(n) = O(n^2)$, so we conclude that $T(n) = \\Theta(n^2)$. h. $T(n) = T(\\sqrt n) + 1$ The easy way to do this is with a change of variables, as on page 86 of the text. Let $m = \\lg n$ and $S(m) = T(2^m)$. $T(2^m) = T(2^{m / 2}) + 1$, so $S(m) = S(m / 2) + 1$. Using the master theorem, $n^{\\log_b a} = n^{\\lg 1} = n^0 = 1$ and $f(n) = 1$. Since $1 = \\Theta(1)$, case 2 applies and $S(m) = \\Theta(\\lg m)$. Therefore, $T(n) = \\Theta(\\lg\\lg n)$.","title":"4-1 Recurrence examples"},{"location":"Chap04/Problems/4-2/","text":"Throughout this book, we assume that parameter passing during procedure calls takes constant time, even if an $N$-element array is being passed. This assumption is valid in most systems because a pointer to the array is passed, not the array itself. This problem examines the implications of three parameter-passing strategies: An array is passed by pointer. Time $= \\Theta(1)$. An array is passed by copying. Time $= \\Theta(N)$, where $N$ is the size of the array. An array is passed by copying only the subrage that might be accessed by the called procedure. Time $= \\Theta(q - p + 1)$ if the subarray $A[p..q]$ is passed. a. Consider the recursive binary search algorithm for finding a number in a sorted array (see Exercise 2.3-5). Give recurrences for the worst-case running times of binary search when arrays are passed using each of the three methods above, and give good upper bounds on the solutions of the recurrences. Let $N$ be the size of the original problems and $n$ be the size of a subproblem. b. Redo part (a) for the $\\text{MERGE-SORT}$ algorithm from Section 2.3.1. a. $T(n) = T(n / 2) + c = \\Theta(\\lg n)$. (master method) $\\Theta(n\\lg n)$. $$ \\begin{aligned} T(n) & = T(n / 2) + cN \\\\ & = 2cN + T(n / 4) \\\\ & = 3cN + T(n / 8) \\\\ & = \\sum_{i = 0}^{\\lg n - 1}(2^icN / 2^i) \\\\ & = cN\\lg n \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ $T(n) = T(n / 2) + cn = \\Theta(n)$. (master method) b. $T(n) = 2T(n / 2) + cn = \\Theta(n\\lg n)$. (master method) $\\Theta(n^2)$. $$ \\begin{aligned} T(n) & = 2T(n / 2) + cn + 2N = 4N + cn + 2c(n / 2) + 4T(n / 4) \\\\ & = 8N + 2cn + 4c(n / 4) + 8T(n / 8) \\\\ & = \\sum_{i = 0}^{\\lg n - 1}(cn + 2^iN) \\\\ & = \\sum_{i = 0}^{\\lg n - 1}cn + N\\sum_{i = 0}^{\\lg n - 1}2^i \\\\ & = cn\\lg n + N\\frac{2^{\\lg n} - 1}{2 - 1} \\\\ & = cn\\lg n + nN - N = \\Theta(nN) \\\\ & = \\Theta(n^2). \\end{aligned} $$ $\\Theta(n\\lg n)$. $$ \\begin{aligned} T(n) & = 2T(n / 2) + cn + 2n / 2 \\\\ & = 2T(n / 2) + (c + 1)n \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$","title":"4-2 Parameter-passing costs"},{"location":"Chap04/Problems/4-3/","text":"Give asymptotic upper and lower bounds for $T(n)$ in each of the following recurrences. Assume that $T(n)$ is constant for sufficiently small $n$. Make your bounds as tight as possible, and justify your answers. a. $T(n) = 4T(n / 3) + n\\lg n$. b. $T(n) = 3T(n / 3) + n / \\lg n$. c. $T(n) = 4T(n / 2) + n^2\\sqrt n$. d. $T(n) = 3T(n / 3 - 2) + n / 2$. e. $T(n) = 2T(n / 2) + n / \\lg n$. f. $T(n) = T(n / 2) + T(n / 4) + T(n / 8) + n$. g. $T(n) = T(n - 1) + 1 / n$. h. $T(n) = T(n - 1) + \\lg n$. i. $T(n) = T(n - 2) + 1 / \\lg n$. j. $T(n) = \\sqrt nT(\\sqrt n) + n$ [This problem is solved only for parts a, c, e, f, g, h, and i.] a. $T(n) = 3T(n / 2) + n\\lg n$ We have $f(n) = n\\lg n$ and $n^{\\log_b a} = n^{\\lg 3} \\approx n^{1.585}$. Since $n\\lg n = O(n^{\\lg 3 - \\epsilon})$ for any $0 < \\epsilon \\le 0.58$, by case 1 of the master theorem, we have $T(n) = \\Theta(n^{\\lg 3})$. b. $\\Theta(n\\lg\\lg n)$. Check subtask 5 for the reasoning. c. $T(n) = 4T(n / 2) + n^2 \\sqrt n$ We have $f(n) = n^2 \\sqrt n = n^{5 / 2}$ and $n^{\\log_b a} = n^{\\lg 4} = n^2$. Since $n^{5 / 2} = \\Omega(n^{2 + \\epsilon})$ for $\\epsilon = 1 / 2$, we look at the regularity condition in case 3 of the master theorem. We have $$af(n / b) = 4(n / 2)^2 \\sqrt{n / 2} = n^{5 / 2} / \\sqrt 2 \\le cn^{5 / 2}$$ for $1 / \\sqrt 2 \\le c < 1$. Case 3 applies, and we have $T(n) = \\Theta(n^2 \\sqrt n)$. d. $\\Theta(n\\lg n)$ by the master method. e. $T(n) = 2T(n / 2) + n / \\lg n$ We can get a guess by means of a recursion tree: We get the sum on each level by observing that at depth $i$, we have $2^i$ nodes, each with a numerator of $n / 2^i$ and a denominator of $\\lg(n / 2^i) = \\lg n - i$, so that the cost at depth $i$ is $$2^i \\cdot \\frac{n / 2^i}{\\lg n - i} = \\frac{n}{\\lg n - i}.$$ The sum for all levels is $$ \\begin{aligned} \\sum_{i = 0}^{\\lg n - 1} \\frac{n}{\\lg n - i} & = \\sum_{i = 1}^{\\lg n} \\frac{n}{i} \\\\ & = n \\sum_{i = 1}^{\\lg n} 1 / i \\\\ & = n \\cdot \\Theta(\\lg\\lg n) & \\text{(by equation (A.7), the harmonic series)} \\\\ & = \\Theta(n\\lg\\lg n). \\end{aligned} $$ We can use this analysis as a guess that $T(n) = \\Theta(n\\lg\\lg n)$. If we were to do a straight substitution proof, it would be rather involved. Instead, we will show by substitution that $T(n) \\le n(1 + H_{\\lfloor \\lg n \\rfloor})$ and $T(n) \\ge n \\cdot H_{\\lceil \\lg n \\rceil}$, where $H_k$ is the $k$th harmonic number: $H_k = 1 / 1 + 1 / 2 + 1 / 3 + \\cdots + 1 / k$. We also define $H_0 = 0$. Since $H_k = \\Theta(\\lg k)$, we have that $H_{\\lfloor \\lg n \\rfloor} = \\Theta(\\lg \\lfloor \\lg n \\rfloor) = \\Theta(\\lg\\lg n)$ and $H_{\\lceil \\lg n \\rceil} = \\Theta(\\lg \\lceil \\lg n \\rceil) = \\Theta(\\lg\\lg n)$. Thus, we will have that $T(n) = \\Theta(n\\lg\\lg n)$. The base case for the proof is for $n = 1$, and we use $T(1) = 1$. Here, $\\lg n = 0$, so that $\\lg n = \\lfloor \\lg n \\rfloor = \\lceil \\lg n \\rceil$. Since $H_0 = 0$, we have $T(1) = 1 \\le 1(1 + H_0)$ and $T(1) = 1 \\ge 0 = 1 \\cdot H_0$. For the upper bound of $T(n) \\le n(1 + H_{\\lfloor \\lg n \\rfloor})$, we have $$ \\begin{aligned} T(n) & = 2T(n / 2) + n / \\lg n \\\\ & \\le 2((n / 2)(1 + H_{\\lfloor \\lg (n / 2) \\rfloor})) + n / \\lg n \\\\ & = n(1 + H_{\\lfloor \\lg n - 1 \\rfloor}) + n / \\lg n \\\\ & = n(1 + H_{\\lfloor \\lg n \\rfloor - 1} + 1 / \\lg n) \\\\ & \\le n(1 + H_{\\lfloor \\lg n \\rfloor - 1} + 1 / \\lfloor \\lg n \\rfloor) \\\\ & = n(1 + H_{\\lfloor \\lg n \\rfloor}), \\end{aligned} $$ where the last line follows from the identity $H_k = H_{k - 1} + 1 / k$. The upper bound of $T(n) \\ge n \\cdot H_{\\lceil \\lg n \\rceil}$ is similar: $$ \\begin{aligned} T(n) & = 2T(n / 2) + n / \\lg n \\\\ & \\ge 2((n / 2) \\cdot H_{\\lceil \\lg (n / 2) \\rceil}) + n / \\lg n \\\\ & = n \\cdot (H_{\\lceil \\lg n - 1 \\rceil}) + n / \\lg n \\\\ & = n \\cdot (H_{\\lceil \\lg n \\rceil - 1} + 1 / \\lg n) \\\\ & \\ge n \\cdot (H_{\\lceil \\lg n \\rceil - 1} + 1 / \\lceil \\lg n \\rceil) \\\\ & = n \\cdot (H_{\\lceil \\lg n \\rceil}). \\end{aligned} $$ Thus, $T(n) = \\Theta(n\\lg\\lg n)$. f. $T(n) = T(n / 2) + T(n / 4) + T(n / 8) + n$ Using the recursion tree shown below, we get a guess of $T(n) = \\Theta(n)$. We use the substitution method to prove that $T(n) = O(n)$. Our inductive hypothesis is that $T(n) \\le cn$ for some constant $c > 0$. We have $$ \\begin{aligned} T(n) & = T(n / 2) + T(n / 4) + T(n / 8) + n \\\\ & \\le cn / 2 + cn / 4 + cn / 8 + n \\\\ & = 7 cn / 8 + n \\\\ & = (1 + 7c / 8) n \\\\ & \\le cn \\quad \\text{if $c \\ge 8$}. \\end{aligned} $$ Therefore, $T(n) = O(n)$. Showing that $T(n) = \\Omega(n)$ is easy: $$T(n) = T(n / 2) + T(n / 4) + T(n / 8) + n \\ge n.$$ Since $T(n) = O(n)$ and $T(n) = \\Omega(n)$, we have that $T(n) = \\Theta(n)$. g. $T(n) = T(n - 1) + 1 / n$ This recurrence corresponds to the harmonic series, so that $T(n) = H_n$, where $H_n = 1 / 1 + 1 / 2 + 1 / 3 + \\cdots + 1 / n$. For the base case, we have $T(1) = 1 = H_1$. For the inductive step, we assume that $T(n - 1) = H_{n - 1}$, and we have $$ \\begin{aligned} T(n) & = T(n - 1) + 1 / n \\\\ & = H_{n - 1} + 1 / n \\\\ & = H_n. \\end{aligned} $$ Since $H_n = \\Theta(\\lg n)$ by equation $\\text{(A.7)}$, we have that $T(n) = \\Theta(\\lg n)$. h. $T(n) = T(n - 1) + \\lg n$ We guess that $T(n) = \\Theta(n\\lg n)$. To prove the upper bound, we will show that $T(n) = O(n\\lg n)$. Our inductive hypothesis is that $T(n) \\le cn\\lg n$ for some constant $c$. We have $$ \\begin{aligned} T(n) & = T(n - 1) + \\lg n \\\\ & \\le c(n - 1) \\lg(n - 1) + \\lg n \\\\ & = cn\\lg(n - 1) - c \\lg(n - 1) + \\lg n \\\\ & \\le cn\\lg(n - 1) - c \\lg(n / 2) + \\lg n & \\text{(since $\\lg(n - 1) \\ge \\lg(n / 2)$ for $n \\ge 2$)} \\\\ & = cn\\lg(n - 1) - c \\lg n + c + \\lg n \\\\ & < cn\\lg n - c \\lg n + c + \\lg n \\\\ & \\le cn\\lg n, \\end{aligned} $$ if $-c \\lg n + c + \\lg n \\le 0$. Equivalently, $$ \\begin{aligned} -c \\lg n + c + \\lg n & \\le 0 \\\\ c & \\le (c - 1) \\lg n \\\\ \\lg n & \\ge c / (c - 1). \\end{aligned} $$ This works for $c = 2$ and all $n \\ge 4$. To prove the lower bound, we will show that $T(n) = \\Omega(n\\lg n)$. Our inductive hypothesis is that $T(n) \\ge cn\\lg n + dn$ for constants $c$ and $d$. We have $$ \\begin{aligned} T(n) & = T(n - 1) + \\lg n \\\\ & \\ge c(n - 1) \\lg(n - 1) + d(n - 1) + \\lg n \\\\ & = cn\\lg(n - 1) - c \\lg(n - 1) + dn - d + \\lg n \\\\ & \\ge cn\\lg(n / 2) - c \\lg(n - 1) + dn - d + \\lg n & \\text{(since $\\lg(n - 1) \\ge \\lg(n / 2)$ for $n \\ge 2$)} \\\\ & = cn\\lg n - cn - c \\lg(n - 1) + dn - d + \\lg n \\\\ & \\ge cn\\lg n, \\end{aligned} $$ if $-cn - c \\lg(n - 1) + dn - d + \\lg n \\ge 0$. Since $$-cn - c \\lg(n - 1) + dn - d + \\lg n > -cn - c\\lg(n - 1) + dn - d + \\lg(n - 1),$$ it suffices to find conditions in which $-cn - c\\lg(n - 1) + dn - d + \\lg(n - 1) \\ge 0$. Equivalently, $$ \\begin{aligned} -cn - c \\lg(n - 1) + dn - d + \\lg(n - 1) & \\ge 0 \\\\ (d - c)n & \\ge (c - 1)\\lg(n - 1) + d. \\end{aligned} $$ This works for $c = 1$, $d = 2$, and all $n \\ge 2$. Since $T(n) = O(n\\lg n)$ and $T(n) = \\Omega(n\\lg n)$, we conclude that $T(n) = \\Theta(n\\lg n)$. i. $T(n) = T(n - 2) + 2\\lg n$ We guess that $T(n) = \\Theta(n\\lg n)$. We show the upper bound of $T(n) = O(n\\lg n)$ by means of the inductive hypothesis $T(n) \\le cn\\lg n$ for some constant $c > 0$. We have $$ \\begin{aligned} T(n) & = T(n - 2) + 2\\lg n \\\\ & \\le c(n - 2)\\lg(n - 2) + 2\\lg n \\\\ & \\le c(n - 2)\\lg n + 2\\lg n \\\\ & = (cn - 2c + 2)\\lg n \\\\ & = cn\\lg n + (2 - 2c)\\lg n \\\\ & \\le cn\\lg n & \\text{if $c > 1$}. \\end{aligned} $$ Therefore, $T(n) = O(n\\lg n)$. For the lower bound of $T(n) = \\Omega(n\\lg n)$, we'll show that $T(n) \\ge cn\\lg n + dn$, for constants $c, d > 0$ to be chosen. We assume that $n \\ge 4$, which implies that $\\lg(n - 2) \\ge \\lg(n / 2)$, $n / 2 \\ge \\lg n$, and $n / 2 \\ge 2$. (We'll use these inequalities as we go along.) We have $$ \\begin{aligned} T(n) & \\ge c(n - 2)\\lg(n - 2) + d(n - 2) + 2\\lg n \\\\ & = cn\\lg(n - 2) - 2c\\lg(n - 2) + dn - 2d + 2\\lg n \\\\ & > cn\\lg(n - 2) - 2c\\lg n + dn - 2d + 2\\lg n & \\text{(since $-\\lg n < -\\lg(n - 2)$)} \\\\ & = cn\\lg(n - 2) - 2(c - 1)\\lg n + dn - 2d & \\text{(by inequality (1) above)} \\\\ & = cn\\lg n - cn - 2(c - 1)\\lg n + dn - 2d \\\\ & \\ge cn\\lg n, \\end{aligned} $$ if $-cn - 2(c - 1)\\lg n + dn - 2d \\ge 0$ or, equivalently, $dn \\ge cn + 2(c - 1)\\lg n + 2d$. Pick any constant $c > 1 / 2$, and then pick any constant $d$ such that $$d \\ge 2(2c - 1).$$ (The requirement that $c > 1 / 2$ means that $d$ is positive.) Then $$d / 2 \\ge 2c - 1 = c + (c - 1),$$ and adding $d / 2$ to both sides, we have $$d \\ge c + (c - 1) + d / 2.$$ Multiplying by $n$ yields $$dn \\ge cn + (c - 1)n + dn / 2,$$ and then both multiplying and dividing the middle term by $2$ gives $$dn \\ge cn + 2(c - 1)n / 2 + dn / 2.$$ Using inequalities (2) and (3) above, we get $$dn \\ge cn + 2(c - 1)\\lg n + 2d,$$ which is what we needed to show. Thus $T(n) = \\Omega(n\\lg n)$. Since $T(n) = O(n\\lg n)$ and $T(n) = \\Omega(n\\lg n)$, we conclude that $T(n) = \\Theta(n\\lg n)$. j. $T(n) = \\sqrt nT(\\sqrt n) + n$ We guess $T(n) \\le cn\\lg\\lg n$, $$ \\begin{aligned} T(n) & \\le \\sqrt nc\\sqrt n\\lg\\lg\\sqrt n + n \\\\ & = cn\\lg\\lg\\sqrt n + n \\\\ & = cn\\lg\\frac{\\lg n}{2} + n \\\\ & = cn\\lg\\lg n - cn\\lg 2 + n \\\\ & = cn\\lg\\lg n + (1 - c)n & (c > 1) \\\\ & \\le cn\\lg\\lg n. & = \\Theta(n\\lg\\lg n) \\end{aligned} $$","title":"4-3 More recurrence examples"},{"location":"Chap04/Problems/4-4/","text":"This problem develops properties of the Fibonacci numbers, which are defined by recurrence $\\text{(3.22)}$. We shall use the technique of generating functions to solve the Fibonacci recurrence. Define the generating function (or formal power series ) $\\mathcal F$ as $$ \\begin{aligned} \\mathcal F(z) & = \\sum_{i = 0}^{\\infty} F_iz^i \\\\ & = 0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + 8z^6 + 13z^7 + 21z^8 + \\cdots, \\end{aligned} $$ where $F_i$ is the $i$th Fibonacci number. a. Show that $\\mathcal F(z) = z + z\\mathcal F(z) + z^2\\mathcal F$. b. Show that $$ \\begin{aligned} \\mathcal F(z) & = \\frac{z}{1 - z - z^2} \\\\ & = \\frac{z}{(1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{1}{\\sqrt 5}\\Big(\\frac{1}{1 - \\phi z} - \\frac{1}{1 - \\hat{\\phi} z}\\Big), \\end{aligned} $$ where $\\phi = \\frac{1 + \\sqrt 5}{2} = 1.61803\\ldots$ and $\\hat\\phi = \\frac{1 - \\sqrt 5}{2} = -0.61803\\ldots$ c. Show that $$\\mathcal F(z) = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt 5}(\\phi^i - \\hat{\\phi}^i)z^i.$$ d. Use part (c) to prove that $F_i = \\phi^i / \\sqrt 5$ for $i > 0$, rounded to the nearest integer. ($\\textit{Hint:}$ Observe that $|\\hat{\\phi}| < 1$.) a. $$ \\begin{aligned} z + z\\mathcal F(z) + z^2\\mathcal F(Z) & = z + z\\sum_{i = 0}^{\\infty} F_iz^i + z^2\\sum_{i = 0}^{\\infty}F_i z^i \\\\ & = z + \\sum_{i = 1}^{\\infty} F_{i - 1}z^i + \\sum_{i = 2}^{\\infty}F_{i - 2} z^i \\\\ & = z + F_1z + \\sum_{i = 2}^{\\infty}(F_{i - 1} + F_{i - 2})z^i \\\\ & = z + F_1z + \\sum_{i = 2}^{\\infty}F_iz^i \\\\ & = \\mathcal F(z). \\end{aligned} $$ b. Note that $\\phi - \\hat\\phi = \\sqrt 5$, $\\phi + \\hat\\phi = 1$ and $\\phi\\hat\\phi = - 1$. $$ \\begin{aligned} \\mathcal F(z) & = \\frac{\\mathcal F(z)(1 - z - z^2)}{1 - z - z^2} \\\\ & = \\frac{\\mathcal F(z) - z\\mathcal F(z) - z^2\\mathcal F(z) - z + z}{1 - z - z^2} \\\\ & = \\frac{\\mathcal F(z) - \\mathcal F(z) + z}{1 - z - z^2} \\\\ & = \\frac{z}{1 - z - z^2} \\\\ & = \\frac{z}{1 - (\\phi + \\hat\\phi)z + \\phi\\hat\\phi z^2} \\\\ & = \\frac{z}{(1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{\\sqrt 5 z}{\\sqrt 5 (1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{(\\phi - \\hat\\phi)z + 1 - 1}{\\sqrt 5 (1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{(1 - \\hat\\phi z) - (1 - \\phi z)}{\\sqrt 5 (1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{1}{\\sqrt 5}\\Big(\\frac{1}{1 - \\phi z} - \\frac{1}{1 - \\hat\\phi z}\\Big). \\end{aligned} $$ c. We have $\\frac{1}{1 - x} = \\sum_{k = 0}^{\\infty}x^k$, when $|x| < 1$, thus $$ \\begin{aligned} \\mathcal F(n) & = \\frac{1}{\\sqrt 5}\\Big(\\frac{1}{1 - \\phi z} - \\frac{1}{1 - \\hat\\phi z}\\Big) \\\\ & = \\frac{1}{\\sqrt 5}\\Big(\\sum_{i = 0}^{\\infty}\\phi^i z^i - \\sum_{i = 0}^{\\infty}\\hat{\\phi}^i z^i\\Big) \\\\ & = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt 5}(\\phi^i - \\hat{\\phi}^i) z^i. \\end{aligned} $$ d. $\\mathcal F(z) = \\sum_{i = 0}^{\\infty}\\alpha_i z^i$ where $\\alpha_i = \\frac{\\phi^i - \\hat{\\phi}^i}{\\sqrt 5}$. From this follows that $\\alpha_i = F_i$, that is $$F_i = \\frac{\\phi^i - \\hat{\\phi}^i}{\\sqrt 5} = \\frac{\\phi^i}{\\sqrt 5} - \\frac{\\hat{\\phi}^i}{\\sqrt 5},$$ For $i = 1$, $\\phi / \\sqrt 5 = (\\sqrt 5 + 5) / 10 > 0.5$. For $i > 2$, $|\\hat{\\phi}^i| < 0.5$.","title":"4-4 Fibonacci numbers"},{"location":"Chap04/Problems/4-5/","text":"Professor Diogenes has $n$ supposedly identical integrated-circuit chips that in principle are capable of testing each other. The professor's test jig accomodates two chips at a time. When the jig is loaded, each chip tests the other and reports whether it is good or bad. A good chip always reports accurately whether the other chip is good or bad, but the professor cannot trust the answer of a bad chip. Thus, the four possible outcomes of a test are as follows: $$ \\begin{array}{lll} \\text{Chip $A$ says} & \\text{Chip $B$ says} & \\text{Conclusion} \\\\ \\hline \\text{$B$ is good} & \\text{$A$ is good} & \\text{both are good, or both are bad} \\\\ \\text{$B$ is good} & \\text{$A$ is bad} & \\text{at least one is bad} \\\\ \\text{$B$ is bad} & \\text{$A$ is good} & \\text{at least one is bad} \\\\ \\text{$B$ is bad} & \\text{$A$ is bad} & \\text{at least one is bad} \\end{array} $$ a. Show that if more than $n / 2$ chips are bad, the professor cannot necessarily determine which chips are good using any strategy based on this kind of pairwise test. Assume that the bad chips can conspire to fool the professor. b. Consider the problem of finding a single good chip from among $n$ chips, assuming that more than $n / 2$ of the chips are good. Show that $\\lfloor n / 2 \\rfloor$ pairwise tests are sufficient to reduce the problem to one of nearly half the size. c. Show that the good chips can be identified with $\\Theta(n)$ pairwise tests, assuming that more than $n / 2$ chips are good. Give and solve the recurrence that describes the number of tests. a. Lets say that there are $g < n / 2$ good chips and $n - g$ bad chips. From this assumption, we can always find a set of good chips $G$ and a set of bad chips $B$ of equal size $g$ since $n - g \\ge g$. Now, assume that chips in $B$ always conspire to fool the professor in the following: \"for any test made by the professor, chips in $B$ declare chips in $B$ as 'good' and chips in $G$ as 'bad'.\" Since the chips in $G$ always report correct answers thus there exists symmetric behaviors, it is not possible to distinguish bad chips from good ones. b. Generalize the original problem to: \"Assume there are no less good chips than bad chips.\" Algorithm: Pairwise test them, and leave the last one alone if the number of chips is odd. If the report says at least one of them is bad, throw both chips away; otherwise, throw one away from each pair. Recursively find one good chip among the remaining chips. The recursion ends when the number of remaining chips is $1$ or $2$. If there is only $1$ chip left, then it is the good chip we desire. If there are $2$ chips left, we make a pairwise test between them. If the report says both are good, we can conclude that both are good chips. Otherwise, one is good and the other is bad and we throw both away. The chip we left alone at step $1$ is a good chip. Explanation: If the number of chips is odd, from assumption we know the number of good chips must be greater than the number of bad chips. We randomly leave one chip alone from the chips, in which good chips are not less than bad chips. Chip pairs that do not say each other is good either have one bad chip or have two bad chips, throwing them away doesn't change the fact that good chips are not less than bad chips. The remaining chip pairs are either both good chips or bad chips, after throwing one chip away in every those pairs, we have reduced the size of the problem to at most half of the original problem size. If the number of good chips is $n$ ($n > 1$) more than that of bad chips, we just throw away the chip we left alone when the number of chips is odd. In this case, the number of good chips is at least one more than that of bad chips, and we can eventually find a good chip as our algorithm claims. If the number of good chips is exactly one more than that of bad chips, there are $2$ cases. We left alone the good chip, and remaining chips are one half good and one half bad. In this case, all the chips will be thrown away eventually. And the chip left alone is the one we desire. We left alone the bad chip, there are more good chips than bad chips in the remaining chips. In this case, we can recursively find a good chip in the remaining chips and the left bad chip will be thrown away at the end. c. As the solution provided in (b), we can find one good chip in $$T(n) \\le T(\\lceil n / 2 \\rceil) + \\lfloor n / 2 \\rfloor.$$ By the master theorem, we have $T(n) = O(n)$. After finding a good chip, we can identify all good chips with that good chip we just found in $n - 1$ tests, so the total number of tests is $$O(n) + n - 1 = \\Theta(n).$$","title":"4-5 Chip testing"},{"location":"Chap04/Problems/4-6/","text":"An $m \\times n$ array $A$ of real numbers is a Monge array if for all $i$, $j$, $k$, and $l$ such that $1 \\le i < k \\le m$ and $1 \\le j < l \\le n$, we have $$A[i, j] + A[k, l] \\le A[i, l] + A[k, j].$$ In other words, whenever we pick two rows and two columns of a Monge array and consider the four elements at the intersections of the rows and columns, the sum of the upper-left and lower-right elements is less than or equal to the sum of the lower-left and upper-right elements. For example, the following array is Monge: $$ \\begin{matrix} 10 & 17 & 13 & 28 & 23 \\\\ 17 & 22 & 16 & 29 & 23 \\\\ 24 & 28 & 22 & 34 & 24 \\\\ 11 & 13 & 6 & 17 & 7 \\\\ 45 & 44 & 32 & 37 & 23 \\\\ 36 & 33 & 19 & 21 & 6 \\\\ 75 & 66 & 51 & 53 & 34 \\end{matrix} $$ a. Prove that an array is Monge if and only if for all $i = 1, 2, \\ldots, m - 1$, and $j = 1, 2, \\ldots, n - 1$ we have $$A[i, j] + A[i + 1,j + 1] \\le A[i, j + 1] + A[i + 1, j].$$ ($\\textit{Hint:}$ For the \"if\" part, use induction seperately on rows and columns.) b. The following array is not Monge. Change one element in order to make it Monge. ($\\textit{Hint:}$ Use part (a).) $$ \\begin{matrix} 37 & 23 & 22 & 32 \\\\ 21 & 6 & 7 & 10 \\\\ 53 & 34 & 30 & 31 \\\\ 32 & 13 & 9 & 6 \\\\ 43 & 21 & 15 & 8 \\end{matrix} $$ c. Let $f(i)$ be the index of the column containing the leftmost minimum element of row $i$. Prove that $f(1) \\le f(2) \\le \\cdots \\le f(m)$ for any $m \\times n$ Monge array. d. Here is a description of a divide-and-conquer algorithm that computes the leftmost minimum element in each row of an $m \\times n$ Monge array $A$: Construct a submatrix $A'$ of $A$ consisting of the even-numbered rows of $A$. Recursively determine the leftmost minimum for each row in $A'$. Then compute the leftmost minimum in the odd-numbered rows of $A$. Explain how to compute the leftmost minimum in the odd-numbered rows of $A$ (given that the leftmost minimum of the even-numbered rows is known) in $O(m + n)$ time. e. Write the recurrence describing the running time of the algorithm described in part (d). Show that its solution is $O(m + n\\log m)$. a. The \"only if\" part is trivial, it follows form the definition of Monge array. As for the \"if\" part, let's first prove that $$ \\begin{aligned} A[i, j] + A[i + 1, j + 1] & \\le A[i, j + 1] + A[i + 1, j] \\\\ \\Rightarrow A[i, j] + A[k, j + 1] & \\le A[i, j + 1] + A[k, j], \\end{aligned} $$ where $i < k$. Let's prove it by induction. The base case of $k = i + 1$ is given. As for the inductive step, we assume it holds for $k = i + n$ and we want to prove it for $k + 1 = i + n + 1$. If we add the given to the assumption, we get $$ \\begin{aligned} A[i, j] + A[k, j + 1] & \\le A[i, j + 1] + A[k, j] & \\text{(assumption)} \\\\ A[k, j] + A[k + 1, j + 1] & \\le A[k, j + 1] + A[k + 1, j] & \\text{(given)} \\\\ \\Rightarrow A[i, j] + A[k, j + 1] + A[k, j] + A[k + 1, j + 1] & \\le A[i, j + 1] + A[k, j] + A[k, j + 1] + A[k + 1, j] \\\\ \\Rightarrow A[i, j] + A[k + 1, j + 1] & \\le A[i, j + 1] + A[k + 1, j] \\end{aligned} $$ b. $$ \\begin{matrix} 37 & 23 & \\mathbf{24} & 32 \\\\ 21 & 6 & 7 & 10 \\\\ 53 & 34 & 30 & 31 \\\\ 32 & 13 & 9 & 6 \\\\ 43 & 21 & 15 & 8 \\\\ \\end{matrix} $$ c. Let $a_i$ and $b_j$ be the leftmost minimal elements on rows $a$ and $b$ and let's assume that $i > j$. Then we have $$A[j, a] + A[i, b] \\le A[i, a] + A[j, b].$$ But $$ \\begin{aligned} A[j, a] \\ge A[i, a] & (a_i \\text{ is minimal}) \\\\ A[i, b] \\ge A[j, b] & (b_j \\text{ is minimal}) \\\\ \\end{aligned} $$ Which implies that $$ \\begin{aligned} A[j, a] + A[i, b] & \\ge A[i, a] + A[j, b] \\\\ A[j, a] + A[i, b] & = A[i, a] + A[j, b] \\end{aligned} $$ Which in turn implies that either: $$ \\begin{aligned} A[j, b] < A[i, b] & \\Rightarrow A[i, a] > A[j, a] \\Rightarrow a_i \\text{ is not minimal} \\\\ A[j, b] = A[i, b] & \\Rightarrow b_j \\text{ is not the leftmost minimal} \\end{aligned} $$ d. If $\\mu_i$ is the index of the $i$-th row's leftmost minimum, then we have $$\\mu_{i - 1} \\le \\mu_i \\le \\mu_{i + 1}.$$ For $i = 2k + 1$, $k \\ge 0$, finding $\\mu_i$ takes $\\mu_{i + 1} - \\mu_{i - 1} + 1$ steps at most, since we only need to compare with those numbers. Thus $$ \\begin{aligned} T(m, n) & = \\sum_{i = 0}^{m / 2 - 1} (\\mu_{2i + 2} - \\mu_{2i} + 1) \\\\ & = \\sum_{i = 0}^{m / 2 - 1} \\mu_{2i + 2} - \\sum_{i = 0}^{m / 2 - 1}\\mu_{2i} + m / 2 \\\\ & = \\sum_{i = 1}^{m / 2} \\mu_{2i} - \\sum_{i = 0}^{m / 2 - 1}\\mu_{2i} + m / 2 \\\\ &= \\mu_m - \\mu_0 + m / 2 \\\\ & = n + m / 2 \\\\ & = O(m + n). \\end{aligned} $$ e. The divide time is $O(1)$, the conquer part is $T(m / 2)$ and the merge part is $O(m + n)$. Thus, $$ \\begin{aligned} T(m) & = T(m / 2) + cn + dm \\\\ & = cn + dm + cn + dm / 2 + cn + dm / 4 + \\cdots \\\\ & = \\sum_{i = 0}^{\\lg m - 1}cn + \\sum_{i = 0}^{\\lg m - 1}\\frac{dm}{2^i} \\\\ & = cn\\lg m + dm\\sum_{i = 0}^{\\lg m - 1} \\\\ & < cn\\lg m + 2dm \\\\ & = O(n\\lg m + m). \\end{aligned} $$","title":"4-6 Monge arrays"},{"location":"Chap05/5.1/","text":"5.1-1 Show that the assumption that we are always able to determine which candidate is best in line 4 of procedure $\\text{HIRE-ASSISTANT}$ implies that we know a total order on the ranks of the candidates. A total order is a partial order that is a total relation $(\\forall a, b \\in A:aRb \\text{ or } bRa)$. A relation is a partial order if it is reflexive, antisymmetric and transitive. Assume that the relation is good or better. Reflexive: This is a bit trivial, but everybody is as good or better as themselves. Transitive: If $A$ is better than $B$ and $B$ is better than $C$, then $A$ is better than $C$. Antisymmetric: If $A$ is better than $B$, then $B$ is not better than $A$. So far we have a partial order. Since we assume we can compare any two candidates, then comparison must be a total relation and thus we have a total order. 5.1-2 $\\star$ Describe an implementation of the procedure $\\text{RANDOM}(a, b)$ that only makes calls to $\\text{RANDOM}(0, 1)$. What is the expected running time of your procedure, as a function of $a$ and $b$? 1 2 3 4 5 6 7 8 RANDOM ( a , b ) if a == b return a mid = ( a + b ) / 2 r = RANDOM ( 0 , 1 ) if r == 0 return RANDOM ( a , floor ( mid )) else return RANDOM ( ceil ( mid ), b ) The expected running time is $\\Theta(\\lg(b - a))$. 5.1-3 $\\star$ Suppose that you want to output $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. At your disposal is a procedure $\\text{BIASED-RANDOM}$, that outputs either $0$ or $1$. It outputs $1$ with some probability $p$ and $0$ with probability $1 - p$, where $0 < p < 1$, but you do not know what $p$ is. Give an algorithm that uses $\\text{BIASED-RANDOM}$ as a subroutine, and returns an unbiased answer, returning $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. What is the expected running time of your algorithm as a function of $p$? To get an unbiased random bit, given only calls to $\\text{BIASED-RANDOM}$, call $\\text{BIASED-RANDOM}$ twice. Repeatedly do so until the two calls return different values, and when this occurs, return the first of the two bits: 1 2 3 4 5 6 UNBIASED - ANSWER () while true x = BIASED - RANDOM () y = BIASED - RANDOM () if x != y return x To see that $\\text{UNBIASED-RANDOM}$ returns $0$ and $1$ each with probability $1 / 2$, observe that the probability that a given iteration returns $0$ is $$\\Pr\\{x = 0 \\text{ and } y = 1\\} = (1 - p)p,$$ and the probability that a given iteration returns $1$ is $$\\Pr\\{x = 1 \\text{ and } y = 0\\} = p(1 - p).$$ (We rely on the bits returned by $\\text{BIASED-RANDOM}$ being independent.) Thus, the probability that a given iteration returns $0$ equals the probability that it returns $1$. Since there is no other way for $\\text{UNBIASED-RANDOM}$ to return a value, it returns $0$ and $1$ each with probability $1 / 2$. Assuming that each iteration takes $O(1)$ time, the expected running time of $\\text{UNBIASED-RANDOM}$ is linear in the expected number of iterations. We can view each iteration as a Bernoulli trial, where \"success\" means that the iteration returns a value. The probability of success equals the probability that $0$ is returned plus the probability that $1$ is returned, or $2p(1 - p)$. The number of trials until a success occurs is given by the geometric distribution, and by equation $\\text{(C.32)}$, the expected number of trials for this scenario is $1 / (2p(1 - p))$. Thus, the expected running time of $\\text{UNBIASED-RANDOM}$ is $\\Theta(1 / (2p(1 - p))$.","title":"5.1 The hiring problem"},{"location":"Chap05/5.1/#51-1","text":"Show that the assumption that we are always able to determine which candidate is best in line 4 of procedure $\\text{HIRE-ASSISTANT}$ implies that we know a total order on the ranks of the candidates. A total order is a partial order that is a total relation $(\\forall a, b \\in A:aRb \\text{ or } bRa)$. A relation is a partial order if it is reflexive, antisymmetric and transitive. Assume that the relation is good or better. Reflexive: This is a bit trivial, but everybody is as good or better as themselves. Transitive: If $A$ is better than $B$ and $B$ is better than $C$, then $A$ is better than $C$. Antisymmetric: If $A$ is better than $B$, then $B$ is not better than $A$. So far we have a partial order. Since we assume we can compare any two candidates, then comparison must be a total relation and thus we have a total order.","title":"5.1-1"},{"location":"Chap05/5.1/#51-2-star","text":"Describe an implementation of the procedure $\\text{RANDOM}(a, b)$ that only makes calls to $\\text{RANDOM}(0, 1)$. What is the expected running time of your procedure, as a function of $a$ and $b$? 1 2 3 4 5 6 7 8 RANDOM ( a , b ) if a == b return a mid = ( a + b ) / 2 r = RANDOM ( 0 , 1 ) if r == 0 return RANDOM ( a , floor ( mid )) else return RANDOM ( ceil ( mid ), b ) The expected running time is $\\Theta(\\lg(b - a))$.","title":"5.1-2 $\\star$"},{"location":"Chap05/5.1/#51-3-star","text":"Suppose that you want to output $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. At your disposal is a procedure $\\text{BIASED-RANDOM}$, that outputs either $0$ or $1$. It outputs $1$ with some probability $p$ and $0$ with probability $1 - p$, where $0 < p < 1$, but you do not know what $p$ is. Give an algorithm that uses $\\text{BIASED-RANDOM}$ as a subroutine, and returns an unbiased answer, returning $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. What is the expected running time of your algorithm as a function of $p$? To get an unbiased random bit, given only calls to $\\text{BIASED-RANDOM}$, call $\\text{BIASED-RANDOM}$ twice. Repeatedly do so until the two calls return different values, and when this occurs, return the first of the two bits: 1 2 3 4 5 6 UNBIASED - ANSWER () while true x = BIASED - RANDOM () y = BIASED - RANDOM () if x != y return x To see that $\\text{UNBIASED-RANDOM}$ returns $0$ and $1$ each with probability $1 / 2$, observe that the probability that a given iteration returns $0$ is $$\\Pr\\{x = 0 \\text{ and } y = 1\\} = (1 - p)p,$$ and the probability that a given iteration returns $1$ is $$\\Pr\\{x = 1 \\text{ and } y = 0\\} = p(1 - p).$$ (We rely on the bits returned by $\\text{BIASED-RANDOM}$ being independent.) Thus, the probability that a given iteration returns $0$ equals the probability that it returns $1$. Since there is no other way for $\\text{UNBIASED-RANDOM}$ to return a value, it returns $0$ and $1$ each with probability $1 / 2$. Assuming that each iteration takes $O(1)$ time, the expected running time of $\\text{UNBIASED-RANDOM}$ is linear in the expected number of iterations. We can view each iteration as a Bernoulli trial, where \"success\" means that the iteration returns a value. The probability of success equals the probability that $0$ is returned plus the probability that $1$ is returned, or $2p(1 - p)$. The number of trials until a success occurs is given by the geometric distribution, and by equation $\\text{(C.32)}$, the expected number of trials for this scenario is $1 / (2p(1 - p))$. Thus, the expected running time of $\\text{UNBIASED-RANDOM}$ is $\\Theta(1 / (2p(1 - p))$.","title":"5.1-3 $\\star$"},{"location":"Chap05/5.2/","text":"5.2-1 In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly one time? What is the probability you hire exactly $n$ times? Since $\\text{HIRE-ASSISTANT}$ always hires candidate 1, it hires exactly once if and only if no candidates other than candidate 1 are hired. This event occurs when candidate 1 is the best candidate of the $n$, which occurs with probability $1 / n$. $\\text{HIRE-ASSISTANT}$ hires $n$ times if each candidate is better than all those who were interviewed (and hired) before. This event occurs precisely when the list of ranks given to the algorithm is $\\langle 1, 2, \\ldots, n \\rangle$, which occurs with probability $1 / n!$. 5.2-2 In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly twice? We make three obervations: Candidate 1 is always hired. The best candidate, i.e., the one whose rank is $n$, is always hired. If the best candidate is candidate 1, then that is the only candidate hired. Therefore, in order for $\\text{HIRE-ASSISTANT}$ to hire exactly twice, candidate 1 must have rank $i \\le n - 1$ and all candidates whose ranks are $i + 1, i + 2, \\ldots, n - 1$ must be interviewed after the candidate whose rank is $n$. (When $i = n - 1$, this second condition vacuously holds.) Let $E_i$ be the event in which candidate 1 has rank $i$; clearly, $\\Pr\\{E_i\\} = 1 / n$ for any given value of $i$. Letting $j$ denote the position in the interview order of the best candidate, let $F$ be the event in which candidates $2, 3, \\ldots, j - 1$ have ranks strictly less than the rank of candidate 1. Given that event $E_i$ has occurred, event $F$ occurs when the best candidate is the first one interviewed out of the $n - i$ candidates whose ranks are $i + 1, i + 2, \\ldots, n$. Thus, $\\Pr\\{F \\mid E_i\\} = 1 / (n - i)$. Our final event is $A$, which occurs when $\\text{HIRE-ASSISTANT}$ hires exactly twice. Noting that the events $E_1, E_2, \\ldots, E_n$ are disjoint, we have $$ \\begin{aligned} A & = F \\cap (E_1 \\cup E_2 \\cup \\cdots \\cup E_{n - 1}) \\\\ & = (F \\cap E_1) \\cup (F \\cap E_2) \\cup \\cdots \\cup (F \\cap E_{n - 1}). \\end{aligned} $$ and $$\\Pr\\{A\\} = \\sum_{i = 1}^{n - 1}\\Pr\\{F \\cap E_i\\}.$$ By equation $\\text{(C.14)}$, $$ \\begin{aligned} \\Pr\\{F \\cap E_i\\} & = \\Pr\\{F | E_i\\}\\Pr\\{E_i\\} \\\\ & = \\frac{1}{n - i} \\cdot \\frac{1}{n}, \\end{aligned} $$ and so $$ \\begin{aligned} \\Pr\\{A\\} & = \\sum_{i = 1}^{n - 1} \\frac{1}{n - i} \\cdot \\frac{1}{n} \\\\ & = \\frac{1}{n} \\sum_{i = 1}^{n - 1} \\frac{1}{n - i} \\\\ & = \\frac{1}{n} \\Big(\\frac{1}{n - 1} + \\frac{1}{n - 2} + \\cdots + \\frac{1}{1}\\Big) \\\\ & = \\frac{1}{n} \\cdot H_{n - 1}, \\end{aligned} $$ where $H_{n - 1}$ is the $n$th harmonic number. 5.2-3 Use indicator random variables to compute the expected value of the sum of $n$ dice. Expectation of a single dice $X_i$ is $$ \\begin{aligned} \\text E[X_k] & = \\sum_{i = 1}^6 i \\Pr\\{X_k = i\\} \\\\ & = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} \\\\ & = \\frac{21}{6} \\\\ & = 3.5. \\end{aligned} $$ As for multiple dices, $$ \\begin{aligned} \\text E[X] & = \\text E\\Bigg[\\sum_{i = 1}^n X_i \\Bigg] \\\\ & = \\sum_{i = 1}^n \\text E[X_i] \\\\ & = \\sum_{i = 1}^n 3.5 \\\\ & = 3.5 \\cdot n. \\end{aligned} $$ 5.2-4 Use indicator random variables to solve the following problem, which is known as the hat-check problem . Each of $n$ customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their hat? Another way to think of the hat-check problem is that we want to determine the expected number of fixed points in a random permutation. (A fixed point of a permutation $\\pi$ is a value $i$ for which $\\pi(i) = i$.) We could enumerate all $n!$ permutations, count the total number of fixed points, and divide by $n!$ to determine the average number of fixed points per permutation. This would be a painstaking process, and the answer would turn out to be $1$. We can use indicator random variables, however, to arrive at the same answer much more easily. Define a random variable $X$ that equals the number of customers that get back their own hat, so that we want to compute $\\text E[X]$. For $i = 1, 2, \\ldots, n$, define the indicator random variable $$X_i = I\\{\\text{customer $i$ gets back his own hat}\\}.$$ Then $X = X_1 + X_2 + \\cdots + X_n$. Since the ordering of hats is random, each customer has a probability of $1 / n$ of getting back his or her own hat. In other words, $\\Pr\\{X_i = 1\\} = 1 / n$, which, by Lemma 5.1, implies that $\\text E[X_i] = 1 / n$. Thus, $$ \\begin{aligned} \\text E[X] & = \\text E \\Bigg[ \\sum_{i = 1}^n X_i \\Bigg] \\\\ & = \\sum_{i = 1}^n \\text E[X_i] & \\text{(linearity of expectation)} \\\\ & = \\sum_{i = 1}^n 1 / n \\\\ & = 1, \\end{aligned} $$ and so we expect that exactly 1 customer gets back his own hat. Note that this is a situation in which the indicator random variables are not independent. For example, if $n = 2$ and $X_1 = 1$, then $X_2$ must also equal $1$. Conversely, if $n = 2$ and $X_1 = 0$, then $X_2$ must also equal $0$. Despite the dependence, $\\Pr\\{X_i = 1\\} = 1 / n$ for all $i$, and linearity of expectation holds. Thus, we can use the technique of indicator random variables even in the presence of dependence. 5.2-5 Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i, j)$ is called an inversion of $A$. (See Problem 2-4 for more on inversions.) Suppose that the elements of $A$ form a uniform random permutation of $\\langle 1, 2, \\ldots, n \\rangle$. Use indicator random variables to compute the expected number of inversions. Let $X_{ij}$ be an indicator random variable for the event where the pair $A[i]$, $A[j]$ for $i < j$ is inverted, i.e., $A[i] > A[j]$. More precisely, we define $X_{ij} = I\\{A[i] > A[j]\\}$ for $1 \\le i < j \\le n$. We have $\\Pr\\{X_{ij} = 1\\} = 1 / 2$, because given two distinct random numbers, the probability that the first is bigger than the second is $1 / 2$. By Lemma 5.1, $E[X_{ij}] = 1 / 2$. Let $X$ be the the random variable denoting the total number of inverted pairs in the array, so that $$X = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij}.$$ We want the expected number of inverted pairs, so we take the expectation of both sides of the above equation to obtain $$\\text E[X] = \\text E \\Bigg[\\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij} \\Bigg].$$ We use linearity of expectation to get $$ \\begin{aligned} \\text E[X] & = \\text E \\Bigg[\\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n X_{ij} \\Bigg] \\\\ & = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\text E[X_{ij}] \\\\ & = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n 1 / 2 \\\\ & = \\binom{n}{2}\\frac{1}{2} \\\\ & = \\frac{n(n - 1)}{2} \\cdot \\frac{1}{2} \\\\ & = \\frac{n(n - 1)}{4}. \\end{aligned} $$ Thus the expected number of inverted pairs is $n(n - 1) / 4$.","title":"5.2 Indicator random variables"},{"location":"Chap05/5.2/#52-1","text":"In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly one time? What is the probability you hire exactly $n$ times? Since $\\text{HIRE-ASSISTANT}$ always hires candidate 1, it hires exactly once if and only if no candidates other than candidate 1 are hired. This event occurs when candidate 1 is the best candidate of the $n$, which occurs with probability $1 / n$. $\\text{HIRE-ASSISTANT}$ hires $n$ times if each candidate is better than all those who were interviewed (and hired) before. This event occurs precisely when the list of ranks given to the algorithm is $\\langle 1, 2, \\ldots, n \\rangle$, which occurs with probability $1 / n!$.","title":"5.2-1"},{"location":"Chap05/5.2/#52-2","text":"In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly twice? We make three obervations: Candidate 1 is always hired. The best candidate, i.e., the one whose rank is $n$, is always hired. If the best candidate is candidate 1, then that is the only candidate hired. Therefore, in order for $\\text{HIRE-ASSISTANT}$ to hire exactly twice, candidate 1 must have rank $i \\le n - 1$ and all candidates whose ranks are $i + 1, i + 2, \\ldots, n - 1$ must be interviewed after the candidate whose rank is $n$. (When $i = n - 1$, this second condition vacuously holds.) Let $E_i$ be the event in which candidate 1 has rank $i$; clearly, $\\Pr\\{E_i\\} = 1 / n$ for any given value of $i$. Letting $j$ denote the position in the interview order of the best candidate, let $F$ be the event in which candidates $2, 3, \\ldots, j - 1$ have ranks strictly less than the rank of candidate 1. Given that event $E_i$ has occurred, event $F$ occurs when the best candidate is the first one interviewed out of the $n - i$ candidates whose ranks are $i + 1, i + 2, \\ldots, n$. Thus, $\\Pr\\{F \\mid E_i\\} = 1 / (n - i)$. Our final event is $A$, which occurs when $\\text{HIRE-ASSISTANT}$ hires exactly twice. Noting that the events $E_1, E_2, \\ldots, E_n$ are disjoint, we have $$ \\begin{aligned} A & = F \\cap (E_1 \\cup E_2 \\cup \\cdots \\cup E_{n - 1}) \\\\ & = (F \\cap E_1) \\cup (F \\cap E_2) \\cup \\cdots \\cup (F \\cap E_{n - 1}). \\end{aligned} $$ and $$\\Pr\\{A\\} = \\sum_{i = 1}^{n - 1}\\Pr\\{F \\cap E_i\\}.$$ By equation $\\text{(C.14)}$, $$ \\begin{aligned} \\Pr\\{F \\cap E_i\\} & = \\Pr\\{F | E_i\\}\\Pr\\{E_i\\} \\\\ & = \\frac{1}{n - i} \\cdot \\frac{1}{n}, \\end{aligned} $$ and so $$ \\begin{aligned} \\Pr\\{A\\} & = \\sum_{i = 1}^{n - 1} \\frac{1}{n - i} \\cdot \\frac{1}{n} \\\\ & = \\frac{1}{n} \\sum_{i = 1}^{n - 1} \\frac{1}{n - i} \\\\ & = \\frac{1}{n} \\Big(\\frac{1}{n - 1} + \\frac{1}{n - 2} + \\cdots + \\frac{1}{1}\\Big) \\\\ & = \\frac{1}{n} \\cdot H_{n - 1}, \\end{aligned} $$ where $H_{n - 1}$ is the $n$th harmonic number.","title":"5.2-2"},{"location":"Chap05/5.2/#52-3","text":"Use indicator random variables to compute the expected value of the sum of $n$ dice. Expectation of a single dice $X_i$ is $$ \\begin{aligned} \\text E[X_k] & = \\sum_{i = 1}^6 i \\Pr\\{X_k = i\\} \\\\ & = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} \\\\ & = \\frac{21}{6} \\\\ & = 3.5. \\end{aligned} $$ As for multiple dices, $$ \\begin{aligned} \\text E[X] & = \\text E\\Bigg[\\sum_{i = 1}^n X_i \\Bigg] \\\\ & = \\sum_{i = 1}^n \\text E[X_i] \\\\ & = \\sum_{i = 1}^n 3.5 \\\\ & = 3.5 \\cdot n. \\end{aligned} $$","title":"5.2-3"},{"location":"Chap05/5.2/#52-4","text":"Use indicator random variables to solve the following problem, which is known as the hat-check problem . Each of $n$ customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their hat? Another way to think of the hat-check problem is that we want to determine the expected number of fixed points in a random permutation. (A fixed point of a permutation $\\pi$ is a value $i$ for which $\\pi(i) = i$.) We could enumerate all $n!$ permutations, count the total number of fixed points, and divide by $n!$ to determine the average number of fixed points per permutation. This would be a painstaking process, and the answer would turn out to be $1$. We can use indicator random variables, however, to arrive at the same answer much more easily. Define a random variable $X$ that equals the number of customers that get back their own hat, so that we want to compute $\\text E[X]$. For $i = 1, 2, \\ldots, n$, define the indicator random variable $$X_i = I\\{\\text{customer $i$ gets back his own hat}\\}.$$ Then $X = X_1 + X_2 + \\cdots + X_n$. Since the ordering of hats is random, each customer has a probability of $1 / n$ of getting back his or her own hat. In other words, $\\Pr\\{X_i = 1\\} = 1 / n$, which, by Lemma 5.1, implies that $\\text E[X_i] = 1 / n$. Thus, $$ \\begin{aligned} \\text E[X] & = \\text E \\Bigg[ \\sum_{i = 1}^n X_i \\Bigg] \\\\ & = \\sum_{i = 1}^n \\text E[X_i] & \\text{(linearity of expectation)} \\\\ & = \\sum_{i = 1}^n 1 / n \\\\ & = 1, \\end{aligned} $$ and so we expect that exactly 1 customer gets back his own hat. Note that this is a situation in which the indicator random variables are not independent. For example, if $n = 2$ and $X_1 = 1$, then $X_2$ must also equal $1$. Conversely, if $n = 2$ and $X_1 = 0$, then $X_2$ must also equal $0$. Despite the dependence, $\\Pr\\{X_i = 1\\} = 1 / n$ for all $i$, and linearity of expectation holds. Thus, we can use the technique of indicator random variables even in the presence of dependence.","title":"5.2-4"},{"location":"Chap05/5.2/#52-5","text":"Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i, j)$ is called an inversion of $A$. (See Problem 2-4 for more on inversions.) Suppose that the elements of $A$ form a uniform random permutation of $\\langle 1, 2, \\ldots, n \\rangle$. Use indicator random variables to compute the expected number of inversions. Let $X_{ij}$ be an indicator random variable for the event where the pair $A[i]$, $A[j]$ for $i < j$ is inverted, i.e., $A[i] > A[j]$. More precisely, we define $X_{ij} = I\\{A[i] > A[j]\\}$ for $1 \\le i < j \\le n$. We have $\\Pr\\{X_{ij} = 1\\} = 1 / 2$, because given two distinct random numbers, the probability that the first is bigger than the second is $1 / 2$. By Lemma 5.1, $E[X_{ij}] = 1 / 2$. Let $X$ be the the random variable denoting the total number of inverted pairs in the array, so that $$X = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij}.$$ We want the expected number of inverted pairs, so we take the expectation of both sides of the above equation to obtain $$\\text E[X] = \\text E \\Bigg[\\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij} \\Bigg].$$ We use linearity of expectation to get $$ \\begin{aligned} \\text E[X] & = \\text E \\Bigg[\\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n X_{ij} \\Bigg] \\\\ & = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\text E[X_{ij}] \\\\ & = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n 1 / 2 \\\\ & = \\binom{n}{2}\\frac{1}{2} \\\\ & = \\frac{n(n - 1)}{2} \\cdot \\frac{1}{2} \\\\ & = \\frac{n(n - 1)}{4}. \\end{aligned} $$ Thus the expected number of inverted pairs is $n(n - 1) / 4$.","title":"5.2-5"},{"location":"Chap05/5.3/","text":"5.3-1 Professor Marceau objects to the loop invariant used in the proof of Lemma 5.5. He questions whether it is true prior to the first iteration. He reasons that we could just as easily declare that an empty subarray contains no $0$-permutations. Therefore, the probability that an empty subarray contains a $0$-permutation should be $0$, thus invalidating the loop invariant prior to the first iteration. Rewrite the procedure $\\text{RANDOMIZE-IN-PLACE}$ so that its associated loop invariant applies to a nonempty subarray prior to the first iteration, and modify the proof of Lemma 5.5 for your procedure. Here's the rewritten procedure: 1 2 3 4 5 RANDOMIZE - IN - PLACE ( A ) n = A . length exchange A [ 1 ] with A [ RANDOM ( 1 , n )] for i = 2 to n exchange A [ i ] with A [ RANDOM ( i , n )] The loop invariant becomes Loop invariant: Just prior to the iteration of the for loop for each value of $i = 2, \\ldots, n$, for each possible $(i - 1)$-permutation, the subarray $A[1..i - 1]$ contains this $(i - 1)$-permutation with probability $(n - i + 1)! / n!$. The maintenance and termination parts remain the same. The initialization part is for the subarray $A[1..1]$, which contains any $1$-permutation with probability $(n - 1)! / n \\ne 1 / n$. 5.3-2 Professor Kelp decides to write a procedure that produces at random any permutation besides the identity permutation. He proposes the following procedure: 1 2 3 4 PERMUTE - WITHOUT - IDENTITY ( A ) n = A . length for i = 1 to n - 1 swap A [ i ] with A [ RANDOM ( i + 1 , n )] Does this code do what Professor Kelp intends? Although $\\text{PERMUTE-WITHOUT-IDENTITY}$ will not produce the identity permutation, there are other permutations that it fails to produce. For example, consider its operation when $n = 3$, when it should be able to produce the $n! - 1 = 5$ nonidentity permutations. The for loop iterates for $i = 1$ and $i = 2$. When $i = 1$, the call to $\\text{RANDOM}$ returns one of two possible values (either $2$ or $3$), and when $i = 2$, the call to $\\text{RANDOM}$ returns just one value $(3)$. Thus, $\\text{PERMUTE-WITHOUT-IDENTITY}$ can produce only $2 \\cdot 1 = 2$ possible permutations, rather than the $5$ that are required. 5.3-3 Suppose that instead of swapping element $A[i]$ with a random element from the subarray $A[i..n]$, we swapped it with a random element from anywhere in the array: 1 2 3 4 PERMUTE - WITH - ALL ( A ) n = A . length for i = 1 to n swap A [ i ] with A [ RANDOM ( 1 , n )] Does this code produce a uniform random permutation? Why or why not? The $\\text{PERMUTE-WITH-ALL}$ procedure does not produce a uniform random permutation. Consider the permutations it produces when $n = 3$. The procedure makes 3 calls to $\\text{RANDOM}$, each of which returns one of 3 values, and so calling $\\text{PERMUTE-WITH-ALL}$ has 27 possible outcomes. Since there are $3! = 6$ permutations, if $\\text{PERMUTE-WITH-ALL}$ did produce a uniform random permutation, then each permutation would occur $1 / 6$ of the time. That would mean that each permutation would have to occur an integer number $m$ times, where $m / 27 = 1 / 6$. No integer $m$ satisfies this condition. In fact, if we were to work out the possible permutations of $\\langle 1, 2, 3 \\rangle$ and how often they occur with $\\text{PERMUTE-WITH-ALL}$, we would get the following probabilities: $$ \\begin{array}{cc} \\text{permutation} & \\text{probability} \\\\ \\hline \\langle 1, 2, 3 \\rangle & 4 / 27 \\\\ \\langle 1, 3, 2 \\rangle & 5 / 27 \\\\ \\langle 2, 1, 3 \\rangle & 5 / 27 \\\\ \\langle 2, 3, 1 \\rangle & 5 / 27 \\\\ \\langle 3, 1, 2 \\rangle & 4 / 27 \\\\ \\langle 3, 2, 1 \\rangle & 4 / 27 \\end{array} $$ Although these probabilities sum to $1$, none are equal to $1 / 6$. 5.3-4 Professor Armstrong suggests the following procedure for generating a uniform random permutation: 1 2 3 4 5 6 7 8 9 10 PERMUTE - BY - CYCLIC ( A ) n = A . length let B [ 1. . n ] be a new array offset = RANDOM ( 1 , n ) for i = 1 to n dest = i + offset if dest > n dest = dest - n B [ dest ] = A [ i ] return B Show that each element $A[i]$ has a $1 / n$ probability of winding up in any particular position in $B$. Then show that Professor Armstrong is mistaken by showing that the resulting permutation is not uniformly random. $\\text{PERMUTE-BY-CYCLE}$ chooses offset as a random integer in the range $1 \\le offset \\le n$, and then it performs a cyclic rotation of the array. That is, $B[((i + offset - 1)\\mod n) + 1] = A[i]$. (The subtraction and addition of $1$ in the index calculation is due to the $1$-origin indexing. If we had used $0$-origin indexing instead, the index calculation would have simplied to $B[(i + offset)\\mod n] = A[i]$ for $i = 0, 1, \\ldots, n - 1$.) Thus, once offset is determined, so is the entire permutation. Since each value of offset occurs with probability $1 / n$, each element $A[i]$ has a probability of ending up in position $B[j]$ with probability $1 / n$. This procedure does not produce a uniform random permutation, however, since it can produce only $n$ different permutations. Thus, $n$ permutations occur with probability $1 / n$, and the remaining $n! - n$ permutations occur with probability $0$. 5.3-5 $\\star$ Prove that in the array $P$ in procedure $\\text{PERMUTE-BY-SORTING}$, the probability that all elements are unique is at least $1 - 1 / n$. Let $\\Pr\\{j\\}$ be the probability that the element with index $j$ is unique. If there are $n^3$ elements, then the $\\Pr\\{j\\} = 1 - \\frac{j - 1}{n^3}$. $$ \\begin{aligned} \\Pr\\{1 \\cap 2 \\cap 3 \\cap \\ldots\\} & = \\Pr\\{1\\} \\cdot \\Pr\\{2 \\mid 1\\} \\cdot \\Pr\\{3 \\mid 1 \\cap 2\\} \\cdots \\\\ & = 1 (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})(1 - \\frac{3}{n^3}) \\cdots \\\\ & \\ge 1 (1 - \\frac{n}{n^3}) (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3}) \\cdots \\\\ & \\ge (1 - \\frac{1}{n^2})^n \\\\ & \\ge 1 - \\frac{1}{n}, \\\\ \\end{aligned} $$ where the last step holds for $(1 - x)^n \\ge 1 - nx$. 5.3-6 Explain how to implement the algorithm $\\text{PERMUTE-BY-SORTING}$ to handle the case in which two or more priorities are identical. That is, your algorithm should produce a uniform random permutation, even if two or more priorities are identical. 1 2 3 4 5 6 PERMUTE - BY - SORTING ( A ) let P [ 1. . n ] be a new array for i = 1 to n P [ i ] = i for i = 1 to n swap P [ i ] with P [ RANDOM ( i , n )] 5.3-7 Suppose we want to create a random sample of the set $\\{1, 2, 3, \\ldots, n\\}$, that is, an $m$-element subset $S$, where $0 \\le m \\le n$, such that each $m$-subset is equally likely to be created. One way would be to set $A[i] = i$ for $i = 1, 2, 3, \\ldots, n$, call $\\text{RANDOMIZE-IN-PLACE}(A)$, and then take just the first $m$ array elements. This method would make $n$ calls to the $\\text{RANDOM}$ procedure. If $n$ is much larger than $m$, we can create a random sample with fewer calls to $\\text{RANDOM}$. Show that the following recursive procedure returns a random $m$-subset $S$ of $\\{1, 2, 3, \\ldots, n\\}$, in which each $m$-subset is equally likely, while making only $m$ calls to $\\text{RANDOM}$: 1 2 3 4 5 6 7 8 9 RANDOM - SAMPLE ( m , n ) if m == 0 return \u00d8 else S = RANDOM - SAMPLE ( m - 1 , n - 1 ) i = RANDOM ( 1 , n ) if i \u2208 S S = S \u222a { n } else S = S \u222a { i } return S Since each recursive call reduces $m$ by $1$ and makes only one call to $\\text{RANDOM}$, it's easy to see that there are a total of $m$ calls to $\\text{RANDOM}$. Moreover, since each recursive call adds exactly one element to the set, it's easy to see that the resulting set $S$ contains exactly $m$ elements. Because the elements of set $S$ are chosen independently of each other, it suffices to show that each of the $n$ values appears in $S$ with probability $m / n$. We use an inductive proof. The inductive hypothesis is that a call to $\\text{RANDOM-SUBSET}(m, n)$ returns a set $S$ of $m$ elements, each appearing with probability $m / n$. The base cases are for $m = 0$ and $m = 1$. When $m = 0$, the returned set is empty, and so it contains each element with probability $0$. When $m = 1$, the returned set has one element, and it is equally likely to be any number in $\\{1, 2, 3, \\ldots, n\\}$ For the inductive step, we assume that the call $\\text{RANDOM-SUBSET}(m - 1, n - 1)$ returns a set $S'$ of $m - 1$ elements in which each value in ${1, 2, 3, \\ldots, n - 1}$ occurs with probability $(m - 1) / (n - 1)$. After the line $i = \\text{RANDOM}(1, n)$, $i$ is equally likely to be any value in ${1, 2, 3, \\ldots, n}$. We consider separately the probabilities that $S$ contains $j < n$ and that $S$ contains $n$. Let $R_j$ be the event that the call $\\text{RANDOM}(1, n)$ returns $j$ , so that $\\Pr\\{R_j\\} = 1 / n$. For $j < n$, the event that $j \\in S$ is the union of two disjoint events: $j \\in S'$, and $j \\notin S'$ and $R_j$ (these events are independent), Thus $$ \\begin{aligned} \\Pr\\{j \\in S\\} & = \\Pr\\{j \\in S'\\} + \\Pr\\{j \\notin S' \\text{ and } R_j\\} & \\text{ (the events are disjoint)} \\\\ & = \\frac{m - 1}{n - 1} + \\Big(1 - \\frac{m - 1}{n - 1}\\Big) \\cdot \\frac{1}{n} & \\text{ (by the inductive hypothesis)} \\\\ & = \\frac{m - 1}{n - 1} + \\Big(\\frac{n - 1}{n - 1} - \\frac{m - 1}{n - 1}\\Big) \\cdot \\frac{1}{n} \\\\ & = \\frac{m - 1}{n - 1} \\cdot \\frac{n}{n} + \\frac{n - m}{n - 1} \\cdot \\frac{1}{n} \\\\ & = \\frac{(m - 1)n + (n - m)}{(n - 1)n} \\\\ & = \\frac{mn - n + n - m}{(n - 1)n} \\\\ & = \\frac{m(n - 1)}{(n - 1)n} \\\\ & = \\frac{m}{n}. \\end{aligned} $$ The event that $n \\in S$ is also the union of two disjoint events: $R_n$, and $R_j$ and $j \\in S'$ for some $j < n$ (these events are independent). $$ \\begin{aligned} \\Pr\\{n \\in S\\} & = \\Pr\\{R_n\\} + \\Pr\\{R_j \\text{ and } j\\in S' \\text{ for some } j < n\\} & \\text{ (the events are disjoint)} \\\\ & = \\frac{1}{n} + \\frac{n - 1}{n} \\cdot \\frac{m - 1}{n - 1} & \\text{ (by the inductive hypothesis)} \\\\ & = \\frac{1}{n} \\cdot \\frac{n - 1}{n - 1} + \\frac{n - 1}{n} \\cdot \\frac{m - 1}{n - 1} \\\\ & = \\frac{n - 1+nm-n - m+1}{n(n - 1)} \\\\ & = \\frac{nm-m}{n(n - 1)} \\\\ & = \\frac{m(n - 1)}{n(n - 1)} \\\\ & = \\frac{m}{n}. \\end{aligned} $$","title":"5.3 Randomized algorithms"},{"location":"Chap05/5.3/#53-1","text":"Professor Marceau objects to the loop invariant used in the proof of Lemma 5.5. He questions whether it is true prior to the first iteration. He reasons that we could just as easily declare that an empty subarray contains no $0$-permutations. Therefore, the probability that an empty subarray contains a $0$-permutation should be $0$, thus invalidating the loop invariant prior to the first iteration. Rewrite the procedure $\\text{RANDOMIZE-IN-PLACE}$ so that its associated loop invariant applies to a nonempty subarray prior to the first iteration, and modify the proof of Lemma 5.5 for your procedure. Here's the rewritten procedure: 1 2 3 4 5 RANDOMIZE - IN - PLACE ( A ) n = A . length exchange A [ 1 ] with A [ RANDOM ( 1 , n )] for i = 2 to n exchange A [ i ] with A [ RANDOM ( i , n )] The loop invariant becomes Loop invariant: Just prior to the iteration of the for loop for each value of $i = 2, \\ldots, n$, for each possible $(i - 1)$-permutation, the subarray $A[1..i - 1]$ contains this $(i - 1)$-permutation with probability $(n - i + 1)! / n!$. The maintenance and termination parts remain the same. The initialization part is for the subarray $A[1..1]$, which contains any $1$-permutation with probability $(n - 1)! / n \\ne 1 / n$.","title":"5.3-1"},{"location":"Chap05/5.3/#53-2","text":"Professor Kelp decides to write a procedure that produces at random any permutation besides the identity permutation. He proposes the following procedure: 1 2 3 4 PERMUTE - WITHOUT - IDENTITY ( A ) n = A . length for i = 1 to n - 1 swap A [ i ] with A [ RANDOM ( i + 1 , n )] Does this code do what Professor Kelp intends? Although $\\text{PERMUTE-WITHOUT-IDENTITY}$ will not produce the identity permutation, there are other permutations that it fails to produce. For example, consider its operation when $n = 3$, when it should be able to produce the $n! - 1 = 5$ nonidentity permutations. The for loop iterates for $i = 1$ and $i = 2$. When $i = 1$, the call to $\\text{RANDOM}$ returns one of two possible values (either $2$ or $3$), and when $i = 2$, the call to $\\text{RANDOM}$ returns just one value $(3)$. Thus, $\\text{PERMUTE-WITHOUT-IDENTITY}$ can produce only $2 \\cdot 1 = 2$ possible permutations, rather than the $5$ that are required.","title":"5.3-2"},{"location":"Chap05/5.3/#53-3","text":"Suppose that instead of swapping element $A[i]$ with a random element from the subarray $A[i..n]$, we swapped it with a random element from anywhere in the array: 1 2 3 4 PERMUTE - WITH - ALL ( A ) n = A . length for i = 1 to n swap A [ i ] with A [ RANDOM ( 1 , n )] Does this code produce a uniform random permutation? Why or why not? The $\\text{PERMUTE-WITH-ALL}$ procedure does not produce a uniform random permutation. Consider the permutations it produces when $n = 3$. The procedure makes 3 calls to $\\text{RANDOM}$, each of which returns one of 3 values, and so calling $\\text{PERMUTE-WITH-ALL}$ has 27 possible outcomes. Since there are $3! = 6$ permutations, if $\\text{PERMUTE-WITH-ALL}$ did produce a uniform random permutation, then each permutation would occur $1 / 6$ of the time. That would mean that each permutation would have to occur an integer number $m$ times, where $m / 27 = 1 / 6$. No integer $m$ satisfies this condition. In fact, if we were to work out the possible permutations of $\\langle 1, 2, 3 \\rangle$ and how often they occur with $\\text{PERMUTE-WITH-ALL}$, we would get the following probabilities: $$ \\begin{array}{cc} \\text{permutation} & \\text{probability} \\\\ \\hline \\langle 1, 2, 3 \\rangle & 4 / 27 \\\\ \\langle 1, 3, 2 \\rangle & 5 / 27 \\\\ \\langle 2, 1, 3 \\rangle & 5 / 27 \\\\ \\langle 2, 3, 1 \\rangle & 5 / 27 \\\\ \\langle 3, 1, 2 \\rangle & 4 / 27 \\\\ \\langle 3, 2, 1 \\rangle & 4 / 27 \\end{array} $$ Although these probabilities sum to $1$, none are equal to $1 / 6$.","title":"5.3-3"},{"location":"Chap05/5.3/#53-4","text":"Professor Armstrong suggests the following procedure for generating a uniform random permutation: 1 2 3 4 5 6 7 8 9 10 PERMUTE - BY - CYCLIC ( A ) n = A . length let B [ 1. . n ] be a new array offset = RANDOM ( 1 , n ) for i = 1 to n dest = i + offset if dest > n dest = dest - n B [ dest ] = A [ i ] return B Show that each element $A[i]$ has a $1 / n$ probability of winding up in any particular position in $B$. Then show that Professor Armstrong is mistaken by showing that the resulting permutation is not uniformly random. $\\text{PERMUTE-BY-CYCLE}$ chooses offset as a random integer in the range $1 \\le offset \\le n$, and then it performs a cyclic rotation of the array. That is, $B[((i + offset - 1)\\mod n) + 1] = A[i]$. (The subtraction and addition of $1$ in the index calculation is due to the $1$-origin indexing. If we had used $0$-origin indexing instead, the index calculation would have simplied to $B[(i + offset)\\mod n] = A[i]$ for $i = 0, 1, \\ldots, n - 1$.) Thus, once offset is determined, so is the entire permutation. Since each value of offset occurs with probability $1 / n$, each element $A[i]$ has a probability of ending up in position $B[j]$ with probability $1 / n$. This procedure does not produce a uniform random permutation, however, since it can produce only $n$ different permutations. Thus, $n$ permutations occur with probability $1 / n$, and the remaining $n! - n$ permutations occur with probability $0$.","title":"5.3-4"},{"location":"Chap05/5.3/#53-5-star","text":"Prove that in the array $P$ in procedure $\\text{PERMUTE-BY-SORTING}$, the probability that all elements are unique is at least $1 - 1 / n$. Let $\\Pr\\{j\\}$ be the probability that the element with index $j$ is unique. If there are $n^3$ elements, then the $\\Pr\\{j\\} = 1 - \\frac{j - 1}{n^3}$. $$ \\begin{aligned} \\Pr\\{1 \\cap 2 \\cap 3 \\cap \\ldots\\} & = \\Pr\\{1\\} \\cdot \\Pr\\{2 \\mid 1\\} \\cdot \\Pr\\{3 \\mid 1 \\cap 2\\} \\cdots \\\\ & = 1 (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})(1 - \\frac{3}{n^3}) \\cdots \\\\ & \\ge 1 (1 - \\frac{n}{n^3}) (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3}) \\cdots \\\\ & \\ge (1 - \\frac{1}{n^2})^n \\\\ & \\ge 1 - \\frac{1}{n}, \\\\ \\end{aligned} $$ where the last step holds for $(1 - x)^n \\ge 1 - nx$.","title":"5.3-5 $\\star$"},{"location":"Chap05/5.3/#53-6","text":"Explain how to implement the algorithm $\\text{PERMUTE-BY-SORTING}$ to handle the case in which two or more priorities are identical. That is, your algorithm should produce a uniform random permutation, even if two or more priorities are identical. 1 2 3 4 5 6 PERMUTE - BY - SORTING ( A ) let P [ 1. . n ] be a new array for i = 1 to n P [ i ] = i for i = 1 to n swap P [ i ] with P [ RANDOM ( i , n )]","title":"5.3-6"},{"location":"Chap05/5.3/#53-7","text":"Suppose we want to create a random sample of the set $\\{1, 2, 3, \\ldots, n\\}$, that is, an $m$-element subset $S$, where $0 \\le m \\le n$, such that each $m$-subset is equally likely to be created. One way would be to set $A[i] = i$ for $i = 1, 2, 3, \\ldots, n$, call $\\text{RANDOMIZE-IN-PLACE}(A)$, and then take just the first $m$ array elements. This method would make $n$ calls to the $\\text{RANDOM}$ procedure. If $n$ is much larger than $m$, we can create a random sample with fewer calls to $\\text{RANDOM}$. Show that the following recursive procedure returns a random $m$-subset $S$ of $\\{1, 2, 3, \\ldots, n\\}$, in which each $m$-subset is equally likely, while making only $m$ calls to $\\text{RANDOM}$: 1 2 3 4 5 6 7 8 9 RANDOM - SAMPLE ( m , n ) if m == 0 return \u00d8 else S = RANDOM - SAMPLE ( m - 1 , n - 1 ) i = RANDOM ( 1 , n ) if i \u2208 S S = S \u222a { n } else S = S \u222a { i } return S Since each recursive call reduces $m$ by $1$ and makes only one call to $\\text{RANDOM}$, it's easy to see that there are a total of $m$ calls to $\\text{RANDOM}$. Moreover, since each recursive call adds exactly one element to the set, it's easy to see that the resulting set $S$ contains exactly $m$ elements. Because the elements of set $S$ are chosen independently of each other, it suffices to show that each of the $n$ values appears in $S$ with probability $m / n$. We use an inductive proof. The inductive hypothesis is that a call to $\\text{RANDOM-SUBSET}(m, n)$ returns a set $S$ of $m$ elements, each appearing with probability $m / n$. The base cases are for $m = 0$ and $m = 1$. When $m = 0$, the returned set is empty, and so it contains each element with probability $0$. When $m = 1$, the returned set has one element, and it is equally likely to be any number in $\\{1, 2, 3, \\ldots, n\\}$ For the inductive step, we assume that the call $\\text{RANDOM-SUBSET}(m - 1, n - 1)$ returns a set $S'$ of $m - 1$ elements in which each value in ${1, 2, 3, \\ldots, n - 1}$ occurs with probability $(m - 1) / (n - 1)$. After the line $i = \\text{RANDOM}(1, n)$, $i$ is equally likely to be any value in ${1, 2, 3, \\ldots, n}$. We consider separately the probabilities that $S$ contains $j < n$ and that $S$ contains $n$. Let $R_j$ be the event that the call $\\text{RANDOM}(1, n)$ returns $j$ , so that $\\Pr\\{R_j\\} = 1 / n$. For $j < n$, the event that $j \\in S$ is the union of two disjoint events: $j \\in S'$, and $j \\notin S'$ and $R_j$ (these events are independent), Thus $$ \\begin{aligned} \\Pr\\{j \\in S\\} & = \\Pr\\{j \\in S'\\} + \\Pr\\{j \\notin S' \\text{ and } R_j\\} & \\text{ (the events are disjoint)} \\\\ & = \\frac{m - 1}{n - 1} + \\Big(1 - \\frac{m - 1}{n - 1}\\Big) \\cdot \\frac{1}{n} & \\text{ (by the inductive hypothesis)} \\\\ & = \\frac{m - 1}{n - 1} + \\Big(\\frac{n - 1}{n - 1} - \\frac{m - 1}{n - 1}\\Big) \\cdot \\frac{1}{n} \\\\ & = \\frac{m - 1}{n - 1} \\cdot \\frac{n}{n} + \\frac{n - m}{n - 1} \\cdot \\frac{1}{n} \\\\ & = \\frac{(m - 1)n + (n - m)}{(n - 1)n} \\\\ & = \\frac{mn - n + n - m}{(n - 1)n} \\\\ & = \\frac{m(n - 1)}{(n - 1)n} \\\\ & = \\frac{m}{n}. \\end{aligned} $$ The event that $n \\in S$ is also the union of two disjoint events: $R_n$, and $R_j$ and $j \\in S'$ for some $j < n$ (these events are independent). $$ \\begin{aligned} \\Pr\\{n \\in S\\} & = \\Pr\\{R_n\\} + \\Pr\\{R_j \\text{ and } j\\in S' \\text{ for some } j < n\\} & \\text{ (the events are disjoint)} \\\\ & = \\frac{1}{n} + \\frac{n - 1}{n} \\cdot \\frac{m - 1}{n - 1} & \\text{ (by the inductive hypothesis)} \\\\ & = \\frac{1}{n} \\cdot \\frac{n - 1}{n - 1} + \\frac{n - 1}{n} \\cdot \\frac{m - 1}{n - 1} \\\\ & = \\frac{n - 1+nm-n - m+1}{n(n - 1)} \\\\ & = \\frac{nm-m}{n(n - 1)} \\\\ & = \\frac{m(n - 1)}{n(n - 1)} \\\\ & = \\frac{m}{n}. \\end{aligned} $$","title":"5.3-7"},{"location":"Chap05/5.4/","text":"5.4-1 How many people must there be in a room before the probability that someone has the same birthday as you do is at least $1 / 2$? How many people must there be before the probability that at least two people have a birthday on July 4 is greater than $1 / 2$? The probability of a person not having the same birthday as me is $(n - 1) / n$. The probability of $k$ people not having the same birthday as me is that, squared. We apply the same approach as the text - we take the complementary event and solve it for $k$, $$ \\begin{aligned} 1 - \\big(\\frac{n - 1}{k}\\big)^k & \\ge \\frac{1}{2} \\\\ \\big(\\frac{n - 1}{k}\\big)^k & \\le \\frac{1}{2} \\\\ k\\lg\\big(\\frac{n - 1}{n}\\big) & \\ge \\lg\\frac{1}{2} \\\\ k = \\frac{\\log(1 / 2)}{\\log(364 / 365)} & \\approx 263. \\end{aligned} $$ As for the other question, $$ \\begin{aligned} \\Pr\\{\\text{2 born on Jul 4}\\} & = 1 - \\Pr\\{\\text{1 born on Jul 4}\\} - \\Pr\\{\\text{0 born on Jul 4}\\} \\\\ & = 1 - \\frac{k}{n}\\big(\\frac{n - 1}{n}\\big)^{k - 1} - \\big(\\frac{n - 1}{n}\\big)^k \\\\ & = 1 - \\big(\\frac{n - 1}{n}\\big)^{k - 1}\\big(\\frac{n + k - 1}{n}\\big). \\end{aligned} $$ Writing a Ruby programme to find the closest integer, we get $115$. 5.4-2 Suppose that we toss balls into $b$ bins until some bin contains two balls. Each toss is independent, and each ball is equally likely to end up in any bin. What is the expected number of ball tosses? This is just a restatement of the birthday problem. I consider this all that needs to be said on this subject. 5.4-3 $\\star$ For the analysis of the birthday paradox, is it important that the birthdays be mutually independent, or is pairwise independence sufficient? Justify your answer. Pairwise independence is enough. It's sufficient for the derivation after $\\text{(5.6)}$. 5.4-4 $\\star$ How many people should be invited to a party in order to make it likely that there are $three$ people with the same birthday? The answer is $88$. I reached it by trial and error. But let's analyze it with indicator random variables. Let $X_{ijk}$ be the indicator random variable for the event of the people with indices $i$, $j$ and $k$ have the same birthday. The probability is $1 / n^2$. Then, $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n X_{ijk} \\\\ & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n \\frac{1}{n^2} \\\\ & = \\binom{n}{3}\\frac{1}{n^2} \\\\ & = \\frac{k(k - 1)(k - 2)}{6n^2}. \\end{aligned} $$ Solving this yields $94$. It's a bit more, but again, indicator random variables are approximate. Finding more commentary online is tricky. 5.4-5 $\\star$ What is the probability that a $k$-string over a set of size $n$ forms a $k$-permutation? How does this question relate to the birthday paradox? $$ \\begin{aligned} \\Pr\\{k\\text{-perm in }n\\} & = 1 \\cdot \\frac{n - 1}{n} \\cdot \\frac{n - 2}{n} \\cdots \\frac{n - k + 1}{n} \\\\ & = \\frac{(n - 1)!}{(n - k)!n^k}. \\end{aligned} $$ This is the complementary event to the birthday problem, that is, the chance of $k$ people have distinct birthdays. 5.4-6 $\\star$ Suppose that $n$ balls are tossed into $n$ bins, where each toss is independent and the ball is equally likely to end up in any bin. What is the expected number of empty bins? What is the expected number of bins with exactly one ball? First we determine the expected number of empty bins. We define a random variable $X$ to be the number of empty bins, so that we want to compute $\\text E[X]$. Next, for $i = 1, 2, \\ldots, n$, we define the indicator random variable $Y_i = I\\{\\text{bin } i \\text{ is empty}\\}$. Thus, $$X = \\sum_{i = 1}^n Y_i,$$ and so $$ \\begin{aligned} \\text E[X] & = \\text E \\Bigg[\\sum\\limits_{i = 1}^n Y_i\\Bigg] & \\\\ & = \\sum\\limits_{i = 1}^n\\text E[Y_i] & \\text{(by linearity of expectation)} \\\\ & = \\sum\\limits_{i = 1}^n\\Pr\\{\\text{bin $i$ is empty}\\}. & \\text{(by Lemma 5.1)} \\end{aligned} $$ Let us focus on a specific bin, say bin $i$. We view a toss as a success if it misses bin $i$ and as a failure if it lands in bin $i$. We have $n$ independent Bernoulli trials, each with probability of success $1 - 1 / n$. In order for bin $i$ to be empty, we need $n$ successes in $n$ trials. Using a binomial distribution, therefore, we have that $$ \\begin{aligned} \\Pr\\{\\text{bin $i$ is empty}\\} & = \\binom{n}{n}\\Big(1 - \\frac{1}{n}\\Big)^n\\Big(\\frac{1}{n}\\Big)^0 \\\\ & = \\Big(1 - \\frac{1}{n}\\Big)^n. \\end{aligned} $$ Thus, $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n\\Big(1 - \\frac{1}{n})^n \\\\ & = n\\Big(1 - \\frac{1}{n}\\Big)^n. \\end{aligned} $$ By equation $\\text{(3.14)}$, as $n$ approaches $\\infty$, the quantity $(1 - 1 / n)^n$ approaches $1/e$, and so $\\text E[X]$ approaches $n/e$. Now we determine the expected number of bins with exactly one ball. We redefine $X$ to be number of bins with exactly one ball, and we redefine $Y_i$ to be $I\\{\\text{bin $i$ gets exactly one ball}\\}$. As before, we find that $$\\text E[X] = \\sum_{i = 1}^n \\Pr\\{\\text{bin $i$ gets exactly one ball}\\}.$$ Again focusing on bin $i$, we need exactly $n - 1$ successes in $n$ independent Bernoulli trials, and so $$ \\begin{aligned} \\Pr\\{\\text{bin $i$ gets exactly one ball}\\} & = \\binom{n}{n - 1} \\Big(1 - \\frac{1}{n}\\Big)^{n - 1} \\Big(\\frac{1}{n}\\Big)^1 \\\\ & = n \\cdot \\Big(1 - \\frac{1}{n} \\Big)^{n - 1} \\frac{1}{n} \\\\ & = \\Big(1 - \\frac{1}{n})^{n - 1}, \\end{aligned} $$ and so $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n \\Big(1 - \\frac{1}{n}\\Big)^{n - 1} \\\\ & = n \\Big(1 - \\frac{1}{n})^{n - 1}. \\end{aligned} $$ Because $$n\\Big(1 - \\frac{1}{n}\\Big)^{n - 1} = \\frac{n(1 - \\frac{1}{n})^n}{1 - \\frac{1}{n}},$$ as $n$ approaches $\\infty$, we find that $\\text E[X]$ approaches $$\\frac{n / e}{1 - 1 / n} = \\frac{n^2}{e(n - 1)}.$$ 5.4-7 $\\star$ Sharpen the lower bound on streak length by showing that in $n$ flips of a fair coin, the probability is less than $1 / n$ that no streak longer than $\\lg n - 2\\lg\\lg n$ consecutive heads occurs. We split up the n flips into $n / s$ groups where we pick $s = \\lg(n) - 2 \\lg(\\lg(n))$. We will show that at least one of these groups comes up all heads with probability at least $\\frac{n - 1}{n}$. So, the probability the group starting in position $i$ comes up all heads is $$\\Pr(A_{i,\\lg n - 2\\lg(\\lg n)}) = \\frac{1}{2^{\\lg n - 2\\lg(\\lg n)}} = \\frac{\\lg n^2}{n}.$$ Since the groups are based of of disjoint sets of IID coin flips, these probabilities are independent. so, $$ \\begin{aligned} \\Pr(\\bigwedge\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) & = \\prod_i\\Pr(\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) \\\\ & = \\Big(1-\\frac{\\lg n^2}{n}\\Big)^{\\frac{n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & \\le e^{-\\frac{\\lg n^2}{\\lg n - 2\\lg(\\lg n)}} \\\\ &= \\frac{1}{n} e^{\\frac{-2\\lg(\\lg n)\\lg n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & = n^{-1-\\frac{2\\lg(\\lg n)}{\\lg n - 2\\lg(\\lg n)}} \\\\ & < n^{-1}. \\end{aligned} $$ Showing that the probability that there is no run of length at least $\\lg n - 2\\lg(\\lg n)$ to be $< \\frac{1}{n}$.","title":"5.4 Probabilistic analysis and further uses of indicator random variables"},{"location":"Chap05/5.4/#54-1","text":"How many people must there be in a room before the probability that someone has the same birthday as you do is at least $1 / 2$? How many people must there be before the probability that at least two people have a birthday on July 4 is greater than $1 / 2$? The probability of a person not having the same birthday as me is $(n - 1) / n$. The probability of $k$ people not having the same birthday as me is that, squared. We apply the same approach as the text - we take the complementary event and solve it for $k$, $$ \\begin{aligned} 1 - \\big(\\frac{n - 1}{k}\\big)^k & \\ge \\frac{1}{2} \\\\ \\big(\\frac{n - 1}{k}\\big)^k & \\le \\frac{1}{2} \\\\ k\\lg\\big(\\frac{n - 1}{n}\\big) & \\ge \\lg\\frac{1}{2} \\\\ k = \\frac{\\log(1 / 2)}{\\log(364 / 365)} & \\approx 263. \\end{aligned} $$ As for the other question, $$ \\begin{aligned} \\Pr\\{\\text{2 born on Jul 4}\\} & = 1 - \\Pr\\{\\text{1 born on Jul 4}\\} - \\Pr\\{\\text{0 born on Jul 4}\\} \\\\ & = 1 - \\frac{k}{n}\\big(\\frac{n - 1}{n}\\big)^{k - 1} - \\big(\\frac{n - 1}{n}\\big)^k \\\\ & = 1 - \\big(\\frac{n - 1}{n}\\big)^{k - 1}\\big(\\frac{n + k - 1}{n}\\big). \\end{aligned} $$ Writing a Ruby programme to find the closest integer, we get $115$.","title":"5.4-1"},{"location":"Chap05/5.4/#54-2","text":"Suppose that we toss balls into $b$ bins until some bin contains two balls. Each toss is independent, and each ball is equally likely to end up in any bin. What is the expected number of ball tosses? This is just a restatement of the birthday problem. I consider this all that needs to be said on this subject.","title":"5.4-2"},{"location":"Chap05/5.4/#54-3-star","text":"For the analysis of the birthday paradox, is it important that the birthdays be mutually independent, or is pairwise independence sufficient? Justify your answer. Pairwise independence is enough. It's sufficient for the derivation after $\\text{(5.6)}$.","title":"5.4-3 $\\star$"},{"location":"Chap05/5.4/#54-4-star","text":"How many people should be invited to a party in order to make it likely that there are $three$ people with the same birthday? The answer is $88$. I reached it by trial and error. But let's analyze it with indicator random variables. Let $X_{ijk}$ be the indicator random variable for the event of the people with indices $i$, $j$ and $k$ have the same birthday. The probability is $1 / n^2$. Then, $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n X_{ijk} \\\\ & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n \\frac{1}{n^2} \\\\ & = \\binom{n}{3}\\frac{1}{n^2} \\\\ & = \\frac{k(k - 1)(k - 2)}{6n^2}. \\end{aligned} $$ Solving this yields $94$. It's a bit more, but again, indicator random variables are approximate. Finding more commentary online is tricky.","title":"5.4-4 $\\star$"},{"location":"Chap05/5.4/#54-5-star","text":"What is the probability that a $k$-string over a set of size $n$ forms a $k$-permutation? How does this question relate to the birthday paradox? $$ \\begin{aligned} \\Pr\\{k\\text{-perm in }n\\} & = 1 \\cdot \\frac{n - 1}{n} \\cdot \\frac{n - 2}{n} \\cdots \\frac{n - k + 1}{n} \\\\ & = \\frac{(n - 1)!}{(n - k)!n^k}. \\end{aligned} $$ This is the complementary event to the birthday problem, that is, the chance of $k$ people have distinct birthdays.","title":"5.4-5 $\\star$"},{"location":"Chap05/5.4/#54-6-star","text":"Suppose that $n$ balls are tossed into $n$ bins, where each toss is independent and the ball is equally likely to end up in any bin. What is the expected number of empty bins? What is the expected number of bins with exactly one ball? First we determine the expected number of empty bins. We define a random variable $X$ to be the number of empty bins, so that we want to compute $\\text E[X]$. Next, for $i = 1, 2, \\ldots, n$, we define the indicator random variable $Y_i = I\\{\\text{bin } i \\text{ is empty}\\}$. Thus, $$X = \\sum_{i = 1}^n Y_i,$$ and so $$ \\begin{aligned} \\text E[X] & = \\text E \\Bigg[\\sum\\limits_{i = 1}^n Y_i\\Bigg] & \\\\ & = \\sum\\limits_{i = 1}^n\\text E[Y_i] & \\text{(by linearity of expectation)} \\\\ & = \\sum\\limits_{i = 1}^n\\Pr\\{\\text{bin $i$ is empty}\\}. & \\text{(by Lemma 5.1)} \\end{aligned} $$ Let us focus on a specific bin, say bin $i$. We view a toss as a success if it misses bin $i$ and as a failure if it lands in bin $i$. We have $n$ independent Bernoulli trials, each with probability of success $1 - 1 / n$. In order for bin $i$ to be empty, we need $n$ successes in $n$ trials. Using a binomial distribution, therefore, we have that $$ \\begin{aligned} \\Pr\\{\\text{bin $i$ is empty}\\} & = \\binom{n}{n}\\Big(1 - \\frac{1}{n}\\Big)^n\\Big(\\frac{1}{n}\\Big)^0 \\\\ & = \\Big(1 - \\frac{1}{n}\\Big)^n. \\end{aligned} $$ Thus, $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n\\Big(1 - \\frac{1}{n})^n \\\\ & = n\\Big(1 - \\frac{1}{n}\\Big)^n. \\end{aligned} $$ By equation $\\text{(3.14)}$, as $n$ approaches $\\infty$, the quantity $(1 - 1 / n)^n$ approaches $1/e$, and so $\\text E[X]$ approaches $n/e$. Now we determine the expected number of bins with exactly one ball. We redefine $X$ to be number of bins with exactly one ball, and we redefine $Y_i$ to be $I\\{\\text{bin $i$ gets exactly one ball}\\}$. As before, we find that $$\\text E[X] = \\sum_{i = 1}^n \\Pr\\{\\text{bin $i$ gets exactly one ball}\\}.$$ Again focusing on bin $i$, we need exactly $n - 1$ successes in $n$ independent Bernoulli trials, and so $$ \\begin{aligned} \\Pr\\{\\text{bin $i$ gets exactly one ball}\\} & = \\binom{n}{n - 1} \\Big(1 - \\frac{1}{n}\\Big)^{n - 1} \\Big(\\frac{1}{n}\\Big)^1 \\\\ & = n \\cdot \\Big(1 - \\frac{1}{n} \\Big)^{n - 1} \\frac{1}{n} \\\\ & = \\Big(1 - \\frac{1}{n})^{n - 1}, \\end{aligned} $$ and so $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n \\Big(1 - \\frac{1}{n}\\Big)^{n - 1} \\\\ & = n \\Big(1 - \\frac{1}{n})^{n - 1}. \\end{aligned} $$ Because $$n\\Big(1 - \\frac{1}{n}\\Big)^{n - 1} = \\frac{n(1 - \\frac{1}{n})^n}{1 - \\frac{1}{n}},$$ as $n$ approaches $\\infty$, we find that $\\text E[X]$ approaches $$\\frac{n / e}{1 - 1 / n} = \\frac{n^2}{e(n - 1)}.$$","title":"5.4-6 $\\star$"},{"location":"Chap05/5.4/#54-7-star","text":"Sharpen the lower bound on streak length by showing that in $n$ flips of a fair coin, the probability is less than $1 / n$ that no streak longer than $\\lg n - 2\\lg\\lg n$ consecutive heads occurs. We split up the n flips into $n / s$ groups where we pick $s = \\lg(n) - 2 \\lg(\\lg(n))$. We will show that at least one of these groups comes up all heads with probability at least $\\frac{n - 1}{n}$. So, the probability the group starting in position $i$ comes up all heads is $$\\Pr(A_{i,\\lg n - 2\\lg(\\lg n)}) = \\frac{1}{2^{\\lg n - 2\\lg(\\lg n)}} = \\frac{\\lg n^2}{n}.$$ Since the groups are based of of disjoint sets of IID coin flips, these probabilities are independent. so, $$ \\begin{aligned} \\Pr(\\bigwedge\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) & = \\prod_i\\Pr(\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) \\\\ & = \\Big(1-\\frac{\\lg n^2}{n}\\Big)^{\\frac{n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & \\le e^{-\\frac{\\lg n^2}{\\lg n - 2\\lg(\\lg n)}} \\\\ &= \\frac{1}{n} e^{\\frac{-2\\lg(\\lg n)\\lg n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & = n^{-1-\\frac{2\\lg(\\lg n)}{\\lg n - 2\\lg(\\lg n)}} \\\\ & < n^{-1}. \\end{aligned} $$ Showing that the probability that there is no run of length at least $\\lg n - 2\\lg(\\lg n)$ to be $< \\frac{1}{n}$.","title":"5.4-7 $\\star$"},{"location":"Chap05/Problems/5-1/","text":"With a $b$-bit counter, we can ordinarily only count up to $2^b - 1$. With R. Morris's probabilistic counting , we can count up to a much larger value at the expense of some loss of precision. We let a counter value of $i$ represent that a count of $n_i$ for $i = 0, 1, \\ldots, 2^b - 1$, where the $n_i$ form an increasing sequence of nonnegative values. We assume that the initial value of the counter is $0$, representing a count of $n_0 = 0$. The $\\text{INCREMENT}$ operation works on a counter containing the value $i$ in a probabilistic manner. If $i = 2^b - 1$, then the operation reports an overflow error. Otherwise, the $\\text{INCREMENT}$ operation increases the counter by $1$ with probability $1 / (n_{i + 1} - n_i)$, and it leaves the counter unchanged with probability $1 - 1 / (n_{i + 1} - n_i)$. If we select $n_i = i$ for all $i \\ge 0$, then the counter is an ordinary one. More interesting situations arise if we select, say, $n_i = 2^{i - 1}$ for $i > 0$ or $n_i = F_i$ (the $i$th Fibonacci number\u2014see Section 3.2). For this problem, assume that $n_{2^b - 1}$ is large enough that the probability of an overflow error is negligible. a. Show that the expected value represented by the counter after $n$ $\\text{INCREMENT}$ operations have been performed is exactly $n$. b. The analysis of the variance of the count represented by the counter depends on the sequence of the $n_i$. Let us consider a simple case: $n_i = 100i$ for all $i \\ge 0$. Estimate the variance in the value represented by the register after $n$ $\\text{INCREMENT}$ operations have been performed. a. To determine the expected value represented by the counter after $n$ $\\text{INCREMENT}$ operations, we define some random variables: For $j = 1, 2, \\ldots, n$, let $X_j$ denote the increase in the value represented by the counter due to the $j$th $\\text{INCREMENT}$ operation. Let $V_n$ be the value represented by the counter after $n$ $\\text{INCREMENT}$ operations. Then $V_n = X_1 + X_2 + \\cdots + X_n$. We want to compute $\\text E[V_n]$. By linearity of expection, $$ \\begin{aligned} \\text E[V_n] & = \\text E[X_1 + X_2 + \\cdots + X_n] \\\\ & = \\text E[X_1] + \\text E[X_2] + \\cdots + \\text E[X_n]. \\end{aligned} $$ We shall show that $\\text E[X_j] = 1$ for $j = 1, 2, \\ldots, n$, which will prove that $\\text E[V_n] = n$. We actually show that $\\text E[X_j] = 1$ in two ways, the second more rigorous than the first: Suppose that at the start of the $j$th $\\text{INCREMENT}$ operation, the counter holds the value $i$, which represents $n_i$. If the counter increases due to this $\\text{INCREMENT}$ operation, then the value it represents increases by $n_{i + 1} - n_i$. The counter increases with probability $1 / (n_{i + 1} - n_i)$, and so $$ \\begin{aligned} \\text E[X_j] & = (0 \\cdot \\Pr\\{\\text{counter does not increase}\\}) + ((n_{i + 1} - n_i) \\cdot \\Pr\\{\\text{counter increases}\\}) \\\\ & = \\Big(0 \\cdot\\Big(1 - \\frac{1}{n_{i + 1} - n_i}\\Big)\\Big) + \\Big((n_{i + 1} - n_i) \\cdot \\frac{1}{n_{i + 1} - n_i}\\Big) \\\\ & = 1, \\end{aligned} $$ and so $\\text E[X_j] = 1$ regardless of the value held by the counter. Let $C_j$ be the random variable denoting the value held in the counter at the start of the $j$th $\\text{INCREMENT}$ operation. Since we can ignore values of $C_j$ greater than $2^b - 1$, we use a formula for conditional expectation: $$ \\begin{aligned} \\text E[X_j] & = \\text E[\\text E[X_j\\mid C_j]] \\\\ & = \\sum_{i = 0}^{2^b - 1} \\text E[X_j \\mid C_j = i] \\cdot \\Pr\\{C_j = i\\}. \\end{aligned} $$ To compute $\\text E[X_j \\mid C_j = i]$, we note that $\\Pr\\{X_j = 0 \\mid C_j = i\\} = 1 - 1 / (n_{i + 1} - n_i)$, $\\Pr\\{X_j = n_{i + 1} - n_i \\mid C_j = i\\} = 1 / (n_{i + 1} - n_i)$, and $\\Pr\\{X_j = k \\mid C_j = i\\} = 0$ for all other $k$. Thus, $$ \\begin{aligned} \\text E[X_j \\mid C_j = i] & = \\sum_k k \\cdot \\Pr\\{X_j = k \\mid C_j = i\\} \\\\ & = \\Big(0 \\cdot \\Big(1 - \\frac{1}{n_{i + 1} - n_i}\\Big)\\Big) + \\Big((n_{i + 1} - n_i) \\cdot \\frac{1}{n_{i + 1} - n_i}\\Big) \\\\ & = 1. \\end{aligned} $$ Therefore, noting that $$\\sum_{i = 0}^{2^b - 1}\\Pr\\{C_j = i\\} = 1,$$ we have $$ \\begin{aligned} \\text E[X_j] & = \\sum_{i = 0}^{2^b - 1}1 \\cdot \\Pr\\{C_j = i\\} \\\\ & = 1. \\end{aligned} $$ Why is the second way more rigorous than the first? Both ways condition on the value held in the counter, but only the second way incorporates the conditioning into the expression for $\\text E[X_j]$. b. Defining $V_n$ and $X_j$ as in part (a), we want to compute $\\text{Var}[V_n]$, where $n_i = 100i$. The $X_j$ are pairwise independent, and so by equation $\\text{(C.29)}$, $$\\text{Var[$V_n$]} = \\text{Var[$X_1$]} + \\text{Var[$X_2$]} + \\cdots + \\text{Var[$X_n$]}.$$ Since $n_i = 100i$, we see that $n_{i + 1} - n_i = 100(i + 1) - 100i = 100$. Therefore, with probability $99 / 100$, the increase in the value represented by the counter due to the $j$th $\\text{INCREMENT}$ operation is $0$, and with probability $1 / 100$, the value represented increases by $100$. Thus, by equation $\\text{(C.27)}$, $$ \\begin{aligned} \\text{Var[$X_j$]} & = \\text E[X_j^2] - \\text E^2[X_j] \\\\ & = \\Big(\\Big(0^2 \\cdot \\frac{99}{100}\\Big) + \\Big(100^2 \\cdot \\frac{1}{100}\\Big)\\Big) - 1^2 \\\\ & = 100 - 1 \\\\ & = 99. \\end{aligned} $$ Summing up the variances of the $X_j$ gives $\\text{Var}[V_n] = 99n$.","title":"5-1 Probabilstic counting"},{"location":"Chap05/Problems/5-2/","text":"The problem examines three algorithms for searching for a value $x$ in an unsorted array $A$ consisting for $n$ elements. Consider the following randomized strategy: pick a random index $i$ into $A$. If $A[i] = x$, then we terminate; otherwise, we continue the search by picking a new random index into $A$. We continue picking random indices into $A$ until we find an index $j$ such that $A[j] = x$ or until we have checked every element of $A$. Note that we pick from the whole set of indices each time, so that we may examine a given element more than once. a. Write pseudocode for a procedure $\\text{RANDOM-SEARCH}$ to implement the strategy above. Be sure that your algorithm terminates when all indices into $A$ have been picked. b. Suppose that there is exactly one index $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that we must pick before we find $x$ and $\\text{RANDOM-SEARCH}$ terminates? c. Generalizing your solution to part (b), suppose that there are $k \\ge 1$ indices $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that we must pick before we find $x$ and $\\text{RANDOM-SEARCH}$ terminates? Your answer should be a function of $n$ and $k$. d. Suppose that there are no indices $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that we must pick before we have checked all elements of $A$ and $\\text{RANDOM-SEARCH}$ terminates? Now consider a deterministic linear search algorithm, which we refer to as $\\text{DETERMINISTIC-SEARCH}$. Specifically, the algorithm searches $A$ for $x$ in order, considering $A[1], A[2], A[3], \\ldots, A[n]$ until either it finds $A[i] = x$ or it reaches the end of the array. Assume that possible permutations of the input array are equally likely. e. Suppose that there is exactly one index $i$ such that $A[i] = x$. What is the average-case running time of $\\text{DETERMINISTIC-SEARCH}$? What is the worst-case running time of $\\text{DETERMINISTIC-SEARCH}$? f. Generalizing your solution to part (e), suppose that there are $k \\ge 1$ indices $i$ such that $A[i] = x$. What is the average-case running time of $\\text{DETERMINISTIC-SEARCH}$? What is the worst-case running time of $\\text{DETERMINISTIC-SEARCH}$? Your answer should be a function of $n$ and $k$. g. Suppose that there are no indices $i$ such that $A[i] = x$. What is the average-case running time of $\\text{DETERMINISTIC-SEARCH}$? What is the worst-case running time of $\\text{DETERMINISTIC-SEARCH}$? Finally, consider a randomized algorithm $\\text{SCRAMBLE-SEARCH}$ that works by first randomly permuting the input array and then running the deterministic linear search given above on the resulting permuting array. h. Letting $k$ be the number of indices $i$ such that $A[i] = x$, give the worst-case and expected running time of $\\text{SCRAMBLE-SEARCH}$ for the cases in which $k = 0$ and $k = 1$. Generalizing your solution to handle the case in which $k \\ge 1$. i. Which of the three searching algorithms would you use? Explain your answer. a. 1 2 3 4 5 6 7 8 9 RANDOM - SEARCH ( x , A , n ) v = \u00d8 while | \u00d8 | != n i = RANDOM ( 1 , n ) if A [ i ] = x return i else v = v \u2229 i return NIL $v$ can be implemented in multiple ways: a hash table, a tree or a bitmap. The last one would probabily perform best and consume the least space. b. $\\text{RANDOM-SEARCH}$ is well-modelled by Bernoulli trials. The expected number of picks is $n$. c. In similar fashion, the expected number of picks is $n / k$. d. This is modelled by the balls and bins problem, explored in section 5.4.2. The answer is $n(\\ln n + O(1))$. e. The worst-case running time is $n$. The average-case is $(n + 1) / 2$ (obviously). f. The worst-case running time is $n - k + 1$. The average-case running time is $(n + 1) / (k + 1)$. Let $X_i$ be an indicator random variable that the $i$th element is a match. $\\Pr\\{X_i\\} = 1 / (k + 1)$. Let $Y$ be an indicator random variable that we have found a match after the first $n - k + 1$ elements ($\\Pr\\{Y\\} = 1$). Thus, $$ \\begin{aligned} \\text E[X] & = \\text E[X_1 + X_2 + \\ldots + X_{n - k} + Y] \\\\ & = 1 + \\sum_{i = 1}^{n - k}\\text E[X_i] = 1 + \\frac{n - k}{k + 1} \\\\ & = \\frac{n + 1}{k + 1}. \\end{aligned} $$ g. Both the worst-case and average case is $n$. h. It's the same as $\\text{DETERMINISTIC-SEARCH}$, only we replace \"average-case\" with \"expected\". i. Definitelly $\\text{DETERMINISTIC-SEARCH}$. $\\text{SCRAMBLE-SEARCH}$ gives better expected results, but for the cost of randomly permuting the array, which is a linear operation. In the same time we could have scanned the full array and reported a result.","title":"5-2 Searching an unsorted array"},{"location":"Chap06/6.1/","text":"6.1-1 What are the minimum and maximum numbers of elements in a heap of height $h$? Since a heap is an almost-complete binary tree (complete at all levels except possibly the lowest), it has at most $2^{h + 1} - 1$ elements (if it is complete) and at least $2^h - 1 + 1 = 2^h$ elements (if the lowest level has just $1$ element and the other levels are complete). 6.1-2 Show that an $n$-element heap has height $\\lfloor \\lg n \\rfloor$. Given an $n$-element heap of height $h$, we know from Exercise 6.1-1 that $$2^h \\le n \\le 2^{h + 1} - 1 < 2^{h + 1}.$$ Thus, $h \\le \\lg n < h + 1$. Since $h$ is an integer, $h = \\lfloor \\lg n \\rfloor$ (by definition of $\\lfloor \\rfloor$). 6.1-3 Show that in any subtree of a max-heap, the root of the subtree contains the largest value occuring anywhere in the subtree. Assume the claim is false\u2014i.e., that there is a subtree whose root is not the largest element in the subtree. Then the maximum element is somewhere else in the subtree, possibly even at more than one location. Let $m$ be the index at which the maximum appears (the lowest such index if the maximum appears more than once). Since the maximum is not at the root of the subtree, node $m$ has a parent. Since the parent of a node has a lower index than the node, and $m$ was chosen to be the smallest index of the maximum value, $A[\\text{PARENT}(m)] < A[m]$. But by the maxheap property, we must have $A[\\text{PARENT}(m)] \\ge A[m]$. So our assumption is false, and the claim is true. 6.1-4 Where in a max-heap might the smallest element reside, assuming that all elements are distinct? In any of the leaves, that is, elements with index $\\lfloor n / 2 \\rfloor + 1$ (see exercise 6.1-7), that is, in the second half of the heap array. 6.1-5 Is an array that is in sorted order a min-heap? Yes. For any index $i$, both $\\text{LEFT}(i)$ and $\\text{RIGHT}(i)$ are larger and thus the elements indexed by them are greater or equal to $A[i]$ (because the array is sorted.) 6.1-6 Is the array with values $\\langle 23, 17, 14, 6, 13, 10, 1, 5, 7, 12 \\rangle$ a max-heap? No. Since $\\text{PARENT}(7)$ is $6$ in the array. This violates the max-heap property. 6.1-7 Show that, with the array representation for sorting an $n$-element heap, the leaves are the nodes indexed by $\\lfloor n / 2 \\rfloor + 1, \\lfloor n / 2 \\rfloor + 2, \\ldots, n$. Let's take the left child of the node indexed by $\\lfloor n / 2 \\rfloor + 1$. $$ \\begin{aligned} \\text{LEFT}(\\lfloor n / 2 \\rfloor + 1) & = 2(\\lfloor n / 2 \\rfloor + 1) \\\\ & > 2(n / 2 - 1) + 2 \\\\ & = n - 2 + 2 \\\\ & = n. \\end{aligned} $$ Since the index of the left child is larger than the number of elements in the heap, the node doesn't have childrens and thus is a leaf. Same goes for all nodes with larger indices. Note that if we take element indexed by $\\lfloor n / 2 \\rfloor$, it will not be a leaf. In case of even number of nodes, it will have a left child with index $n$ and in the case of odd number of nodes, it will have a left child with index $n - 1$ and a right child with index $n$. This makes the number of leaves in a heap of size $n$ equal to $\\lceil n / 2 \\rceil$.","title":"6.1 Heaps"},{"location":"Chap06/6.1/#61-1","text":"What are the minimum and maximum numbers of elements in a heap of height $h$? Since a heap is an almost-complete binary tree (complete at all levels except possibly the lowest), it has at most $2^{h + 1} - 1$ elements (if it is complete) and at least $2^h - 1 + 1 = 2^h$ elements (if the lowest level has just $1$ element and the other levels are complete).","title":"6.1-1"},{"location":"Chap06/6.1/#61-2","text":"Show that an $n$-element heap has height $\\lfloor \\lg n \\rfloor$. Given an $n$-element heap of height $h$, we know from Exercise 6.1-1 that $$2^h \\le n \\le 2^{h + 1} - 1 < 2^{h + 1}.$$ Thus, $h \\le \\lg n < h + 1$. Since $h$ is an integer, $h = \\lfloor \\lg n \\rfloor$ (by definition of $\\lfloor \\rfloor$).","title":"6.1-2"},{"location":"Chap06/6.1/#61-3","text":"Show that in any subtree of a max-heap, the root of the subtree contains the largest value occuring anywhere in the subtree. Assume the claim is false\u2014i.e., that there is a subtree whose root is not the largest element in the subtree. Then the maximum element is somewhere else in the subtree, possibly even at more than one location. Let $m$ be the index at which the maximum appears (the lowest such index if the maximum appears more than once). Since the maximum is not at the root of the subtree, node $m$ has a parent. Since the parent of a node has a lower index than the node, and $m$ was chosen to be the smallest index of the maximum value, $A[\\text{PARENT}(m)] < A[m]$. But by the maxheap property, we must have $A[\\text{PARENT}(m)] \\ge A[m]$. So our assumption is false, and the claim is true.","title":"6.1-3"},{"location":"Chap06/6.1/#61-4","text":"Where in a max-heap might the smallest element reside, assuming that all elements are distinct? In any of the leaves, that is, elements with index $\\lfloor n / 2 \\rfloor + 1$ (see exercise 6.1-7), that is, in the second half of the heap array.","title":"6.1-4"},{"location":"Chap06/6.1/#61-5","text":"Is an array that is in sorted order a min-heap? Yes. For any index $i$, both $\\text{LEFT}(i)$ and $\\text{RIGHT}(i)$ are larger and thus the elements indexed by them are greater or equal to $A[i]$ (because the array is sorted.)","title":"6.1-5"},{"location":"Chap06/6.1/#61-6","text":"Is the array with values $\\langle 23, 17, 14, 6, 13, 10, 1, 5, 7, 12 \\rangle$ a max-heap? No. Since $\\text{PARENT}(7)$ is $6$ in the array. This violates the max-heap property.","title":"6.1-6"},{"location":"Chap06/6.1/#61-7","text":"Show that, with the array representation for sorting an $n$-element heap, the leaves are the nodes indexed by $\\lfloor n / 2 \\rfloor + 1, \\lfloor n / 2 \\rfloor + 2, \\ldots, n$. Let's take the left child of the node indexed by $\\lfloor n / 2 \\rfloor + 1$. $$ \\begin{aligned} \\text{LEFT}(\\lfloor n / 2 \\rfloor + 1) & = 2(\\lfloor n / 2 \\rfloor + 1) \\\\ & > 2(n / 2 - 1) + 2 \\\\ & = n - 2 + 2 \\\\ & = n. \\end{aligned} $$ Since the index of the left child is larger than the number of elements in the heap, the node doesn't have childrens and thus is a leaf. Same goes for all nodes with larger indices. Note that if we take element indexed by $\\lfloor n / 2 \\rfloor$, it will not be a leaf. In case of even number of nodes, it will have a left child with index $n$ and in the case of odd number of nodes, it will have a left child with index $n - 1$ and a right child with index $n$. This makes the number of leaves in a heap of size $n$ equal to $\\lceil n / 2 \\rceil$.","title":"6.1-7"},{"location":"Chap06/6.2/","text":"6.2-1 Using figure 6.2 as a model, illustrate the operation of $\\text{MAX-HEAPIFY}(A, 3)$ on the array $A = \\langle 27, 17, 3, 16, 13, 10, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle$. $$ \\begin{aligned} \\langle 27, 17, 3, 16, 13, 10,1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 3, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 9, 1, 5, 7, 12, 4, 8, 3, 0 \\rangle \\\\ \\end{aligned} $$ 6.2-2 Starting with the procedure $\\text{MAX-HEAPIFY}$, write pseudocode for the procedure $\\text{MIN-HEAPIFY}(A, i)$, which performs the corresponding manipulation on a min-heap. How does the running time of $\\text{MIN-HEAPIFY}$ compare to that of $\\text{MAX-HEAPIFY}$? 1 2 3 4 5 6 7 8 9 10 11 MIN - HEAPIFY ( A , i ) l = LEFT ( i ) r = RIGHT ( i ) if l \u2264 A . heap - size and A [ l ] < A [ i ] smallest = l else smallest = i if r \u2264 A . heap - size and A [ r ] < A [ smallest ] smallest = r if smallest != i exchange A [ i ] with A [ smallest ] MIN - HEAPIFY ( A , smallest ) The running time is the same. Actually, the algorithm is the same with the exceptions of two comparisons and some names. 6.2-3 What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ when the element $A[i]$ is larger than its children? No effect. The comparisons are carried out, $A[i]$ is found to be largest and the procedure just returns. 6.2-4 What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ for $i > A.heap\\text-size / 2$? No effect. In that case, it is a leaf. Both $\\text{LEFT}$ and $\\text{RIGHT}$ return values that fail the comparison with the heap size and $i$ is stored in largest. Afterwards the procedure just returns. 6.2-5 The code for $\\text{MAX-HEAPIFY}$ is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, which might cause some compilers to produce inefficient code. Write an efficient $\\text{MAX-HEAPIFY}$ that uses an iterative control construct (a loop) instead of recursion. 1 2 3 4 5 6 7 8 9 10 11 12 13 MAX - HEAPIFY ( A , i ) while true left = LEFT ( i ) right = RIGHT ( i ) if left < A . heap - size and A . nodes [ left ] > A . nodes [ i ] largest = left else largest = i if right < A . heap - size and A . nodes [ right ] > A . nodes [ largest ] largest = right if largest == i return exchange A . nodes [ i ] with A . nodes [ largest ] i = largest 6.2-6 Show that the worst-case running time of $\\text{MAX-HEAPIFY}$ on a heap of size $n$ is $\\Omega(\\lg n)$. ($\\textit{Hint:}$ For a heap with $n$ nodes, give node values that cause $\\text{MAX-HEAPIFY}$ to be called recursively at every node on a simple path from the root down to a leaf.) If you put a value at the root that is less than every value in the left and right subtrees, then $\\text{MAX-HEAPIFY}$ will be called recursively until a leaf is reached. To make the recursive calls traverse the longest path to a leaf, choose values that make $\\text{MAX-HEAPIFY}$ always recurse on the left child. It follows the left branch when the left child is greater than or equal to the right child, so putting $0$ at the root and $1$ at all the other nodes, for example, will accomplish that. With such values, $\\text{MAX-HEAPIFY}$ will be called $h$ times (where $h$ is the heap height, which is the number of edges in the longest path from the root to a leaf), so its running time will be $\\Theta(h)$ (since each call does $\\Theta(1)$ work), which is $\\Theta(\\lg n)$. Since we have a case in which $\\text{MAX-HEAPIFY}$'s running time is $\\Theta(\\lg n)$, its worst-case running time is \u007f$\\Omega(\\lg n)$.","title":"6.2 Maintaining the heap property"},{"location":"Chap06/6.2/#62-1","text":"Using figure 6.2 as a model, illustrate the operation of $\\text{MAX-HEAPIFY}(A, 3)$ on the array $A = \\langle 27, 17, 3, 16, 13, 10, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle$. $$ \\begin{aligned} \\langle 27, 17, 3, 16, 13, 10,1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 3, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 9, 1, 5, 7, 12, 4, 8, 3, 0 \\rangle \\\\ \\end{aligned} $$","title":"6.2-1"},{"location":"Chap06/6.2/#62-2","text":"Starting with the procedure $\\text{MAX-HEAPIFY}$, write pseudocode for the procedure $\\text{MIN-HEAPIFY}(A, i)$, which performs the corresponding manipulation on a min-heap. How does the running time of $\\text{MIN-HEAPIFY}$ compare to that of $\\text{MAX-HEAPIFY}$? 1 2 3 4 5 6 7 8 9 10 11 MIN - HEAPIFY ( A , i ) l = LEFT ( i ) r = RIGHT ( i ) if l \u2264 A . heap - size and A [ l ] < A [ i ] smallest = l else smallest = i if r \u2264 A . heap - size and A [ r ] < A [ smallest ] smallest = r if smallest != i exchange A [ i ] with A [ smallest ] MIN - HEAPIFY ( A , smallest ) The running time is the same. Actually, the algorithm is the same with the exceptions of two comparisons and some names.","title":"6.2-2"},{"location":"Chap06/6.2/#62-3","text":"What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ when the element $A[i]$ is larger than its children? No effect. The comparisons are carried out, $A[i]$ is found to be largest and the procedure just returns.","title":"6.2-3"},{"location":"Chap06/6.2/#62-4","text":"What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ for $i > A.heap\\text-size / 2$? No effect. In that case, it is a leaf. Both $\\text{LEFT}$ and $\\text{RIGHT}$ return values that fail the comparison with the heap size and $i$ is stored in largest. Afterwards the procedure just returns.","title":"6.2-4"},{"location":"Chap06/6.2/#62-5","text":"The code for $\\text{MAX-HEAPIFY}$ is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, which might cause some compilers to produce inefficient code. Write an efficient $\\text{MAX-HEAPIFY}$ that uses an iterative control construct (a loop) instead of recursion. 1 2 3 4 5 6 7 8 9 10 11 12 13 MAX - HEAPIFY ( A , i ) while true left = LEFT ( i ) right = RIGHT ( i ) if left < A . heap - size and A . nodes [ left ] > A . nodes [ i ] largest = left else largest = i if right < A . heap - size and A . nodes [ right ] > A . nodes [ largest ] largest = right if largest == i return exchange A . nodes [ i ] with A . nodes [ largest ] i = largest","title":"6.2-5"},{"location":"Chap06/6.2/#62-6","text":"Show that the worst-case running time of $\\text{MAX-HEAPIFY}$ on a heap of size $n$ is $\\Omega(\\lg n)$. ($\\textit{Hint:}$ For a heap with $n$ nodes, give node values that cause $\\text{MAX-HEAPIFY}$ to be called recursively at every node on a simple path from the root down to a leaf.) If you put a value at the root that is less than every value in the left and right subtrees, then $\\text{MAX-HEAPIFY}$ will be called recursively until a leaf is reached. To make the recursive calls traverse the longest path to a leaf, choose values that make $\\text{MAX-HEAPIFY}$ always recurse on the left child. It follows the left branch when the left child is greater than or equal to the right child, so putting $0$ at the root and $1$ at all the other nodes, for example, will accomplish that. With such values, $\\text{MAX-HEAPIFY}$ will be called $h$ times (where $h$ is the heap height, which is the number of edges in the longest path from the root to a leaf), so its running time will be $\\Theta(h)$ (since each call does $\\Theta(1)$ work), which is $\\Theta(\\lg n)$. Since we have a case in which $\\text{MAX-HEAPIFY}$'s running time is $\\Theta(\\lg n)$, its worst-case running time is \u007f$\\Omega(\\lg n)$.","title":"6.2-6"},{"location":"Chap06/6.3/","text":"6.3-1 Using figure 6.3 as a model, illustrate the operation of $\\text{BUILD-MAX-HEAP}$ on the array $A = \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle$. $$ \\begin{aligned} \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle \\\\ \\langle 5, 3, 17, 22, 84, 19, 6, 10, 9 \\rangle \\\\ \\langle 5, 3, 19, 22, 84, 17, 6, 10, 9 \\rangle \\\\ \\langle 5, 84, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 5, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 5, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 10, 3, 17, 6, 5, 9 \\rangle \\\\ \\end{aligned} $$ 6.3-2 Why do we want the loop index $i$ in line 2 of $\\text{BUILD-MAX-HEAP}$ to decrease from $\\lfloor A.length / 2 \\rfloor$ to $1$ rather than increase from $1$ to $\\lfloor A.length/2 \\rfloor$? Otherwise we won't be allowed to call $\\text{MAX-HEAPIFY}$, since it will fail the condition of having the subtrees be max-heaps. That is, if we start with $1$, there is no guarantee that $A[2]$ and $A[3]$ are roots of max-heaps. 6.3-3 Show that there are at most $\\lceil n / 2^{h + 1} \\rceil$ nodes of height $h$ in any $n$-element heap. Let $H$ be the height of the heap. Two subtleties to beware of: Be careful not to confuse the height of a node (longest distance from a leaf) with its depth (distance from the root). If the heap is not a complete binary tree (bottom level is not full), then the nodes at a given level (depth) don't all have the same height. For example, although all nodes at depth $H$ have height $0$, nodes at depth $H - 1$ can have either height $0$ or height $1$. For a complete binary tree, it's easy to show that there are $\\lceil n / 2^{h + 1}\\rceil$ nodes of height $h$. But the proof for an incomplete tree is tricky and is not derived from the proof for a complete tree. Proof By induction on $h$. Basis\uff1a Show that it's true for $h = 0$ (i.e., that # of leaves $\\le \\lceil n / 2^{h + 1} \\rceil = \\lceil n / 2 \\rceil$ In fact, we'll show that the # of leaves $= \\lceil n / 2 \\rceil$. The tree leaves (nodes at height $0$) are at depths $H$ and $H - 1$. They consist of all nodes at depth $H$, and the nodes at depth $H - 1$ that are not parents of depth-$H$ nodes. Let $x$ be the number of nodes at depth $H$\u2014that is, the number of nodes in the bottom (possibly incomplete) level. Note that $n - x$ is odd, because the $n - x$ nodes above the bottom level form a complete binary tree, and a complete binary tree has an odd number of nodes ($1$ less than a power of $2$). Thus if $n$ is odd, $x$ is even, and if $n$ is even, $x$ is odd. To prove the base case, we must consider separately the case in which $n$ is even ($x$ is odd) and the case in which $n$ is odd ($x$ is even). Here are two ways to do this: The first requires more cleverness, and the second requires more algebraic manipulation. First method of proving the base case: If $n$ is odd, then $x$ is even, so all nodes have siblings\u2014i.e., all internal nodes have $2$ children. Thus (see Exercise B.5-3),# of internal nodes $=$ # of leaves $- 1$. So, $n =$ # of nodes $=$ # of leaves $+$ # of internal nodes $ = 2 \\cdot$ # of leaves $- 1$. Thus, # of leaves $= (n + 1) / 2$. (The latter equality holds because $n$ is odd.) If $n$ is even, then $x$ is odd, and some leaf doesn't have a sibling. If we gave it a sibling, we would have $n + 1$ nodes, where $n + 1$ is odd, so the case we analyzed above would apply. Observe that we would also increase the number of leaves by $1$, since we added a node to a parent that already had a child. By the odd-node case above, # of leaves $+ 1 = \\lceil (n + 1) / 2 \\rceil = \\lceil n / 2 \\rceil + 1$. (The latter equality holds because $n$ is even.) In either case, # of leaves = $\\lceil n / 2 \\rceil$. Second method of proving the base case: Note that at any depth $d < H$ there are $2^d$ nodes, because all such tree levels are complete. If $x$ is even, there are $x / 2$ nodes at depth $H - 1$ that are parents of depth $H$ nodes, hence $2^{H - 1} - x / 2$ nodes at depth $H - 1$ that are not parents of depth-$H$ nodes. Thus, $$ \\begin{aligned} \\text{total \\# of height-$0$ nodes} & = x + 2^{H - 1} - x / 2 \\\\ & = 2^{H - 1} + x / 2 \\\\ & = (2^H + x) / 2 \\\\ & = \\lceil (2^H + x - 1) / 2 \\rceil & \\text{(because $x$ is even)} \\\\ & = \\lceil n / 2 \\rceil \\end{aligned} $$ ($n = 2^H + x - 1$ because the complete tree down to depth $H - 1$ has $2^H - 1$ nodes and depth $H$ has $x$ nodes.) If $x$ is odd, by an argument similar to the even case, we see that $$ \\begin{aligned} \\text{total \\# of height-$0$ nodes} & = x + 2^{H - 1} - (x + 1) / 2 \\\\ & = 2^{H - 1} + (x - 1) / 2 \\\\ & = (2^H + x - 1) / 2 \\\\ & = n / 2 \\\\ & = \\lceil n / 2 \\rceil & \\text{(because $x$ is odd $\\Rightarrow n$ is even)}. \\end{aligned} $$ Inductive step: Show that if it's true for height $H - 1$, it's true for $h$. Let $n_h$ be the number of nodes at height $h$ in the $n$-node tree $T$. Consider the tree $T'$ formed by removing the leaves of $T$. It has $n' = n - n_0$ nodes. We know from the base case that $n_0 = \\lceil n / 2 \\rceil$, so $n' = n - n_0 = n - \\lceil n / 2 \\rceil = \\lfloor n / 2 \\rfloor$. Note that the nodes at height $h$ in $T$ would be at height $h - 1$ if the leaves of the tree were removed\u2014that is, they are at height $h - 1$ in $T'$. Letting $n'_{h - 1}$ denote the number of nodes at height $h - 1$ in $T'$, we have $$n_h = n'_{h - 1}.$$ By induction, we can bound $n'_{h - 1}$: $$n_h = n'_{h - 1} \\le \\lceil n' / 2^h \\rceil = \\big\\lceil\\lfloor n / 2 \\rfloor / 2^h \\big\\rceil \\le \\lceil (n / 2) / 2^h \\rceil = \\lceil n / 2^{h + 1} \\rceil.$$ Alternative solution An alternative solution relies on four facts: Every node not on the unique simple path from the last leaf to the root is the root of a complete binary subtree. A node that is the root of a complete binary subtree and has height $h$ is the ancestor of $2^h$ leaves. By Exercise 6.1-7, an n-element heap has $\\lceil n / 2 \\rceil$ leaves. For nonnegative reals $a$ and $b$, we have $\\lceil a \\rceil \\cdot b \\ge \\lceil ab \\rceil$. The proof is by contradiction. Assume that an $n$-element heap contains at least $\\lceil n / 2^{h + 1} \\rceil + 1$ nodes of height $h$. Exactly one node of height $h$ is on the unique simple path from the last leaf to the root, and the subtree rooted at this node has at least one leaf (that being the last leaf). All other nodes of height $h$, of which the heap contains at least $\\lceil n / 2^{h + 1} \\rceil$, are the roots of complete binary subtrees, and each such node is the root of a subtree with $2^h$ leaves. Moreover, each subtree whose root is at height $h$ is disjoint. Therefore, the number of leaves in the entire heap is at least $$ \\begin{aligned} \\Big\\lceil \\frac{n}{2^{h + 1}} \\Big\\rceil \\cdot 2^h + 1 & \\ge \\Big\\lceil \\frac{n}{2^{h + 1}} \\cdot 2^h \\Big\\rceil + 1 \\\\ & = \\Big\\lceil \\frac{n}{2} \\Big\\rceil + 1, \\end{aligned} $$ which contradicts the property that an $n$-element heap has $\\lceil n / 2 \\rceil$ leaves.","title":"6.3 Building a heap"},{"location":"Chap06/6.3/#63-1","text":"Using figure 6.3 as a model, illustrate the operation of $\\text{BUILD-MAX-HEAP}$ on the array $A = \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle$. $$ \\begin{aligned} \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle \\\\ \\langle 5, 3, 17, 22, 84, 19, 6, 10, 9 \\rangle \\\\ \\langle 5, 3, 19, 22, 84, 17, 6, 10, 9 \\rangle \\\\ \\langle 5, 84, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 5, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 5, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 10, 3, 17, 6, 5, 9 \\rangle \\\\ \\end{aligned} $$","title":"6.3-1"},{"location":"Chap06/6.3/#63-2","text":"Why do we want the loop index $i$ in line 2 of $\\text{BUILD-MAX-HEAP}$ to decrease from $\\lfloor A.length / 2 \\rfloor$ to $1$ rather than increase from $1$ to $\\lfloor A.length/2 \\rfloor$? Otherwise we won't be allowed to call $\\text{MAX-HEAPIFY}$, since it will fail the condition of having the subtrees be max-heaps. That is, if we start with $1$, there is no guarantee that $A[2]$ and $A[3]$ are roots of max-heaps.","title":"6.3-2"},{"location":"Chap06/6.3/#63-3","text":"Show that there are at most $\\lceil n / 2^{h + 1} \\rceil$ nodes of height $h$ in any $n$-element heap. Let $H$ be the height of the heap. Two subtleties to beware of: Be careful not to confuse the height of a node (longest distance from a leaf) with its depth (distance from the root). If the heap is not a complete binary tree (bottom level is not full), then the nodes at a given level (depth) don't all have the same height. For example, although all nodes at depth $H$ have height $0$, nodes at depth $H - 1$ can have either height $0$ or height $1$. For a complete binary tree, it's easy to show that there are $\\lceil n / 2^{h + 1}\\rceil$ nodes of height $h$. But the proof for an incomplete tree is tricky and is not derived from the proof for a complete tree. Proof By induction on $h$. Basis\uff1a Show that it's true for $h = 0$ (i.e., that # of leaves $\\le \\lceil n / 2^{h + 1} \\rceil = \\lceil n / 2 \\rceil$ In fact, we'll show that the # of leaves $= \\lceil n / 2 \\rceil$. The tree leaves (nodes at height $0$) are at depths $H$ and $H - 1$. They consist of all nodes at depth $H$, and the nodes at depth $H - 1$ that are not parents of depth-$H$ nodes. Let $x$ be the number of nodes at depth $H$\u2014that is, the number of nodes in the bottom (possibly incomplete) level. Note that $n - x$ is odd, because the $n - x$ nodes above the bottom level form a complete binary tree, and a complete binary tree has an odd number of nodes ($1$ less than a power of $2$). Thus if $n$ is odd, $x$ is even, and if $n$ is even, $x$ is odd. To prove the base case, we must consider separately the case in which $n$ is even ($x$ is odd) and the case in which $n$ is odd ($x$ is even). Here are two ways to do this: The first requires more cleverness, and the second requires more algebraic manipulation. First method of proving the base case: If $n$ is odd, then $x$ is even, so all nodes have siblings\u2014i.e., all internal nodes have $2$ children. Thus (see Exercise B.5-3),# of internal nodes $=$ # of leaves $- 1$. So, $n =$ # of nodes $=$ # of leaves $+$ # of internal nodes $ = 2 \\cdot$ # of leaves $- 1$. Thus, # of leaves $= (n + 1) / 2$. (The latter equality holds because $n$ is odd.) If $n$ is even, then $x$ is odd, and some leaf doesn't have a sibling. If we gave it a sibling, we would have $n + 1$ nodes, where $n + 1$ is odd, so the case we analyzed above would apply. Observe that we would also increase the number of leaves by $1$, since we added a node to a parent that already had a child. By the odd-node case above, # of leaves $+ 1 = \\lceil (n + 1) / 2 \\rceil = \\lceil n / 2 \\rceil + 1$. (The latter equality holds because $n$ is even.) In either case, # of leaves = $\\lceil n / 2 \\rceil$. Second method of proving the base case: Note that at any depth $d < H$ there are $2^d$ nodes, because all such tree levels are complete. If $x$ is even, there are $x / 2$ nodes at depth $H - 1$ that are parents of depth $H$ nodes, hence $2^{H - 1} - x / 2$ nodes at depth $H - 1$ that are not parents of depth-$H$ nodes. Thus, $$ \\begin{aligned} \\text{total \\# of height-$0$ nodes} & = x + 2^{H - 1} - x / 2 \\\\ & = 2^{H - 1} + x / 2 \\\\ & = (2^H + x) / 2 \\\\ & = \\lceil (2^H + x - 1) / 2 \\rceil & \\text{(because $x$ is even)} \\\\ & = \\lceil n / 2 \\rceil \\end{aligned} $$ ($n = 2^H + x - 1$ because the complete tree down to depth $H - 1$ has $2^H - 1$ nodes and depth $H$ has $x$ nodes.) If $x$ is odd, by an argument similar to the even case, we see that $$ \\begin{aligned} \\text{total \\# of height-$0$ nodes} & = x + 2^{H - 1} - (x + 1) / 2 \\\\ & = 2^{H - 1} + (x - 1) / 2 \\\\ & = (2^H + x - 1) / 2 \\\\ & = n / 2 \\\\ & = \\lceil n / 2 \\rceil & \\text{(because $x$ is odd $\\Rightarrow n$ is even)}. \\end{aligned} $$ Inductive step: Show that if it's true for height $H - 1$, it's true for $h$. Let $n_h$ be the number of nodes at height $h$ in the $n$-node tree $T$. Consider the tree $T'$ formed by removing the leaves of $T$. It has $n' = n - n_0$ nodes. We know from the base case that $n_0 = \\lceil n / 2 \\rceil$, so $n' = n - n_0 = n - \\lceil n / 2 \\rceil = \\lfloor n / 2 \\rfloor$. Note that the nodes at height $h$ in $T$ would be at height $h - 1$ if the leaves of the tree were removed\u2014that is, they are at height $h - 1$ in $T'$. Letting $n'_{h - 1}$ denote the number of nodes at height $h - 1$ in $T'$, we have $$n_h = n'_{h - 1}.$$ By induction, we can bound $n'_{h - 1}$: $$n_h = n'_{h - 1} \\le \\lceil n' / 2^h \\rceil = \\big\\lceil\\lfloor n / 2 \\rfloor / 2^h \\big\\rceil \\le \\lceil (n / 2) / 2^h \\rceil = \\lceil n / 2^{h + 1} \\rceil.$$ Alternative solution An alternative solution relies on four facts: Every node not on the unique simple path from the last leaf to the root is the root of a complete binary subtree. A node that is the root of a complete binary subtree and has height $h$ is the ancestor of $2^h$ leaves. By Exercise 6.1-7, an n-element heap has $\\lceil n / 2 \\rceil$ leaves. For nonnegative reals $a$ and $b$, we have $\\lceil a \\rceil \\cdot b \\ge \\lceil ab \\rceil$. The proof is by contradiction. Assume that an $n$-element heap contains at least $\\lceil n / 2^{h + 1} \\rceil + 1$ nodes of height $h$. Exactly one node of height $h$ is on the unique simple path from the last leaf to the root, and the subtree rooted at this node has at least one leaf (that being the last leaf). All other nodes of height $h$, of which the heap contains at least $\\lceil n / 2^{h + 1} \\rceil$, are the roots of complete binary subtrees, and each such node is the root of a subtree with $2^h$ leaves. Moreover, each subtree whose root is at height $h$ is disjoint. Therefore, the number of leaves in the entire heap is at least $$ \\begin{aligned} \\Big\\lceil \\frac{n}{2^{h + 1}} \\Big\\rceil \\cdot 2^h + 1 & \\ge \\Big\\lceil \\frac{n}{2^{h + 1}} \\cdot 2^h \\Big\\rceil + 1 \\\\ & = \\Big\\lceil \\frac{n}{2} \\Big\\rceil + 1, \\end{aligned} $$ which contradicts the property that an $n$-element heap has $\\lceil n / 2 \\rceil$ leaves.","title":"6.3-3"},{"location":"Chap06/6.4/","text":"6.4-1 Using figure 6.4 as a model, illustrate the operation of $\\text{HEAPSORT}$ on the array $A = \\langle 5, 13, 2, 25, 7, 17, 20, 8, 4 \\rangle$. 6.4-2 Argue the correctness of $\\text{HEAPSORT}$ using the following loop invariant: At the start of each iteration of the for loop of lines 2-5, the subarray $A[1..i]$ is a max-heap containing the $i$ smallest elements of $A[1..n]$, and the subarray $A[i + 1..n]$ contains the $n - i$ largest elements of $A[1..n]$, sorted. Initialization: The subarray $A[i + 1..n]$ is empty, thus the invariant holds. Maintenance: $A[1]$ is the largest element in $A[1..i]$ and it is smaller than the elements in $A[i + 1..n]$. When we put it in the $i$th position, then $A[i..n]$ contains the largest elements, sorted. Decreasing the heap size and calling $\\text{MAX-HEAPIFY}$ turns $A[1..i - 1]$ into a max-heap. Decrementing $i$ sets up the invariant for the next iteration. Termination: After the loop $i = 1$. This means that $A[2..n]$ is sorted and $A[1]$ is the smallest element in the array, which makes the array sorted. 6.4-3 What is the running time of $\\text{HEAPSORT}$ on an array $A$ of length $n$ that is already sorted in increasing order? What about decreasing order? Both of them are $\\Theta(n\\lg n)$. If the array is sorted in increasing order, the algorithm will need to convert it to a heep that will take $O(n)$. Afterwards, however, there are $n - 1$ calls to $\\text{MAX-HEAPIFY}$ and each one will perform the full $\\lg k$ operations. Since: $$\\sum_{i = 1}^{n - 1}\\lg k = \\lg((n - 1)!) = \\Theta(n\\lg n).$$ Same goes for decreasing order. $\\text{BUILD-MAX-HEAP}$ will be faster (by a constant factor), but the computation time will be dominated by the loop in $\\text{HEAPSORT}$, which is $\\Theta(n\\lg n)$. 6.4-4 Show that the worst-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This is essentially the first part of exercise 6.4-3. Whenever we have an array that is already sorted, we take linear time to convert it to a max-heap and then $n\\lg n$ time to sort it. 6.4-5 $\\star$ Show that when all elements are distinct, the best-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This proved to be quite tricky. My initial solution was wrong. Also, heapsort appeared in 1964, but the lower bound was proved by Schaffer and Sedgewick in 1992. It's evil to put this an exercise. Let's assume that the heap is a full binary tree with $n = 2^k - 1$. There are $2^{k - 1}$ leaves and $2^{k - 1} - 1$ inner nodes. Let's look at sorting the first $2^{k - 1}$ elements of the heap. Let's consider their arrangement in the heap and color the leaves to be red and the inner nodes to be blue. The colored nodes are a subtree of the heap (otherwise there would be a contradiction). Since there are $2^{k - 1}$ colored nodes, at most $2^{k - 2}$ are red, which means that at least $2^{k - 2} - 1$ are blue. While the red nodes can jump directly to the root, the blue nodes need to travel up before they get removed. Let's count the number of swaps to move the blue nodes to the root. The minimal case of swaps is when there are $2^{k - 2} - 1$ blue nodes and they are arranged in a binary tree. If there are $d$ such blue nodes, then there would be $i = \\lg d$ levels, each containing $2^i$ nodes with length $i$. Thus the number of swaps is, $$\\sum_{i = 0}^{\\lg d}i2^i = 2 + (\\lg d - 2)2^{\\lg d} = \\Omega(d\\lg d).$$ And now for a lazy (but cute) trick. We've figured out a tight bound on sorting half of the heap. We have the following recurrence: $$T(n) = T(n / 2) + \\Omega(n\\lg n).$$ Applying the master method, we get that $T(n) = \\Omega(n\\lg n)$.","title":"6.4 The heapsort algorithm"},{"location":"Chap06/6.4/#64-1","text":"Using figure 6.4 as a model, illustrate the operation of $\\text{HEAPSORT}$ on the array $A = \\langle 5, 13, 2, 25, 7, 17, 20, 8, 4 \\rangle$.","title":"6.4-1"},{"location":"Chap06/6.4/#64-2","text":"Argue the correctness of $\\text{HEAPSORT}$ using the following loop invariant: At the start of each iteration of the for loop of lines 2-5, the subarray $A[1..i]$ is a max-heap containing the $i$ smallest elements of $A[1..n]$, and the subarray $A[i + 1..n]$ contains the $n - i$ largest elements of $A[1..n]$, sorted. Initialization: The subarray $A[i + 1..n]$ is empty, thus the invariant holds. Maintenance: $A[1]$ is the largest element in $A[1..i]$ and it is smaller than the elements in $A[i + 1..n]$. When we put it in the $i$th position, then $A[i..n]$ contains the largest elements, sorted. Decreasing the heap size and calling $\\text{MAX-HEAPIFY}$ turns $A[1..i - 1]$ into a max-heap. Decrementing $i$ sets up the invariant for the next iteration. Termination: After the loop $i = 1$. This means that $A[2..n]$ is sorted and $A[1]$ is the smallest element in the array, which makes the array sorted.","title":"6.4-2"},{"location":"Chap06/6.4/#64-3","text":"What is the running time of $\\text{HEAPSORT}$ on an array $A$ of length $n$ that is already sorted in increasing order? What about decreasing order? Both of them are $\\Theta(n\\lg n)$. If the array is sorted in increasing order, the algorithm will need to convert it to a heep that will take $O(n)$. Afterwards, however, there are $n - 1$ calls to $\\text{MAX-HEAPIFY}$ and each one will perform the full $\\lg k$ operations. Since: $$\\sum_{i = 1}^{n - 1}\\lg k = \\lg((n - 1)!) = \\Theta(n\\lg n).$$ Same goes for decreasing order. $\\text{BUILD-MAX-HEAP}$ will be faster (by a constant factor), but the computation time will be dominated by the loop in $\\text{HEAPSORT}$, which is $\\Theta(n\\lg n)$.","title":"6.4-3"},{"location":"Chap06/6.4/#64-4","text":"Show that the worst-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This is essentially the first part of exercise 6.4-3. Whenever we have an array that is already sorted, we take linear time to convert it to a max-heap and then $n\\lg n$ time to sort it.","title":"6.4-4"},{"location":"Chap06/6.4/#64-5-star","text":"Show that when all elements are distinct, the best-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This proved to be quite tricky. My initial solution was wrong. Also, heapsort appeared in 1964, but the lower bound was proved by Schaffer and Sedgewick in 1992. It's evil to put this an exercise. Let's assume that the heap is a full binary tree with $n = 2^k - 1$. There are $2^{k - 1}$ leaves and $2^{k - 1} - 1$ inner nodes. Let's look at sorting the first $2^{k - 1}$ elements of the heap. Let's consider their arrangement in the heap and color the leaves to be red and the inner nodes to be blue. The colored nodes are a subtree of the heap (otherwise there would be a contradiction). Since there are $2^{k - 1}$ colored nodes, at most $2^{k - 2}$ are red, which means that at least $2^{k - 2} - 1$ are blue. While the red nodes can jump directly to the root, the blue nodes need to travel up before they get removed. Let's count the number of swaps to move the blue nodes to the root. The minimal case of swaps is when there are $2^{k - 2} - 1$ blue nodes and they are arranged in a binary tree. If there are $d$ such blue nodes, then there would be $i = \\lg d$ levels, each containing $2^i$ nodes with length $i$. Thus the number of swaps is, $$\\sum_{i = 0}^{\\lg d}i2^i = 2 + (\\lg d - 2)2^{\\lg d} = \\Omega(d\\lg d).$$ And now for a lazy (but cute) trick. We've figured out a tight bound on sorting half of the heap. We have the following recurrence: $$T(n) = T(n / 2) + \\Omega(n\\lg n).$$ Applying the master method, we get that $T(n) = \\Omega(n\\lg n)$.","title":"6.4-5 $\\star$"},{"location":"Chap06/6.5/","text":"6.5-1 Illustrate the operation $\\text{HEAP-EXTRACT-MAX}$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Extract the max node $15$, then move $1$ to the top of the heap. Since $13 > 9 > 1$, swap $1$ and $13$. Since $12 > 5 > 1$, swap $1$ and $12$. Since $6 > 2 > 1$, swap $1$ and $6$. 6.5-2 Illustrate the operation of $\\text{MAX-HEAP-INSERT}(A, 10)$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Since $\\text{MAX-HEAP-INSERT}(A, 10)$ is called, we append a node assigned value $-\\infty$. Update the $key$ value of the new node. Since the parent $key$ is smaller than $10$, the nodes are swapped. Since the parent $key$ is smaller than $10$, the nodes are swapped. 6.5-3 Write pseudocode for the procedures $\\text{HEAP-MINIMUM}$, $\\text{HEAP-EXTRACT-MIN}$, $\\text{HEAP-DECREASE-KEY}$, and $\\text{MIN-HEAP-INSERT}$ that implement a min-priority queue with a min-heap. 1 2 HEAP - MINIMUM ( A ) return A [ 1 ] 1 2 3 4 5 6 7 8 HEAP - EXTRACT - MIN ( A ) if A . heap - size < 1 error \"heap underflow\" min = A [ 1 ] A [ 1 ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 MIN - HEAPIFY ( A , 1 ) return min 1 2 3 4 5 6 7 HEAP - DECREASE - KEY ( A , i , key ) if key > A [ i ] error \"new key is larger than current key\" A [ i ] = key while i > 1 and A [ PAREANT ( i )] > A [ i ] exchange A [ i ] with A [ PARENT ( i )] i = PARENT ( i ) 1 2 3 4 MIN - HEAP - INSERT ( A , key ) A . heap - size = A . heap - size + 1 A [ A . heap - size ] = \u221e HEAP - DECREASE - KEY ( A , A . heap - size , key ) 6.5-4 Why do we bother setting the key of the inserted node to $-\\infty$ in line 2 of $\\text{MAX-HEAP-INSERT}$ when the next thing we do is increase its key to the desired value? In order to pass the guard clause. Otherwise we have to drop the check if $key < A[i]$. 6.5-5 Argue the correctness of $\\text{HEAP-INCREASE-KEY}$ using the following loop invariant: At the start of each iteration of the while loop of lines 4-6, the subarray $A[1 ..A.heap\\text-size]$ satisfies the max-heap property, except that there may be one violation: $A[i]$ may be larger than $A[\\text{PARENT}(i)]$. You may assume that the subarray $A[1..A.heap\\text-size]$ satisfies the max-heap property at the time $\\text{HEAP-INCREASE-KEY}$ is called. Initialization: $A$ is a heap except that $A[i]$ might be larger that it's parent, because it has been modified. $A[i]$ is larger than its children, because otherwise the guard clause would fail and the loop will not be entered (the new value is larger than the old value and the old value is larger than the children). Maintenance: When we exchange $A[i]$ with its parent, the max-heap property is satisfied except that now $A[\\text{PARENT}(i)]$ might be larger than its parent. Changing $i$ to its parent maintains the invariant. Termination: The loop terminates whenever the heap is exhausted or the max-heap property for $A[i]$ and its parent is preserved. At the loop termination, $A$ is a max-heap. 6.5-6 Each exchange operation on line 5 of $\\text{HEAP-INCREASE-KEY}$ typically requires three assignments. Show how to use the idea of the inner loop of $\\text{INSERTION-SORT}$ to reduce the three assignments down to just one assignment. Change the procedure to the following: 1 2 3 4 5 6 7 HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] error \"new key is smaller than current key\" while i > 1 and A [ PARENT ( i )] < key A [ i ] = A [ PARENT ( i )] i = PARENT ( i ) A [ i ] = key 6.5-7 Show how to implement a first-in, first-out queue with a priority queue. Show how to implement a stack with a priority queue. (Queues and stacks are defined in section 10.1). Both are simple. For a stack we keep adding elements in increasing priority, while in a queue we add them in decreasing priority. For the stack we can set the new priority to $\\text{HEAP-MAXIMUM}(A) + 1$. For the queue we need to keep track of it and decrease it on every insertion. Both are not very efficient. Furthermore, if the priority can overflow or underflow, so will eventually need to reassign priorities. 6.5-8 The operation $\\text{HEAP-DELETE}(A, i)$ deletes the item in node $i$ from heap $A$. Give an implementation of $\\text{HEAP-DELETE}$ that runs in $O(\\lg n)$ time for an $n$-element max-heap. 1 2 3 4 5 6 7 HEAP - DELETE ( A , i ) if A [ i ] > A [ h . heap - size ] A [ i ] = A [ A . heap - size ] MAX - HEAPIFY ( A , i ) else HEAP - INCREASE - KEY ( A , i , A [ A . heap - size ]) A . heap - size = A . heap - size - 1 Note: The following algorithm is wrong. For example, given an array $A = [15, 7, 9, 1, 2, 3, 8]$ which is a max-heap, and if we delete $A[5] = 2$, then it will fail. 1 2 3 4 HEAP - DELETE ( A , i ) A [ i ] = A [ A . heap - size ] MAX - HEAPIFY ( A , i ) A . heap - size = A . heap - size - 1 before: 1 2 3 4 5 15 / \\ 7 9 / \\ / \\ 1 2 3 8 after (which is wrong since $8 > 7$ violates the max-heap property): 1 2 3 4 5 15 / \\ 7 9 / \\ / 1 8 3 6.5-9 Give an $O(n\\lg k)$-time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. ($\\textit{Hint:}$ Use a min-heap for $k$-way merging.) We take one element of each list and put it in a min-heap. Along with each element we have to track which list we took it from. When merging, we take the minimum element from the heap and insert another element off the list it came from (unless the list is empty). We continue until we empty the heap. We have $n$ steps and at each step we're doing an insertion into the heap, which is $\\lg k$.","title":"6.5 Priority queues"},{"location":"Chap06/6.5/#65-1","text":"Illustrate the operation $\\text{HEAP-EXTRACT-MAX}$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Extract the max node $15$, then move $1$ to the top of the heap. Since $13 > 9 > 1$, swap $1$ and $13$. Since $12 > 5 > 1$, swap $1$ and $12$. Since $6 > 2 > 1$, swap $1$ and $6$.","title":"6.5-1"},{"location":"Chap06/6.5/#65-2","text":"Illustrate the operation of $\\text{MAX-HEAP-INSERT}(A, 10)$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Since $\\text{MAX-HEAP-INSERT}(A, 10)$ is called, we append a node assigned value $-\\infty$. Update the $key$ value of the new node. Since the parent $key$ is smaller than $10$, the nodes are swapped. Since the parent $key$ is smaller than $10$, the nodes are swapped.","title":"6.5-2"},{"location":"Chap06/6.5/#65-3","text":"Write pseudocode for the procedures $\\text{HEAP-MINIMUM}$, $\\text{HEAP-EXTRACT-MIN}$, $\\text{HEAP-DECREASE-KEY}$, and $\\text{MIN-HEAP-INSERT}$ that implement a min-priority queue with a min-heap. 1 2 HEAP - MINIMUM ( A ) return A [ 1 ] 1 2 3 4 5 6 7 8 HEAP - EXTRACT - MIN ( A ) if A . heap - size < 1 error \"heap underflow\" min = A [ 1 ] A [ 1 ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 MIN - HEAPIFY ( A , 1 ) return min 1 2 3 4 5 6 7 HEAP - DECREASE - KEY ( A , i , key ) if key > A [ i ] error \"new key is larger than current key\" A [ i ] = key while i > 1 and A [ PAREANT ( i )] > A [ i ] exchange A [ i ] with A [ PARENT ( i )] i = PARENT ( i ) 1 2 3 4 MIN - HEAP - INSERT ( A , key ) A . heap - size = A . heap - size + 1 A [ A . heap - size ] = \u221e HEAP - DECREASE - KEY ( A , A . heap - size , key )","title":"6.5-3"},{"location":"Chap06/6.5/#65-4","text":"Why do we bother setting the key of the inserted node to $-\\infty$ in line 2 of $\\text{MAX-HEAP-INSERT}$ when the next thing we do is increase its key to the desired value? In order to pass the guard clause. Otherwise we have to drop the check if $key < A[i]$.","title":"6.5-4"},{"location":"Chap06/6.5/#65-5","text":"Argue the correctness of $\\text{HEAP-INCREASE-KEY}$ using the following loop invariant: At the start of each iteration of the while loop of lines 4-6, the subarray $A[1 ..A.heap\\text-size]$ satisfies the max-heap property, except that there may be one violation: $A[i]$ may be larger than $A[\\text{PARENT}(i)]$. You may assume that the subarray $A[1..A.heap\\text-size]$ satisfies the max-heap property at the time $\\text{HEAP-INCREASE-KEY}$ is called. Initialization: $A$ is a heap except that $A[i]$ might be larger that it's parent, because it has been modified. $A[i]$ is larger than its children, because otherwise the guard clause would fail and the loop will not be entered (the new value is larger than the old value and the old value is larger than the children). Maintenance: When we exchange $A[i]$ with its parent, the max-heap property is satisfied except that now $A[\\text{PARENT}(i)]$ might be larger than its parent. Changing $i$ to its parent maintains the invariant. Termination: The loop terminates whenever the heap is exhausted or the max-heap property for $A[i]$ and its parent is preserved. At the loop termination, $A$ is a max-heap.","title":"6.5-5"},{"location":"Chap06/6.5/#65-6","text":"Each exchange operation on line 5 of $\\text{HEAP-INCREASE-KEY}$ typically requires three assignments. Show how to use the idea of the inner loop of $\\text{INSERTION-SORT}$ to reduce the three assignments down to just one assignment. Change the procedure to the following: 1 2 3 4 5 6 7 HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] error \"new key is smaller than current key\" while i > 1 and A [ PARENT ( i )] < key A [ i ] = A [ PARENT ( i )] i = PARENT ( i ) A [ i ] = key","title":"6.5-6"},{"location":"Chap06/6.5/#65-7","text":"Show how to implement a first-in, first-out queue with a priority queue. Show how to implement a stack with a priority queue. (Queues and stacks are defined in section 10.1). Both are simple. For a stack we keep adding elements in increasing priority, while in a queue we add them in decreasing priority. For the stack we can set the new priority to $\\text{HEAP-MAXIMUM}(A) + 1$. For the queue we need to keep track of it and decrease it on every insertion. Both are not very efficient. Furthermore, if the priority can overflow or underflow, so will eventually need to reassign priorities.","title":"6.5-7"},{"location":"Chap06/6.5/#65-8","text":"The operation $\\text{HEAP-DELETE}(A, i)$ deletes the item in node $i$ from heap $A$. Give an implementation of $\\text{HEAP-DELETE}$ that runs in $O(\\lg n)$ time for an $n$-element max-heap. 1 2 3 4 5 6 7 HEAP - DELETE ( A , i ) if A [ i ] > A [ h . heap - size ] A [ i ] = A [ A . heap - size ] MAX - HEAPIFY ( A , i ) else HEAP - INCREASE - KEY ( A , i , A [ A . heap - size ]) A . heap - size = A . heap - size - 1 Note: The following algorithm is wrong. For example, given an array $A = [15, 7, 9, 1, 2, 3, 8]$ which is a max-heap, and if we delete $A[5] = 2$, then it will fail. 1 2 3 4 HEAP - DELETE ( A , i ) A [ i ] = A [ A . heap - size ] MAX - HEAPIFY ( A , i ) A . heap - size = A . heap - size - 1 before: 1 2 3 4 5 15 / \\ 7 9 / \\ / \\ 1 2 3 8 after (which is wrong since $8 > 7$ violates the max-heap property): 1 2 3 4 5 15 / \\ 7 9 / \\ / 1 8 3","title":"6.5-8"},{"location":"Chap06/6.5/#65-9","text":"Give an $O(n\\lg k)$-time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. ($\\textit{Hint:}$ Use a min-heap for $k$-way merging.) We take one element of each list and put it in a min-heap. Along with each element we have to track which list we took it from. When merging, we take the minimum element from the heap and insert another element off the list it came from (unless the list is empty). We continue until we empty the heap. We have $n$ steps and at each step we're doing an insertion into the heap, which is $\\lg k$.","title":"6.5-9"},{"location":"Chap06/Problems/6-1/","text":"We can build a heap by repeatedly calling $\\text{MAX-HEAP-INSERT}$ to insert the elements into the heap. Consider the following variation of the $\\text{BUILD-MAX-HEAP}$ procedure: 1 2 3 4 BUILD - MAX - HEAP ' ( A ) A . heap - size = 1 for i = 2 to A . length MAX - HEAP - INSERT ( A , A [ i ]) a. Do the procedures $\\text{BUILD-MAX-HEAP}$ and $\\text{BUILD-MAX-HEAP}'$ always create the same heap when run on the same input array? Prove that they do, or provide a counterexample. b. Show that in the worst case, $\\text{BUILD-MAX-HEAP}'$ requires $\\Theta(n\\lg n)$ time to build a $n$-element heap. a. The procedures $\\text{BUILD-MAX-HEAP}$ and $\\text{BUILD-MAX-HEAP}'$ do not always create the same heap when run on the same input array. Consider the following counterexample. Input array $A = \\langle 1, 2, 3 \\rangle$: $\\text{BUILD-MAX-HEAP}(A)$: $A = \\langle 3, 2, 1 \\rangle$. $\\text{BUILD-MAX-HEAP}'(A)$: $A = \\langle 3, 1, 2 \\rangle$. b. An upper bound of $O(n\\lg n)$ time follows immediately from there being $n - 1$ calls to $\\text{MAX-HEAP-INSERT}$, each taking $O(\\lg n)$ time. For a lower bound of \u007f$\\Omega(n\\lg n)$, consider the case in which the input array is given in strictly increasing order. Each call to $\\text{MAX-HEAP-INSERT}$ causes $\\text{HEAP-INCREASE-KEY}$ to go all the way up to the root. Since the depth of node $i$ is $\\lfloor \\lg i \\rfloor$, the total time is $$ \\begin{aligned} \\sum_{i = 1}^n \\Theta(\\lfloor \\lg i \\rfloor) & \\ge \\sum_{i = \\lceil n / 2 \\rceil}^n \\Theta(\\lfloor \\lg \\lceil n / 2 \\rceil\\rfloor) \\\\ & \\ge \\sum_{i = \\lceil n / 2 \\rceil}^n \\Theta(\\lfloor \\lg (n / 2) \\rfloor) \\\\ & = \\sum_{i = \\lceil n / 2 \\rceil}^n \\Theta(\\lfloor \\lg n - 1 \\rfloor) \\\\ & \\ge n / 2 \\cdot \\Theta(\\lg n) \\\\ & = \\Omega(n\\lg n). \\end{aligned} $$ In the worst case, therefore, $\\text{BUILD-MAX-HEAP}'$ requires $\\Theta(n\\lg n)$ time to build an $n$-element heap.","title":"6-1 Building a heap using insertion"},{"location":"Chap06/Problems/6-2/","text":"A $d$-ary heap is like a binary heap, but (with one possible exception) non-leaf nodes have $d$ children instead of $2$ children. a. How would you represent a $d$-ary heap in an array? b. What is the height of a $d$-ary heap of $n$ elements in terms of $n$ and $d$? c. Give an efficient implementation of $\\text{EXTRACT-MAX}$ in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$. d. Give an efficient implementation of $\\text{INSERT}$ in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$. e. Give an efficient implementation of $\\text{INCREASE-KEY}(A, i, k)$, which flags an error if $k < A[i]$, but otherwise sets $A[i] = k$ and then updates the $d$-ary max-heap structure appropriately. Analyze its running time in terms of $d$ and $n$. a. We can represent a $d$-ary heap in a $1$-dimensional array as follows. The root resides in $A[1]$, its $d$ children reside in order in $A[2]$ through $A[d + 1]$, their children reside in order in $A[d + 2]$ through $A[d^2 + d + 1]$, and so on. The following two procedures map a node with index $i$ to its parent and to its $j$ th child (for $1 \\le j \\le d$), respectively. 1 2 d - ARY - PARENT ( i ) return floor (( i - 2 ) / d + 1 ) 1 2 d - ARY - CHILD ( i , j ) return d ( i - 1 ) + j + 1 To convince yourself that these procedures really work, verify that $$\\text{$d$-ARY-PARENT($d$-ARY-CHILD($i, j$)) = $i$},$$ for any $1 \\le j \\le d$. Notice that the binary heap procedures are a special case of the above procedures when $d = 2$. b. Since each node has $d$ children, the height of a $d$-ary heap with $n$ nodes is $\\Theta(\\log_d n) = \\Theta(\\lg n / \\lg d)$. c. The procedure $\\text{HEAP-EXTRACT-MAX}$ given in the text for binary heaps works fine for $d$-ary heaps too. The change needed to support $d$-ary heaps is in $\\text{MAX-HEAPIFY}$, which must compare the argument node to all d children instead of just $2$ children. The running time of $\\text{HEAP-EXTRACT-MAX}$ is still the running time for $\\text{MAX-HEAPIFY}$, but that now takes worst-case time proportional to the product of the height of the heap by the number of children examined at each node (at most $d$), namely $\\Theta(d \\log_d n) = \\Theta(d\\lg n / \\lg d)$. d. The procedure $\\text{MAX-HEAP-INSERT}$ given in the text for binary heaps works fine for $d$-ary heaps too, assuming that $\\text{HEAP-INCREASE-KEY}$ works for $d$-ary heaps. The worst-case running time is still $\\Theta(h)$, where $h$ is the height of the heap. (Since only parent pointers are followed, the number of children a node has is irrelevant.) For a $d$-ary heap, this is $\\Theta(\\log_d n) = \\Theta(\\lg n / \\lg d)$. e. The $\\text{HEAP-INCREASE-KEY}$ procedure with two small changes works for $d$-ary heaps. First, because the problem specifies that the new key is given by the parameter $k$, change instances of the variable $key$ to $k$. Second, change calls of $\\text{PARENT}$ to calls of $d\\text{-ARY-PARENT}$ from part (a). In the worst case, the entire height of the tree must be traversed, so the worst-case running time is $\\Theta(h) = \\Theta(\\log_d n) = \\Theta(\\lg n / \\lg d)$.","title":"6-2 Analysis of $d$-ary heaps"},{"location":"Chap06/Problems/6-3/","text":"An $m \\times n$ Young tableau is an $m \\times n$ matrix such that the entries of each row are in sorted order from left to right and the entries of each column are in sorted order from top to bottom. Some of the entries of a Young tableau may be $\\infty$, which we treat as nonexistent elements. Thus, a Young tableau can be used to hold $r \\le mn$ finite numbers. a. Draw $4 \\times 4$ tableau containing the elements $\\{9, 16, 3, 2, 4, 8, 5, 14, 12\\}$. b. Argue that an $m \\times n$ Young tableau $Y$ is empty if $Y[1, 1] = \\infty$. Argue that $Y$ is full (contains $mn$ elements) if $Y[m, n] < \\infty$. c. Give an algorithm to implement $\\text{EXTRACT-MIN}$ on a nonempty $m \\times n$ Young tableau that runs in $O(m + n)$ time. Your algorithm should use a recursive subroutine that solves an $m \\times n$ problem by recursively solving either an $(m - 1) \\times n$ or an $m \\times (n - 1)$ subproblem. ($\\textit{Hint:}$ Think about $\\text{MAX-HEAPIFY}$.) Define $T(p)$ where $p = m + n$, to be the maximum running time of $\\text{EXTRACT-MIN}$ on any $m \\times n$ Young tableau. Give and solve a recurrence relation for $T(p)$ that yields the $O(m + n)$ time bound. d. Show how to insert a new element into a nonfull $m \\times n$ Young tableau in $O(m + n)$ time. e. Using no other sorting method as a subroutine, show how to use an $n \\times n$ Young tableau to sort $n^2$ numbers in $O(n^3)$ time. f. Give an $O(m + n)$-time algorithm to determine whether a given number is stored in a given $m \\times n$ Young tableau. a. $$ \\begin{matrix} 2 & 3 & 12 & 14 \\\\ 4 & 8 & 16 & \\infty \\\\ 5 & 9 & \\infty & \\infty \\\\ \\infty & \\infty & \\infty & \\infty \\end{matrix} $$ b. If the top left element is $\\infty$, then all the elements on the first row need to be $\\infty$. But if this is the case, all other elements need to be $\\infty$ because they are larger than the first element on their column. If the bottom right element is smaller than $\\infty$, all the elements on the bottom row need to be smaller than $\\infty$. But so are the other elements in the tableau, because each is smaller than the bottom element of its column. c. The $A[1, 1]$ is the smallest elemnt. We store it, so we can return it later and then replace is with $\\infty$. This breaks the Young tableau property and we need to perform a procedure, similar to $\\text{MAX-HEAPIFY}$ to restore it. We compare $A[i, j]$ with each of its neighbours and exchange it with the smallest. This restores the property for $A[i, j]$ but reduces the problem to either $A[i, j + 1]$ or $A[i + 1, j]$. We terminate when $A[i, j]$ is smaller than its neighbours. The relation in question is $$T(p) = T(p - 1) + O(1) = T(p - 2) + O(1) + O(1) = \\cdots = O(p).$$ d. The algorithm is very similar to the previous, except that we start with the bottom right element of the tableau and move it upwards and leftwards to the correct position. The asymptotic analysis is the same. e. We can sort by starting with an empty tableau and inserting all the $n^2$ elements in it. Each insertion is $O(n + n) = O(n)$. The complexity is $n^2O(n) = O(n^3)$. Afterwards we can take them one by one and put them back in the original array which has the same complexity. In total, its $O(n^3)$. We can also do it in place if we allow for \"partial\" tableaus where only a portion of the top rows (and a portion of the last of them) is in the tableau. Then we can build the tableau in place and then start putting each minimal element to the end. This would be asymptotically equal, but use constant memory. It would also sort the array in reverse. f. We from the lower-left corner. We check the current element $current$ with the one we're looking for $key$ and move up if $current > key$ and right if $current < key$. We declare success if $current = key$ and otherwise terminate if we walk off the tableau.","title":"6-3 Young tableaus"},{"location":"Chap07/7.1/","text":"7.1-1 Using figure 7.1 as a model, illustrate the operation of $\\text{PARTITION}$ on the array $A = \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle$. $$ \\begin{aligned} \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 19, 13, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 19, 12, 13, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 12, 13, 19, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 19, 12, 21, 13, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 12, 21, 13, 19, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 \\rangle \\end{aligned} $$ 7.1-2 What value of $q$ does $\\text{PARTITION}$ return when all elements in the array $A[p..r]$ have the same value? Modify $\\text{PARTITION}$ so that $q = \\lfloor (p + r) / 2 \\rfloor$ when all elements in the array $A[p..r]$ have the same value. It returns $r$. We can modify $\\text{PARTITION}$ by counting the number of comparisons in which $A[j] = A[r]$ and then subtracting half that number from the pivot index. 7.1-3 Give a brief argument that the running time of $\\text{PARTITION}$ on a subarray of size $n$ is $\\Theta(n)$. There is a for statement whose body executes $r - 1 - p = \\Theta(n)$ times. In the worst case every time the body of the if is executed, but it takes constant time and so does the code outside of the loop. Thus the running time is $\\Theta(n)$. 7.1-4 How would you modify $\\text{QUICKSORT}$ to sort into nonincreasing order? We only need to flip the condition on line 4.","title":"7.1 Description of quicksort"},{"location":"Chap07/7.1/#71-1","text":"Using figure 7.1 as a model, illustrate the operation of $\\text{PARTITION}$ on the array $A = \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle$. $$ \\begin{aligned} \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 19, 13, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 19, 12, 13, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 12, 13, 19, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 19, 12, 21, 13, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 12, 21, 13, 19, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 \\rangle \\end{aligned} $$","title":"7.1-1"},{"location":"Chap07/7.1/#71-2","text":"What value of $q$ does $\\text{PARTITION}$ return when all elements in the array $A[p..r]$ have the same value? Modify $\\text{PARTITION}$ so that $q = \\lfloor (p + r) / 2 \\rfloor$ when all elements in the array $A[p..r]$ have the same value. It returns $r$. We can modify $\\text{PARTITION}$ by counting the number of comparisons in which $A[j] = A[r]$ and then subtracting half that number from the pivot index.","title":"7.1-2"},{"location":"Chap07/7.1/#71-3","text":"Give a brief argument that the running time of $\\text{PARTITION}$ on a subarray of size $n$ is $\\Theta(n)$. There is a for statement whose body executes $r - 1 - p = \\Theta(n)$ times. In the worst case every time the body of the if is executed, but it takes constant time and so does the code outside of the loop. Thus the running time is $\\Theta(n)$.","title":"7.1-3"},{"location":"Chap07/7.1/#71-4","text":"How would you modify $\\text{QUICKSORT}$ to sort into nonincreasing order? We only need to flip the condition on line 4.","title":"7.1-4"},{"location":"Chap07/7.2/","text":"7.2-1 Use the substitution method to prove that the recurrence $T(n) = T(n - 1) + \\Theta(n)$ has the solution $T(n) = \\Theta(n^2)$, as claimed at the beginning of section 7.2. We represent $\\Theta(n)$ as $c_2n$ and we guess that $T(n) \\le c_1n^2$, $$ \\begin{aligned} T(n) & = T(n - 1) + c_2n \\\\ & \\le c_1(n - 1)^2 + c_2n \\\\ & = c_1n^2 - 2c_1n + c_1 + c_2n & (2c_1 > c_2, n \\ge c_1 / (2c_1 - c_2)) \\\\ & \\le c_1n^2. \\end{aligned} $$ 7.2-2 What is the running time of $\\text{QUICKSORT}$ when all elements of the array $A$ have the same value? It is $\\Theta(n^2)$, since one of the partitions is always empty (see exercise 7.1-2.) 7.2-3 Show that the running time of $\\text{QUICKSORT}$ is $\\Theta(n^2)$ when the array $A$ contains distict elements and is sorted in decreasing order. $\\text{PARTITION}$ does a \"worst-case partitioning\" when the elements are in decreasing order. It reduces the size of the subarray under consideration by only $1$ at each step, which we've seen has running time $\\Theta(n^2)$. In particular, $\\text{PARTITION}$, given a subarray $A[p..r]$ of distinct elements in decreasing order, produces an empty partition in $A[p..q - 1]$, puts the pivot (originally in $A[r]$) into $A[p]$, and produces a partition $A[p + 1..r]$ with only one fewer element than $A[p..r]$. The recurrence for $\\text{QUICKSORT}$ becomes $T(n) = T(n - 1) + \\Theta(n)$, which has the solution $T(n) = \\Theta(n^2)$. 7.2-4 Banks often record transactions on an account in order of the times of the transactions, but many people like to receive their bank statements with checks listed in order by check numbers. People usually write checks in order by check number, and merchants usually cash the with reasonable dispatch. The problem of converting time-of-transaction ordering to check-number ordering is therefore the problem of sorting almost-sorted input. Argue that the procedure $\\text{INSERTION-SORT}$ would tend to beat the procedure $\\text{QUICKSORT}$ on this problem. The more sorted the array is, the less work insertion sort will do. Namely, $\\text{INSERTION-SORT}$ is $\\Theta(n + d)$, where $d$ is the number of inversions in the array. In the example above the number of inversions tends to be small so insertion sort will be close to linear. On the other hand, if $\\text{PARTITION}$ does picks a pivot that does not participate in an inversion, it will produce and empty partition. Since there is a small number of inversions, $\\text{QUICKSORT}$ is very likely to produce empty partitions. 7.2-5 Suppose that the splits at every level of quicksort are in proportion $1 - \\alpha$ to $\\alpha$, where $0 < \\alpha \\le 1 / 2$ is a constant. Show that the minumum depth of a leaf in the recursion tree is approximately $-\\lg n / \\lg\\alpha$ and the maximum depth is approximately $-\\lg n / \\lg(1 - \\alpha)$. (Don't worry about integer round-off.) The minimum depth follows a path that always takes the smaller part of the partition\u2014i.e., that multiplies the number of elements by $\\alpha$. One iteration reduces the number of elements from $n$ to $\\alpha n$, and $i$ iterations reduces the number of elements to $\\alpha^i n$. At a leaf, there is just one remaining element, and so at a minimum-depth leaf of depth $m$, we have $\\alpha^m n = 1$. Thus, $\\alpha^m = 1 / n$. Taking logs, we get $m\\lg\\alpha = -\\lg n$, or $m = -\\lg n / \\lg\\alpha$. Similarly, maximum depth corresponds to always taking the larger part of the partition, i.e., keeping a fraction $1 - \\alpha$ of the elements each time. The maximum depth $M$ is reached when there is one element left, that is, when $(1 - \\alpha)^M n = 1$. Thus, $M = -\\lg n / \\lg (1 - \\alpha)$. All these equations are approximate because we are ignoring floors and ceilings. 7.2-6 $\\star$ Argue that for any constant $0 < \\alpha \\le 1 / 2$, the probability is approximately $1 - 2\\alpha$ that on a random input array, $\\text{PARTITION}$ produces a split more balanced than $1 - \\alpha$ to $\\alpha$. In order to produce a worse split than $\\alpha$ to $1 - \\alpha$, $\\text{PARTITION}$ must pick a pivot that will be either within the smallest $\\alpha n$ elements or the largest $\\alpha n$ elements. The probability of either is (approximately) $\\alpha n / n = \\alpha$ and the probability of both is $2\\alpha$. Thus, the probability of having a better partition is the complement, $1 - 2\\alpha$.","title":"7.2 Performance of quicksort"},{"location":"Chap07/7.2/#72-1","text":"Use the substitution method to prove that the recurrence $T(n) = T(n - 1) + \\Theta(n)$ has the solution $T(n) = \\Theta(n^2)$, as claimed at the beginning of section 7.2. We represent $\\Theta(n)$ as $c_2n$ and we guess that $T(n) \\le c_1n^2$, $$ \\begin{aligned} T(n) & = T(n - 1) + c_2n \\\\ & \\le c_1(n - 1)^2 + c_2n \\\\ & = c_1n^2 - 2c_1n + c_1 + c_2n & (2c_1 > c_2, n \\ge c_1 / (2c_1 - c_2)) \\\\ & \\le c_1n^2. \\end{aligned} $$","title":"7.2-1"},{"location":"Chap07/7.2/#72-2","text":"What is the running time of $\\text{QUICKSORT}$ when all elements of the array $A$ have the same value? It is $\\Theta(n^2)$, since one of the partitions is always empty (see exercise 7.1-2.)","title":"7.2-2"},{"location":"Chap07/7.2/#72-3","text":"Show that the running time of $\\text{QUICKSORT}$ is $\\Theta(n^2)$ when the array $A$ contains distict elements and is sorted in decreasing order. $\\text{PARTITION}$ does a \"worst-case partitioning\" when the elements are in decreasing order. It reduces the size of the subarray under consideration by only $1$ at each step, which we've seen has running time $\\Theta(n^2)$. In particular, $\\text{PARTITION}$, given a subarray $A[p..r]$ of distinct elements in decreasing order, produces an empty partition in $A[p..q - 1]$, puts the pivot (originally in $A[r]$) into $A[p]$, and produces a partition $A[p + 1..r]$ with only one fewer element than $A[p..r]$. The recurrence for $\\text{QUICKSORT}$ becomes $T(n) = T(n - 1) + \\Theta(n)$, which has the solution $T(n) = \\Theta(n^2)$.","title":"7.2-3"},{"location":"Chap07/7.2/#72-4","text":"Banks often record transactions on an account in order of the times of the transactions, but many people like to receive their bank statements with checks listed in order by check numbers. People usually write checks in order by check number, and merchants usually cash the with reasonable dispatch. The problem of converting time-of-transaction ordering to check-number ordering is therefore the problem of sorting almost-sorted input. Argue that the procedure $\\text{INSERTION-SORT}$ would tend to beat the procedure $\\text{QUICKSORT}$ on this problem. The more sorted the array is, the less work insertion sort will do. Namely, $\\text{INSERTION-SORT}$ is $\\Theta(n + d)$, where $d$ is the number of inversions in the array. In the example above the number of inversions tends to be small so insertion sort will be close to linear. On the other hand, if $\\text{PARTITION}$ does picks a pivot that does not participate in an inversion, it will produce and empty partition. Since there is a small number of inversions, $\\text{QUICKSORT}$ is very likely to produce empty partitions.","title":"7.2-4"},{"location":"Chap07/7.2/#72-5","text":"Suppose that the splits at every level of quicksort are in proportion $1 - \\alpha$ to $\\alpha$, where $0 < \\alpha \\le 1 / 2$ is a constant. Show that the minumum depth of a leaf in the recursion tree is approximately $-\\lg n / \\lg\\alpha$ and the maximum depth is approximately $-\\lg n / \\lg(1 - \\alpha)$. (Don't worry about integer round-off.) The minimum depth follows a path that always takes the smaller part of the partition\u2014i.e., that multiplies the number of elements by $\\alpha$. One iteration reduces the number of elements from $n$ to $\\alpha n$, and $i$ iterations reduces the number of elements to $\\alpha^i n$. At a leaf, there is just one remaining element, and so at a minimum-depth leaf of depth $m$, we have $\\alpha^m n = 1$. Thus, $\\alpha^m = 1 / n$. Taking logs, we get $m\\lg\\alpha = -\\lg n$, or $m = -\\lg n / \\lg\\alpha$. Similarly, maximum depth corresponds to always taking the larger part of the partition, i.e., keeping a fraction $1 - \\alpha$ of the elements each time. The maximum depth $M$ is reached when there is one element left, that is, when $(1 - \\alpha)^M n = 1$. Thus, $M = -\\lg n / \\lg (1 - \\alpha)$. All these equations are approximate because we are ignoring floors and ceilings.","title":"7.2-5"},{"location":"Chap07/7.2/#72-6-star","text":"Argue that for any constant $0 < \\alpha \\le 1 / 2$, the probability is approximately $1 - 2\\alpha$ that on a random input array, $\\text{PARTITION}$ produces a split more balanced than $1 - \\alpha$ to $\\alpha$. In order to produce a worse split than $\\alpha$ to $1 - \\alpha$, $\\text{PARTITION}$ must pick a pivot that will be either within the smallest $\\alpha n$ elements or the largest $\\alpha n$ elements. The probability of either is (approximately) $\\alpha n / n = \\alpha$ and the probability of both is $2\\alpha$. Thus, the probability of having a better partition is the complement, $1 - 2\\alpha$.","title":"7.2-6 $\\star$"},{"location":"Chap07/7.3/","text":"7.3-1 Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time? We may be interested in the worst-case performance, but in that case, the randomization is irrelevant: it won't improve the worst case. What randomization can do is make the chance of encountering a worst-case scenario small. 7.3-2 When $\\text{RANDOMIZED-QUICKSORT}$ runs, how many calls are made to the random number generator $\\text{RANDOM}$ in the worst case? How about in the best case? Give your answer in terms of $\\Theta$-notation. In the worst case, the number of calls to $\\text{RANDOM}$ is $$T(n) = T(n - 1) + 1 = n = \\Theta(n).$$ As for the best case, $$T(n) = 2T(n / 2) + 1 = \\Theta(n).$$ This is not too surprising, because each third element (at least) gets picked as pivot.","title":"7.3 A randomized version of quicksort"},{"location":"Chap07/7.3/#73-1","text":"Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time? We may be interested in the worst-case performance, but in that case, the randomization is irrelevant: it won't improve the worst case. What randomization can do is make the chance of encountering a worst-case scenario small.","title":"7.3-1"},{"location":"Chap07/7.3/#73-2","text":"When $\\text{RANDOMIZED-QUICKSORT}$ runs, how many calls are made to the random number generator $\\text{RANDOM}$ in the worst case? How about in the best case? Give your answer in terms of $\\Theta$-notation. In the worst case, the number of calls to $\\text{RANDOM}$ is $$T(n) = T(n - 1) + 1 = n = \\Theta(n).$$ As for the best case, $$T(n) = 2T(n / 2) + 1 = \\Theta(n).$$ This is not too surprising, because each third element (at least) gets picked as pivot.","title":"7.3-2"},{"location":"Chap07/7.4/","text":"7.4-1 Show that in the recurrence $$ \\begin{aligned} T(n) & = \\max\\limits_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n), \\\\ T(n) & = \\Omega(n^2). \\end{aligned} $$ We guess $T(n) \\ge cn^2 - 2n$, $$ \\begin{aligned} T(n) & = \\max_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n) \\\\ & \\ge \\max_{0 \\le q \\le n - 1} (cq^2 - 2q + c(n - q - 1)^2 - 2n - 2q -1) + \\Theta(n) \\\\ & \\ge c\\max_{0 \\le q \\le n - 1} (q^2 + (n - q - 1)^2 - (2n + 4q + 1) / c) + \\Theta(n) \\\\ & \\ge cn^2 - c(2n - 1) + \\Theta(n) \\\\ & \\ge cn^2 - 2cn + 2c & (c \\le 1) \\\\ & \\ge cn^2 - 2n. \\end{aligned} $$ 7.4-2 Show that quicksort's best-case running time is $\\Omega(n\\lg n)$. To show that quicksort's best-case running time is \u007f$\\Omega(n\\lg n)$, we use a technique similar to the one used in Section 7.4.1 to show that its worst-case running time is $O(n^2)$. Let $T(n)$ be the best-case time for the procedure $\\text{QUICKSORT}$ on an input of size $n$. We have the recurrence $$T(n) = \\min\\limits_{1 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n).$$ We guess that $T(n) \\ge cn\\lg n$ for some constant $c$. Substituting this guess into the recurrence, we obtain $$ \\begin{aligned} T(n) & \\ge \\min_{1 \\le q \\le n - 1} (cq\\lg q + c(n - q - 1)\\lg(n - q - 1)) + \\Theta(n) \\\\ & = c \\cdot \\min_{1 \\le q \\le n - 1} (q\\lg q + (n - q - 1)\\lg(n - q - 1)) + \\Theta(n). \\end{aligned} $$ As we'll show below, the expression $q\\lg q + (n - q - 1)\\lg(n - q - 1)$ achieves a minimum over the range $1 \\le q \\le n - 1$ when $q = n - q - 1$, or $q = (n - 1) / 2$, since the first derivative of the expression with respect to $q$ is $0$ when $q = (n - 1) / 2$ and the second derivative of the expression is positive. (It doesn't matter that $q$ is not an integer when $n$ is even, since we're just trying to determine the minimum value of a function, knowing that when we constrain $q$ to integer values, the function's value will be no lower.) Choosing $q = (n - 1) / 2$ gives us the bound $$ \\begin{aligned} \\min_{1 \\le q \\le n - 1} (q\\lg q + (n - q - 1)\\lg(n - q - 1)) & \\ge \\frac{n - 1}{2}\\lg\\frac{n - 1}{2} + \\Big(n - \\frac{n - 1}{2} - 1\\Big)\\lg\\Big(n - \\frac{n - 1}{2} - 1\\Big) \\\\ & = (n - 1)\\lg\\frac{n - 1}{2}. \\end{aligned} $$ Continuing with our bounding of $T(n)$, we obtain, for $n \\ge 2$, $$ \\begin{aligned} T(n) & = c(n - 1)\\lg\\frac{n - 1}{2} + \\Theta(n) \\\\ & = c(n - 1)\\lg(n - 1) - c(n - 1) + \\Theta(n) \\\\ & = cn\\lg(n - 1) - c\\lg(n - 1) - c(n - 1) + \\Theta(n) \\\\ & \\ge cn\\lg(n / 2) - c\\lg(n - 1) - c(n - 1) + \\Theta(n) & \\text{(since $n \\ge 2$)} \\\\ & = cn\\lg n - cn - c\\lg(n - 1) - cn + c + \\Theta(n) \\\\ & = cn\\lg n - (2cn + c\\lg(n - 1) - c) + \\Theta(n) \\\\ & \\ge cn\\lg n, \\end{aligned} $$ since we can pick the constant $c$ small enough so that the $\\Theta(n)$ term dominates the quantity $2cn + c\\lg(n - 1) - c$. Thus, the best-case running time of quicksort is $\\Omega(n\\lg n)$. Letting $f(q) = q\\lg q + (n - q - 1)\\lg(n - q - 1)$, we now show how to find the minimum value of this function in the range $1 \\le q \\le n - 1$. We need to find the value of $q$ for which the derivative of $f$ with respect to $q$ is $0$. We rewrite this function as $$f(q) = \\frac{q\\ln q + (n - q - 1)\\ln(n - q - 1)}{\\ln 2},$$ and so $$ \\begin{aligned} f'(q) & = \\frac{d}{dq}\\Big(\\frac{q\\ln q + (n - q - 1)\\ln(n - q - 1)}{\\ln 2}\\Big) \\\\ & = \\frac{\\ln q + 1 - \\ln(n - q - 1) - 1}{\\ln 2} \\\\ & = \\frac{\\ln q - \\ln(n - q - 1)}{\\ln 2}. \\end{aligned} $$ The derivative $f'(q)$ is $0$ when $q = n - q - 1$, or when $q = (n - 1) / 2$. To verify that $q = (n - 1) / 2$ is indeed a minimum (not a maximum or an in\ufb02ection point), we need to check that the second derivative of $f$ is positive at $q = (n - 1) / 2$: $$ \\begin{aligned} f''(q) & = \\frac{d}{dq}\\Big(\\frac{\\ln q - \\ln(n - q - 1)}{\\ln 2}\\Big) \\\\ & = \\frac{1}{\\ln 2}\\Big(\\frac{1}{q} + \\frac{1}{n - q - 1}\\Big) \\\\ f''\\Big(\\frac{n - 1}{2}\\Big) & = \\frac{1}{\\ln 2}\\Big(\\frac{2}{n - 1} + \\frac{2}{n - 1}\\Big) \\\\ & = \\frac{1}{\\ln 2} \\cdot \\frac{4}{n - 1} \\\\ & > 0. & \\text{(since $n \\ge 2$)} \\end{aligned} $$ 7.4-3 Show that the expression $q^2 + (n - q - 1)^2$ achieves a maximum over $q = 0, 1, \\ldots, n - 1$ when $q = 0$ and $q = n - 1$. $$ \\begin{aligned} f(q) & = q^2 + (n - q - 1)^2 \\\\ f'(q) & = 2q - 2(n - q - 1) = 4q - 2n + 2 \\\\ f''(q) & = 4. \\\\ \\end{aligned} $$ $f'(q) = 0$ when $q = \\frac{1}{2}n - \\frac{1}{4}$. $f'(q)$ is also continious. $\\forall q: f''(q) > 0$, which means that $f'(q)$ is negative left of $f'(q) = 0$ and positive right of it, which means that this is a local minima. In this case, $f(q)$ is decreasing in the beginning of the interval and increasing in the end, which means that those two points are the only candidates for a maximum in the interval. $$ \\begin{aligned} f(0) & = (n - 1)^2 \\\\ f(n - 1) & = (n - 1)^2 + 0^2. \\end{aligned} $$ 7.4-4 Show that $\\text{RANDOMIZED-QUICKSORT}$'s expected running time is $\\Omega(n\\lg n)$. We use the same reasoning for the expected number of comparisons, we just take in in a different direction. $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ & = \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{k + 1} & (k \\ge 1) \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\Omega(\\lg n) \\\\ & = \\Omega(n\\lg n). \\end{aligned} $$ Using the master method, we get the solution $\\Theta(n\\lg n)$. 7.4-5 We can improve the running time of quicksort in practice by taking advantage of the fast running time of insertion sort when its input is \"nearly\" sorted. Upon calling quicksort on a subarray with fewer than $k$ elements, let it simply return without sorting the subarray. After the top-level call to quicksort returns, run insertion sort on the entire array to finish the sorting process. Argue that this sorting algorithm runs in $O(nk + n\\lg(n / k))$ expected time. How should we pick $k$, both in theory and practice? In the quicksort part of the proposed algorithm, the recursion stops at level $\\lg(n / k)$, which makes the expected running time $O(n\\lg(n / k))$. However, this leaves $n / k$ non-sorted, non - intersecting subarrays of (maximum) length $k$. Because of the nature of the insertion sort algorithm, it will first sort fully one such subarray before consider the next one. Thus, it has the same complexity as sorting each of those arrays, that is $\\frac{n}{k}O(k^2) = O(nk)$. In theory, if we ignore the constant factors, we need to solve $$ \\begin{aligned} & n\\lg n \\ge nk + n\\lg{n / k} \\\\ \\Rightarrow & \\lg n \\ge k + \\lg n - \\lg k \\\\ \\Rightarrow & \\lg k \\ge k. \\end{aligned} $$ Which is not possible. If we add the constant factors, we get $$ \\begin{aligned} & c_qn\\lg n \\ge c_ink + c_qn\\lg(n / k) \\\\ \\Rightarrow & c_q\\lg n \\ge c_ik + c_q\\lg n - c_q\\lg k \\\\ \\Rightarrow & \\lg k \\ge \\frac{c_i}{c_q}k. \\end{aligned} $$ Which indicates that there might be a good candidate. Furthermore, the lower-order terms should be taken into consideration too. In practice, $k$ should be chosed by experiment. 7.4-6 $\\star$ Consider modifying the $\\text{PARTITION}$ procedure by randomly picking three elements from array $A$ and partitioning about their median (the middle value of the three elements). Approximate the probability of getting at worst an $\\alpha$-to-$(1 - \\alpha)$ split, as a function of $\\alpha$ in the range $0 < \\alpha < 1$. First, for simplicity's sake, let's assume that we can pick the same element twice. Let's also assume that $0 < \\alpha \\le 1 / 2$. In order to get such a split, two out of three elements need need to be in the smallest $\\alpha n$ elements. The probability of having one is $\\alpha n / n = \\alpha$. The probability of having exactly two is $\\alpha^2 - \\alpha^3$. There are three ways in which two elements can be in the smallest $\\alpha n$ and one way in which all three can be in the smallest $\\alpha n$ so the probability of getting such a median is $3\\alpha^2 - 2\\alpha^3$. We will get the same split if the median is in the largest $\\alpha n$. Since the two events are mutually exclusive, the probability is $$\\Pr\\{\\text{OK split}\\} = 6\\alpha^2 - 4\\alpha^3 = 2\\alpha^2(3 - 2\\alpha).$$","title":"7.4 Analysis of quicksort"},{"location":"Chap07/7.4/#74-1","text":"Show that in the recurrence $$ \\begin{aligned} T(n) & = \\max\\limits_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n), \\\\ T(n) & = \\Omega(n^2). \\end{aligned} $$ We guess $T(n) \\ge cn^2 - 2n$, $$ \\begin{aligned} T(n) & = \\max_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n) \\\\ & \\ge \\max_{0 \\le q \\le n - 1} (cq^2 - 2q + c(n - q - 1)^2 - 2n - 2q -1) + \\Theta(n) \\\\ & \\ge c\\max_{0 \\le q \\le n - 1} (q^2 + (n - q - 1)^2 - (2n + 4q + 1) / c) + \\Theta(n) \\\\ & \\ge cn^2 - c(2n - 1) + \\Theta(n) \\\\ & \\ge cn^2 - 2cn + 2c & (c \\le 1) \\\\ & \\ge cn^2 - 2n. \\end{aligned} $$","title":"7.4-1"},{"location":"Chap07/7.4/#74-2","text":"Show that quicksort's best-case running time is $\\Omega(n\\lg n)$. To show that quicksort's best-case running time is \u007f$\\Omega(n\\lg n)$, we use a technique similar to the one used in Section 7.4.1 to show that its worst-case running time is $O(n^2)$. Let $T(n)$ be the best-case time for the procedure $\\text{QUICKSORT}$ on an input of size $n$. We have the recurrence $$T(n) = \\min\\limits_{1 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n).$$ We guess that $T(n) \\ge cn\\lg n$ for some constant $c$. Substituting this guess into the recurrence, we obtain $$ \\begin{aligned} T(n) & \\ge \\min_{1 \\le q \\le n - 1} (cq\\lg q + c(n - q - 1)\\lg(n - q - 1)) + \\Theta(n) \\\\ & = c \\cdot \\min_{1 \\le q \\le n - 1} (q\\lg q + (n - q - 1)\\lg(n - q - 1)) + \\Theta(n). \\end{aligned} $$ As we'll show below, the expression $q\\lg q + (n - q - 1)\\lg(n - q - 1)$ achieves a minimum over the range $1 \\le q \\le n - 1$ when $q = n - q - 1$, or $q = (n - 1) / 2$, since the first derivative of the expression with respect to $q$ is $0$ when $q = (n - 1) / 2$ and the second derivative of the expression is positive. (It doesn't matter that $q$ is not an integer when $n$ is even, since we're just trying to determine the minimum value of a function, knowing that when we constrain $q$ to integer values, the function's value will be no lower.) Choosing $q = (n - 1) / 2$ gives us the bound $$ \\begin{aligned} \\min_{1 \\le q \\le n - 1} (q\\lg q + (n - q - 1)\\lg(n - q - 1)) & \\ge \\frac{n - 1}{2}\\lg\\frac{n - 1}{2} + \\Big(n - \\frac{n - 1}{2} - 1\\Big)\\lg\\Big(n - \\frac{n - 1}{2} - 1\\Big) \\\\ & = (n - 1)\\lg\\frac{n - 1}{2}. \\end{aligned} $$ Continuing with our bounding of $T(n)$, we obtain, for $n \\ge 2$, $$ \\begin{aligned} T(n) & = c(n - 1)\\lg\\frac{n - 1}{2} + \\Theta(n) \\\\ & = c(n - 1)\\lg(n - 1) - c(n - 1) + \\Theta(n) \\\\ & = cn\\lg(n - 1) - c\\lg(n - 1) - c(n - 1) + \\Theta(n) \\\\ & \\ge cn\\lg(n / 2) - c\\lg(n - 1) - c(n - 1) + \\Theta(n) & \\text{(since $n \\ge 2$)} \\\\ & = cn\\lg n - cn - c\\lg(n - 1) - cn + c + \\Theta(n) \\\\ & = cn\\lg n - (2cn + c\\lg(n - 1) - c) + \\Theta(n) \\\\ & \\ge cn\\lg n, \\end{aligned} $$ since we can pick the constant $c$ small enough so that the $\\Theta(n)$ term dominates the quantity $2cn + c\\lg(n - 1) - c$. Thus, the best-case running time of quicksort is $\\Omega(n\\lg n)$. Letting $f(q) = q\\lg q + (n - q - 1)\\lg(n - q - 1)$, we now show how to find the minimum value of this function in the range $1 \\le q \\le n - 1$. We need to find the value of $q$ for which the derivative of $f$ with respect to $q$ is $0$. We rewrite this function as $$f(q) = \\frac{q\\ln q + (n - q - 1)\\ln(n - q - 1)}{\\ln 2},$$ and so $$ \\begin{aligned} f'(q) & = \\frac{d}{dq}\\Big(\\frac{q\\ln q + (n - q - 1)\\ln(n - q - 1)}{\\ln 2}\\Big) \\\\ & = \\frac{\\ln q + 1 - \\ln(n - q - 1) - 1}{\\ln 2} \\\\ & = \\frac{\\ln q - \\ln(n - q - 1)}{\\ln 2}. \\end{aligned} $$ The derivative $f'(q)$ is $0$ when $q = n - q - 1$, or when $q = (n - 1) / 2$. To verify that $q = (n - 1) / 2$ is indeed a minimum (not a maximum or an in\ufb02ection point), we need to check that the second derivative of $f$ is positive at $q = (n - 1) / 2$: $$ \\begin{aligned} f''(q) & = \\frac{d}{dq}\\Big(\\frac{\\ln q - \\ln(n - q - 1)}{\\ln 2}\\Big) \\\\ & = \\frac{1}{\\ln 2}\\Big(\\frac{1}{q} + \\frac{1}{n - q - 1}\\Big) \\\\ f''\\Big(\\frac{n - 1}{2}\\Big) & = \\frac{1}{\\ln 2}\\Big(\\frac{2}{n - 1} + \\frac{2}{n - 1}\\Big) \\\\ & = \\frac{1}{\\ln 2} \\cdot \\frac{4}{n - 1} \\\\ & > 0. & \\text{(since $n \\ge 2$)} \\end{aligned} $$","title":"7.4-2"},{"location":"Chap07/7.4/#74-3","text":"Show that the expression $q^2 + (n - q - 1)^2$ achieves a maximum over $q = 0, 1, \\ldots, n - 1$ when $q = 0$ and $q = n - 1$. $$ \\begin{aligned} f(q) & = q^2 + (n - q - 1)^2 \\\\ f'(q) & = 2q - 2(n - q - 1) = 4q - 2n + 2 \\\\ f''(q) & = 4. \\\\ \\end{aligned} $$ $f'(q) = 0$ when $q = \\frac{1}{2}n - \\frac{1}{4}$. $f'(q)$ is also continious. $\\forall q: f''(q) > 0$, which means that $f'(q)$ is negative left of $f'(q) = 0$ and positive right of it, which means that this is a local minima. In this case, $f(q)$ is decreasing in the beginning of the interval and increasing in the end, which means that those two points are the only candidates for a maximum in the interval. $$ \\begin{aligned} f(0) & = (n - 1)^2 \\\\ f(n - 1) & = (n - 1)^2 + 0^2. \\end{aligned} $$","title":"7.4-3"},{"location":"Chap07/7.4/#74-4","text":"Show that $\\text{RANDOMIZED-QUICKSORT}$'s expected running time is $\\Omega(n\\lg n)$. We use the same reasoning for the expected number of comparisons, we just take in in a different direction. $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ & = \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{k + 1} & (k \\ge 1) \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\Omega(\\lg n) \\\\ & = \\Omega(n\\lg n). \\end{aligned} $$ Using the master method, we get the solution $\\Theta(n\\lg n)$.","title":"7.4-4"},{"location":"Chap07/7.4/#74-5","text":"We can improve the running time of quicksort in practice by taking advantage of the fast running time of insertion sort when its input is \"nearly\" sorted. Upon calling quicksort on a subarray with fewer than $k$ elements, let it simply return without sorting the subarray. After the top-level call to quicksort returns, run insertion sort on the entire array to finish the sorting process. Argue that this sorting algorithm runs in $O(nk + n\\lg(n / k))$ expected time. How should we pick $k$, both in theory and practice? In the quicksort part of the proposed algorithm, the recursion stops at level $\\lg(n / k)$, which makes the expected running time $O(n\\lg(n / k))$. However, this leaves $n / k$ non-sorted, non - intersecting subarrays of (maximum) length $k$. Because of the nature of the insertion sort algorithm, it will first sort fully one such subarray before consider the next one. Thus, it has the same complexity as sorting each of those arrays, that is $\\frac{n}{k}O(k^2) = O(nk)$. In theory, if we ignore the constant factors, we need to solve $$ \\begin{aligned} & n\\lg n \\ge nk + n\\lg{n / k} \\\\ \\Rightarrow & \\lg n \\ge k + \\lg n - \\lg k \\\\ \\Rightarrow & \\lg k \\ge k. \\end{aligned} $$ Which is not possible. If we add the constant factors, we get $$ \\begin{aligned} & c_qn\\lg n \\ge c_ink + c_qn\\lg(n / k) \\\\ \\Rightarrow & c_q\\lg n \\ge c_ik + c_q\\lg n - c_q\\lg k \\\\ \\Rightarrow & \\lg k \\ge \\frac{c_i}{c_q}k. \\end{aligned} $$ Which indicates that there might be a good candidate. Furthermore, the lower-order terms should be taken into consideration too. In practice, $k$ should be chosed by experiment.","title":"7.4-5"},{"location":"Chap07/7.4/#74-6-star","text":"Consider modifying the $\\text{PARTITION}$ procedure by randomly picking three elements from array $A$ and partitioning about their median (the middle value of the three elements). Approximate the probability of getting at worst an $\\alpha$-to-$(1 - \\alpha)$ split, as a function of $\\alpha$ in the range $0 < \\alpha < 1$. First, for simplicity's sake, let's assume that we can pick the same element twice. Let's also assume that $0 < \\alpha \\le 1 / 2$. In order to get such a split, two out of three elements need need to be in the smallest $\\alpha n$ elements. The probability of having one is $\\alpha n / n = \\alpha$. The probability of having exactly two is $\\alpha^2 - \\alpha^3$. There are three ways in which two elements can be in the smallest $\\alpha n$ and one way in which all three can be in the smallest $\\alpha n$ so the probability of getting such a median is $3\\alpha^2 - 2\\alpha^3$. We will get the same split if the median is in the largest $\\alpha n$. Since the two events are mutually exclusive, the probability is $$\\Pr\\{\\text{OK split}\\} = 6\\alpha^2 - 4\\alpha^3 = 2\\alpha^2(3 - 2\\alpha).$$","title":"7.4-6 $\\star$"},{"location":"Chap07/Problems/7-1/","text":"The version of $\\text{PARTITION}$ given in this chapter is not the original partitioning algorithm. Here is the original partition algorithm, which is due to C.A.R. Hoare: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 HOARE - PARTITION ( A , p , r ) x = A [ p ] i = p - 1 j = r + 1 while true repeat j = j - 1 until A [ j ] \u2264 x repeat i = i + 1 until A [ i ] \u2265 x if i < j exchange A [ i ] with A [ j ] else return j a. Demonstrate the operation of $\\text{HOARE-PARTITION}$ on the array $A = \\langle 13, 19, 9, 5, 12, 8, 7, 4, 11, 2, 6, 21 \\rangle$, showing the values of the array and auxiliary values after each iteration of the while loop in lines 4-13. The next three questions ask you to give a careful argument that the procedure $\\text{HOARE-PARTITION}$ is correct. Assuming that the subarray $A[p..r]$ contains at least two elements, prove the following: b. The indices $i$ and $j$ are such that we never access an element of $A$ outside the subarray $A[p..r]$. c. When $\\text{HOARE-PARTITION}$ terminates, it returns a value $j$ such that $p \\le j < r$. d. Every element of $A[p..j]$ is less than or equal to every element of $A[j + 1..r]$ when $\\text{HOARE-PARTITION}$ terminates. The $\\text{PARTITION}$ procedure in section 7.1 separates the pivot value (originally in $A[r]$) from the two partitions it forms. The $\\text{HOARE-PARTITION}$ procedure, on the other hand, always places the pivot value (originally in $A[p]$) into one of the two parititions $A[p..j]$ and $A[j + 1..r]$. Since $p \\le j < r$, this split is always nontrivial. e. Rewrite the $\\text{QUICKSORT}$ procedure to use $\\text{HOARE-PARTITION}$. a. After the end of the loop, the variables have the following values: $x = 13$, $j = 9$ and $i = 10$. b. Because when $\\text{HOARE-PARTITION}$ is running, $p \\le i < j \\le r$ will always hold, $i$, $j$ won't access any element of $A$ outside the subarray $A[p..r]$. c. When $i \\ge j$, $\\text{HOARE-PARTITION}$ terminates, so $p \\le j < r$. d. When $\\text{HOARE-PARTITION}$ terminates, $A[p..j] \\le x \\le A[j + 1..r]$. e. 1 2 3 4 5 QUICKSORT ( A , p , r ) if p < r q = HOARE - PARTITION ( A , p , r ) QUICKSORT ( A , p , q ) QUICKSORT ( A , q + 1 , r )","title":"7-1 Hoare partition correctness"},{"location":"Chap07/Problems/7-2/","text":"The analysis of the expected running time of randomized quicksort in section 7.4.2 assumes that all element values are distinct. In this problem. we examine what happens when they are not. a. Suppose that all element values are equal. What would be randomized quick-sort's running time in this case? b. The $\\text{PARTITION}$ procedure returns an index $q$ such that each element of $A[p..q - 1]$ is less than or equal to $A[q]$ and each element of $A[q + 1..r]$ is greater than $A[q]$. Modify the $\\text{PARTITION}$ procedure to produce a procedure $\\text{PARTITION}'(A, p, r)$ which permutes the elements of $A[p..r]$ and returns two indices $q$ and $t$ where $p \\le q \\le t \\le r$, such that all elements of $A[q..t]$ are equal, each element of $A[p..q - 1]$ is less than $A[q]$, and each element of $A[t + 1..r]$ is greater than $A[q]$. Like $\\text{PARTITION}$, your $\\text{PARTITION}'$ procedure should take $\\Theta(r - p)$ time. c. Modify the $\\text{RANDOMIZED-QUICKSORT}$ procedure to call $\\text{PARTITION}'$, and name the new procedure $\\text{RANDOMIZED-QUICKSORT}'$. Then modify the $\\text{QUICKSORT}$ procedure to produce a procedure $\\text{QUICKSORT}'(p, r)$ that calls $\\text{RANDOMIZED-PARTITION}'$ and recurses only on partitions of elements not know to be equal to each other. d. Using $\\text{QUICKSORT}'$, how would you adjust the analysis of section 7.4.2 to avoid the assumption that all elements are distinct? a. If all elements are equal, then when $\\text{PARTITION}$ returns, $q = r$ and all elements in $A[p..q - 1]$ are equal. We get the recurrence $T(n) = T(n - 1) + T(0) + \\Theta(n)$ for the running time, and so $T(n) = \\Theta(n^2)$. b. The $\\text{PARTITION}'$ procedure: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PARTITION ' ( A , p , r ) x = A [ p ] i = h = p for j = p + 1 to r // Invariant: A[p..i - 1] < x, A[i..h] = x, A[h + 1..j - 1] > x, A[j..r] unknown. if A [ j ] < x y = A [ j ] A [ j ] = A [ h + 1 ] A [ h + 1 ] = A [ i ] A [ i ] = y i = i + 1 h = h + 1 else if A [ j ] == x exchange A [ h + 1 ] with A [ j ] h = h + 1 return ( i , h ) c. $\\text{RANDOMIZED-PARTITION}'$ is the same as $\\text{RANDOMIZED-PARTITION}$, but with the call to $\\text{PARTITION}$ replaced by a call to $\\text{PARTITION}'$. 1 2 3 4 5 QUICKSORT ' ( A , p , r ) if q < r ( q , t ) = RANDOMIZED - PARTITION ' ( A , q , r ) QUICKSORT ' ( A , p , q - 1 ) QUICKSORT ' ( A , t + 1 , r ) d. Putting elements equal to the pivot in the same partition as the pivot can only help us, because we do not recurse on elements equal to the pivot. Thus, the subproblem sizes with $\\text{QUICKSORT}'$, even with equal elements, are no larger than the subproblem sizes with $\\text{QUICKSORT}$ when all elements are distinct.","title":"7-2 Quicksort with equal element values"},{"location":"Chap07/Problems/7-3/","text":"An alternative analysis of the running time of randomized quicksort focuses on the expected running time of each individual recursive call to $\\text{RANDOMIZED-QUICKSORT}$, rather than on the number of comparisons performed. a. Argue that, given an array of size $n$, the probability that any particular element is chosen as the pivot is $1 / n$. Use this to define indicator random variables $$X_i = I\\{i\\text{th smallest element is chosen as the pivot}\\}.$$ What is $\\text E[X_i]$? b. Let $T(n)$ be a random variable denoting the running time of quicksort on an array of size $n$. Argue that $$\\text E[T(n)] = \\text E\\bigg[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))\\bigg]. \\tag{7.5}$$ c. Show that we can rewrite equation $\\text{(7.5)}$ as $$\\text E[T(n)] = \\frac{2}{n}\\sum_{q = 2}^{n - 1}\\text E[T(q)] + \\Theta(n). \\tag{7.6}$$ d. Show that $$\\sum_{k = 2}^{n - 1}k\\lg k \\le \\frac{1}{2}n^2\\lg n - \\frac{1}{8}n^2. \\tag{7.7}$$ ($\\textit{Hint:}$ Split the summation into two parts, one for $k = 2, 3, \\ldots, \\lceil n / 2 \\rceil - 1$ and one for $k = \\lceil n / 2 \\rceil, \\ldots, n - 1$.) e. Using the bound from equation $\\text{(7.7)}$, show that the recurrence in equation $\\text{(7.6)}$ has the solution $\\text E[T(n)] = \\Theta(n\\lg n)$. ($\\textit{Hint:}$ Show, by substitution, that $\\text E[T(n)] \\le an\\lg n$ for sufficiently large $n$ and for some positive constant $a$.) a. Since the pivot is selected as a random element in the array, which has size $n$, the probabilities of any particular element being selected are all equal, and add to one, so, are all $\\frac{1}{n}$. As such, $\\text E[X_i] = \\Pr\\{i \\text{ smallest is picked}\\} = \\frac{1}{n}$. b. We can apply linearity of expectation over all of the events $X_i$. Suppose we have a particular $X_i$ be true, then, we will have one of the sub arrays be length $i - 1$, and the other be $n - i$, and will of course still need linear time to run the partition procedure. This corresponds exactly to the summand in equation $\\text{(7.5)}$. c. $$ \\begin{aligned} & \\text E\\Bigg[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n)) \\Bigg] \\\\ & = \\sum_{q = 1}^n \\text E[X_q(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ & = \\sum_{q = 1}^n(T(q - 1) + T(n - q) + \\Theta(n))/n \\\\ & = \\Theta(n) + \\frac{1}{n} \\sum_{q = 1}^n(T(q - 1)+T(n - 1)) \\\\ & = \\Theta(n) + \\frac{1}{n} \\Big(\\sum_{q = 1}^n T(q - 1) + \\sum_{q = 1}^n T (n - q) \\Big) \\\\ & = \\Theta(n) + \\frac{1}{n} \\Big(\\sum_{q = 1}^n T(q - 1) + \\sum_{q = 1}^n T (q - 1) \\Big) \\\\ & = \\Theta(n) + \\frac{2}{n} \\sum_{q = 1}^n T(q - 1) \\\\ & = \\Theta(n) + \\frac{2}{n} \\sum_{q = 0}^{n - 1} T(q) \\\\ & = \\Theta(n) + \\frac{2}{n} \\sum_{q = 2}^{n - 1} T(q). \\end{aligned} $$ d. We will prove this inequality in a different way than suggested by the hint. If we let $f(k) = k\\lg k$ treated as a continuous function, then $f'(k) = \\lg k + 1$. Note now that the summation written out is the left hand approximation of the integral of $f(k)$ from $2$ to $n$ with step size $1$. By integration by parts, the anti-derivative of $k\\lg k$ is $$\\frac{1}{\\lg 2}(\\frac{k^2}{2}\\ln k-\\frac{k^2}{4}).$$ So, plugging in the bounds and subtracting, we get $\\frac{n^2\\lg n}{2} - \\frac{n^2}{4\\ln 2} - 1$. Since $f$ has a positive derivative over the entire interval that the integral is being evaluated over, the left hand rule provides a underapproximation of the integral, so, we have that $$ \\begin{aligned} \\sum_{k = 2}^{n - 1} k\\lg k & \\le \\frac{n^2\\lg n}{2} - \\frac{n^2}{4\\ln 2} - 1 \\\\ & \\le \\frac{n^2\\lg n}{2} - \\frac{n^2}{8}, \\end{aligned} $$ where the last inequality uses the fact that $\\ln 2 > 1 / 2$. e. Assume by induction that $T(q) \\le q \\lg(q) + \\Theta(n)$. Combining $\\text{(7.6)}$ and $\\text{(7.7)}$, we have $$ \\begin{aligned} \\text E[T(n)] & = \\frac{2}{n} \\sum_{q = 2}^{n - 1} \\text E[T(q)] + \\Theta(n) \\\\ & \\le \\frac{2}{n} \\sum_{q = 2}^{n - 1}(q\\lg q + \\Theta(n)) + \\Theta(n) \\\\ & \\le \\frac{2}{n} \\sum_{q = 2}^{n - 1}q\\lg q + \\frac{2}{n}\\Theta(n) + \\Theta(n) \\\\ & \\le \\frac{2}{n}(\\frac{1}{2}n^2\\lg n - \\frac{1}{8}n^2) + \\Theta(n) \\\\ & = n\\lg n -\\frac{1}{4}n + \\Theta(n) \\\\ & = n\\lg n+\\Theta(n). \\end{aligned} $$","title":"7-3 Alternative quicksort analysis"},{"location":"Chap07/Problems/7-4/","text":"The $\\text{QUICKSORT}$ algorithm of Section 7.1 contains two recursive calls to itself. After $\\text{QUICKSORT}$ calls $\\text{PARTITION}$, it recursively sorts the left subarray and then it recursively sorts the right subarray. The second recursive call in $\\text{QUICKSORT}$ is not really necessary; we can avoid it by using an iterative control structure. This technique, called tail recursion , is provided automatically by good compilers. Consider the following version of quicksort, which simulates tail recursion: 1 2 3 4 5 6 TAIL - RECURSIVE - QUICKSORT ( A , p , r ) while p < r // Partition and sort left subarray. q = PARTITION ( A , p , r ) TAIL - RECURSIVE - QUICKSORT ( A , p , q - 1 ) p = q + 1 a. Argue that $\\text{TAIL-RECURSIVE-QUICKSORT}(A, 1, A.length)$ correctly sorts the array $A$. Compilers usually execute recursive procedures by using a stack that contains pertinent information, including the parameter values, for each recursive call. The information for the most recent call is at the top of the stack, and the information for the initial call is at the bottom. Upon calling a procedure, its information is pushed onto the stack; when it terminates, its information is popped . Since we assume that array parameters are represented by pointers, the information for each procedure call on the stack requires $O(1)$ stack space. The stack depth is the maximum amount of stack space used at any time during a computation. b. Describe a scenario in which $\\text{TAIL-RECURSIVE-QUICKSORT}$'s stack depth is $\\Theta(n)$ on an $n$-element input array. c. Modify the code for $\\text{TAIL-RECURSIVE-QUICKSORT}$ so that the worst-case stack depth is $\\Theta(\\lg n)$. Maintain the $O(n\\lg n)$ expected running time of the algorithm. a. $\\text{QUICKSORT}'$ does exactly what $\\text{QUICKSORT}$ does; hence it sorts correctly. $\\text{QUICKSORT}$ and $\\text{QUICKSORT}'$ do the same partitioning, and then each calls itself with arguments $A$, $p$, $q - 1$. $\\text{QUICKSORT}$ then calls itself again, with arguments $A$, $q + 1$, $r$. $\\text{QUICKSORT}'$ instead sets $p = q + 1$ and performs another iteration of its while loop. This executes the same operations as calling itself with $A$, $q + 1$, $r$, because in both cases, the first and third arguments ($A$ and $r$) have the same values as before, and $p$ has the old value of $q + 1$. b. The stack depth of $\\text{QUICKSORT}'$ will be $\\Theta(n)$ on an $n$-element input array if there are $\\Theta(n)$ recursive calls to $\\text{QUICKSORT}'$. This happens if every call to $\\text{PARTITION}(A, p, r)$ returns $q = r$. The sequence of recursive calls in this scenario is $$ \\begin{aligned} & \\text{QUICKSORT$'(A, 1, n)$}, \\\\ & \\text{QUICKSORT$'(A, 1, n - 1)$}, \\\\ & \\text{QUICKSORT$'(A, 1, n - 2)$}, \\\\ & \\quad\\quad\\vdots \\\\ & \\text{QUICKSORT$'(A, 1, 1)$}. \\end{aligned} $$ Any array that is already sorted in increasing order will cause $\\text{QUICKSORT}'$ to behave this way. c. The problem demonstrated by the scenario in part (b) is that each invocation of $\\text{QUICKSORT}'$ calls $\\text{QUICKSORT}'$ again with almost the same range. To avoid such behavior, we must change $\\text{QUICKSORT}'$ so that the recursive call is on a smaller interval of the array. The following variation of $\\text{QUICKSORT}'$ checks which of the two subarrays returned from $\\text{PARTITION}$ is smaller and recurses on the smaller subarray, which is at most half the size of the current array. Since the array size is reduced by at least half on each recursive call, the number of recursive calls, and hence the stack depth, is $\\Theta(\\lg n)$ in the worst case. Note that this method works no matter how partitioning is performed (as long as the $\\text{PARTITION}$ procedure has the same functionality as the procedure given in Section 7.1). 1 2 3 4 5 6 7 8 9 QUICKSORT '' ( A , p , r ) while p < r // Partition and sort the small subarray first. q = PARTITION ( A , p , r ) if q - p < r - q QUICKSORT '' ( A , p , q - 1 ) p = q + 1 else QUICKSORT '' ( A , q + 1 , r ) r = q - 1 The expected running time is not affected, because exactly the same work is done as before: the same partitions are produced, and the same subarrays are sorted.","title":"7-4 Stack depth for quicksort"},{"location":"Chap07/Problems/7-5/","text":"One way to improve the $\\text{RANDOMIZED-QUICKSORT}$ procedure is to partition around a pivot that is chosen more carefully than by picking a random element from the subarray. One common approach is the median-of-3 method: choose the pivot as the median (middle element) of a set of 3 elements randomly selected from the subarray. (See exercise 7.4-6.) For this problem, let us assume that the elements of the input array $A[1..n]$ are distinct and that $n \\ge 3$. We denote the sorted output array by $A'[1..n]$. Using the median-of-3 method to choose the pivot element $x$, define $p_i = \\Pr\\{x = A'[i]\\}$. a. Give an exact formula for $p_i$ as a function of $n$ and $i$ for $i = 2, 3, \\ldots, n - 1$. (Note that $p_1 = p_n = 0$.) b. By what amount have we increased the likelihood of choosing the pivot as $x = A'[\\lfloor (n + 1) / 2 \\rfloor]$, the median of $A[1..n]$, compared with the ordinary implementation? Assume that $n \\to \\infty$, and give the limiting ratio of these probabilities. c. If we define a \"good\" split to mean choosing the pivot as $x = A'[i]$, where $n / 3 \\le i \\le 2n / 3$, by what amount have we increased the likelihood of getting a good split compared with the ordinary implementation? ($\\textit{Hint:}$ Approximate the sum by an integral.) d. Argue that in the $\\Omega(n\\lg n)$ running time of quicksort, the median-of-3 method affects only the constant factor. a. $p_i$ is the probability that a randomly selected subset of size three has the $A'[i]$ as it's middle element. There are 6 possible orderings of the three elements selected. So, suppose that $S'$ is the set of three elements selected. We will compute the probability that the second element of $S'$ is $A'[i]$ among all possible $3$-sets we can pick, since there are exactly six ordered $3$-sets corresponding to each $3$-set, these probabilities will be equal. We will compute the probability that $S'[2] = A[i]$. For any such $S'$, we would need to select the first element from $[i - 1]$ and the third from ${i + 1, \\ldots , n}$. So, there are $(i - 1)(n - i)$ such $3$-sets. The total number of $3$-sets is $\\binom{n}{3} = \\frac{n(n - 1)(n - 2)}{6}$. So, $$p_i = \\frac{6(n - i)(i - 1)}{n(n - 1)(n - 2)}.$$ b. If we let $i = \\lfloor \\frac{n + 1}{2} \\rfloor$, the previous result gets us an increase of $$\\frac{6(\\lfloor\\frac{n - 1}{2}\\rfloor)(n - \\lfloor\\frac{n + 1}{2}\\rfloor)}{n(n - 1)(n - 2)} - \\frac{1}{n}$$ in the limit $n$ going to infinity, we get $$\\lim_{n \\to \\infty} \\frac{\\frac{6(\\lfloor \\frac{n - 1}{2} \\rfloor)(n - \\lfloor \\frac{n + 1}{2} \\rfloor)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} = \\frac{3}{2}.$$ c. To save the messiness, suppose $n$ is a multiple of $3$. We will approximate the sum as an integral, so, $$ \\begin{aligned} \\sum_{i = n / 3}^{2n / 3} & \\approx \\int_{n / 3}^{2n / 3} \\frac{6(-x^2 + nx + x - n)}{n(n - 1)(n - 2)}dx \\\\ & = \\frac{6(-7n^3 / 81 + 3n^3 / 18 + 3n^2 / 18 - n^2 / 3)}{n(n - 1)(n - 2)}, \\end{aligned} $$ which, in the limit $n$ goes to infinity, is $\\frac{13}{27}$ which is a constant that $>\\frac{1}{3}$ as it was in the original randomized quicksort implementation. d. Since the new algorithm always has a \"bad\" choice that is within a constant factor of the original quicksort, it will still have a reasonable probability that the randomness leads us into a bad situation, so, it will still be $n\\lg n$.","title":"7-5 Median-of-3 partition"},{"location":"Chap07/Problems/7-6/","text":"Consider the problem in which we do not know the numbers exactly. Instead, for each number, we know an interval on the real line to which it belongs. That is, we are given $n$ closed intervals of the form $[a_i, b_i]$, where $a_i \\le b_i$. We wish to fuzzy-sort these intervals, i.e., to produce a permutation $\\langle i_1, i_2, \\ldots, i_n \\rangle$ of the intervals such that for $j = 1, 2, \\ldots, n$, there exists $c_j \\in [a_{i_j}, b_{i_j}]$ satisfying $c_1 \\le c_2 \\le \\cdots \\le c_n$. a. Design a randomized algorithm for fuzzy-sorting $n$ intervals. Your algorithm should have the general structure of an algorithm that quicksorts the left endpoints (the $a_i$ values), but it should take advantage of overlapping intervals to improve the running time. (As the intervals overlap more and more, the problem of fuzzy-sorting the intervals becoes progressively easier. Your algorithm should take advantage of such overlapping, to the extend that it exists.) b. Argue that your algorithm runs in expected time $\\Theta(n\\lg n)$ in general, but runs in expected time $\\Theta(n)$ when all of the intervals overlap (i.e., when there exists a value $x$ such that $x \\in [a_i, b_i]$ for all $i$). Your algorithm should not be checking for this case explicitly; rather, its performance should naturally improve as the amount of overlap increases. a. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 FUZZY - PARTITION ( A , p , r ) x = A [ r ] exchange A [ r ] with A [ p ] i = p - 1 k = p for j = p + 1 to r - 1 if b [ j ] < x . a i = i + 1 k = i + 2 exchange A [ i ] with A [ j ] exchange A [ k ] with A [ j ] if b [ j ] \u2265 x . a or a [ j ] \u2264 x . b x . a = max ( a [ j ], x . a ) and x . b = min ( b [ j ], x . b ) k = k + 1 exchange A [ k ] with A [ j ] exchange A [ i + 1 ] with A [ r ] return i + 1 and k + 1 When intervals overlap we treat them as equal elements, thus cutting down on the time required to sort. b. For distinct intervals the algorithm runs exactly as regular quicksort does, so its expected runtime will be $\\Theta(n\\lg n)$ in general. If all of the intervals overlap then the condition on line 12 will be satisfied for every iteration of the for loop. Thus the algorithm returns $p$ and $r$, so only empty arrays remain to be sorted. $\\text{FUZZY-PARTITION}$ will only be called a single time, and since its runtime remains $\\Theta(n)$, the total expected runtime is $\\Theta(n)$.","title":"7-6 Fuzzy sorting of intervals"},{"location":"Chap08/8.1/","text":"8.1-1 What is the smallest possible depth of a leaf in a decision tree for a comparison sort? For a permutation $a_1 \\le a_2 \\le \\ldots \\le a_n$, there are $n - 1$ pairs of relative ordering, thus the smallest possible depth is $n - 1$. 8.1-2 Obtain asymptotically tight bounds on $\\lg(n!)$ without using Stirling's approximation. Instead, evaluate the summation $\\sum_{k = 1}^n \\lg k$ using techniques from Section A.2. $$ \\begin{aligned} \\sum_{k = 1}^n \\lg k & \\le \\sum_{k = 1}^n \\lg n \\\\ & = n\\lg n. \\end{aligned} $$ $$ \\begin{aligned} \\sum_{k = 1}^n \\lg k & = \\sum_{k = 2}^{n / 2} \\lg k + \\sum_{k = n / 2}^n \\lg k \\\\ & \\ge \\sum_{k = 1}^{n / 2} 1 + \\sum_{k = n / 2}^n \\lg n / 2 \\\\ & = \\frac{n}{2} + \\frac{n}{2}(\\lg n - 1) \\\\ & = \\frac{n}{2}\\lg n. \\end{aligned} $$ 8.1-3 Show that there is no comparison sort whose running time is linear for at least half of the $n!$ inputs of length $n$. What about a fraction of $1 / n$ of the inputs of length $n$? What about a fraction $1 / 2^n$? If the sort runs in linear time for $m$ input permutations, then the height $h$ of the portion of the decision tree consisting of the $m$ corresponding leaves and their ancestors is linear. Use the same argument as in the proof of Theorem 8.1 to show that this is impossible for $m = n! / 2$, $n! / n$, or $n! / 2^n$. We have $2^h \\ge m$, which gives us $h \\ge \\lg m$. For all the possible m's given here, $\\lg m = \\Omega(n\\lg n)$, hence $h = \\Omega(n\\lg n)$. In particular, $$ \\begin{aligned} \\lg \\frac{n!}{2} & = \\lg n! - 1 \\ge n\\lg n - n\\lg e - 1, \\\\ \\lg \\frac{n!}{n} & = \\lg n! - \\lg n \\ge n\\lg n - n\\lg e - \\lg n, \\\\ \\lg \\frac{n!}{2^n} & = \\lg n! - n \\ge n\\lg n - n\\lg e - n. \\\\ \\end{aligned} $$ 8.1-4 Suppose that you are given a sequence of $n$ elements to sort. The input sequence consists of $n / k$ subsequences, each containing $k$ elements. The elements in a given subsequence are all smaller than the elements in the succeeding subsequence and larger than the elements in the preceding subsequence. Thus, all that is needed to sort the whole sequence of length $n$ is to sort the $k$ elements in each of the $n / k$ subsequences. Show an $\\Omega(n\\lg k)$ lower bound on the number of comparisons needed to solve this variant of the sorting problem. ($\\textit{Hint:}$ It is not rigorous to simply combine the lower bounds for the individual subsequences.) Let $S$ be a sequence of $n$ elements divided into $n / k$ subsequences each of length $k$ where all of the elements in any subsequence are larger than all of the elements of a preceding subsequence and smaller than all of the elements of a succeeding subsequence. Claim Any comparison-based sorting algorithm to sort $s$ must take $\\Omega(n\\lg k)$ time in the worst case. Proof First notice that, as pointed out in the hint, we cannot prove the lower bound by multiplying together the lower bounds for sorting each subsequence. That would only prove that there is no faster algorithm that sorts the subsequences independently . This was not what we are asked to prove; we cannot introduce any extra assumptions. Now, consider the decision tree of height $h$ for any comparison sort for $S$. Since the elements of each subsequence can be in any order, any of the $k!$ permutations correspond to the final sorted order of a subsequence. And, since there are $n / k$ such subsequences, each of which can be in any order, there are $(k!)^{n / k}$ permutations of $S$ that could correspond to the sorting of some input order. Thus, any decision tree for sorting $S$ must have at least $(k!)^{n / k}$ leaves. Since a binary tree of height $h$ has no more than $2^h$ leaves, we must have $2^h \\ge (k!)^{n / k}$ or $h \\ge \\lg((k!)^{n / k})$. We therefore obtain $$ \\begin{aligned} h & = \\lg((k!)^{n / k}) \\\\ & = (n / k)\\lg(k!) \\\\ & \\ge (n / k)\\lg((k/k)^{k / 2}) \\\\ & = (n / 2)\\lg(k / 2). \\end{aligned} $$ The third line comes from $k!$ having its $k / 2$ largest terms being at least $k / 2$ each. (We implicitly assume here that $k$ is even. We could adjust with \ufb02oors and ceilings if $k$ were odd.) Since there exists at least one path in any decision tree for sorting $S$ that has length at least $(n / 2)\\lg(k / 2)$, the worst-case running time of any comparison-based sorting algorithm for $S$ is $\\Omega(n\\lg k)$.","title":"8.1 Lower bounds for sorting"},{"location":"Chap08/8.1/#81-1","text":"What is the smallest possible depth of a leaf in a decision tree for a comparison sort? For a permutation $a_1 \\le a_2 \\le \\ldots \\le a_n$, there are $n - 1$ pairs of relative ordering, thus the smallest possible depth is $n - 1$.","title":"8.1-1"},{"location":"Chap08/8.1/#81-2","text":"Obtain asymptotically tight bounds on $\\lg(n!)$ without using Stirling's approximation. Instead, evaluate the summation $\\sum_{k = 1}^n \\lg k$ using techniques from Section A.2. $$ \\begin{aligned} \\sum_{k = 1}^n \\lg k & \\le \\sum_{k = 1}^n \\lg n \\\\ & = n\\lg n. \\end{aligned} $$ $$ \\begin{aligned} \\sum_{k = 1}^n \\lg k & = \\sum_{k = 2}^{n / 2} \\lg k + \\sum_{k = n / 2}^n \\lg k \\\\ & \\ge \\sum_{k = 1}^{n / 2} 1 + \\sum_{k = n / 2}^n \\lg n / 2 \\\\ & = \\frac{n}{2} + \\frac{n}{2}(\\lg n - 1) \\\\ & = \\frac{n}{2}\\lg n. \\end{aligned} $$","title":"8.1-2"},{"location":"Chap08/8.1/#81-3","text":"Show that there is no comparison sort whose running time is linear for at least half of the $n!$ inputs of length $n$. What about a fraction of $1 / n$ of the inputs of length $n$? What about a fraction $1 / 2^n$? If the sort runs in linear time for $m$ input permutations, then the height $h$ of the portion of the decision tree consisting of the $m$ corresponding leaves and their ancestors is linear. Use the same argument as in the proof of Theorem 8.1 to show that this is impossible for $m = n! / 2$, $n! / n$, or $n! / 2^n$. We have $2^h \\ge m$, which gives us $h \\ge \\lg m$. For all the possible m's given here, $\\lg m = \\Omega(n\\lg n)$, hence $h = \\Omega(n\\lg n)$. In particular, $$ \\begin{aligned} \\lg \\frac{n!}{2} & = \\lg n! - 1 \\ge n\\lg n - n\\lg e - 1, \\\\ \\lg \\frac{n!}{n} & = \\lg n! - \\lg n \\ge n\\lg n - n\\lg e - \\lg n, \\\\ \\lg \\frac{n!}{2^n} & = \\lg n! - n \\ge n\\lg n - n\\lg e - n. \\\\ \\end{aligned} $$","title":"8.1-3"},{"location":"Chap08/8.1/#81-4","text":"Suppose that you are given a sequence of $n$ elements to sort. The input sequence consists of $n / k$ subsequences, each containing $k$ elements. The elements in a given subsequence are all smaller than the elements in the succeeding subsequence and larger than the elements in the preceding subsequence. Thus, all that is needed to sort the whole sequence of length $n$ is to sort the $k$ elements in each of the $n / k$ subsequences. Show an $\\Omega(n\\lg k)$ lower bound on the number of comparisons needed to solve this variant of the sorting problem. ($\\textit{Hint:}$ It is not rigorous to simply combine the lower bounds for the individual subsequences.) Let $S$ be a sequence of $n$ elements divided into $n / k$ subsequences each of length $k$ where all of the elements in any subsequence are larger than all of the elements of a preceding subsequence and smaller than all of the elements of a succeeding subsequence. Claim Any comparison-based sorting algorithm to sort $s$ must take $\\Omega(n\\lg k)$ time in the worst case. Proof First notice that, as pointed out in the hint, we cannot prove the lower bound by multiplying together the lower bounds for sorting each subsequence. That would only prove that there is no faster algorithm that sorts the subsequences independently . This was not what we are asked to prove; we cannot introduce any extra assumptions. Now, consider the decision tree of height $h$ for any comparison sort for $S$. Since the elements of each subsequence can be in any order, any of the $k!$ permutations correspond to the final sorted order of a subsequence. And, since there are $n / k$ such subsequences, each of which can be in any order, there are $(k!)^{n / k}$ permutations of $S$ that could correspond to the sorting of some input order. Thus, any decision tree for sorting $S$ must have at least $(k!)^{n / k}$ leaves. Since a binary tree of height $h$ has no more than $2^h$ leaves, we must have $2^h \\ge (k!)^{n / k}$ or $h \\ge \\lg((k!)^{n / k})$. We therefore obtain $$ \\begin{aligned} h & = \\lg((k!)^{n / k}) \\\\ & = (n / k)\\lg(k!) \\\\ & \\ge (n / k)\\lg((k/k)^{k / 2}) \\\\ & = (n / 2)\\lg(k / 2). \\end{aligned} $$ The third line comes from $k!$ having its $k / 2$ largest terms being at least $k / 2$ each. (We implicitly assume here that $k$ is even. We could adjust with \ufb02oors and ceilings if $k$ were odd.) Since there exists at least one path in any decision tree for sorting $S$ that has length at least $(n / 2)\\lg(k / 2)$, the worst-case running time of any comparison-based sorting algorithm for $S$ is $\\Omega(n\\lg k)$.","title":"8.1-4"},{"location":"Chap08/8.2/","text":"8.2-1 Using Figure 8.2 as a model, illustrate the operation of $\\text{COUNTING-SORT}$ on the array $A = \\langle 6, 0, 2, 0, 1, 3, 4, 6, 1, 3, 2 \\rangle$. We have that $C = \\langle 2, 4, 6, 8, 9, 9, 11 \\rangle$. Then, after successive iterations of the loop on lines 10-12, we have $$ \\begin{aligned} B & = \\langle, , , , , 2, , , , , \\rangle, \\\\ B & = \\langle, , , , , 2, , 3, , , \\rangle, \\\\ B & = \\langle, , , 1, , 2, , 3, , , \\rangle \\end{aligned} $$ and at the end, $$B = \\langle 0, 0, 1, 1, 2, 2, 3, 3, 4, 6, 6 \\rangle.$$ 8.2-2 Prove that $\\text{COUNTING-SORT}$ is stable. Consider two elements in the input array, $A[s]$ and $A[s + 1]$, such that $A[s] = A[s + 1]$, $1 \\le s \\le n - 1$. After the execution of the final fo $r$ loop in $\\text{COUNTING-SORT}$, $B[p] = A[s + 1]$ and $B[p - 1] = A[s]$, $2 \\le p \\le n$. $A[s]$ and $A[s + 1]$ appear in the output array $B$ in the same order as they appear in $A$. Therefore, $\\text{COUNTING-SORT}$ is stable. 8.2-3 Suppose that we were to rewrite the for loop header in line 10 of the $\\text{COUNTING-SORT}$ as 1 10 for j = 1 to A . length Show that the algorithm still works properly. Is the modified algorithm stable? [The following solution also answers Exercise 8.2-2.] Notice that the correctness argument in the text does not depend on the order in which $A$ is processed. The algorithm is correct no matter what order is used! But the modified algorithm is not stable. As before, in the final for loop an element equal to one taken from $A$ earlier is placed before the earlier one (i.e., at a lower index position) in the output array $B$. The original algorithm was stable because an element taken from $A$ later started out with a lower index than one taken earlier. But in the modified algorithm, an element taken from $A$ later started out with a higher index than one taken earlier. In particular, the algorithm still places the elements with value $k$ in positions $C[k - 1] + 1$ through $C[k]$, but in the reverse order of their appearance in $A$. 8.2-4 Describe an algorithm that, given n integers in the range $0$ to $k$, preprocesses its input and then answers any query about how many of the $n$ integers fall into a range $[a..b]$ in $O(1)$ time. Your algorithm should use $\\Theta(n + k)$ preprocessing time. Compute the $C$ array as is done in counting sort. The number of integers in the range $[a..b]$ is $C[b] - C[a - 1]$, where we interpret $C[-1]$ as $0$.","title":"8.2 Counting sort"},{"location":"Chap08/8.2/#82-1","text":"Using Figure 8.2 as a model, illustrate the operation of $\\text{COUNTING-SORT}$ on the array $A = \\langle 6, 0, 2, 0, 1, 3, 4, 6, 1, 3, 2 \\rangle$. We have that $C = \\langle 2, 4, 6, 8, 9, 9, 11 \\rangle$. Then, after successive iterations of the loop on lines 10-12, we have $$ \\begin{aligned} B & = \\langle, , , , , 2, , , , , \\rangle, \\\\ B & = \\langle, , , , , 2, , 3, , , \\rangle, \\\\ B & = \\langle, , , 1, , 2, , 3, , , \\rangle \\end{aligned} $$ and at the end, $$B = \\langle 0, 0, 1, 1, 2, 2, 3, 3, 4, 6, 6 \\rangle.$$","title":"8.2-1"},{"location":"Chap08/8.2/#82-2","text":"Prove that $\\text{COUNTING-SORT}$ is stable. Consider two elements in the input array, $A[s]$ and $A[s + 1]$, such that $A[s] = A[s + 1]$, $1 \\le s \\le n - 1$. After the execution of the final fo $r$ loop in $\\text{COUNTING-SORT}$, $B[p] = A[s + 1]$ and $B[p - 1] = A[s]$, $2 \\le p \\le n$. $A[s]$ and $A[s + 1]$ appear in the output array $B$ in the same order as they appear in $A$. Therefore, $\\text{COUNTING-SORT}$ is stable.","title":"8.2-2"},{"location":"Chap08/8.2/#82-3","text":"Suppose that we were to rewrite the for loop header in line 10 of the $\\text{COUNTING-SORT}$ as 1 10 for j = 1 to A . length Show that the algorithm still works properly. Is the modified algorithm stable? [The following solution also answers Exercise 8.2-2.] Notice that the correctness argument in the text does not depend on the order in which $A$ is processed. The algorithm is correct no matter what order is used! But the modified algorithm is not stable. As before, in the final for loop an element equal to one taken from $A$ earlier is placed before the earlier one (i.e., at a lower index position) in the output array $B$. The original algorithm was stable because an element taken from $A$ later started out with a lower index than one taken earlier. But in the modified algorithm, an element taken from $A$ later started out with a higher index than one taken earlier. In particular, the algorithm still places the elements with value $k$ in positions $C[k - 1] + 1$ through $C[k]$, but in the reverse order of their appearance in $A$.","title":"8.2-3"},{"location":"Chap08/8.2/#82-4","text":"Describe an algorithm that, given n integers in the range $0$ to $k$, preprocesses its input and then answers any query about how many of the $n$ integers fall into a range $[a..b]$ in $O(1)$ time. Your algorithm should use $\\Theta(n + k)$ preprocessing time. Compute the $C$ array as is done in counting sort. The number of integers in the range $[a..b]$ is $C[b] - C[a - 1]$, where we interpret $C[-1]$ as $0$.","title":"8.2-4"},{"location":"Chap08/8.3/","text":"8.3-1 Using Figure 8.3 as a model, illustrate the operation of $\\text{RADIX-SORT}$ on the following list of English words: COW, DOG, SEA, RUG, ROW, MOB, BOX, TAB, BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX. $$ \\begin{array}{cccc} 0 & 1 & 2 & 3 \\\\ \\hline \\text{COW} & \\text{SE$\\textbf{A}$} & \\text{T$\\textbf{A}$B} & \\text{$\\textbf{B}$AR} \\\\ \\text{DOG} & \\text{TE$\\textbf{A}$} & \\text{B$\\textbf{A}$R} & \\text{$\\textbf{B}$IG} \\\\ \\text{SEA} & \\text{MO$\\textbf{B}$} & \\text{E$\\textbf{A}$R} & \\text{$\\textbf{B}$OX} \\\\ \\text{RUG} & \\text{TA$\\textbf{B}$} & \\text{T$\\textbf{A}$R} & \\text{$\\textbf{C}$OW} \\\\ \\text{ROW} & \\text{DO$\\textbf{G}$} & \\text{S$\\textbf{E}$A} & \\text{$\\textbf{D}$IG} \\\\ \\text{MOB} & \\text{RU$\\textbf{G}$} & \\text{T$\\textbf{E}$A} & \\text{$\\textbf{D}$OG} \\\\ \\text{BOX} & \\text{DI$\\textbf{G}$} & \\text{D$\\textbf{I}$G} & \\text{$\\textbf{E}$AR} \\\\ \\text{TAB} & \\text{BI$\\textbf{G}$} & \\text{B$\\textbf{I}$G} & \\text{$\\textbf{F}$OX} \\\\ \\text{BAR} & \\text{BA$\\textbf{R}$} & \\text{M$\\textbf{O}$B} & \\text{$\\textbf{M}$OB} \\\\ \\text{EAR} & \\text{EA$\\textbf{R}$} & \\text{D$\\textbf{O}$G} & \\text{$\\textbf{N}$OW} \\\\ \\text{TAR} & \\text{TA$\\textbf{R}$} & \\text{C$\\textbf{O}$W} & \\text{$\\textbf{R}$OW} \\\\ \\text{DIG} & \\text{CO$\\textbf{W}$} & \\text{R$\\textbf{O}$W} & \\text{$\\textbf{R}$UG} \\\\ \\text{BIG} & \\text{RO$\\textbf{W}$} & \\text{N$\\textbf{O}$W} & \\text{$\\textbf{S}$EA} \\\\ \\text{TEA} & \\text{NO$\\textbf{W}$} & \\text{B$\\textbf{O}$X} & \\text{$\\textbf{T}$AB} \\\\ \\text{NOW} & \\text{BO$\\textbf{X}$} & \\text{F$\\textbf{O}$X} & \\text{$\\textbf{T}$AR} \\\\ \\text{FOX} & \\text{FO$\\textbf{X}$} & \\text{R$\\textbf{U}$G} & \\text{$\\textbf{T}$EA} \\\\ \\end{array} $$ 8.3-2 Which of the following sorting algorithms are stable: insertion sort, merge sort, heapsort, and quicksort? Give a simple scheme that makes any sorting algorithm stable. How much additional time and space does your scheme entail? Insertion sort is stable. When inserting $A[j]$ into the sorted sequence $A[1..j - 1]$, we do it the following way: compare $A[j]$ to $A[i]$, starting with $i = j - 1$ and going down to $i = 1$. Continue at long as $A[j] < A[i]$. Merge sort as defined is stable, because when two elements compared are equal, the tie is broken by taking the element from array $L$ which keeps them in the original order. Heapsort and quicksort are not stable. One scheme that makes a sorting algorithm stable is to store the index of each element (the element's place in the original ordering) with the element. When comparing two elements, compare them by their values and break ties by their indices. Additional space requirements: For $n$ elements, their indices are $1 \\ldots n$. Each can be written in $\\lg n$ bits, so together they take $O(n\\lg n)$ additional space. Additional time requirements: The worst case is when all elements are equal. The asymptotic time does not change because we add a constant amount of work to each comparison. 8.3-3 Use induction to prove that radix sort works. Where does your proof need the assumption that the intermediate sort is stable? Basis: If $d = 1$, there's only one digit, so sorting on that digit sorts the array. Inductive step: Assuming that radix sort works for $d - 1$ digits, we'll show that it works for $d$ digits. Radix sort sorts separately on each digit, starting from digit $1$. Thus, radix sort of $d$ digits, which sorts on digits $1, \\ldots, d$ is equivalent to radix sort of the low-order $d - 1$ digits followed by a sort on digit $d$. By our induction hypothesis, the sort of the low-order $d - 1$ digits works, so just before the sort on digit $d$, the elements are in order according to their low-order $d - 1$ digits. The sort on digit $d$ will order the elements by their $d$th digit. Consider two elements, $a$ and $b$, with dth digits $a_d$ and $b_d$ respectively. If $a_d < b_d$, the sort will put $a$ before $b$, which is correct, since $a < b$ regardless of the low-order digits. If $a_d > b_d$, the sort will put $a$ after $b$, which is correct, since $a > b$ regardless of the low-order digits. If $a_d = b_d$, the sort will leave $a$ and $b$ in the same order they were in, because it is stable. But that order is already correct, since the correct order of $a$ and $b$ is determined by the low-order $d - 1$ digits when their dth digits are equal, and the elements are already sorted by their low-order $d - 1$ digits. If the intermediate sort were not stable, it might rearrange elements whose $d$th digits were equal\u2014elements that were in the right order after the sort on their lower-order digits. 8.3-4 Show how to sort $n$ integers in the range $0$ to $n^3 - 1$ in $O(n)$ time. Treat the numbers as $3$-digit numbers in radix $n$. Each digit ranges from $0$ to $n - 1$. Sort these $3$-digit numbers with radix sort. There are $3$ calls to counting sort, each taking $\\Theta(n + n) = \\Theta(n)$ time, so that the total time is $\\Theta(n)$. 8.3-5 $\\star$ In the first card-sorting algorithm in this section, exactly how many sorting passes are needed to sort $d$-digit decimal numbers in the worst case? How many piles of cards would an operator need to keep track of in the worst case? Since a pass consists of one iteration of the loop on line 1\u20132, only $d$ passes are needed. Since each of the digits can be one of ten decimal numbers, the most number of piles that would be needed to be kept track of is $10$.","title":"8.3 Radix sort"},{"location":"Chap08/8.3/#83-1","text":"Using Figure 8.3 as a model, illustrate the operation of $\\text{RADIX-SORT}$ on the following list of English words: COW, DOG, SEA, RUG, ROW, MOB, BOX, TAB, BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX. $$ \\begin{array}{cccc} 0 & 1 & 2 & 3 \\\\ \\hline \\text{COW} & \\text{SE$\\textbf{A}$} & \\text{T$\\textbf{A}$B} & \\text{$\\textbf{B}$AR} \\\\ \\text{DOG} & \\text{TE$\\textbf{A}$} & \\text{B$\\textbf{A}$R} & \\text{$\\textbf{B}$IG} \\\\ \\text{SEA} & \\text{MO$\\textbf{B}$} & \\text{E$\\textbf{A}$R} & \\text{$\\textbf{B}$OX} \\\\ \\text{RUG} & \\text{TA$\\textbf{B}$} & \\text{T$\\textbf{A}$R} & \\text{$\\textbf{C}$OW} \\\\ \\text{ROW} & \\text{DO$\\textbf{G}$} & \\text{S$\\textbf{E}$A} & \\text{$\\textbf{D}$IG} \\\\ \\text{MOB} & \\text{RU$\\textbf{G}$} & \\text{T$\\textbf{E}$A} & \\text{$\\textbf{D}$OG} \\\\ \\text{BOX} & \\text{DI$\\textbf{G}$} & \\text{D$\\textbf{I}$G} & \\text{$\\textbf{E}$AR} \\\\ \\text{TAB} & \\text{BI$\\textbf{G}$} & \\text{B$\\textbf{I}$G} & \\text{$\\textbf{F}$OX} \\\\ \\text{BAR} & \\text{BA$\\textbf{R}$} & \\text{M$\\textbf{O}$B} & \\text{$\\textbf{M}$OB} \\\\ \\text{EAR} & \\text{EA$\\textbf{R}$} & \\text{D$\\textbf{O}$G} & \\text{$\\textbf{N}$OW} \\\\ \\text{TAR} & \\text{TA$\\textbf{R}$} & \\text{C$\\textbf{O}$W} & \\text{$\\textbf{R}$OW} \\\\ \\text{DIG} & \\text{CO$\\textbf{W}$} & \\text{R$\\textbf{O}$W} & \\text{$\\textbf{R}$UG} \\\\ \\text{BIG} & \\text{RO$\\textbf{W}$} & \\text{N$\\textbf{O}$W} & \\text{$\\textbf{S}$EA} \\\\ \\text{TEA} & \\text{NO$\\textbf{W}$} & \\text{B$\\textbf{O}$X} & \\text{$\\textbf{T}$AB} \\\\ \\text{NOW} & \\text{BO$\\textbf{X}$} & \\text{F$\\textbf{O}$X} & \\text{$\\textbf{T}$AR} \\\\ \\text{FOX} & \\text{FO$\\textbf{X}$} & \\text{R$\\textbf{U}$G} & \\text{$\\textbf{T}$EA} \\\\ \\end{array} $$","title":"8.3-1"},{"location":"Chap08/8.3/#83-2","text":"Which of the following sorting algorithms are stable: insertion sort, merge sort, heapsort, and quicksort? Give a simple scheme that makes any sorting algorithm stable. How much additional time and space does your scheme entail? Insertion sort is stable. When inserting $A[j]$ into the sorted sequence $A[1..j - 1]$, we do it the following way: compare $A[j]$ to $A[i]$, starting with $i = j - 1$ and going down to $i = 1$. Continue at long as $A[j] < A[i]$. Merge sort as defined is stable, because when two elements compared are equal, the tie is broken by taking the element from array $L$ which keeps them in the original order. Heapsort and quicksort are not stable. One scheme that makes a sorting algorithm stable is to store the index of each element (the element's place in the original ordering) with the element. When comparing two elements, compare them by their values and break ties by their indices. Additional space requirements: For $n$ elements, their indices are $1 \\ldots n$. Each can be written in $\\lg n$ bits, so together they take $O(n\\lg n)$ additional space. Additional time requirements: The worst case is when all elements are equal. The asymptotic time does not change because we add a constant amount of work to each comparison.","title":"8.3-2"},{"location":"Chap08/8.3/#83-3","text":"Use induction to prove that radix sort works. Where does your proof need the assumption that the intermediate sort is stable? Basis: If $d = 1$, there's only one digit, so sorting on that digit sorts the array. Inductive step: Assuming that radix sort works for $d - 1$ digits, we'll show that it works for $d$ digits. Radix sort sorts separately on each digit, starting from digit $1$. Thus, radix sort of $d$ digits, which sorts on digits $1, \\ldots, d$ is equivalent to radix sort of the low-order $d - 1$ digits followed by a sort on digit $d$. By our induction hypothesis, the sort of the low-order $d - 1$ digits works, so just before the sort on digit $d$, the elements are in order according to their low-order $d - 1$ digits. The sort on digit $d$ will order the elements by their $d$th digit. Consider two elements, $a$ and $b$, with dth digits $a_d$ and $b_d$ respectively. If $a_d < b_d$, the sort will put $a$ before $b$, which is correct, since $a < b$ regardless of the low-order digits. If $a_d > b_d$, the sort will put $a$ after $b$, which is correct, since $a > b$ regardless of the low-order digits. If $a_d = b_d$, the sort will leave $a$ and $b$ in the same order they were in, because it is stable. But that order is already correct, since the correct order of $a$ and $b$ is determined by the low-order $d - 1$ digits when their dth digits are equal, and the elements are already sorted by their low-order $d - 1$ digits. If the intermediate sort were not stable, it might rearrange elements whose $d$th digits were equal\u2014elements that were in the right order after the sort on their lower-order digits.","title":"8.3-3"},{"location":"Chap08/8.3/#83-4","text":"Show how to sort $n$ integers in the range $0$ to $n^3 - 1$ in $O(n)$ time. Treat the numbers as $3$-digit numbers in radix $n$. Each digit ranges from $0$ to $n - 1$. Sort these $3$-digit numbers with radix sort. There are $3$ calls to counting sort, each taking $\\Theta(n + n) = \\Theta(n)$ time, so that the total time is $\\Theta(n)$.","title":"8.3-4"},{"location":"Chap08/8.3/#83-5-star","text":"In the first card-sorting algorithm in this section, exactly how many sorting passes are needed to sort $d$-digit decimal numbers in the worst case? How many piles of cards would an operator need to keep track of in the worst case? Since a pass consists of one iteration of the loop on line 1\u20132, only $d$ passes are needed. Since each of the digits can be one of ten decimal numbers, the most number of piles that would be needed to be kept track of is $10$.","title":"8.3-5 $\\star$"},{"location":"Chap08/8.4/","text":"8.4-1 Using Figure 8.4 as a model, illustrate the operation of $\\text{BUCKET-SORT}$ on the array $A = \\langle .79, .13, .16, .64, .39, .20, .89, .53, .71, .42 \\rangle$. $$ \\begin{array}{cl} R & \\\\ \\hline 0 & \\\\ 1 & .13 .16 \\\\ 2 & .20 \\\\ 3 & .39 \\\\ 4 & .42 \\\\ 5 & .53 \\\\ 6 & .64 \\\\ 7 & \\\\ 8 & .79 .71 \\\\ 9 & .89 \\\\ \\end{array} $$ $$A = \\langle.13, .16, .20, .39, .42, .53, .64, .71, .79, .89 \\rangle.$$ 8.4-2 Explain why the worst-case running time for bucket sort is $\\Theta(n^2)$. What simple change to the algorithm preserves its linear average-case running time and makes its worst-case running time $O(n\\lg n)$? The worst-case running time for the bucket-sort algorithm occurs when the assumption of uniformly distributed input does not hold. If, for example, all the input ends up in the first bucket, then in the insertion sort phase it needs to sort all the input, which takes $O(n^2)$. A simple change that will preserve the linear expected running time and make the worst-case running time $O(n\\lg n)$ is to use a worst-case $O(n\\lg n)$-time algorithm, such as merge sort, instead of insertion sort when sorting the buckets. 8.4-3 Let $X$ be a random variable that is equal to the number of heads in two flips of a fair coin. What is $\\text E[X^2]$? What is $\\text E^2[X]$? $$ \\begin{aligned} \\text E[X] & = 2 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1 \\\\ \\text E[X^2] & = 4 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1.5 \\\\ \\text E^2[X] & = \\text E[X] \\cdot \\text E[X] = 1 \\cdot 1 = 1. \\end{aligned} $$ 8.4-4 $\\star$ We are given $n$ points in the unit circle, $p_i = (x_i, y_i)$, such that $0 < x_i^2 + y_i^2 \\le 1$ for $i = 1, 2, \\ldots, n$. Suppose that the points are uniformly distributed; that is, the probability of finding a point in any region of the circle is proportional to the area of that region. Design an algorithm with an average-case running time of $\\Theta(n)$ to sort the $n$ points by their distances $d_i = \\sqrt{x_i^2 + y_i^2}$ from the origin. ($\\textit{Hint:}$ Design the bucket sizes in $\\text{BUCKET-SORT}$ to re\ufb02ect the uniform distribution of the points in the unit circle.) Bucket sort by radius, $$ \\begin{aligned} \\pi r_i^2 & = \\frac{i}{n} \\cdot \\pi 1^2 \\\\ r_i & = \\sqrt{\\frac{i}{n}}. \\end{aligned} $$ 8.4-5 $\\star$ A probability distribution function $P(x)$ for a random variable $X$ is defined by $P(x) = \\Pr\\{X \\le x\\}$. Suppose that we draw a list of $n$ random variables $X_1, X_2, \\ldots, X_n$ from a continuous probability distribution function $P$ that is computable in $O(1)$ time. Give an algorithm that sorts these numbers in linear average-case time. Bucket sort by $p_i$, so we have $n$ buckets: $[p_0, p_1), [p_1, p_2), \\dots, [p_{n - 1}, p_n)$. Note that not all buckets are the same size, which is ok as to ensure linear run time, the inputs should on average be uniformly distributed amongst all buckets, of which the intervals defined with $p_i$ will do so. $p_i$ is defined as follows: $$P(p_i) = \\frac{i}{n}.$$","title":"8.4 Bucket sort"},{"location":"Chap08/8.4/#84-1","text":"Using Figure 8.4 as a model, illustrate the operation of $\\text{BUCKET-SORT}$ on the array $A = \\langle .79, .13, .16, .64, .39, .20, .89, .53, .71, .42 \\rangle$. $$ \\begin{array}{cl} R & \\\\ \\hline 0 & \\\\ 1 & .13 .16 \\\\ 2 & .20 \\\\ 3 & .39 \\\\ 4 & .42 \\\\ 5 & .53 \\\\ 6 & .64 \\\\ 7 & \\\\ 8 & .79 .71 \\\\ 9 & .89 \\\\ \\end{array} $$ $$A = \\langle.13, .16, .20, .39, .42, .53, .64, .71, .79, .89 \\rangle.$$","title":"8.4-1"},{"location":"Chap08/8.4/#84-2","text":"Explain why the worst-case running time for bucket sort is $\\Theta(n^2)$. What simple change to the algorithm preserves its linear average-case running time and makes its worst-case running time $O(n\\lg n)$? The worst-case running time for the bucket-sort algorithm occurs when the assumption of uniformly distributed input does not hold. If, for example, all the input ends up in the first bucket, then in the insertion sort phase it needs to sort all the input, which takes $O(n^2)$. A simple change that will preserve the linear expected running time and make the worst-case running time $O(n\\lg n)$ is to use a worst-case $O(n\\lg n)$-time algorithm, such as merge sort, instead of insertion sort when sorting the buckets.","title":"8.4-2"},{"location":"Chap08/8.4/#84-3","text":"Let $X$ be a random variable that is equal to the number of heads in two flips of a fair coin. What is $\\text E[X^2]$? What is $\\text E^2[X]$? $$ \\begin{aligned} \\text E[X] & = 2 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1 \\\\ \\text E[X^2] & = 4 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1.5 \\\\ \\text E^2[X] & = \\text E[X] \\cdot \\text E[X] = 1 \\cdot 1 = 1. \\end{aligned} $$","title":"8.4-3"},{"location":"Chap08/8.4/#84-4-star","text":"We are given $n$ points in the unit circle, $p_i = (x_i, y_i)$, such that $0 < x_i^2 + y_i^2 \\le 1$ for $i = 1, 2, \\ldots, n$. Suppose that the points are uniformly distributed; that is, the probability of finding a point in any region of the circle is proportional to the area of that region. Design an algorithm with an average-case running time of $\\Theta(n)$ to sort the $n$ points by their distances $d_i = \\sqrt{x_i^2 + y_i^2}$ from the origin. ($\\textit{Hint:}$ Design the bucket sizes in $\\text{BUCKET-SORT}$ to re\ufb02ect the uniform distribution of the points in the unit circle.) Bucket sort by radius, $$ \\begin{aligned} \\pi r_i^2 & = \\frac{i}{n} \\cdot \\pi 1^2 \\\\ r_i & = \\sqrt{\\frac{i}{n}}. \\end{aligned} $$","title":"8.4-4 $\\star$"},{"location":"Chap08/8.4/#84-5-star","text":"A probability distribution function $P(x)$ for a random variable $X$ is defined by $P(x) = \\Pr\\{X \\le x\\}$. Suppose that we draw a list of $n$ random variables $X_1, X_2, \\ldots, X_n$ from a continuous probability distribution function $P$ that is computable in $O(1)$ time. Give an algorithm that sorts these numbers in linear average-case time. Bucket sort by $p_i$, so we have $n$ buckets: $[p_0, p_1), [p_1, p_2), \\dots, [p_{n - 1}, p_n)$. Note that not all buckets are the same size, which is ok as to ensure linear run time, the inputs should on average be uniformly distributed amongst all buckets, of which the intervals defined with $p_i$ will do so. $p_i$ is defined as follows: $$P(p_i) = \\frac{i}{n}.$$","title":"8.4-5 $\\star$"},{"location":"Chap08/Problems/8-1/","text":"In this problem, we prove a probabilistic $\\Omega(n\\lg n)$ lower bound on the running time of any deterministic or randomized comparison sort on $n$ distinct input elements. We begin by examining a deterministic comparison sort $A$ with decision tree $T_A$. We assume that every permutation of $A$'s inputs is equally likely. a. Suppose that each leaf of $T_A$ is labeled with the probability that it is reached given a random input. Prove that exactly $n!$ leaves are labeled $1 / n!$ and that the rest are labeled $0$. b. Let $D(T)$ denote the external path length of a decision tree $T$; that is, $D(T)$ is the sum of the depths of all the leaves of $T$. Let $T$ be a decision tree with $k > 1$ leaves, and let $LT$ and $RT$ be the left and right subtrees of $T$. Show that $D(T) = D(LT) + D(RT)+k$. c. Let $d(k)$ be the minimum value of $D(T)$ over all decision trees $T$ with $k > 1$ leaves. Show that $d(k) = \\min _{1 \\le i \\le k - 1} \\{ d(i) + d(k - i) + k \\}$. ($\\textit{Hint:}$ Consider a decision tree $T$ with $k$ leaves that achieves the minimum. Let $i_0$ be the number of leaves in $LT$ and $k - i_0$ the number of leaves in $RT$.) d. Prove that for a given value of $k > 1$ and $i$ in the range $1 \\le i \\le k - 1$, the function $i\\lg i + (k - i) \\lg(k - i)$ is minimized at $i = k / 2$. Conclude that $d(k) = \\Omega(k\\lg k)$. e. Prove that $D(T_A) = \\Omega(n!\\lg(n!))$, and conclude that the average-case time to sort $n$ elements is $\\Omega(n\\lg n)$. Now, consider a randomized comparison sort $B$. We can extend the decision-tree model to handle randomization by incorporating two kinds of nodes: ordinary comparison nodes and \"randomization\" nodes. A randomization node models a random choice of the form $\\text{RANDOM}(1, r)$ made by algorithm $B$; the node has $r$ children, each of which is equally likely to be chosen during an execution of the algorithm. f. Show that for any randomized comparison sort $B$, there exists a deterministic comparison sort $A$ whose expected number of comparisons is no more than those made by $B$. a. For a comparison algorithm $A$ to sort, no two input permutations can reach the same leaf of the decision tree, so there must be at least $n!$ leaves reached in $T_A$, one for each possible input permutation. Since $A$ is a deterministic algorithm, it must always reach the same leaf when given a particular permutation as input, so at most $n!$ leaves are reached (one for each permutation). Therefore exactly $n!$ leaves are reached, one for each input permutation. These $n!$ leaves will each have probability $1 / n!$, since each of the $n!$ possible permutations is the input with the probability $1 / n!$. Any remaining leaves will have probability $0$, since they are not reached for any input. Without loss of generality, we can assume for the rest of this problem that paths leading only to $0$-probability leaves aren't in the tree, since they cannot affect the running time of the sort. That is, we can assume that $T_A$ consists of only the $n!$ leaves labeled $1 / n!$ and their ancestors. b. If $k > 1$, then the root of $T$ is not a leaf. This implies that all of $T$'s leaves are leaves in $LT$ and $RT$. Since every leaf at depth $h$ in $LT$ or $RT$ has depth $h + 1$ in $T$, $D(T)$ must be the sum of $D(LT)$, $D(RT)$, and $k$, the total number of leaves. To prove this last assertion, let $d_T(x) =$ depth of node $x$ in tree $T$. Then, $$ \\begin{aligned} D(T) & = \\sum_{x \\in T} d_T(x) \\\\ & = \\sum_{x \\in LT} d_T(x) + \\sum_{x \\in RT} d_T(x) \\\\ & = \\sum_{x \\in LT} (d_{LT}(x) + 1) + \\sum_{x \\in RT} (d_{RT}(x) + 1) \\\\ & = \\sum_{x \\in LT} d_{LT}(x) + \\sum_{x \\in RT} d_{RT}(x) + \\sum_{x \\in T} 1 \\\\ & = D(LT) + D(RT) + k. \\\\ \\end{aligned} $$ c. To show that $d(k) = \\min_{1\\le i\\le k - 1}{d(i) + d(k - i) + k}$ we will show separately that $$ \\begin{aligned} & d(k) \\le \\min_{1\\le i\\le k - 1}{d(i) + d(k - i) + k} \\\\ \\text{and } & d(k) \\ge \\min_{1\\le i\\le k - 1}{d(i) + d(k - i) + k}. \\end{aligned} $$ To show that $d(k) \\le \\min_{1\\le i\\le k - 1}{d(i) + d(k - i) + k}$, we need only show that $d(k) \\le d(i) + d(k - i) + k$, for $i = 1, 2, \\ldots, k - 1$. For any $i$ from $1$ to $k - 1$ we can find trees $RT$ with $i$ leaves and $LT$ with $k - i$ leaves such that $D(RT) = d(i)$ and $D(LT) = d(k - i)$. Construct $T$ such that $RT$ and $LT$ are the right and left subtrees of $T$'s root respectively. Then $$ \\begin{aligned} d(k) & \\le D(T) & \\text{(by definition of $d$ as min $D(T)$ value)} \\\\ & = D(RT) + D(LT) + k & \\text{(by part (b))} \\\\ & = d(i) + d(k - i) + k. & \\text{(by choice of $RT$ and $LT$)} \\end{aligned} $$ To show that $d(k) \\ge \\min_{1\\le i\\le k - 1}{d(i) + d(k - i) + d}$, we need only show that $d(k) \\ge d(i) + d(k - i) + k$, for some $i$ in $\\{1, 2, \\ldots, k - 1\\}$. Take the tree $T$ with $k$ leaves such that $D(T) = d(k)$, let $RT$ and $LT$ be $T$'s right and left subtree, respectively, and let $i$ be the number of leaves in $RT$. Then $k - i$ is the number of leaves in $LT$ and $$ \\begin{aligned} d(k) & = D(T) & \\text{(by choice of $T$)} \\\\ & = D(RT) + D(LT) + k & \\text{(by part (b))} \\\\ & \\ge d(i) + d(k - i) + k. & \\text{(by definition of $d$ as min $D(T)$ value)} \\end{aligned} $$ Neither $i$ nor $k - i$ can be $0$ (and hence $1 \\le i \\le k - 1$), since if one of these were $0$, either $RT$ or $LT$ would contain all $k$ leaves of $T$, and that $k$-leaf subtree would have a $D$ equal to $D(T) - k$ (by part (b)), contradicting the choice of $T$ as the $k$-leaf tree with the minimum $D$. d. Let $f_k(i) = i\\lg i + (k - i)\\lg(k - i)$. To find the value of $i$ that minimizes $f_k$, find the $i$ for which the derivative of $f_k$ with respect to $i$ is $0$: $$ \\begin{aligned} f_k'(i) & = \\frac{d}{di} \\Big(\\frac{i\\ln i + (k - i)\\ln(k - i)}{\\ln 2}\\Big) \\\\ & = \\frac{\\ln i + 1 - \\ln(k - i) - 1}{\\ln 2} \\\\ & = \\frac{\\ln i - \\ln(k - i)}{\\ln 2} \\end{aligned} $$ is $0$ at $i = k / 2$. To verify this is indeed a minimum (not a maximum), check that the second derivative of $f_k$ is positive at $i = k / 2$: $$ \\begin{aligned} f_k''(i) & = \\frac{d}{di}\\Big(\\frac{\\ln i - \\ln(k - i)}{\\ln 2}\\Big) \\\\ & = \\frac{1}{\\ln 2}\\Big(\\frac{1}{i} + \\frac{1}{k - i}\\Big). \\end{aligned} $$ $$ \\begin{aligned} f_k''(k / 2) & = \\frac{1}{\\ln 2}\\Big(\\frac{2}{k} + \\frac{2}{k}\\Big) \\\\ & = \\frac{1}{\\ln 2} \\cdot \\frac{4}{k} \\\\ & > 0 & \\text{since $k > 1$}. \\end{aligned} $$ Now we use substitution to prove $d(k) = \\Omega(k\\lg k)$. The base case of the induction is satisfied because $d(1) \\ge 0 = c \\cdot 1 \\cdot \\lg 1$ for any constant $c$. For the inductive step we assume that $d(i) \\ge ci\\lg i$ for $1 \\le i \\le k - 1$, where $c$ is some constant to be determined. $$ \\begin{aligned} d(k) & = \\min_{1\\le i\\le k - 1} {d(i) + d(k - i) + k} \\\\ & \\ge \\min_{1\\le i\\le k - 1} {c(i\\lg i + (k - i)\\lg(k - i)) + k} \\\\ & = \\min_{1\\le i\\le k - 1} {cf_k(i) + k} \\\\ & = c\\Big(\\frac{k}{2}\\lg\\frac{k}{2}\\Big(k - \\frac{k}{2}\\Big)\\lg\\Big(k - \\frac{k}{2}\\Big)\\Big) + k \\\\ & = ck\\lg\\Big(\\frac{k}{2}\\Big) + k \\\\ & = c(k\\lg k - k) + k \\\\ & = ck\\lg k + (k - ck) \\\\ & \\ge ck\\lg k & \\text{if $c \\le 1$}, \\end{aligned} $$ and so $d(k) = \\Omega(k\\lg k)$. e. Using the result of part (d) and the fact that $T_A$ (as modified in our solution to part (a)) has $n!$ leaves, we can conclude that $$D(T_A) \\ge d(n!) = \\Omega(n!\\lg(n!)).$$ $D(T_A)$ is the sum of the decision-tree path lengths for sorting all input permutations, and the path lengths are proportional to the run time. Since the $n!$ permutations have equal probability $1 / n!$, the expected time to sort $n$ random elements (1 input permutation) is the total time for all permutations divided by $n!$: $$\\frac{\\Omega(n!\\lg(n!))}{n!} = \\Omega(\\lg(n!)) = \\Omega(n\\lg n).$$ f. We will show how to modify a randomized decision tree (algorithm) to define a deterministic decision tree (algorithm) that is at least as good as the randomized one in terms of the average number of comparisons. At each randomized node, pick the child with the smallest subtree (the subtree with the smallest average number of comparisons on a path to a leaf). Delete all the other children of the randomized node and splice out the randomized node itself. The deterministic algorithm corresponding to this modified tree still works, because the randomized algorithm worked no matter which path was taken from each randomized node. The average number of comparisons for the modified algorithm is no larger than the average number for the original randomized tree, since we discarded the higher-average subtrees in each case. In particular, each time we splice out a randomized node, we leave the overall average less than or equal to what it was, because the same set of input permutations reaches the modified subtree as before, but those inputs are handled in less than or equal to average time than before, and the rest of the tree is unmodified. The randomized algorithm thus takes at least as much time on average as the corresponding deterministic one. (We've shown that the expected running time for a deterministic comparison sort is \u007f$\\Omega(n\\lg n)$, hence the expected time for a randomized comparison sort is also $\\Omega(n\\lg n)$).","title":"8-1 Probabilistic lower bounds on comparison sorting"},{"location":"Chap08/Problems/8-2/","text":"Suppose that we have an array of $n$ data records to sort and that the key of each record has the value $0$ or $1$. An algorithm for sorting such a set of records might possess some subset of the following three desirable characteristics: The algorithm runs in $O(n)$ time. The algorithm is stable. The algorithm sorts in place, using no more than a constant amount of storage space in addition to the original array. a. Give an algorithm that satisfies criteria 1 and 2 above. b. Give an algorithm that satisfies criteria 1 and 3 above. c. Give an algorithm that satisfies criteria 2 and 3 above. d. Can you use any of your sorting algorithms from parts (a)\u2013(c) as the sorting method used in line 2 of $\\text{RADIX-SORT}$, so that $\\text{RADIX-SORT}$ sorts $n$ records with $b$-bit keys in $O(bn)$ time? Explain how or why not. e. Suppose that the $n$ records have keys in the range from $1$ to $k$. Show how to modify counting sort so that it sorts the records in place in $O(n + k)$ time. You may use $O(k)$ storage outside the input array. Is your algorithm stable? ($\\textit{Hint:}$ How would you do it for $k = 3$?) a. Counting-Sort. b. Quicksort-Partition. c. Insertion-Sort. d. (a) Yes. (b) No. (c) No. e. Using $O(k)$ outside the input-arr. 1 2 3 4 5 6 7 8 9 10 11 COUNTING - SORT ( A , k ) let C [ 0. . k ] be a new array for i = 0 to k C [ i ] = 0 for i = 1 to A . length C [ A [ i ]] = C [ A [ i ]] + 1 // C[i] now contains the number of elements equal to i p = 0 for i = 0 to k for j = 1 to C [ i ] p = p + 1 A [ p ] = i Not stable, in place, in $O(n + k)$.","title":"8-2 Sorting in place in linear time"},{"location":"Chap08/Problems/8-3/","text":"a. You are given an array of integers, where different integers may have different numbers of digits, but the total number of digits over all the integers in the array is $n$. Show how to sort the array in $O(n)$ time. b. You are given an array of strings, where different strings may have different numbers of characters, but the total number of characters over all the strings is $n$. Show how to sort the strings in $O(n)$ time. (Note that the desired order here is the standard alphabetical order; for example, $\\text a < \\text{ab} < \\text b$.) a. The usual, unadorned radix sort algorithm will not solve this problem in the required time bound. The number of passes, $d$, would have to be the number of digits in the largest integer. Suppose that there are $m$ integers; we always have $m \\le n$. In the worst case, we would have one integer with $n / 2$ digits and $n / 2$ integers with one digit each. We assume that the range of a single digit is constant. Therefore, we would have $d = n / 2$ and $m = n / 2 + 1$, and so the running time would be $\\Theta(dm) = \\Theta(n^2)$. Let us assume without loss of generality that all the integers are positive and have no leading zeros. (If there are negative integers or $0$, deal with the positive numbers, negative numbers, and $0$ separately.) Under this assumption, we can observe that integers with more digits are always greater than integers with fewer digits. Thus, we can first sort the integers by number of digits (using counting sort), and then use radix sort to sort each group of integers with the same length. Noting that each integer has between $1$ and $n$ digits, let $m_i$ be the number of integers with $i$ digits, for $i = 1, 2, \\ldots, n$. Since there are $n$ digits altogether, we have $\\sum_{i = 1}^n i \\cdot m_i = n$. It takes $O(n)$ time to compute how many digits all the integers have and, once the numbers of digits have been computed, it takes $O(m + n) = O(n)$ time to group the integers by number of digits. To sort the group with $m_i$ digits by radix sort takes $\\Theta(i \\cdot m_i)$ time. The time to sort all groups, therefore, is $$ \\begin{aligned} \\sum_{i = 1}^n \\Theta(i\\cdot m_i) & = \\Theta\\Bigg(\\sum_{i = 1}^n i \\cdot m_i\\Bigg) \\\\ & = \\Theta(n). \\end{aligned} $$ b. One way to solve this problem is by a radix sort from right to left. Since the strings have varying lengths, however, we have to pad out all strings that are shorter than the longest string. The padding is on the right end of the string, and it's with a special character that is lexicographically less than any other character (e.g., in C, the character '\\0' with ASCII value $0$). Of course, we don't have to actually change any string; if we want to know the $j$th character of a string whose length is $k$, then if $j > k$, the $j$th character is the pad character. Unfortunately, this scheme does not always run in the required time bound. Suppose that there are $m$ strings and that the longest string has $d$ characters. In the worst case, one string has $n / 2$ characters and, before padding, $n / 2$ strings have one character each. As in part (a), we would have $d = n / 2$ and $m = n / 2 + 1$. We still have to examine the pad characters in each pass of radix sort, even if we don't actually create them in the strings. Assuming that the range of a single character is constant, the running time of radix sort would be $\\Theta(dm) = \\Theta(n^2)$. To solve the problem in $O(n)$ time, we use the property that, if the first letter of string $x$ is lexicographically less that the first letter of string $y$, then $x$ is lexicographically less than $y$, regardless of the lengths of the two strings. We take advantage of this property by sorting the strings on the first letter, using counting sort. We take an empty string as a special case and put it first. We gather together all strings with the same first letter as a group. Then we recurse, within each group, based on each string with the first letter removed. The correctness of this algorithm is straightforward. Analyzing the running time is a bit trickier. Let us count the number of times that each string is sorted by a call of counting sort. Suppose that the ith string, $s_i$, has length $l_i$. Then $s_i$ is sorted by at most $l_i + 1$ counting sorts. (The \"+1\" is because it may have to be sorted as an empty string at some point; for example, ab and a end up in the same group in the first pass and are then ordered based on b and the empty string in the second pass. The string a is sorted its length, $1$, time plus one more time.) A call of counting sort on t strings takes $\\Theta(t)$ time (remembering that the number of different characters on which we are sorting is a constant.) Thus, the total time for all calls of counting sort is $$ \\begin{aligned} O\\Bigg(\\sum_{i = 1}^m (l_i + 1)\\Bigg) & = O\\Bigg(\\sum_{i = 1}^m (l_i + m)\\Bigg) \\\\ & = O(n + m) \\\\ & = O(n), \\end{aligned} $$ where the second line follows from $\\sum_{i = 1}^m l_i = n$, and the last line is because $m \\le n$.","title":"8-3 Sorting variable-length items"},{"location":"Chap08/Problems/8-4/","text":"Suppose that you are given $n$ red and $n$ blue water jugs, all of different shapes and sizes. All red jugs hold different amounts of water, as do the blue ones. Moreover, for every red jug, there is a blue jug that holds the same amount of water, and vice versa. Your task is to find a grouping of the jugs into pairs of red and blue jugs that hold the same amount of water. To do so, you may perform the following operation: pick a pair of jugs in which one is red and one is blue, fill the red jug with water, and then pour the water into the blue jug. This operation will tell you whether the red or the blue jug can hold more water, or that they have the same volume. Assume that such a comparison takes one time unit. Your goal is to find an algorithm that makes a minimum number of comparisons to determine the grouping. Remember that you may not directly compare two red jugs or two blue jugs. a. Describe a deterministic algorithm that uses $\\Theta(n^2)$ comparisons to group the jugs into pairs. b. Prove a lower bound of $\\Omega(n\\lg n)$ for the number of comparisons that an algorithm solving this problem must make. c. Give a randomized algorithm whose expected number of comparisons is $O(n\\lg n)$, and prove that this bound is correct. What is the worst-case number of comparisons for your algorithm? a. Compare each red jug with each blue jug. Since there are $n$ red jugs and $n$ blue jugs, that will take $\\Theta(n^2)$ comparisons in the worst case. b. To solve the problem, an algorithm has to perform a series of comparisons until it has enough information to determine the matching. We can view the computation of the algorithm in terms of a decision tree. Every internal node is labeled with two jugs (one red, one blue) which we compare, and has three outgoing edges (red jug smaller, same size, or larger than the blue jug). The leaves are labeled with a unique matching of jugs. The height of the decision tree is equal to the worst-case number of comparisons the algorithm has to make to determine the matching. To bound that size, let us first compute the number of possible matchings for n red and n blue jugs. If we label the red jugs from $1$ to $n$ and we also label the blue jugs from $1$ to $n$ before starting the comparisons, every outcome of the algorithm can be represented as a set $$\\text{\\{$i, \\pi(i): 1 \\le i \\le n$ and $\\pi$ is a permutation on \\{$1, \\ldots, n$\\}\\}},$$ which contains the pairs of red jugs (first component) and blue jugs (second component) that are matched up. Since every permutation $\\pi$ corresponds to a different outcome, there must be exactly $n!$ different results. Now we can bound the height $h$ of our decision tree. Every tree with a branching factor of $3$ (every inner node has at most three children) has at most $3^h$ leaves. Since the decison tree must have at least $n!$ children, it follows that $$3^h \\ge n! \\ge (n / e)^n \\Rightarrow h \\ge n\\log_3 n - n\\log_3 e = \\Omega(n\\lg n).$$ So any algorithm solving the problem must use $\\Omega(n\\lg n)$ comparisons. c. Assume that the red jugs are labeled with numbers $1, 2, \\ldots, n$ and so are the blue jugs. The numbers are arbitrary and do not correspond to the volumes of jugs, but are just used to refer to the jugs in the algorithm description. Moreover, the output of the algorithm will consist of $n$ distinct pairs $(i, j)$, where the red jug $i$ and the blue jug $j$ have the same volume. The procedure $\\text{MATCH-JUGS}$ takes as input two sets representing jugs to be matched: $R \\subseteq \\{1, \\ldots, n\\}$, representing red jugs, and $B \\subseteq \\{1, \\ldots, n\\}$, representing blue jugs. We will call the procedure only with inputs that can be matched; one necessary condition is that $|R| = |B|$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 MATCH - JUGS ( R , B ) if | R | == 0 // sets are empty return if | R | == 1 // sets contain just one jug each let R = { r } and B = { b } output \"(r, b)\" return else r = a randomly chosen jug in R compare r to every jug of B B [ < ] = the set of jugs in B that are smaller than r B [ > ] = the set of jugs in B that are larger than r b = the one jug in B with the same size as r compare b to every jug of R - { r } R [ < ] = the set of jugs in R that are smaller than b R [ > ] = the set of jugs in R that are larger than b output \"(r, b)\" MATCH - JUGS ( R [ < ], B [ < ]) MATCH - JUGS ( R [ > ], B [ > ]) Correctness can be seen as follows (remember that $|R| = |B|$ in each call). Once we pick $r$ randomly from $R$, there will be a matching among the jugs in volume smaller than $r$ (which are in the sets $R_<$ and $B_<$), and likewise between the jugs larger than $r$ (which are in $R_>$ and $B_>$). Termination is also easy to see: since $|R_<| + |R_>| < |R|$ in every recursive step, the size of the first parameter reduces with every recursive call. It eventually must reach $0$ or $1$, in which case the recursion terminates. What about the running time? The analysis of the expected number of comparisons is similar to that of the quicksort algorithm in Section 7.4.2. Let us order the jugs as $r_1, \\ldots, r_n$ and $b_1, \\ldots,b_n$ where $r_i < r_{i + 1}$ and $b_i < b_{i + 1}$ for $i = 1, \\ldots, n$, and $r_i = b_i$. Our analysis uses indicator random variables $$X_{ij} = \\text I\\{\\text{red jug $r_i$ is compared to blue jug $b_j$}\\}.$$ As in quicksort, a given pair $r_i$ and $b_j$ is compared at most once. When we compare $r_i$ to every jug in $B$, jug $r_i$ will not be put in either $R_<$ or $R_>$. When we compare $b_i$ to every jug in $R - \\{r_i\\}$, jug $b_i$ is not put into either $B_<$ or $B_>$. The total number of comparisons is $$X = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n X_{ij}.$$ To calculate the expected value of $X$, we follow the quicksort analysis to arrive at $$\\text E[X] = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\Pr\\{r_i \\text{ is compared to } b_j\\}.$$ As in the quicksort analysis, once we choose a jug $r_k$ such that $r_i < r_k < b_j$, we will put $r_i$ in $R_<$ and $b_j$ in $R_>$, and so $r_i$ and $b_j$ will never be compared again. Let us denote $R_{ij} = \\{r_i, \\ldots, r_j\\}$. Then jugs $r_i$ and $b_j$ will be compared if and only if the first jug in $R_{ij}$ to be chosen is either $r_i$ or $r_j$. Still following the quicksort analysis, until a jug from $R_{ij}$ is chosen, the entire set $R_{ij}$ is together. Any jug in $R_{ij}$ is equally likely to be first one chosen. Since $|R_{ij}| = j - i + 1$, the probability of any given jug being the first one chosen in $R_{ij}$ is $1 / (j - i + 1)$. The remainder of the analysis is the same as the quicksort analysis, and we arrive at the solution of $O(n\\lg n)$ comparisons. Just like in quicksort, in the worst case we always choose the largest (or smallest) jug to partition the sets, which reduces the set sizes by only $1$. The running time then obeys the recurrence $T(n) = T(n - 1) + \\Theta(n)$, and the number of comparisons we make in the worst case is $T(n) = \\Theta(n^2)$.","title":"8-4 Water jugs"},{"location":"Chap08/Problems/8-5/","text":"Suppose that, instead of sorting an array, we just require that the elements increase on average. More precisely, we call an $n$-element array $A$ k-sorted if, for all $i = 1, 2, \\ldots, n - k$, the following holds: $$\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\le \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}.$$ a. What does it mean for an array to be $1$-sorted? b. Give a permutation of the numbers $1, 2, \\ldots, 10$ that is $2$-sorted, but not sorted. c. Prove that an $n$-element array is $k$-sorted if and only if $A[i] \\le A[i + k]$ for all $i = 1, 2, \\ldots, n - k$. d. Give an algorithm that $k$-sorts an $n$-element array in $O(n\\lg (n / k))$ time. We can also show a lower bound on the time to produce a $k$-sorted array, when $k$ is a constant. e. Show that we can sort a $k$-sorted array of length $n$ in $O(n\\lg k)$ time. ($\\textit{Hint:}$ Use the solution to Exercise 6.5-9.) f. Show that when $k$ is a constant, $k$-sorting an $n$-element array requires $\\Omega(n\\lg n)$ time. ($\\textit{Hint:}$ Use the solution to the previous part along with the lower bound on comparison sorts.) a. Ordinary sorting b. $2, 1, 4, 3, 6, 5, 8, 7, 10, 9$. c. $$ \\begin{aligned} \\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} & \\le \\frac{\\sum_{j = i + 1}^{i + k}A[j]}{k} \\\\ \\sum_{j = i}^{i + k- 1 } A[j] & \\le \\sum_{j = i + 1}^{i + k} A[j] \\\\ A[i] & \\le A[i + k]. \\end{aligned} $$ d. Shell-Sort, i.e., We split the $n$-element array into $k$ part. For each part, we use Insertion-Sort (or Quicksort) to sort in $O(n / k \\lg(n / k))$ time. Therefore, the total running time is $k \\cdot O(n / k \\lg(n / k)) = O(n\\lg(n / k))$. e. Using a heap, we can sort a $k$-sorted array of length $n$ in $O(n\\lg k)$ time. (The height of the heap is $\\lg k$, the solution to Exercise 6.5-9.) f. The lower bound of sorting each part is $\\Omega(n / k\\lg(n / k))$, so the total lower bound is $\\Theta(n\\lg n(/k))$. Since $k$ is a constant, therefore $\\Theta(n\\lg(n / k)) = \\Omega(n\\lg n)$.","title":"8-5 Average sorting"},{"location":"Chap08/Problems/8-6/","text":"The problem of merging two sorted lists arises frequently. We have seen a procedure for it as the subroutine $\\text{MERGE}$ in Section 2.3.1. In this problem, we will prove a lower bound of $2n - 1$ on the worst-case number of comparisons required to merge two sorted lists, each containing $n$ items. First we will show a lower bound of $2n - o(n)$ comparisons by using a decision tree. a. Given $2n$ numbers, compute the number of possible ways to divide them into two sorted lists, each with $n$ numbers. b. Using a decision tree and your answer to part (a), show that any algorithm that correctly merges two sorted lists must perform at least $2n - o(n)$ comparisons. Now we will show a slightly tighter $2n - 1$ bound. c. Show that if two elements are consecutive in the sorted order and from different lists, then they must be compared. d. Use your answer to the previous part to show a lower bound of $2n - 1$ comparisons for merging two sorted lists. a. There are $\\binom{2n}{n}$ ways to divide $2n$ numbers into two sorted lists, each with $n$ numbers. b. Based on Exercise C.1.13, $$ \\begin{aligned} \\binom{2n}{n} & \\le 2^h \\\\ h & \\ge \\lg\\frac{(2n)!}{(n!)^2} \\\\ & = \\lg (2n!) - 2\\lg (n!) \\\\ & = \\Theta(2n\\lg 2n) - 2\\Theta(n\\lg n) \\\\ & = \\Theta(2n). \\end{aligned} $$ c. We have to know the order of the two consecutive elements. d. Let list $A = 1, 3, 5, \\ldots, 2n - 1$ and $B = 2, 4, 6, \\ldots, 2n$. By part (c), we must compare $1$ with $2$, $2$ with $3$, $3$ with $4$, and so on up until we compare $2n - 1$ with $2n$. This amounts to a total of $2n - 1$ comparisons.","title":"8-6 Lower bound on merging sorted lists"},{"location":"Chap08/Problems/8-7/","text":"A compare-exchange operation on two array elements $A[i]$ and $A[j]$, where $i < j$, has the form 1 2 3 COMPARE - EXCHANGE ( A , i , j ) if A [ i ] > A [ j ] exchange A [ i ] with A [ j ] After the compare-exchange operation, we know that $A[i] \\le A[j]$. An oblivious compare-exchange algorithm operates solely by a sequence of prespecified compare-exchange operations. The indices of the positions compared in the sequence must be determined in advance, and although they can depend on the number of elements being sorted, they cannot depend on the values being sorted, nor can they depend on the result of any prior compare-exchange operation. For example, here is insertion sort expressed as an oblivious compare-exchange algorithm: 1 2 3 4 INSERTION - SORT ( A ) for j = 2 to A . length for i = j - 1 downto 1 COMPARE - EXCHANGE ( A , i , i + 1 ) The 0-1 sorting lemma provides a powerful way to prove that an oblivious compare-exchange algorithm produces a sorted result. It states that if an oblivious compare-exchange algorithm correctly sorts all input sequences consisting of only $0$s and $1$s, then it correctly sorts all inputs containing arbitrary values. You will prove the $0$-$1$ sorting lemma by proving its contrapositive: if an oblivious compare-exchange algorithm fails to sort an input containing arbitrary values, then it fails to sort some $0$-$1$ input. Assume that an oblivious compare-exchange algorithm $\\text X$ fails to correctly sort the array $A[1..n]$. Let $A[p]$ be the smallest value in $A$ that algorithm $\\text X$ puts into the wrong location, and let $A[q]$ be the value that algorithm $\\text X$ moves to the location into which $A[p]$ should have gone. Define an array $B[1..n]$ of $0$s and $1$s as follows: $$ B[i] = \\begin{cases} 0 & \\text{if $A[i] \\le A[p]$}, \\\\ 1 & \\text{if $A[i] > A[p]$}. \\end{cases} $$ a. Argue that $A[q] > A[p]$, so that $B[p] = 0$ and $B[q] = 1$. b. To complete the proof of the $0$-$1$ sorting lemma, prove that algorithm $\\text X$ fails to sort array $B$ correctly. Now you will use the $0$-$1$ sorting lemma to prove that a particular sorting algorithm works correctly. The algorithm, columnsort , works on a rectangular array of $n$ elements. The array has $r$ rows and $s$ columns (so that $n = rs$), subject to three restrictions: $r$ must be even, $s$ must be a divisor of $r$, and $r \\ge 2 s^2$. When columnsort completes, the array is sorted in column-major order : reading down the columns, from left to right, the elements monotonically increase. Columnsort operates in eight steps, regardless of the value of $n$. The odd steps are all the same: sort each column individually. Each even step is a fixed permutation. Here are the steps: Sort each column. Transpose the array, but reshape it back to $r$ rows and $s$ columns. In other words, turn the leftmost column into the top $r / s$ rows, in order; turn the next column into the next $r / s$ rows, in order; and so on. Sort each column. Perform the inverse of the permutation performed in step 2. Sort each column. Shift the top half of each column into the bottom half of the same column, and shift the bottom half of each column into the top half of the next column to the right. Leave the top half of the leftmost column empty. Shift the bottom half of the last column into the top half of a new rightmost column, and leave the bottom half of this new column empty. Sort each column. Perform the inverse of the permutation performed in step 6. Figure 8.5 shows an example of the steps of columnsort with $r = 6$ and $s = 3$. (Even though this example violates the requirement that $r \\ge 2s^2$, it happens to work.) c. Argue that we can treat columnsort as an oblivious compare-exchange algorithm, even if we do not know what sorting method the odd steps use. Although it might seem hard to believe that columnsort actually sorts, you will use the $0$-$1$ sorting lemma to prove that it does. The $0$-$1$ sorting lemma applies because we can treat columnsort as an oblivious compare-exchange algorithm. A couple of definitions will help you apply the $0$-$1$ sorting lemma. We say that an area of an array is clean if we know that it contains either all $0$s or all $1$s. Otherwise, the area might contain mixed $0$s and $1$s, and it is dirty . From here on, assume that the input array contains only $0$s and $1$s, and that we can treat it as an array with $r$ rows and $s$ columns. d. Prove that after steps 1\u20133, the array consists of some clean rows of $0$s at the top, some clean rows of $1$s at the bottom, and at most $s$ dirty rows between them. e. Prove that after step 4, the array, read in column - major order, starts with a clean area of $0$s, ends with a clean area of $1$s, and has a dirty area of at most $s^2$ elements in the middle. f. Prove that steps 5\u20138 produce a fully sorted $0$-$1$ output. Conclude that columnsort correctly sorts all inputs containing arbitrary values. g. Now suppose that $s$ does not divide $r$. Prove that after steps 1\u20133, the array consists of some clean rows of $0$s at the top, some clean rows of $1$s at the bottom, and at most $2s - 1$ dirty rows between them. How large must $r$ be, compared with $s$, for columnsort to correctly sort when $s$ does not divide $r$? h. Suggest a simple change to step 1 that allows us to maintain the requirement that $r \\ge 2s^2$ even when $s$ does not divide $r$, and prove that with your change, columnsort correctly sorts. a. $A[q]$ must go the wrong place, because it goes where $A[p]$ should go. Since $A[p]$ is the smallest value in array $A$ that goes to the wrong array location, $A[p]$ must be smaller than $A[q]$. b. From how we have defined the array $B$, we have that if $A[i] \\le A[j]$ then $B[i] \\le B[j]$. Therefore, algorithm $\\text X$ performs the same sequence of exchanges on array $B$ as it does on array $A$. The output produced on array $A$ is of the form $\\ldots A[q] \\ldots A[p] \\ldots$, and so the output produced on array $B$ is of the form $\\ldots B[q] \\ldots B[p] \\ldots$, or $\\ldots 1 \\ldots 0 \\ldots$. Hence algorithm $\\text X$ fails to sort array $B$ correctly. c. The even steps perform fixed permutations. The odd steps sort each column by some sorting algorithm, which might not be an oblivious compare-exchange algorithm. But the result of sorting each column would be the same as if we did use an oblivious compare-exchange algorithm. d. After step 1, each column has $0$s on top and $1$s on the bottom, with at most one transition between $0$s and $1$s, and it is a $0 \\to 1$ transition. (As we read the array in column - major order, all $1 \\to 0$ transitions occur between adjacent columns.) After step 2, therefore, each consecutive group of $r / s$ rows, read in row-major order, has at most one transition, and again it is a $0 \\to 1$ transition. All $1 \\to 0$ transitions occur at the end of a group of $r / s$ rows. Since there are s groups of $r / s$ rows, there are at most $s$ dirty rows, and the rest of the rows are clean. Step 3 moves the $0$s to the top rows and the $1$s to the bottom rows. The $s$ dirty rows are somewhere in the middle. e. The dirty area after step 3 is at most $s$ rows high and $s$ columns wide, and so its area is at most $s^2$. Step 4 turns the clean $0$s in the top rows into a clean area on the left, the clean $1$s in the bottom rows into a clean area on the right, and the dirty area of size $s^2$ is between the two clean areas. f. First, we argue that if the dirty area after step 4 has size at most $r / 2$, then steps 5\u20138 complete the sorting. If the dirty area has size at most $r / 2$ (half a column), then it either resides entirely in one column or it resides in the bottom half of one column and the top half of the next column. In the former case, step 5 sorts the column containing the dirty area, and steps 6\u20138 maintain that the array is sorted. In the latter case, step 5 cannot increase the size of the dirty area, step 6 moves the entire dirty area into the same column, step 7 sorts it, and step 8 moves it back. Second, we argue that the dirty area after step 4 has size at most $r / 2$. But that follows immediately from the requirement that $r \\ge 2s^2$ and the property that after step 4, the dirty area has size at most $s^2$. g. If $s$ does not divide $r$, then after step 2, we can see up to $s$ $0 \\to 1$ transitions and $s - 1$ $1 \\to 0$ transitions in the rows. After step 3, we would have up to $2s - 1$ dirty rows, for a dirty area size of at most $2s^2 - s$. To push the correctness proof through, we need $2s^2 - s \\le r / 2$, or $r \\ge 4s^2 - 2s$. h. We can reduce the number of transitions in the rows after step 2 back down to at most $s$ by sorting every other column in reverse order in step 1. Now if we have a transition (either $1 \\to 0$ or $0 \\to 1$) between columns after step 1, then either one of the columns had all $1$s or the other had all $0$s, in which case we would not have a transition within one of the columns.","title":"8-7 The $0$-$1$ sorting lemma and columnsort"},{"location":"Chap09/9.1/","text":"9.1-1 Show that the second smallest of $n$ elements can be found with $n + \\lceil \\lg n \\rceil - 2$ comparisons in the worst case. ($\\textit{Hint:}$ Also find the smallest element.) The smallest of $n$ numbers can be found with $n - 1$ comparisons by conducting a tournament as follows: Compare all the numbers in pairs. Only the smaller of each pair could possibly be the smallest of all $n$, so the problem has been reduced to that of finding the smallest of $\\lceil n / 2 \\rceil$ numbers. Compare those numbers in pairs, and so on, until there's just one number left, which is the answer. To see that this algorithm does exactly $n - 1$ comparisons, notice that each number except the smallest loses exactly once. To show this more formally, draw a binary tree of the comparisons the algorithm does. The $n$ numbers are the leaves, and each number that came out smaller in a comparison is the parent of the two numbers that were compared. Each non-leaf node of the tree represents a comparison, and there are $n - 1$ internal nodes in an $n$-leaf full binary tree (see Exercise (B.5-3)), so exactly $n - 1$ comparisons are made. In the search for the smallest number, the second smallest number must have come out smallest in every comparison made with it until it was eventually compared with the smallest. So the second smallest is among the elements that were compared with the smallest during the tournament. To find it, conduct another tournament (as above) to find the smallest of these numbers. At most $\\lceil \\lg n \\rceil$ (the height of the tree of comparisons) elements were compared with the smallest, so finding the smallest of these takes $\\lceil \\lg n \\rceil - 1$ comparisons in the worst case. The total number of comparisons made in the two tournaments was $$n - 1 + \\lceil \\lg n \\rceil - 1 = n + \\lceil \\lg n \\rceil - 2$$ in the worst case. 9.1-2 $\\star$ Prove the lower bound of $\\lceil 3n / 2 \\rceil - 2$ comparisons in the worst case to find both the maximum and minimum of $n$ numbers. ($\\textit{Hint:}$ Consider how many numbers are potentially either the maximum or minimum, and investigate how a comparison affects these counts.) If $n$ is odd, there are $$ \\begin{aligned} 1 + \\frac{3(n-3)}{2} + 2 & = \\frac{3n}{2} - \\frac{3}{2} \\\\ & = (\\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - \\frac{1}{2}) - \\frac{3}{2} \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons. If $n$ is even, there are $$ \\begin{aligned} 1 + \\frac{3(n - 2)}{2} & = \\frac{3n}{2} - 2 \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons.","title":"9.1 Minimum and maximum"},{"location":"Chap09/9.1/#91-1","text":"Show that the second smallest of $n$ elements can be found with $n + \\lceil \\lg n \\rceil - 2$ comparisons in the worst case. ($\\textit{Hint:}$ Also find the smallest element.) The smallest of $n$ numbers can be found with $n - 1$ comparisons by conducting a tournament as follows: Compare all the numbers in pairs. Only the smaller of each pair could possibly be the smallest of all $n$, so the problem has been reduced to that of finding the smallest of $\\lceil n / 2 \\rceil$ numbers. Compare those numbers in pairs, and so on, until there's just one number left, which is the answer. To see that this algorithm does exactly $n - 1$ comparisons, notice that each number except the smallest loses exactly once. To show this more formally, draw a binary tree of the comparisons the algorithm does. The $n$ numbers are the leaves, and each number that came out smaller in a comparison is the parent of the two numbers that were compared. Each non-leaf node of the tree represents a comparison, and there are $n - 1$ internal nodes in an $n$-leaf full binary tree (see Exercise (B.5-3)), so exactly $n - 1$ comparisons are made. In the search for the smallest number, the second smallest number must have come out smallest in every comparison made with it until it was eventually compared with the smallest. So the second smallest is among the elements that were compared with the smallest during the tournament. To find it, conduct another tournament (as above) to find the smallest of these numbers. At most $\\lceil \\lg n \\rceil$ (the height of the tree of comparisons) elements were compared with the smallest, so finding the smallest of these takes $\\lceil \\lg n \\rceil - 1$ comparisons in the worst case. The total number of comparisons made in the two tournaments was $$n - 1 + \\lceil \\lg n \\rceil - 1 = n + \\lceil \\lg n \\rceil - 2$$ in the worst case.","title":"9.1-1"},{"location":"Chap09/9.1/#91-2-star","text":"Prove the lower bound of $\\lceil 3n / 2 \\rceil - 2$ comparisons in the worst case to find both the maximum and minimum of $n$ numbers. ($\\textit{Hint:}$ Consider how many numbers are potentially either the maximum or minimum, and investigate how a comparison affects these counts.) If $n$ is odd, there are $$ \\begin{aligned} 1 + \\frac{3(n-3)}{2} + 2 & = \\frac{3n}{2} - \\frac{3}{2} \\\\ & = (\\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - \\frac{1}{2}) - \\frac{3}{2} \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons. If $n$ is even, there are $$ \\begin{aligned} 1 + \\frac{3(n - 2)}{2} & = \\frac{3n}{2} - 2 \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons.","title":"9.1-2 $\\star$"},{"location":"Chap09/9.2/","text":"9.2-1 Show that $\\text{RANDOMIZED-SELECT}$ never makes a recursive call to a $0$-length array. Calling a $0$-length array would mean that the second and third arguments are equal. So, if the call is made on line 8, we would need that $p = q - 1$, which means that $q - p + 1 = 0$. However, $i$ is assumed to be a nonnegative number, and to be executing line 8, we would need that $i < k = q - p + 1 = 0$, a contradiction. The other possibility is that the bad recursive call occurs on line 9. This would mean that $q + 1 = r$. To be executing line 9, we need that $i > k = q - p + 1 = r - p$. This would be a nonsensical original call to the array though because we are asking for the ith element from an array of strictly less size. 9.2-2 Argue that the indicator random variable $X_k$ and the value $T(\\max(k - 1, n - k))$ are independent. The probability that $X_k$ is equal to $1$ is unchanged when we know the max of $k - 1$ and $n - k$. In other words, $\\Pr\\{X_k = a \\mid \\max(k - 1, n - k) = m\\} = \\Pr\\{X_k = a\\}$ for $a = 0, 1$ and $m = k - 1, n - k$ so $X_k$ and $\\max(k - 1, n - k)$ are independent. By C.3-5, so are $X_k$ and $T(\\max(k - 1, n - k))$. 9.2-3 Write an iterative version of $\\text{RANDOMIZED-SELECT}$. 1 2 3 4 5 6 7 8 9 10 PARTITION ( A , p , r ) x = A [ r ] i = p for k = p - 1 to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap A [ i ] with A [ r ] return i 1 2 3 4 RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p - 1 , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) 1 2 3 4 5 6 7 8 9 10 11 12 13 RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return A [ q ] if i < k r = q else p = q i = i - k 9.2-4 Suppose we use $\\text{RANDOMIZED-SELECT}$ to select the minimum element of the array $A = \\langle 3, 2, 9, 0, 7, 5, 4, 8, 6, 1 \\rangle$. Describe a sequence of partitions that results in a worst-case performance of $\\text{RANDOMIZED-SELECT}$. When the partition selected is always the maximum element of the array we get worst-case performance. In the example, the sequence would be $\\langle 9, 8, 7, 6, 5, 4, 3, 2, 1, 0 \\rangle$.","title":"9.2 Selection in expected linear time"},{"location":"Chap09/9.2/#92-1","text":"Show that $\\text{RANDOMIZED-SELECT}$ never makes a recursive call to a $0$-length array. Calling a $0$-length array would mean that the second and third arguments are equal. So, if the call is made on line 8, we would need that $p = q - 1$, which means that $q - p + 1 = 0$. However, $i$ is assumed to be a nonnegative number, and to be executing line 8, we would need that $i < k = q - p + 1 = 0$, a contradiction. The other possibility is that the bad recursive call occurs on line 9. This would mean that $q + 1 = r$. To be executing line 9, we need that $i > k = q - p + 1 = r - p$. This would be a nonsensical original call to the array though because we are asking for the ith element from an array of strictly less size.","title":"9.2-1"},{"location":"Chap09/9.2/#92-2","text":"Argue that the indicator random variable $X_k$ and the value $T(\\max(k - 1, n - k))$ are independent. The probability that $X_k$ is equal to $1$ is unchanged when we know the max of $k - 1$ and $n - k$. In other words, $\\Pr\\{X_k = a \\mid \\max(k - 1, n - k) = m\\} = \\Pr\\{X_k = a\\}$ for $a = 0, 1$ and $m = k - 1, n - k$ so $X_k$ and $\\max(k - 1, n - k)$ are independent. By C.3-5, so are $X_k$ and $T(\\max(k - 1, n - k))$.","title":"9.2-2"},{"location":"Chap09/9.2/#92-3","text":"Write an iterative version of $\\text{RANDOMIZED-SELECT}$. 1 2 3 4 5 6 7 8 9 10 PARTITION ( A , p , r ) x = A [ r ] i = p for k = p - 1 to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap A [ i ] with A [ r ] return i 1 2 3 4 RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p - 1 , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) 1 2 3 4 5 6 7 8 9 10 11 12 13 RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return A [ q ] if i < k r = q else p = q i = i - k","title":"9.2-3"},{"location":"Chap09/9.2/#92-4","text":"Suppose we use $\\text{RANDOMIZED-SELECT}$ to select the minimum element of the array $A = \\langle 3, 2, 9, 0, 7, 5, 4, 8, 6, 1 \\rangle$. Describe a sequence of partitions that results in a worst-case performance of $\\text{RANDOMIZED-SELECT}$. When the partition selected is always the maximum element of the array we get worst-case performance. In the example, the sequence would be $\\langle 9, 8, 7, 6, 5, 4, 3, 2, 1, 0 \\rangle$.","title":"9.2-4"},{"location":"Chap09/9.3/","text":"9.3-1 In the algorithm $\\text{SELECT}$, the input elements are divided into groups of $5$. Will the algorithm work in linear time if they are divided into groups of $7$? Argue that $\\text{SELECT}$ does not run in linear time if groups of $3$ are used. For groups of $7$, the algorithm still works in linear time. The number of elements greater than $x$ (and similarly, the number less than $x$) is at least $$4\\Bigg(\\Bigg\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{7} \\Big\\rceil\\Bigg\\rceil - 2\\Bigg) \\ge \\frac{2n}{7} - 8,$$ and the recurrence becomes $$T(n) \\le T(\\lceil n/7 \\rceil) + T(5n/7 + 8) + O(n),$$ which can be shown to be $O(n)$ by substitution, as for the groups of $5$ case in the text. For groups of $3$, however, the algorithm no longer works in linear time. The number of elements greater than $x$, and the number of elements less than $x$, is at least $$2\\Bigg(\\Bigg\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{3} \\Big\\rceil\\Bigg\\rceil - 2\\Bigg) \\ge \\frac{n}{3} - 4,$$ and the recurrence becomes $$T(n) \\le T(\\lceil n / 3 \\rceil) + T(2n / 3 + 4) + O(n),$$ which does not have a linear solution. We can prove that the worst-case time for groups of $3$ is \u007f$\\Omega(n\\lg n)$. We do so by deriving a recurrence for a particular case that takes \u007f$\\Omega(n\\lg n)$ time. In counting up the number of elements greater than $x$ (and similarly, the number less than $x$), consider the particular case in which there are exactly $\\Big\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{3} \\Big\\rceil\\Big\\rceil$ groups with medians $\\ge x$ and in which the \"leftover\" group does contribute 2 elements greater than $x$. Then the number of elements greater than $x$ is exactly $2\\Big(\\Big\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{3} \\Big\\rceil\\Big\\rceil - 1\\Big) + 1$ (the $-1$ discounts $x$'s group, as usual, and the $+1$ is contributed by $x$'s group) $= 2\\lceil n / 6 \\rceil - 1$, and the recursive step for elements $\\le x$ has $n - (2 \\lceil n / 6 \\rceil - 1) \\ge n - (2(n / 6 + 1) - 1) = 2n / 3 - 1$ elements. Observe also that the $O(n)$ term in the recurrence is really $\\Theta(n)$, since the partitioning in step 4 takes $\\Theta(n)$ (not just $O(n)$) time. Thus, we get the recurrence $$ \\begin{aligned} T(n) & \\ge T(\\lceil n / 3 \\rceil) + T(2n / 3 - 1) + \\Theta(n) \\\\ & \\ge T(n / 3) + T(2n / 3 - 1) + \\Theta(n), \\end{aligned} $$ from which you can show that $T(n) \\ge cn\\lg n$ by substitution. You can also see that $T(n)$ is nonlinear by noticing that each level of the recursion tree sums to $n$. [In fact, any odd group size $\\ge 5$ works in linear time.] 9.3-2 Analyze $\\text{SELECT}$ to show that if $n \\ge 140$, then at least $\\lceil n / 4 \\rceil$ elements are greater than the median-of-medians $x$ and at least $\\lceil n / 4 \\rceil$ elements are less than $x$. $$ \\begin{aligned} \\frac{3n}{10} - 6 & \\ge \\lceil \\frac{n}{4} \\rceil \\\\ \\frac{3n}{10} - 6 & \\ge \\frac{n}{4} + 1 \\\\ 12n - 240 & \\ge 10n + 40 \\\\ n & \\ge 140. \\end{aligned} $$ 9.3-3 Show how quicksort can be made to run in $O(n\\lg n)$ time in the worst case, assuming that all elements are distinct. A modification to quicksort that allows it to run in $O(n\\lg n)$ time in the worst case uses the deterministic $\\text{PARTITION}$ algorithm that was modified to take an element to partition around as an input parameter. $\\text{SELECT}$ takes an array $A$, the bounds $p$ and $r$ of the subarray in $A$, and the rank $i$ of an order statistic, and in time linear in the size of the subarray $A[p..r]$ it returns the ith smallest element in $A[p..r]$. 1 2 3 4 5 6 7 BEST - CASE - QUICKSORT ( A , p , r ) if p < r i = floor (( r - p + 1 ) / 2 ) x = SELECT ( A , p , r , i ) q = PARTITION ( x ) BEST - CASE - QUICKSORT ( A , p , q - 1 ) BEST - CASE - QUICKSORT ( A , q + 1 , r ) For an $n$-element array, the largest subarray that $\\text{BEST-CASE-QUICKSORT}$ recurses on has $n / 2$ elements. This situation occurs when $n = r - p + 1$ is even; then the subarray $A[q + 1..r]$ has $n / 2$ elements, and the subarray $A[p..q - 1]$ has $n / 2 - 1$ elements. Because $\\text{BEST-CASE-QUICKSORT}$ always recurses on subarrays that are at most half the size of the original array, the recurrence for the worst-case running time is $T(n) \\le 2T(n / 2) + \\Theta(n) = O(n\\lg n)$. 9.3-4 $\\star$ Suppose that an algorithm uses only comparisons to find the $i$th smallest element in a set of $n$ elements. Show that it can also find the $i - 1$ smaller elements and $n - i$ larger elements without performing additional comparisons. Create a graph with $n$ vertices and draw a directed edge from vertex $i$ to vertex $j$ if the $i$th and $j$th elements of the array are compared in the algorithm and we discover that $A[i] \\ge A[j]$. Observe that $A[i]$ is one of the $i - 1$ smaller elements if there exists a path from $x$ to $i$ in the graph, and $A[i]$ is one of the $n - i$ larger elements if there exists a path from $i$ to $x$ in the graph. Every vertex $i$ must either lie on a path to or from $x$ because otherwise the algorithm can't distinguish between $i \\le x$ and $i \\ge x$. Moreover, if a vertex $i$ lies on both a path to $x$ and a path from $x$, then it must be such that $x \\le A[i] \\le x$,so $x = A[i]$. In this case, we can break ties arbitrarily. 9.3-5 Suppose that you have a \"black-box\" worst-case linear-time median subroutine. Give a simple, linear-time algorithm that solves the selection problem for an arbitrary order statistic. We assume that are given a procedure $\\text{MEDIAN}$ that takes as parameters an array $A$ and subarray indices $p$ and $r$, and returns the value of the median element of $A[p..r]$ in $O(n)$ time in the worst case. Given $\\text{MEDIAN}$, here is a linear-time algorithm $\\text{SELECT}'$ for finding the $i$th smallest element in $A[p..r]$. This algorithm uses the deterministic $\\text{PARTITION}$ algorithm that was modified to take an element to partition around as an input parameter. 1 2 3 4 5 6 7 8 9 10 11 SELECT ' ( A , p , r , i ) if p == r return A [ p ] x = MEDIAN ( A , p , r ) q = PARTITION ( x ) k = q - p + 1 if i == k return A [ q ] else if i < k return SELECT ' ( A , p , q - 1 , i ) else return SELECT ' ( A , q + 1 , r , i - k ) Because $x$ is the median of $A[p..r]$, each of the subarrays $A[p..q - 1]$ and $A[q + 1..r]$ has at most half the number of elements of $A[p..r]$. The recurrence for the worst-case running time of $\\text{SELECT}'$ is $T(n) \\le T(n / 2) + O(n) = O(n)$. 9.3-6 The $k$th quantiles of an $n$-element set are the $k - 1$ order statistics that divide the sorted set into $k$ equal-sized sets (to within $1$). Give an $O(n\\lg k)$-time algorithm to list the $k$th quantiles of a set. Pre-calculate the positions of the quantiles in $O(k)$, we use the $O(n)$ select algorithm to find the $\\lfloor k / 2 \\rfloor$th position, after that the elements are divided into two sets by the pivot the $\\lfloor k / 2 \\rfloor$th position, we do it recursively in the two sets to find other positions. Since the maximum depth is $\\lceil \\lg k \\rceil$, the total running time is $O(n\\lg k)$. 1 2 3 4 5 6 7 8 9 10 PARTITION ( A , p , r ) x = A [ r ] i = p for k = p to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap a [ i ] with a [ r ] return i 1 2 3 4 RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) 1 2 3 4 5 6 7 8 9 10 11 12 13 RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return p , A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return q , A [ q ] if i < k r = q else p = q + 1 i = i - k 1 2 3 4 5 6 7 8 9 10 k - QUANTITLES - SUB ( A , p , r , pos , f , e , quantiles ) if f + 1 > e return mid = ( f + e ) / 2 q , val = RANDOMIZED - SELECT ( A , p , r , pos [ mid ) quantiles [ mid ] = val k = q - p + 1 for i = mid + 1 to e pos [ i ] = pos [ i ] - k k - QUANTILES - SUB ( A , q + 1 , r , pos , mid + 1 , e , quantiles ) 1 2 3 4 5 6 7 8 9 10 11 k - QUANTITLES ( A , k ) num = A . size () / k mod = A . size () % k pos = num [ 1. . k ] for i = 1 to mod pos [ i ] = pos [ i ] + 1 for i = 1 to k pos [ i ] = pos [ i ] + pos [ i - 1 ] quantiles = [ 1. . k ] k - QUANTITLES - SUB ( A , 0 , A . length , pos , 0 , pos . size (), quantiles ) return quantiles 9.3-7 Describe an $O(n)$-time algorithm that, given a set $S$ of $n$ distinct numbers and a positive integer $k \\le n$, determines the $k$ numbers in $S$ that are closest to the median of $S$. Find the median in $O(n)$; create a new array, each element is the absolute value of the original value subtract the median; find the $k$th smallest number in $O(n)$, then the desired values are the elements whose absolute difference with the median is less than or equal to the $k$th smallest number in the new array. 9.3-8 Let $X[1..n]$ and $Y[1..n]$ be two arrays, each containing $n$ numbers already in sorted order. Give an $O(\\lg n)$-time algorithm to find the median of all $2n$ elements in arrays $X$ and $Y$. Let's start out by supposing that the median (the lower median, since we know we have an even number of elements) is in $X$. Let's call the median value $m$, and let's suppose that it's in $X[k]$. Then $k$ elements of $X$ are less than or equal to $m$ and $n - k$ elements of X are greater than or equal to m. We know that in the two arrays combined, there must be $n$ elements less than or equal to $m$ and $n$ elements greater than or equal to $m$, and so there must be $n - k$ elements of $Y$ that are less than or equal to $m$ and $n - (n - k)=k$ elements of $Y$ that are greater than or equal to $m$. Thus, we can check that $X[k]$ is the lower median by checking whether $Y[n - k] \\le X[k] \\le Y[n - k + 1]$. A boundary case occurs for $k = n$. Then $n - k = 0$, and there is no array entry $Y[0]$; we only need to check that $X[n] \\le Y[1]$. Now, if the median is in $X$ but is not in $X[k]$, then the above condition will not hold. If the median is in $X[k']$ , where $k' < k$, then $X[k]$ is above the median, and $Y[n - k + 1] < X[k]$. Conversely, if the median is in $X[k'']$, where $k'' > k$, then $X[k]$ is below the median, and $X[k] < Y[n - k]$. Thus, we can use a binary search to determine whether there is an $X[k]$ such that either $k < n$ and $X[n - k] \\le Y[k] \\le X[n - k + 1]$ or $k = n$ and $X[k] \\le Y[n - k + 1]$; if we find such an $X[k]$, then it is the median. Otherwise, we know that the median is in $Y$, and we use a binary search to find a $Y[k]$ such that either $k < n$ and $X[n - k] \\le Y[k] \\le X[n - k + 1]$ or $k = n$ is the median. Since each binary search takes $O(\\lg n)$ time, we spend a total of $O(\\lg n)$ time. Here's how we write the algorithm in pseudocode: 1 2 3 4 5 6 TWO - ARRAY - MEDIAN ( X , Y ) n = X . length // n also equals Y.length median = FIND - MEDIAN ( X , Y , n , 1 , n ) if median == NOT - FOUND median = FIND - MEDIAN ( Y , X , n , 1 , n ) return median 1 2 3 4 5 6 7 8 9 10 11 FIND - MEDIAN ( A , B , n , low , hign ) if low > high return NOT - FOUND else k = floor (( low + high ) / 2 ) if k == n and A [ n ] \u2264 B [ 1 ] return A [ n ] else if k < n and B [ n - k ] \u2264 A [ k ] \u2264 B [ n - k + 1 ] return A [ k ] else A [ k ] > B [ n - k + 1 ] return FIND - MEDIAN ( A , B , n , low , k - 1 ) else return FIND - MEDIAN ( A , B , n , k + 1 , high ) 9.3-9 Professor Olay is consulting for an oil company, which is planning a large pipeline running east to west through an oil field of $n$ wells. The company wants to connect a spur pipeline from each well directly to the main pipeline along a shortest route (either north or south), as shown in Figure 9.2. Given the $x$- and $y$-coordinates of the wells, how should the professor pick the optimal location of the main pipeline, which would be the one that minimizes the total length of the spurs? Show how to determine the optimal location in linear time. In order to find the optimal placement for Professor Olay's pipeline, we need only find the median(s) of the $y$-coordinates of his oil wells, as the following proof explains. Claim The optimal $y$-coordinate for Professor Olay's east-west oil pipeline is as follows: If $n$ is even, then on either the oil well whose $y$-coordinate is the lower median or the one whose $y$-coordinate is the upper median, or anywhere between them. If $n$ is odd, then on the oil well whose $y$-coordinate is the median. Proof We examine various cases. In each case, we will start out with the pipeline at a particular $y$-coordinate and see what happens when we move it. We'll denote by $s$ the sum of the north-south spurs with the pipeline at the starting location, and $s'$ will denote the sum after moving the pipeline. We start with the case in which n is even. Let us start with the pipeline somewhere on or between the two oil wells whose $y$-coordinates are the lower and upper medians. If we move the pipeline by a vertical distance $d$ without crossing either of the median wells, then $n / 2$ of the wells become $d$ farther from the pipeline and $n / 2$ become $d$ closer, and so $s' = s + dn / 2 - dn / 2 = s$; thus, all locations on or between the two medians are equally good. Now suppose that the pipeline goes through the oil well whose $y$-coordinate is the upper median. What happens when we increase the $y$-coordinate of the pipeline by $d > 0$ units, so that it moves above the oil well that achieves the upper median? All oil wells whose $y$-coordinates are at or below the upper median become d units farther from the pipeline, and there are at least $n / 2 + 1$ such oil wells (the upper median, and every well at or below the lower median). There are at most $n / 2 - 1$ oil wells whose $y$-coordinates are above the upper median, and each of these oil wells becomes at most d units closer to the pipeline when it moves up. Thus, we have a lower bound on $s'$ of $s' \\ge s + d(n / 2 + 1) - d(n / 2 - 1) = s + 2d > s$. We conclude that moving the pipeline up from the oil well at the upper median increases the total spur length. A symmetric argument shows that if we start with the pipeline going through the oil well whose $y$-coordinate is the lower median and move it down, then the total spur length increases. We see, therefore, that when $n$ is even, an optimal placement of the pipeline is anywhere on or between the two medians. Now we consider the case when $n$ is odd. We start with the pipeline going through the oil well whose $y$-coordinate is the median, and we consider what happens when we move it up by $d > 0$ units. All oil wells at or below the median become $d$ units farther from the pipeline, and there are at least $(n + 1) / 2$ such wells (the one at the median and the $(n - 1) / 2$ at or below the median. There are at most $(n - 1) / 2$ oil wells above the median, and each of these becomes at most d units closer to the pipeline. We get a lower bound on $s'$ of $s' \\ge s + d(n + 1) / 2 - d(n - 1) / 2 = s + d > s$, and we conclude that moving the pipeline up from the oil well at the median increases the total spur length. A symmetric argument shows that moving the pipeline down from the median also increases the total spur length, and so the optimal placement of the pipeline is on the median. (claim) Since we know we are looking for the median, we can use the linear-time median-finding algorithm.","title":"9.3 Selection in worst-case linear time"},{"location":"Chap09/9.3/#93-1","text":"In the algorithm $\\text{SELECT}$, the input elements are divided into groups of $5$. Will the algorithm work in linear time if they are divided into groups of $7$? Argue that $\\text{SELECT}$ does not run in linear time if groups of $3$ are used. For groups of $7$, the algorithm still works in linear time. The number of elements greater than $x$ (and similarly, the number less than $x$) is at least $$4\\Bigg(\\Bigg\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{7} \\Big\\rceil\\Bigg\\rceil - 2\\Bigg) \\ge \\frac{2n}{7} - 8,$$ and the recurrence becomes $$T(n) \\le T(\\lceil n/7 \\rceil) + T(5n/7 + 8) + O(n),$$ which can be shown to be $O(n)$ by substitution, as for the groups of $5$ case in the text. For groups of $3$, however, the algorithm no longer works in linear time. The number of elements greater than $x$, and the number of elements less than $x$, is at least $$2\\Bigg(\\Bigg\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{3} \\Big\\rceil\\Bigg\\rceil - 2\\Bigg) \\ge \\frac{n}{3} - 4,$$ and the recurrence becomes $$T(n) \\le T(\\lceil n / 3 \\rceil) + T(2n / 3 + 4) + O(n),$$ which does not have a linear solution. We can prove that the worst-case time for groups of $3$ is \u007f$\\Omega(n\\lg n)$. We do so by deriving a recurrence for a particular case that takes \u007f$\\Omega(n\\lg n)$ time. In counting up the number of elements greater than $x$ (and similarly, the number less than $x$), consider the particular case in which there are exactly $\\Big\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{3} \\Big\\rceil\\Big\\rceil$ groups with medians $\\ge x$ and in which the \"leftover\" group does contribute 2 elements greater than $x$. Then the number of elements greater than $x$ is exactly $2\\Big(\\Big\\lceil \\frac{1}{2} \\Big\\lceil \\frac{n}{3} \\Big\\rceil\\Big\\rceil - 1\\Big) + 1$ (the $-1$ discounts $x$'s group, as usual, and the $+1$ is contributed by $x$'s group) $= 2\\lceil n / 6 \\rceil - 1$, and the recursive step for elements $\\le x$ has $n - (2 \\lceil n / 6 \\rceil - 1) \\ge n - (2(n / 6 + 1) - 1) = 2n / 3 - 1$ elements. Observe also that the $O(n)$ term in the recurrence is really $\\Theta(n)$, since the partitioning in step 4 takes $\\Theta(n)$ (not just $O(n)$) time. Thus, we get the recurrence $$ \\begin{aligned} T(n) & \\ge T(\\lceil n / 3 \\rceil) + T(2n / 3 - 1) + \\Theta(n) \\\\ & \\ge T(n / 3) + T(2n / 3 - 1) + \\Theta(n), \\end{aligned} $$ from which you can show that $T(n) \\ge cn\\lg n$ by substitution. You can also see that $T(n)$ is nonlinear by noticing that each level of the recursion tree sums to $n$. [In fact, any odd group size $\\ge 5$ works in linear time.]","title":"9.3-1"},{"location":"Chap09/9.3/#93-2","text":"Analyze $\\text{SELECT}$ to show that if $n \\ge 140$, then at least $\\lceil n / 4 \\rceil$ elements are greater than the median-of-medians $x$ and at least $\\lceil n / 4 \\rceil$ elements are less than $x$. $$ \\begin{aligned} \\frac{3n}{10} - 6 & \\ge \\lceil \\frac{n}{4} \\rceil \\\\ \\frac{3n}{10} - 6 & \\ge \\frac{n}{4} + 1 \\\\ 12n - 240 & \\ge 10n + 40 \\\\ n & \\ge 140. \\end{aligned} $$","title":"9.3-2"},{"location":"Chap09/9.3/#93-3","text":"Show how quicksort can be made to run in $O(n\\lg n)$ time in the worst case, assuming that all elements are distinct. A modification to quicksort that allows it to run in $O(n\\lg n)$ time in the worst case uses the deterministic $\\text{PARTITION}$ algorithm that was modified to take an element to partition around as an input parameter. $\\text{SELECT}$ takes an array $A$, the bounds $p$ and $r$ of the subarray in $A$, and the rank $i$ of an order statistic, and in time linear in the size of the subarray $A[p..r]$ it returns the ith smallest element in $A[p..r]$. 1 2 3 4 5 6 7 BEST - CASE - QUICKSORT ( A , p , r ) if p < r i = floor (( r - p + 1 ) / 2 ) x = SELECT ( A , p , r , i ) q = PARTITION ( x ) BEST - CASE - QUICKSORT ( A , p , q - 1 ) BEST - CASE - QUICKSORT ( A , q + 1 , r ) For an $n$-element array, the largest subarray that $\\text{BEST-CASE-QUICKSORT}$ recurses on has $n / 2$ elements. This situation occurs when $n = r - p + 1$ is even; then the subarray $A[q + 1..r]$ has $n / 2$ elements, and the subarray $A[p..q - 1]$ has $n / 2 - 1$ elements. Because $\\text{BEST-CASE-QUICKSORT}$ always recurses on subarrays that are at most half the size of the original array, the recurrence for the worst-case running time is $T(n) \\le 2T(n / 2) + \\Theta(n) = O(n\\lg n)$.","title":"9.3-3"},{"location":"Chap09/9.3/#93-4-star","text":"Suppose that an algorithm uses only comparisons to find the $i$th smallest element in a set of $n$ elements. Show that it can also find the $i - 1$ smaller elements and $n - i$ larger elements without performing additional comparisons. Create a graph with $n$ vertices and draw a directed edge from vertex $i$ to vertex $j$ if the $i$th and $j$th elements of the array are compared in the algorithm and we discover that $A[i] \\ge A[j]$. Observe that $A[i]$ is one of the $i - 1$ smaller elements if there exists a path from $x$ to $i$ in the graph, and $A[i]$ is one of the $n - i$ larger elements if there exists a path from $i$ to $x$ in the graph. Every vertex $i$ must either lie on a path to or from $x$ because otherwise the algorithm can't distinguish between $i \\le x$ and $i \\ge x$. Moreover, if a vertex $i$ lies on both a path to $x$ and a path from $x$, then it must be such that $x \\le A[i] \\le x$,so $x = A[i]$. In this case, we can break ties arbitrarily.","title":"9.3-4 $\\star$"},{"location":"Chap09/9.3/#93-5","text":"Suppose that you have a \"black-box\" worst-case linear-time median subroutine. Give a simple, linear-time algorithm that solves the selection problem for an arbitrary order statistic. We assume that are given a procedure $\\text{MEDIAN}$ that takes as parameters an array $A$ and subarray indices $p$ and $r$, and returns the value of the median element of $A[p..r]$ in $O(n)$ time in the worst case. Given $\\text{MEDIAN}$, here is a linear-time algorithm $\\text{SELECT}'$ for finding the $i$th smallest element in $A[p..r]$. This algorithm uses the deterministic $\\text{PARTITION}$ algorithm that was modified to take an element to partition around as an input parameter. 1 2 3 4 5 6 7 8 9 10 11 SELECT ' ( A , p , r , i ) if p == r return A [ p ] x = MEDIAN ( A , p , r ) q = PARTITION ( x ) k = q - p + 1 if i == k return A [ q ] else if i < k return SELECT ' ( A , p , q - 1 , i ) else return SELECT ' ( A , q + 1 , r , i - k ) Because $x$ is the median of $A[p..r]$, each of the subarrays $A[p..q - 1]$ and $A[q + 1..r]$ has at most half the number of elements of $A[p..r]$. The recurrence for the worst-case running time of $\\text{SELECT}'$ is $T(n) \\le T(n / 2) + O(n) = O(n)$.","title":"9.3-5"},{"location":"Chap09/9.3/#93-6","text":"The $k$th quantiles of an $n$-element set are the $k - 1$ order statistics that divide the sorted set into $k$ equal-sized sets (to within $1$). Give an $O(n\\lg k)$-time algorithm to list the $k$th quantiles of a set. Pre-calculate the positions of the quantiles in $O(k)$, we use the $O(n)$ select algorithm to find the $\\lfloor k / 2 \\rfloor$th position, after that the elements are divided into two sets by the pivot the $\\lfloor k / 2 \\rfloor$th position, we do it recursively in the two sets to find other positions. Since the maximum depth is $\\lceil \\lg k \\rceil$, the total running time is $O(n\\lg k)$. 1 2 3 4 5 6 7 8 9 10 PARTITION ( A , p , r ) x = A [ r ] i = p for k = p to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap a [ i ] with a [ r ] return i 1 2 3 4 RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) 1 2 3 4 5 6 7 8 9 10 11 12 13 RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return p , A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return q , A [ q ] if i < k r = q else p = q + 1 i = i - k 1 2 3 4 5 6 7 8 9 10 k - QUANTITLES - SUB ( A , p , r , pos , f , e , quantiles ) if f + 1 > e return mid = ( f + e ) / 2 q , val = RANDOMIZED - SELECT ( A , p , r , pos [ mid ) quantiles [ mid ] = val k = q - p + 1 for i = mid + 1 to e pos [ i ] = pos [ i ] - k k - QUANTILES - SUB ( A , q + 1 , r , pos , mid + 1 , e , quantiles ) 1 2 3 4 5 6 7 8 9 10 11 k - QUANTITLES ( A , k ) num = A . size () / k mod = A . size () % k pos = num [ 1. . k ] for i = 1 to mod pos [ i ] = pos [ i ] + 1 for i = 1 to k pos [ i ] = pos [ i ] + pos [ i - 1 ] quantiles = [ 1. . k ] k - QUANTITLES - SUB ( A , 0 , A . length , pos , 0 , pos . size (), quantiles ) return quantiles","title":"9.3-6"},{"location":"Chap09/9.3/#93-7","text":"Describe an $O(n)$-time algorithm that, given a set $S$ of $n$ distinct numbers and a positive integer $k \\le n$, determines the $k$ numbers in $S$ that are closest to the median of $S$. Find the median in $O(n)$; create a new array, each element is the absolute value of the original value subtract the median; find the $k$th smallest number in $O(n)$, then the desired values are the elements whose absolute difference with the median is less than or equal to the $k$th smallest number in the new array.","title":"9.3-7"},{"location":"Chap09/9.3/#93-8","text":"Let $X[1..n]$ and $Y[1..n]$ be two arrays, each containing $n$ numbers already in sorted order. Give an $O(\\lg n)$-time algorithm to find the median of all $2n$ elements in arrays $X$ and $Y$. Let's start out by supposing that the median (the lower median, since we know we have an even number of elements) is in $X$. Let's call the median value $m$, and let's suppose that it's in $X[k]$. Then $k$ elements of $X$ are less than or equal to $m$ and $n - k$ elements of X are greater than or equal to m. We know that in the two arrays combined, there must be $n$ elements less than or equal to $m$ and $n$ elements greater than or equal to $m$, and so there must be $n - k$ elements of $Y$ that are less than or equal to $m$ and $n - (n - k)=k$ elements of $Y$ that are greater than or equal to $m$. Thus, we can check that $X[k]$ is the lower median by checking whether $Y[n - k] \\le X[k] \\le Y[n - k + 1]$. A boundary case occurs for $k = n$. Then $n - k = 0$, and there is no array entry $Y[0]$; we only need to check that $X[n] \\le Y[1]$. Now, if the median is in $X$ but is not in $X[k]$, then the above condition will not hold. If the median is in $X[k']$ , where $k' < k$, then $X[k]$ is above the median, and $Y[n - k + 1] < X[k]$. Conversely, if the median is in $X[k'']$, where $k'' > k$, then $X[k]$ is below the median, and $X[k] < Y[n - k]$. Thus, we can use a binary search to determine whether there is an $X[k]$ such that either $k < n$ and $X[n - k] \\le Y[k] \\le X[n - k + 1]$ or $k = n$ and $X[k] \\le Y[n - k + 1]$; if we find such an $X[k]$, then it is the median. Otherwise, we know that the median is in $Y$, and we use a binary search to find a $Y[k]$ such that either $k < n$ and $X[n - k] \\le Y[k] \\le X[n - k + 1]$ or $k = n$ is the median. Since each binary search takes $O(\\lg n)$ time, we spend a total of $O(\\lg n)$ time. Here's how we write the algorithm in pseudocode: 1 2 3 4 5 6 TWO - ARRAY - MEDIAN ( X , Y ) n = X . length // n also equals Y.length median = FIND - MEDIAN ( X , Y , n , 1 , n ) if median == NOT - FOUND median = FIND - MEDIAN ( Y , X , n , 1 , n ) return median 1 2 3 4 5 6 7 8 9 10 11 FIND - MEDIAN ( A , B , n , low , hign ) if low > high return NOT - FOUND else k = floor (( low + high ) / 2 ) if k == n and A [ n ] \u2264 B [ 1 ] return A [ n ] else if k < n and B [ n - k ] \u2264 A [ k ] \u2264 B [ n - k + 1 ] return A [ k ] else A [ k ] > B [ n - k + 1 ] return FIND - MEDIAN ( A , B , n , low , k - 1 ) else return FIND - MEDIAN ( A , B , n , k + 1 , high )","title":"9.3-8"},{"location":"Chap09/9.3/#93-9","text":"Professor Olay is consulting for an oil company, which is planning a large pipeline running east to west through an oil field of $n$ wells. The company wants to connect a spur pipeline from each well directly to the main pipeline along a shortest route (either north or south), as shown in Figure 9.2. Given the $x$- and $y$-coordinates of the wells, how should the professor pick the optimal location of the main pipeline, which would be the one that minimizes the total length of the spurs? Show how to determine the optimal location in linear time. In order to find the optimal placement for Professor Olay's pipeline, we need only find the median(s) of the $y$-coordinates of his oil wells, as the following proof explains. Claim The optimal $y$-coordinate for Professor Olay's east-west oil pipeline is as follows: If $n$ is even, then on either the oil well whose $y$-coordinate is the lower median or the one whose $y$-coordinate is the upper median, or anywhere between them. If $n$ is odd, then on the oil well whose $y$-coordinate is the median. Proof We examine various cases. In each case, we will start out with the pipeline at a particular $y$-coordinate and see what happens when we move it. We'll denote by $s$ the sum of the north-south spurs with the pipeline at the starting location, and $s'$ will denote the sum after moving the pipeline. We start with the case in which n is even. Let us start with the pipeline somewhere on or between the two oil wells whose $y$-coordinates are the lower and upper medians. If we move the pipeline by a vertical distance $d$ without crossing either of the median wells, then $n / 2$ of the wells become $d$ farther from the pipeline and $n / 2$ become $d$ closer, and so $s' = s + dn / 2 - dn / 2 = s$; thus, all locations on or between the two medians are equally good. Now suppose that the pipeline goes through the oil well whose $y$-coordinate is the upper median. What happens when we increase the $y$-coordinate of the pipeline by $d > 0$ units, so that it moves above the oil well that achieves the upper median? All oil wells whose $y$-coordinates are at or below the upper median become d units farther from the pipeline, and there are at least $n / 2 + 1$ such oil wells (the upper median, and every well at or below the lower median). There are at most $n / 2 - 1$ oil wells whose $y$-coordinates are above the upper median, and each of these oil wells becomes at most d units closer to the pipeline when it moves up. Thus, we have a lower bound on $s'$ of $s' \\ge s + d(n / 2 + 1) - d(n / 2 - 1) = s + 2d > s$. We conclude that moving the pipeline up from the oil well at the upper median increases the total spur length. A symmetric argument shows that if we start with the pipeline going through the oil well whose $y$-coordinate is the lower median and move it down, then the total spur length increases. We see, therefore, that when $n$ is even, an optimal placement of the pipeline is anywhere on or between the two medians. Now we consider the case when $n$ is odd. We start with the pipeline going through the oil well whose $y$-coordinate is the median, and we consider what happens when we move it up by $d > 0$ units. All oil wells at or below the median become $d$ units farther from the pipeline, and there are at least $(n + 1) / 2$ such wells (the one at the median and the $(n - 1) / 2$ at or below the median. There are at most $(n - 1) / 2$ oil wells above the median, and each of these becomes at most d units closer to the pipeline. We get a lower bound on $s'$ of $s' \\ge s + d(n + 1) / 2 - d(n - 1) / 2 = s + d > s$, and we conclude that moving the pipeline up from the oil well at the median increases the total spur length. A symmetric argument shows that moving the pipeline down from the median also increases the total spur length, and so the optimal placement of the pipeline is on the median. (claim) Since we know we are looking for the median, we can use the linear-time median-finding algorithm.","title":"9.3-9"},{"location":"Chap09/Problems/9-1/","text":"Given a set of $n$ numbers, we wish to find the $i$ largest in sorted order using a comparison-based algorithm. Find the algorithm that implements each of the following methods with the best asymptotic worst-case running time, and analyze the running times of the algorithms in terms of $n$ and $i$ . a. Sort the numbers, and list the $i$ largest. b. Build a max-priority queue from the numbers, and call $\\text{EXTRACT-MAX}$ $i$ times. c. Use an order-statistic algorithm to find the $i$th largest number, partition around that number, and sort the $i$ largest numbers. We assume that the numbers start out in an array. a. Sort the numbers using merge sort or heapsort, which take $\\Theta(n\\lg n)$ worst-case time. (Don't use quicksort or insertion sort, which can take $\\Theta(n^2)$ time.) Put the $i$ largest elements (directly accessible in the sorted array) into the output array, taking $\\Theta(i)$ time. Total worst-case running time: $\\Theta(n\\lg n + i) = \\Theta(n\\lg n)$ (because $i \\le n$). b. Implement the priority queue as a heap. Build the heap using $\\text{BUILD-HEAP}$, which takes $\\Theta(n)$ time, then call $\\text{HEAP-EXTRACT-MAX}$ $i$ times to get the $i$ largest elements, in $\\Theta(i\\lg n)$ worst-case time, and store them in reverse order of extraction in the output array. The worst-case extraction time is $\\Theta(i\\lg n)$ because $i$ extractions from a heap with $O(n)$ elements takes $i \\cdot O(\\lg n) = O(i\\lg n)$ time, and half of the $i$ extractions are from a heap with $\\ge n / 2$ elements, so those $i / 2$ extractions take $(i / 2)\\Omega(\\lg n / 2)) = \\Omega(i\\lg n)$ time in the worst case. Total worst-case running time: $\\Theta(n + i\\lg n)$. c. Use the $\\text{SELECT}$ algorithm of Section 9.3 to find the $i$th largest number in $\\Theta(n)$ time. Partition around that number in $\\Theta(n)$ time. Sort the i largest numbers in $\\Theta(i\\lg i)$ worst-case time (with merge sort or heapsort). Total worst-case running time: $\\Theta(n + i\\lg i)$. Note that method (c) is always asymptotically at least as good as the other two methods, and that method (b) is asymptotically at least as good as (a). (Comparing (c) to (b) is easy, but it is less obvious how to compare (c) and (b) to (a). (c) and (b) are asymptotically at least as good as (a) because $n$, $i\\lg i$, and $i\\lg n$ are all $O(n\\lg n)$. The sum of two things that are $O(n\\lg n)$ is also $O(n\\lg n)$.)","title":"9-1 Largest $i$ numbers in sorted order"},{"location":"Chap09/Problems/9-2/","text":"For $n$ distinct elements $x_1, x_2, \\ldots, x_n$ with positive weights $w_1, w_2, \\ldots, w_n$ such that $\\sum_{i = 1}^n w_i = 1$, the weighted (lower) median is the element $x_k$ satisfying $$\\sum_{x_i < x_k} w_i < \\frac{1}{2}$$ and $$\\sum_{x_i > x_k} w_i \\le \\frac{1}{2}.$$ For example, if the elements are $0.1, 0.35, 0.05, 0.1, 0.15, 0.05, 0.2$ and each element equals its weight (that is, $w_i = x_i$ for $i = 1, 2, \\ldots, 7$), then the median is $0.1$, but the weighted median is $0.2$. a. Argue that the median of $x_1, x_2, \\ldots, x_n$ is the weighted median of the $x_i$ with weights $w_i = 1 / n$ for $i = 1, 2, \\ldots, n$. b. Show how to compute the weighted median of $n$ elements in $O(n\\lg n)$ worstcase time using sorting. c. Show how to compute the weighted median in $\\Theta(n)$ worst-case time using a linear-time median algorithm such as $\\text{SELECT}$ from Section 9.3. The post-office location problem is defined as follows. We are given $n$ points $p_1, p_2, \\ldots, p_n$ with associated weights $w_1, w_2, \\ldots, w_n$. We wish to find a point $p$ (not necessarily one of the input points) that minimizes the sum $\\sum_{i = 1}^n w_i d(p, p_i)$, where $d(a, b)$ is the distance between points $a$ and $b$. d. Argue that the weighted median is a best solution for the $1$-dimensional postoffice location problem, in which points are simply real numbers and the distance between points $a$ and $b$ is $d(a, b) = |a - b|$. e. Find the best solution for the $2$-dimensional post-office location problem, in which the points are $(x,y)$ coordinate pairs and the distance between points $a = (x_1, y_1)$ and $b = (x_2, y_2)$ is the Manhattan distance given by $d(a, b) = |x_1 - x_2| + |y_1 - y_2|$. a. The median $x$ of the elements $x_1, x_2, \\ldots, x_n$, is an element $x = x_k$ satisfying $|\\{x_i: 1\\le i\\le n \\text{ and } x_i < x\\}| \\le n / 2$ and $|\\{x_i: 1 \\le i \\le n \\text{ and } x_i > x\\}| \\le n / 2$. If each element $x_i$ is assigned a weight $x_i = 1 / n$, then we get $$ \\begin{aligned} \\sum_{x_i < x} w_i & = \\sum_{x_i < x} \\frac{1}{n} \\\\ & = \\frac{1}{n} \\cdot \\sum_{x_i < x} 1 \\\\ & = \\frac{1}{n} \\cdot |{x_i: 1\\le i\\le n\\text{ and } x_i < x}| \\\\ & \\le \\frac{1}{n} \\cdot \\frac{n}{2} \\\\ & = \\frac{1}{2}, \\end{aligned} $$ and $$ \\begin{aligned} \\sum_{x_i > x} w_i & = \\sum_{x_i > x} \\frac{1}{n} \\\\ & = \\frac{1}{n} \\cdot \\sum_{x_i > x} 1 \\\\ & = \\frac{1}{n} \\cdot |{x_i: 1\\le i\\le n\\text{ and } x_i > x}| \\\\ & \\le \\frac{1}{n} \\cdot \\frac{n}{2} \\\\ & = \\frac{1}{2}, \\end{aligned} $$ which proves that $x$ is also the weighted median of $x_1, x_2, \\ldots, x_n$ with weights $x_i = 1 / n$, for $i = 1, 2, \\ldots, n$. b. We first sort the $n$ elements into increasing order by $x_i$ values. Then we scan the array of sorted $x_i$'s, starting with the smallest element and accumulating weights as we scan, until the total exceeds $1 / 2$. The last element, say $x_k$, whose weight caused the total to exceed $1 / 2$, is the weighted median. Notice that the total weight of all elements smaller than $x_k$ is less than $1 / 2$, because $x_k$ was the first element that caused the total weight to exceed $1 / 2$. Similarly, the total weight of all elements larger than $x_k$ is also less than $1 / 2$, because the total weight of all the other elements exceeds $1 / 2$. The sorting phase can be done in $O(n\\lg n)$ worst-case time (using merge sort or heapsort), and the scanning phase takes $O(n)$ time. The total running time in the worst case, therefore, is $O(n\\lg n)$. c. We find the weighted median in $\\Theta(n)$ worst-case time using the $\\Theta(n)$ worst-case median algorithm in Section 9.3. (Although the first paragraph of the section only claims an $O(n)$ upper bound, it is easy to see that the more precise running time of $\\Theta(n)$ applies as well, since steps 1, 2, and 4 of $\\text{SELECT}$ actually take $\\Theta(n)$ time.) The weighted-median algorithm works as follows. If $n \\le 2$, we just return the brute-force solution. Otherwise, we proceed as follows. We find the actual median $x_k$ of the $n$ elements and then partition around it. We then compute the total weights of the two halves. If the weights of the two halves are each strictly less than $1 / 2$, then the weighted median is $x_k$ . Otherwise, the weighted median should be in the half with total weight exceeding $1 / 2$. The total weight of the \"light\" half is lumped into the weight of $x_k$ , and the search continues within the half that weighs more than $1 / 2$. Here's pseudocode, which takes as input a set $X = \\{x_1, x_2, \\ldots, x_n\\}$: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 WEIGHTED - MEDIAN ( X ) if n == 1 return x [ 1 ] else if n == 2 if w [ 1 ] \u2265 w [ 2 ] return x [ 1 ] else return x [ 2 ] else find the median x [ k ] of X = { x [ 1 ], x [ 2 ], ..., x [ n ]} partition the set X around x [ k ] compute W [ L ] = sum ( x [ i ] < x [ k ]) w [ i ] and W [ G ] = sum ( x [ i ] > x [ k ]) w [ i ] if W [ L ] < 1 / 2 and W [ G ] < 1 / 2 return x [ k ] else if W [ L ] > 1 / 2 w [ k ] = w [ k ] + W [ G ] X ' = { x [ i ] \u2208 X : x [ i ] \u2264 x [ k ]} return WEIGHTED - MEDIAN ( X ' ) else w [ k ] = w [ k ] + W [ L ] X ' = { x [ i ] \u2208 X : x [ i ] \u2265 x [ k ]} return WEIGHTED - MEDIAN ( X ' ) The recurrence for the worst-case running time of $\\text{WEIGHTED-MEDIAN}$ is $T(n) = T(n / 2 + 1) + \\Theta(n)$, since there is at most one recursive call on half the number of elements, plus the median element $x_k$, and all the work preceding the recursive call takes $\\Theta(n)$ time. The solution of the recurrence is $T (n) = \\Theta(n)$. d. Let the $n$ points be denoted by their coordinates $x_1, x_2, \\ldots, x_n$, let the corresponding weights be $w_1, w_2, \\ldots, w_n$, and let $x = x_k$ be the weighted median. For any point $p$, let $f(p) = \\sum_{i = 1}^n w_i |p - x_i|$; we want to find a point $p$ such that $f(p)$ is minimum. Let $y$ be any point (real number) other than $x$. We show the optimality of the weighted median $x$ by showing that $f(y) - f(x) \\ge 0$. We examine separately the cases in which $y > x$ and $x > y$. For any $x$ and $y$, we have $$ \\begin{aligned} f(y) - f(x) & = \\sum_{i = 1}^n w_i |y - x_i| - \\sum_{i = 1}^n w_i |x - x_i| \\\\ & = \\sum_{i = 1}^n w_i (|y - x_i| - |x - x_i|). \\end{aligned} $$ When $y > x$, we bound the quantity $|y - x_i| - |x - x_i|$ from below by examining three cases: $x < y \\le x_i$: Here, $|x - y| + |y - x_i| = |x - x_i|$ and $|x - y| = y - x$, which imply that $|y - x_i| - |x - x_i| = -|x - y| = x - y$. $x < x_i \\le y$: Here, $|y - x_i| \\ge 0$ and $|x_i - x| \\le y - x$, which imply that $|y - x_i| - |x - x_i| \\ge -(y - x) = x - y$. $x_i \\le x < y$: Here, $|x - x_i| + |y - x| = |y - x_i|$ and $|y - x| = y - x$, which imply that $y - x_i| - |x - x_i| = |y - x| = y - x$. Separating out the first two cases, in which $x < x_i$, from the third case, in which $x \\ge x_i$, we get $$ \\begin{aligned} f(y) - f(x) & = \\sum_{i = 1}^n w_i(|y - x_i| - |x - x_i|) \\\\ & \\ge \\sum_{x < x_i} w_i (x - y) + \\sum_{x \\ge x_i} w_i (y - x) \\\\ & = (y - x) \\Bigg(\\sum_{x \\ge x_i} w_i - \\sum_{x < x_i} w_i \\Bigg). \\end{aligned} $$ The property that $\\sum_{x_i < x} w_i < 1 / 2$ implies that $\\sum_{x \\ge x_i} w_i \\ge 1 / 2$. This fact, combined with $y - x > 0$ and $\\sum_{x < x_i} w_i \\le 1 / 2$, yields that $f(y) - f(x) \\ge 0$. When $x > y$, we bound the quantity $|y - x_i| - |x - x_i|$ from below by examining three cases: $x_i \\le y < x$: Here, $|y - x_i| + |x - y| = |x - x_i|$ and $|x - y| = x - y$, which imply that $|y - x_i| - |x - x_i| = -|x - y| = y - x$. $y \\le x_i < x$: Here, $|y - x_i| \\ge 0$ and $|x - x_i| \\le x - y$, which imply that $|y - x_i| - |x - x_i| \\ge -(x - y) = y - x$. $y < x \\le x_i$: Here, $|x - y| + |x - x_i| = |y - x_i|$ and $|x - y| = x - y$, which imply that $y - x_i| - |x - x_i| = |x - y| = x - y$. Separating out the first two cases, in which $x > x_i$, from the third case, in which $x \\le x_i$, we get $$ \\begin{aligned} f(y) - f(x) & = \\sum_{i = 1}^n w_i(|y - x_i| - |x - x_i|) \\\\ & \\ge \\sum_{x > x_i} w_i(y - x) + \\sum_{x \\ge x_i} w_i(x - y) \\\\ & = (x - y) \\Bigg(\\sum_{x \\le x_i} w_i - \\sum_{x > x_i} w_i \\Bigg). \\end{aligned} $$ The property that $sum_{x_i > x} w_i \\le 1 / 2$ implies that $\\sum_{x \\le x_i} w_i > 1 / 2$. This fact, combined with $x - y > 0$ and $\\sum_{x > x_i} w_i < 1 / 2$, yields that $f(y) - f(x) > 0$. e. We are given $n$ $2$-dimensional points $p_1, p_2, \\ldots, p_n$, where each $p_i$ is a pair of real numbers $p_i = (x_i, y_i)$, and positive weights $w_1, w_2, \\ldots, w_n$. The goal is to find a point $p = (x, y)$ that minimizes the sum $$f(x, y) = \\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|).$$ We can express the cost function of the two variables, $f(x, y)$, as the sum of two functions of one variable each: $f(x, y) = g(x) + h(y)$, where $g(x) = \\sum_{i = 1}^n w_i |x - x_i|$, and $h(y) = \\sum_{i = 1}^n w_i |y - y_i|$. The goal of finding a point $p = (x, y)$ that minimizes the value of $f(x, y)$ can be achieved by treating each dimension independently, because $g$ does not depend on $y$ and $h$ does not depend on $x$. Thus, $$ \\begin{aligned} \\min_{x, y} f(x, y) & = \\min_{x, y} (g(x) + h(y)) \\\\ & = \\min_x \\Big(\\min_y(g(x) + h(y))\\Big) \\\\ & = \\min_x \\Big(g(x) + \\min_y h(y)\\Big) \\\\ & = \\min_x g(x) + \\min_y h(y). \\end{aligned} $$ Consequently, finding the best location in $2$ dimensions can be done by finding the weighted median $x_k$ of the $x$-coordinates and then finding the weighted median $y_j$ of the $y$-coordinates. The point $(x_k, y_j)$ is an optimal solution for the $2$-dimensional post-office location problem.","title":"9-2 Weighted median"},{"location":"Chap09/Problems/9-3/","text":"We showed that the worst-case number $T(n)$ of comparisons used by $\\text{SELECT}$ to select the $i$th order statistic from $n$ numbers satisfies $T(n) = \\Theta(n)$, but the constant hidden by the $\\Theta$-notation is rather large. When $i$ is small relative to $n$, we can implement a different procedure that uses $\\text{SELECT}$ as a subroutine but makes fewer comparisons in the worst case. a. Describe an algorithm that uses $U_i(n)$ comparisons to find the $i$th smallest of $n$ elements, where $$ U_i(n) = \\begin{cases} T(n) & \\text{if $i \\ge n / 2$}, \\\\ \\lfloor n / 2 \\rfloor + U_i(\\lceil n / 2 \\rceil) + T(2i) & \\text{otherwise}. \\end{cases} $$ ($\\textit{Hint:}$ Begin with $\\lfloor n / 2 \\rfloor$ disjoint pairwise comparisons, and recurse on the set containing the smaller element from each pair.) b. Show that, if $i < n / 2$, then $U_i(n) = n + O(T(2i)\\lg(n / i))$. c. Show that if $i$ is a constant less than $n / 2$, then $U_i(n) = n + O(\\lg n)$. d. Show that if $i = n / k$ for $k \\ge 2$, then $U_i(n) = n + O(T(2n / k)\\lg k)$. a. Our algorithm relies on a particular property of $\\text{SELECT}$: that not only does it return the $i$th smallest element, but that it also partitions the input array so that the first $i$ positions contain the $i$ smallest elements (though not necessarily in sorted order). To see that $\\text{SELECT}$ has this property, observe that there are only two ways in which returns a value: when $n = 1$, and when immediately after partitioning in step 4, it finds that there are exactly $i$ elements on the low side of the partition. Taking the hint from the book, here is our modified algorithm to select the $i$th smallest element of $n$ elements. Whenever it is called with $i \\ge n / 2$, it just calls $\\text{SELECT}$ and returns its result; in this case, $U_i(n) = T(n)$. When $i < n / 2$, our modified algorithm works as follows. Assume that the input is in a subarray $A[p + 1..p + n]$, and let $m = \\lfloor n / 2 \\rfloor$. In the initial call, $p = 1$. Divide the input as follows. If $n$ is even, divide the input into two parts: $A[p + 1..p + m]$ and $A[p + m + 1..p + n]$. If $n$ is odd, divide the input into three parts: $A[p + 1..p + m]$, $A[p + m + 1..p + n - 1]$, and $A[p + n]$ as a leftover piece. Compare $A[p + i]$ and $A[p + i + m]$ for $i = 1, 2, \\ldots, m$, putting the smaller of the the two elements into $A[p + i + m]$ and the larger into $A[p + i]$. Recursively find the $i$th smallest element in $A[p + m + 1..p + n]$, but with an additional action performed by the partitioning procedure: whenever it exchanges $A[j]$ and $A[k]$ (where $p + m + 1 \\le j$, $k \\le p + 2m$), it also exchanges $A[j - m]$ and $A[k - m]$. The idea is that after recursively finding the $i$th smallest element in $A[p + m + 1..p + n]$, the subarray $A[p + m + 1..p + m + i]$ contains the $i$ smallest elements that had been in $A[p + m + 1..p + n]$ and the subarray $A[p + 1..p + i]$ contains their larger counterparts, as found in step 1. The $i$th smallest element of $A[p + 1..p + n]$ must be either one of the $i$ smallest, as placed into $A[p + m + 1..p + m + i]$, or it must be one of the larger counterparts, as placed into $A[p + 1..p + i]$. Collect the subarrays $A[p + 1..p + i]$ and $A[p + m + 1..p + m + i]$ into a single array $B[1..2i]$, call $\\text{SELECT}$ to find the $i$th smallest element of $B$, and return the result of this call to $\\text{SELECT}$. The number of comparisons in each step is as follows: No comparisons. $m = \\lfloor n / 2 \\lfloor$ comparisons. Since we recurse on $A[p + m + 1..p + n]$, which has $dn / 2e$ elements, the number of comparisons is $U_i(\\lceil n / 2 \\rceil)$. Since we call $\\text{SELECT}$ on an array with $2i$ elements, the number of comparisons is $T(2i)$. Thus, when $i < n / 2$, the total number of comparisons is $\\lfloor n / 2 \\rfloor + U_i(\\lceil n / 2 \\rceil) + T(2i)$. b. We show by substitution that if $i < n / 2$, then $U_i(n) = n + O(T(2i)\\lg(n / i))$. In particular, we show that $$ \\begin{aligned} U_i(n) & \\le n + cT (2i)\\lg(n / i) - d(\\lg\\lg n)T(2i) \\\\ & = n + cT(2i)\\lg n - cT(2i)\\lg i - d(\\lg\\lg n)T(2i) \\end{aligned} $$ for some positive constant $c$, some positive constant $d$ to be chosen later, and $n \\ge 4$. We have $$ \\begin{aligned} U_i(n) & = \\lfloor n / 2 \\rfloor + U_i(\\lceil n / 2 \\rceil) + T(2i) \\\\ & \\le \\lfloor n / 2 \\rfloor + \\lceil n / 2 \\rceil + cT(2i)\\lg\\lceil n / 2 \\rceil - cT(2i)\\lg i - d(\\lg\\lg\\lceil n / 2 \\rceil)T(2i) \\\\ & = n + cT(2i)\\lg\\lceil n / 2 \\rceil - cT(2i)\\lg i - d(\\lg\\lg\\lceil n / 2 \\rceil)T(2i) \\\\ & \\le n + cT(2i)\\lg(n / 2 + 1) - cT(2i)\\lg i - d(\\lg\\lg(n / 2))T(2i) \\\\ & = n + cT(2i)\\lg(n / 2 + 1) - cT(2i)\\lg i - d(\\lg(\\lg n - 1))T(2i) \\\\ & \\le n + cT(2i)\\lg n - cT(2i)\\lg i - d(\\lg\\lg n)T(2i). \\end{aligned} $$ If $$cT(2i)\\lg(n)2 + 1) - d(\\lg(\\lg n - 1))T(2i) \\le cT(2i)\\lg n - d(\\lg\\lg n)T(2i),$$ simple algebraic manipulations gives the following sequence of equivalent conditions: $$ \\begin{aligned} cT(2i)\\lg(n / 2 + 1) - d(\\lg(\\lg n - 1))T(2i) & \\le cT(2i)\\lg n - d(\\lg\\lg n)T(2i) \\\\ c\\lg(n / 2 + 1) - d(\\lg(\\lg n - 1)) & \\le c\\lg n - d(\\lg\\lg n) \\\\ c(\\lg(n / 2 + 1) - \\lg n) & \\le d(\\lg(\\lg n - 1) - \\lg\\lg n) \\\\ c\\Bigg(\\lg\\frac{n / 2 + 1}{n}\\Bigg) & \\le d\\lg\\frac{\\lg n - 1}{\\lg n} \\\\ c\\Bigg(\\lg\\Bigg(\\frac{1}{2} + \\frac{1}{n}\\Bigg)\\Bigg) & \\le d\\lg\\frac{\\lg n - 1}{\\lg n}. \\end{aligned} $$ Observe that $1 / 2 + 1 / n$ decreases as $n$ increases, but $(\\lg n - 1) / \\lg n$ increases as $n$ increases. When $n = 4$, we have $1 / 2 + 1 / n = 3 / 4$ and $(\\lg n - 1) = \\lg n = 1 / 2$. Thus, we just need to choose $d$ such that $c\\lg(3 / 4) \\le d\\lg(1 / 2)$ or, equivalently, $c\\lg(3 / 4) \\le -d$. Multiplying both sides by $-1$, we get $d \\le -c\\lg(3 / 4) = c\\lg(4 / 3)$. Thus, any value of $d$ that is at most $c\\lg(4 / 3)$ suffices. c. When $i$ is a constant, $T(2i) = O(1)$ and $\\lg(n / i) = \\lg n - \\lg i = O(\\lg n)$. Thus, when $i$ is a constant less than $n / 2$, we have that $$ \\begin{aligned} U_i(n) & = n + O(T(2i)\\lg(n / i)) \\\\ & = n + O(O(1) \\cdot O(\\lg n)) \\\\ & = n + O(\\lg n). \\end{aligned} $$ d. Suppose that $i = n / k$ for $k \\ge 2$. Then $i \\le n / 2$. If $k > 2$, then $i < n / 2$, and we have $$ \\begin{aligned} U_i(n) & = n + O(T(2i)\\lg(n / i)) \\\\ & = n + O(T(2n / k)\\lg(n/(n / k)) \\\\ & = n + O(T(2n / k)\\lg k). \\end{aligned} $$ If $k = 2$, then $n = 2i$ and $\\lg k = 1$. We have $$ \\begin{aligned} U_i(n) & = T(n) \\\\ & = n + (T(n) - n) \\\\ & \\le n + (T(2i) - n) \\\\ & = n + (T(2n / k) - n) \\\\ & = n + (T(2n / k)\\lg k - n) \\\\ & = n + O(T(2n / k)\\lg k). \\end{aligned} $$","title":"9-3 Small order statistics"},{"location":"Chap09/Problems/9-4/","text":"In this problem, we use indicator random variables to analyze the $\\text{RANDOMIZED-SELECT}$ procedure in a manner akin to our analysis of $\\text{RANDOMIZED-QUICKSORT}$ in Section 7.4.2. As in the quicksort analysis, we assume that all elements are distinct, and we rename the elements of the input array $A$ as $z_1, z_2, \\ldots, z_n$, where $z_i$ is the $i$th smallest element. Thus, the call $\\text{RANDOMIZED-SELECT}(A, 1, n, k)$ returns $z_k$. For $1 \\le i < j \\le n$, let $$X_{ijk} = \\text{I \\{$z_i$ is compared with $z_j$ sometime during the execution of the algorithm to find $z_k$\\}}.$$ a. Give an exact expression for $\\text E[X_{ijk}]$. ($\\textit{Hint:}$ Your expression may have different values, depending on the values of $i$, $j$, and $k$.) b. Let $X_k$ denote the total number of comparisons between elements of array $A$ when finding $z_k$. Show that $$\\text E[X_k] \\le 2 \\Bigg (\\sum_{i = 1}^{k}\\sum_{j = k}^n \\frac{1}{j - i + 1} + \\sum_{j = k + 1}^{n} \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k-2} \\frac{k - i - 1}{k - i + 1} \\Bigg).$$ c. Show that $\\text E[X_k] \\le 4n$. d. Conclude that, assuming all elements of array $A$ are distinct, $\\text{RANDOMIZED-SELECT}$ runs in expected time $O(n)$. a. As in the quicksort analysis, elements $z_i$ and $z_j$ will not be compared with each other if any element in $\\{z_{i + 1}, z_{i + 2}, \\ldots, z_{j - 1}\\}$ is chosen as a pivot element before either $z_i$ or $z_j$, because $z_i$ and $z_j$ would then lie in separate partitions. There can be another reason that $z_i$ and $z_j$ might not be compared, however. Suppose that $k < i$, so that $z_k < z_i$ , and suppose further that the element chosen as the pivot is $z_l$, where $k \\le l < i$. In this case, because $k \\le l$, the recursion won't consider elements indexed higher than $l$. Therefore, the recursion will never look at $z_i$ or $z_j$ , and they will never be compared with each other. Similarly, if $j < k$ and the pivot element $z_l$ is such that $j < l \\le k$, then the recursion won't consider elements indexed less than $l$, and again $z_i$ and $z_j$ will never be compared with each other. The final case is when $i \\le k \\le j$ (but disallowing $i = j$), so that $z_i \\le z_k \\le z_j$; in this case, we have the same analysis as for quicksort: $z_i$ and $z_j$ are compared with each other only if one of them is chosen as the pivot element. Getting back to the case in which $k < i$, it is again true that $z_i$ and $z_j$ are compared with each other only if one of them is chosen as the pivot element. As we know, they won't be compared with each other if the pivot element is between them, and we argued above that they won't be compared with each other if the pivot element is $z_l$ for $l < i$. Similarly, when $j < k$, elements $z_i$ and $z_j$ are compared with each other only if one of them is chosen as the pivot element. Now we need to compute the probability that $z_i$ and $z_j$ are compared with each other. Let $Z_{ijk}$ be the set of elements that includes $z_i, \\ldots, z_j$, along with $z_k, \\ldots, z_{i - 1}$ if $k < i$ or $z_{j + 1}, \\ldots, z_k$ if $j < k$. In other words, $$ Z_{ijk} = \\begin{cases} {z_i, z_{i + 1}, \\ldots, z_j} & \\text{if $i \\le k \\le j$}, \\\\ {z_k, z_{k + 1}, \\ldots, z_j} & \\text{if $k < i$}, \\\\ {z_i, z_{i + 1}, \\ldots, z_k} & \\text{if $j < k$}. \\end{cases} $$ With this definition of $Z_{ijk}$, we have that $$|Z_{ijk}| = \\max(j - i + 1, j - k + 1, k - i + 1).$$ As in the quicksort analysis, we observe that until an element from $Z_{ijk}$ is chosen as the pivot, the whole set $Z_{ijk}$ is together in the same partition, and so each element of $Z_{ijk}$ is equally likely to be the first one chosen as the pivot. Letting $C$ be the event that $z_i$ is compared with $z_j$ during the execution of the algorithm, we have that $$ \\begin{aligned} \\text E[X_{ijk}] & = \\Pr\\{C\\} \\\\ & = \\Pr\\{z_i \\text{ or } z_j \\text{ is the first pivot chosen from } Z_{ijk}\\} \\\\ & = \\Pr\\{z_i \\text{ is the first pivot chosen from } Z_{ijk}\\} + \\Pr\\{z_j \\text{ is the first pivot chosen from } Z_{ijk}\\} \\\\ & = \\frac{1}{|Z_{ijk}|} + \\frac{1}{|Z_{ijk}|} \\\\ & = \\frac{2}{\\max(j - i + 1, j - k + 1, k - i + 1)}. \\end{aligned} $$ b. Adding up all the possible pairs that might be compared gives $$X_k = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ijk},$$ and so, by linearity of expectation, we have $$ \\begin{aligned} \\text E[X_k] & = \\text E \\Bigg[\\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n X_{ijk} \\Bigg] \\\\ & = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n \\text E[X_{ijk}] \\\\ & = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n \\frac{2}{\\max(j - i + 1, j - k + 1, k - i + 1)}. \\end{aligned} $$ We break this sum into the same three cases as before: $i \\le k \\le j$, $k < i$ and $j < k$. With $k$ fixed, we vary $i$ and $j$. We get an inequality because we cannot have $i = k = j$, but our summation will allow it: $$ \\begin{aligned} \\text E[X_k] & \\le 2\\Bigg(\\sum_{i = 1}^k \\sum_{j = k}^n \\frac{1}{j - i + 1} + \\sum_{j = k + 1}^n \\sum_{i = k + 1}^{j - 1} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\sum_{j = i + 1}^{k - 1} \\frac{1}{k - i + 1}\\Bigg) \\\\ & = 2\\Bigg(\\sum_{i = 1}^k \\sum_{j = k}^n \\frac{1}{j - i + 1} + \\sum_{j = k + 1}^n \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}\\Bigg). \\end{aligned} $$ c. First, let's focus on the latter two summations. Each one sums fractions that are strictly less than 1. The middle summation has $n - k$ terms, and the right-hand summation has $k - 2$ terms, and so the latter two summations sum to less than $n$. Now we look at the first summation. Let $m = j - i$. There is only one way for $m$ to equal $0$: if $i = k = j$. There are only two ways for $m$ to equal $1$: if $i = k - 1$ and $j = k$, or if $i = k$ and $j = k + 1$. There are only three ways for $m$ to equal $2$: if $i = k - 2$ and $j = k$, if $i = k - 1$ and $j = k + 1$, or if $i = k$ and $j = k + 2$. Continuing on, we see that there are at most $m + 1$ ways for $j - i$ to equal $m$. Since $j - i \\le n - 1$, we can rewrite the first summation as $$\\sum_{m = 0}^{n - 1} \\frac{m + 1}{m + 1} = n.$$ Thus, we have $$ \\begin{aligned} \\text E[X_k] & < 2(n + n) \\\\ & = 4n. \\end{aligned} $$ d. To show that $\\text{RANDOMIZED-SELECT}$ runs in expected time $O(n)$, we adapt Lemma 7.1 for $\\text{RANDOMIZED-SELECT}$. The adaptation is trivial: just replace the variable $X$ in the lemma statement by the random variable $X_k$ that we just analyzed. Thus, the expected running time of $\\text{RANDOMIZED-SELECT}$ is $O(n + X_k) = O(n)$.","title":"9-4 Alternative analysis of randomized selection"},{"location":"Chap10/10.1/","text":"10.1-1 Using Figure 10.1 as a model, illustrate the result of each operation in the sequence $\\text{PUSH}(S, 4)$, $\\text{PUSH}(S, 1)$, $\\text{PUSH}(S, 3)$, $\\text{POP}(S)$, $\\text{PUSH}(S, 8)$, and $\\text{POP}(S)$ on an initially empty stack $S$ stored in array $S[1..6]$. $$ \\begin{array}{l|ccc} \\text{PUSH($S, 4$)} & 4 & & \\\\ \\text{PUSH($S, 1$)} & 4 & 1 & \\\\ \\text{PUSH($S, 3$)} & 4 & 1 & 3 \\\\ \\text{POP($S$)} & 4 & 1 & \\\\ \\text{PUSH($S, 8$)} & 4 & 1 & 8 \\\\ \\text{POP($S$)} & 4 & 1 & \\end{array} $$ 10.1-2 Explain how to implement two stacks in one array $A[1..n]$ in such a way that neither stack overflows unless the total number of elements in both stacks together is $n$. The $\\text{PUSH}$ and $\\text{POP}$ operations should run in $O(1)$ time. The first stack starts at $1$ and grows up towards n, while the second starts form $n$ and grows down towards $1$. Stack overflow happens when an element is pushed when the two stack pointers are adjacent. 10.1-3 Using Figure 10.2 as a model, illustrate the result of each operation in the sequence $\\text{ENQUEUE}(Q, 4)$, $\\text{ENQUEUE}(Q ,1)$, $\\text{ENQUEUE}(Q, 3)$, $\\text{DEQUEUE}(Q)$, $\\text{ENQUEUE}(Q, 8)$, and $\\text{DEQUEUE}(Q)$ on an initially empty queue $Q$ stored in array $Q[1..6]$. $$ \\begin{array}{l|cccc} \\text{ENQUEUE($Q, 4$)} & 4 & & & \\\\ \\text{ENQUEUE($Q, 1$)} & 4 & 1 & & \\\\ \\text{ENQUEUE($Q, 3$)} & 4 & 1 & 3 & \\\\ \\text{DEQUEUE($Q$)} & & 1 & 3 & \\\\ \\text{ENQUEUE($Q, 8$)} & & 1 & 3 & 8 \\\\ \\text{DEQUEUE($Q$)} & & & 3 & 8 \\end{array} $$ 10.1-4 Rewrite $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ to detect underflow and overflow of a queue. To detect underflow and overflow of a queue, we can implement $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ first. 1 2 3 4 QUEUE - EMPTY ( Q ) if Q . head == Q . tail return true else return false 1 2 3 4 QUEUE - FULL ( Q ) if Q . head == Q . tail + 1 or ( Q . head == 1 and Q . tail == Q . length ) return true else return false 1 2 3 4 5 6 7 8 ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 1 2 3 4 5 6 7 8 9 DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x 10.1-5 Whereas a stack allows insertion and deletion of elements at only one end, and a queue allows insertion at one end and deletion at the other end, a deque (double-ended queue) allows insertion and deletion at both ends. Write four $O(1)$-time procedures to insert elements into and delete elements from both ends of a deque implemented by an array. The procedures $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ are implemented in Exercise 10.1-4. 1 2 3 4 5 6 7 8 HEAD - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else if Q . head == 1 Q . head = Q . length else Q . head = Q . head - 1 Q [ Q . head ] = x 1 2 3 4 5 6 7 8 TAIL - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 1 2 3 4 5 6 7 8 9 HEAD - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x 1 2 3 4 5 6 7 8 9 TAIL - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else if Q . tail == 1 Q . tail = Q . length else Q . tail = Q . tail - 1 x = Q [ Q . tail ] return x 10.1-6 Show how to implement a queue using two stacks. Analyze the running time of the queue operations. $\\text{ENQUEUE}$: $\\Theta(1)$. $\\text{DEQUEUE}$: worst $O(n)$, amortized $\\Theta(1)$. Let the two stacks be $A$ and $B$. $\\text{ENQUEUE}$ pushes elements on $B$. $\\text{DEQUEUE}$ pops elements from $A$. If $A$ is empty, the contents of $B$ are transfered to $A$ by popping them out of $B$ and pushing them to $A$. That way they appear in reverse order and are popped in the original. A $\\text{DEQUEUE}$ operation can perform in $\\Theta(n)$ time, but that will happen only when $A$ is empty. If many $\\text{ENQUEUE}$s and $\\text{DEQUEUE}$s are performed, the total time will be linear to the number of elements, not to the largest length of the queue. 10.1-7 Show how to implement a stack using two queues. Analyze the running time of the stack operations. $\\text{PUSH}$: $\\Theta(1)$. $\\text{POP}$: $\\Theta(n)$. We have two queues and mark one of them as active. $\\text{PUSH}$ queues an element on the active queue. $\\text{POP}$ should dequeue all but one element of the active queue and queue them on the inactive. The roles of the queues are then reversed, and the final element left in the (now) inactive queue is returned. The $\\text{PUSH}$ operation is $\\Theta(1)$, but the $\\text{POP}$ operation is $\\Theta(n)$ where $n$ is the number of elements in the stack.","title":"10.1 Stacks and queues"},{"location":"Chap10/10.1/#101-1","text":"Using Figure 10.1 as a model, illustrate the result of each operation in the sequence $\\text{PUSH}(S, 4)$, $\\text{PUSH}(S, 1)$, $\\text{PUSH}(S, 3)$, $\\text{POP}(S)$, $\\text{PUSH}(S, 8)$, and $\\text{POP}(S)$ on an initially empty stack $S$ stored in array $S[1..6]$. $$ \\begin{array}{l|ccc} \\text{PUSH($S, 4$)} & 4 & & \\\\ \\text{PUSH($S, 1$)} & 4 & 1 & \\\\ \\text{PUSH($S, 3$)} & 4 & 1 & 3 \\\\ \\text{POP($S$)} & 4 & 1 & \\\\ \\text{PUSH($S, 8$)} & 4 & 1 & 8 \\\\ \\text{POP($S$)} & 4 & 1 & \\end{array} $$","title":"10.1-1"},{"location":"Chap10/10.1/#101-2","text":"Explain how to implement two stacks in one array $A[1..n]$ in such a way that neither stack overflows unless the total number of elements in both stacks together is $n$. The $\\text{PUSH}$ and $\\text{POP}$ operations should run in $O(1)$ time. The first stack starts at $1$ and grows up towards n, while the second starts form $n$ and grows down towards $1$. Stack overflow happens when an element is pushed when the two stack pointers are adjacent.","title":"10.1-2"},{"location":"Chap10/10.1/#101-3","text":"Using Figure 10.2 as a model, illustrate the result of each operation in the sequence $\\text{ENQUEUE}(Q, 4)$, $\\text{ENQUEUE}(Q ,1)$, $\\text{ENQUEUE}(Q, 3)$, $\\text{DEQUEUE}(Q)$, $\\text{ENQUEUE}(Q, 8)$, and $\\text{DEQUEUE}(Q)$ on an initially empty queue $Q$ stored in array $Q[1..6]$. $$ \\begin{array}{l|cccc} \\text{ENQUEUE($Q, 4$)} & 4 & & & \\\\ \\text{ENQUEUE($Q, 1$)} & 4 & 1 & & \\\\ \\text{ENQUEUE($Q, 3$)} & 4 & 1 & 3 & \\\\ \\text{DEQUEUE($Q$)} & & 1 & 3 & \\\\ \\text{ENQUEUE($Q, 8$)} & & 1 & 3 & 8 \\\\ \\text{DEQUEUE($Q$)} & & & 3 & 8 \\end{array} $$","title":"10.1-3"},{"location":"Chap10/10.1/#101-4","text":"Rewrite $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ to detect underflow and overflow of a queue. To detect underflow and overflow of a queue, we can implement $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ first. 1 2 3 4 QUEUE - EMPTY ( Q ) if Q . head == Q . tail return true else return false 1 2 3 4 QUEUE - FULL ( Q ) if Q . head == Q . tail + 1 or ( Q . head == 1 and Q . tail == Q . length ) return true else return false 1 2 3 4 5 6 7 8 ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 1 2 3 4 5 6 7 8 9 DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x","title":"10.1-4"},{"location":"Chap10/10.1/#101-5","text":"Whereas a stack allows insertion and deletion of elements at only one end, and a queue allows insertion at one end and deletion at the other end, a deque (double-ended queue) allows insertion and deletion at both ends. Write four $O(1)$-time procedures to insert elements into and delete elements from both ends of a deque implemented by an array. The procedures $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ are implemented in Exercise 10.1-4. 1 2 3 4 5 6 7 8 HEAD - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else if Q . head == 1 Q . head = Q . length else Q . head = Q . head - 1 Q [ Q . head ] = x 1 2 3 4 5 6 7 8 TAIL - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 1 2 3 4 5 6 7 8 9 HEAD - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x 1 2 3 4 5 6 7 8 9 TAIL - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else if Q . tail == 1 Q . tail = Q . length else Q . tail = Q . tail - 1 x = Q [ Q . tail ] return x","title":"10.1-5"},{"location":"Chap10/10.1/#101-6","text":"Show how to implement a queue using two stacks. Analyze the running time of the queue operations. $\\text{ENQUEUE}$: $\\Theta(1)$. $\\text{DEQUEUE}$: worst $O(n)$, amortized $\\Theta(1)$. Let the two stacks be $A$ and $B$. $\\text{ENQUEUE}$ pushes elements on $B$. $\\text{DEQUEUE}$ pops elements from $A$. If $A$ is empty, the contents of $B$ are transfered to $A$ by popping them out of $B$ and pushing them to $A$. That way they appear in reverse order and are popped in the original. A $\\text{DEQUEUE}$ operation can perform in $\\Theta(n)$ time, but that will happen only when $A$ is empty. If many $\\text{ENQUEUE}$s and $\\text{DEQUEUE}$s are performed, the total time will be linear to the number of elements, not to the largest length of the queue.","title":"10.1-6"},{"location":"Chap10/10.1/#101-7","text":"Show how to implement a stack using two queues. Analyze the running time of the stack operations. $\\text{PUSH}$: $\\Theta(1)$. $\\text{POP}$: $\\Theta(n)$. We have two queues and mark one of them as active. $\\text{PUSH}$ queues an element on the active queue. $\\text{POP}$ should dequeue all but one element of the active queue and queue them on the inactive. The roles of the queues are then reversed, and the final element left in the (now) inactive queue is returned. The $\\text{PUSH}$ operation is $\\Theta(1)$, but the $\\text{POP}$ operation is $\\Theta(n)$ where $n$ is the number of elements in the stack.","title":"10.1-7"},{"location":"Chap10/10.2/","text":"10.2-1 Can you implement the dynamic-set operation $\\text{INSERT}$ on a singly linked list in $O(1)$ time? How about $\\text{DELETE}$? $\\text{INSERT}$: can be implemented in constant time by prepending it to the list. 1 2 3 LIST - INSERT ( L , x ) x . next = L . head L . head = x $\\text{DELETE}$: you can copy the value from the successor to element you want to delete, and then you can delete the successor in $O(1)$ time. This solution is not good in situations when you have a large object, in that case copying the whole object will be a bad idea. 10.2-2 Implement a stack using a singly linked list $L$. The operations $\\text{PUSH}$ and $\\text{POP}$ should still take $O(1)$ time. 1 2 3 4 STACK - EMPTY ( L ) if L . head == NIL return true else return false $\\text{PUSH}$: adds an element in the beginning of the list. 1 2 3 PUSH ( L , x ) x . next = L . head L . head = x $\\text{POP}$: removes the first element from the list. 1 2 3 4 5 6 7 POP ( L ) if STACK - EMPTY ( L ) error \"underflow\" else x = L . head L . head = L . head . next return x 10.2-3 Implement a queue by a singly linked list $L$. The operations $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ should still take $O(1)$ time. 1 2 3 4 QUEUE - EMPTY ( L ) if L . head == NIL return true else return false $\\text{ENQUEUE}$: inserts an element at the end of the list. In this case we need to keep track of the last element of the list. We can do that with a sentinel. 1 2 3 4 5 6 ENQUEUE ( L , x ) if QUEUE - EMPTY ( L ) L . head = x else L . tail . next = x L . tail = x x . next = NIL $\\text{DEQUEUE}$: removes an element from the beginning of the list. 1 2 3 4 5 6 7 8 9 DEQUEUE ( L ) if QUEUE - EMPTY ( L ) error \"underflow\" else x = L . head if L . head == L . tail L . tail = NIL L . head = L . head . next return x 10.2-4 As written, each loop iteration in the $\\text{LIST-SEARCH}'$ procedure requires two tests: one for $x \\ne L.nil$ and one for $x.key \\ne k$. Show how to eliminate the test for $x \\ne L.nil$ in each iteration. 1 2 3 4 5 6 LIST - SEARCH ' ( L , k ) x = L . nil . next L . nil . key = k while x . key != k x = x . next return x 10.2-5 Implement the dictionary operations $\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$ using singly linked, circular lists. What are the running times of your procedures? $\\text{INSERT}$: $O(1)$. 1 2 3 LIST - INSERT '' ( L , x ) x . next = L . nil . next L . nil . next = x $\\text{DELETE}$: $O(n)$. 1 2 3 4 5 6 7 LIST - DELETE '' ( L , x ) prev = L . nil while prev . next != x if prev . next == L . nil error \"element not exist\" prev = prev . next prev . next = x . next $\\text{SEARCH}$: $O(n)$. 1 2 3 4 5 LIST - SEARCH '' ( L , k ) x = L . nil . next while x != L . nil and x . key != k x = x . next return x 10.2-6 The dynamic-set operation $\\text{UNION}$ takes two disjoint sets $S_1$ and $S_2$ as input, and it returns a set $S = S_1 \\cup S_2$ consisting of all the elements of $S_1$ and $S_2$. The sets $S_1$ and $S_2$ are usually destroyed by the operation. Show how to support $\\text{UNION}$ in $O(1)$ time using a suitable list data structure. If both sets are a doubly linked lists, we just point link the last element of the first list to the first element in the second. If the implementation uses sentinels, we need to destroy one of them. 1 2 3 4 5 LIST - UNION ( L [ 1 ], L [ 2 ]) L [ 2 ]. nil . next . prev = L [ 1 ]. nil . prev L [ 1 ]. nil . prev . next = L [ 2 ]. nil . next L [ 2 ]. nil . prev . next = L [ 1 ]. nil L [ 1 ]. nil . prev = L [ 2 ]. nil . prev 10.2-7 Give a $\\Theta(n)$-time nonrecursive procedure that reverses a singly linked list of $n$ elements. The procedure should use no more than constant storage beyond that needed for the list itself. 1 2 3 4 5 6 7 8 9 LIST - REVERSE ( L ) p [ 1 ] = NIL p [ 2 ] = L . head while p [ 2 ] != NIL p [ 3 ] = p [ 2 ]. next p [ 2 ]. next = p [ 1 ] p [ 1 ] = p [ 2 ] p [ 2 ] = p [ 3 ] L . head = p [ 1 ] 10.2-8 $\\star$ Explain how to implement doubly linked lists using only one pointer value $x.np$ per item instead of the usual two ($next$ and $prev$). Assume all pointer values can be interpreted as $k$-bit integers, and define $x.np$ to be $x.np = x.next \\text{ XOR } x.prev$, the $k$-bit \"exclusive-or\" of $x.next$ and $x.prev$. (The value $\\text{NIL}$ is represented by $0$.) Be sure to describe what information you need to access the head of the list. Show how to implement the $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ operations on such a list. Also show how to reverse such a list in $O(1)$ time. 1 2 3 4 5 6 7 8 LIST - SEARCH ( L , k ) prev = NIL x = L . head while x != NIL and x . key != k next = prev XOR x . np prev = x x = next return x 1 2 3 4 5 6 7 LIST - INSERT ( L , x ) x . np = NIL XOR L . tail if L . tail != NIL L . tail . np = ( L . tail . np XOR NIL ) XOR x // tail.prev XOR x if L . head == NIL L . head = x L . tail = x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 LIST - DELETE ( L , x ) y = L . head prev = NIL while y != NIL next = prev XOR y . np if y != x prev = y y = next else if prev != NIL prev . np = ( prev . np XOR y ) XOR next // prev.prev XOR next else L . head = next if next != NIL next . np = prev XOR ( y XOR next . np ) // prev XOR next.next else L . tail = prev 1 2 3 4 LIST - REVERSE ( L ) tmp = L . head L . head = L . tail L . tail = tmp","title":"10.2 Linked lists"},{"location":"Chap10/10.2/#102-1","text":"Can you implement the dynamic-set operation $\\text{INSERT}$ on a singly linked list in $O(1)$ time? How about $\\text{DELETE}$? $\\text{INSERT}$: can be implemented in constant time by prepending it to the list. 1 2 3 LIST - INSERT ( L , x ) x . next = L . head L . head = x $\\text{DELETE}$: you can copy the value from the successor to element you want to delete, and then you can delete the successor in $O(1)$ time. This solution is not good in situations when you have a large object, in that case copying the whole object will be a bad idea.","title":"10.2-1"},{"location":"Chap10/10.2/#102-2","text":"Implement a stack using a singly linked list $L$. The operations $\\text{PUSH}$ and $\\text{POP}$ should still take $O(1)$ time. 1 2 3 4 STACK - EMPTY ( L ) if L . head == NIL return true else return false $\\text{PUSH}$: adds an element in the beginning of the list. 1 2 3 PUSH ( L , x ) x . next = L . head L . head = x $\\text{POP}$: removes the first element from the list. 1 2 3 4 5 6 7 POP ( L ) if STACK - EMPTY ( L ) error \"underflow\" else x = L . head L . head = L . head . next return x","title":"10.2-2"},{"location":"Chap10/10.2/#102-3","text":"Implement a queue by a singly linked list $L$. The operations $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ should still take $O(1)$ time. 1 2 3 4 QUEUE - EMPTY ( L ) if L . head == NIL return true else return false $\\text{ENQUEUE}$: inserts an element at the end of the list. In this case we need to keep track of the last element of the list. We can do that with a sentinel. 1 2 3 4 5 6 ENQUEUE ( L , x ) if QUEUE - EMPTY ( L ) L . head = x else L . tail . next = x L . tail = x x . next = NIL $\\text{DEQUEUE}$: removes an element from the beginning of the list. 1 2 3 4 5 6 7 8 9 DEQUEUE ( L ) if QUEUE - EMPTY ( L ) error \"underflow\" else x = L . head if L . head == L . tail L . tail = NIL L . head = L . head . next return x","title":"10.2-3"},{"location":"Chap10/10.2/#102-4","text":"As written, each loop iteration in the $\\text{LIST-SEARCH}'$ procedure requires two tests: one for $x \\ne L.nil$ and one for $x.key \\ne k$. Show how to eliminate the test for $x \\ne L.nil$ in each iteration. 1 2 3 4 5 6 LIST - SEARCH ' ( L , k ) x = L . nil . next L . nil . key = k while x . key != k x = x . next return x","title":"10.2-4"},{"location":"Chap10/10.2/#102-5","text":"Implement the dictionary operations $\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$ using singly linked, circular lists. What are the running times of your procedures? $\\text{INSERT}$: $O(1)$. 1 2 3 LIST - INSERT '' ( L , x ) x . next = L . nil . next L . nil . next = x $\\text{DELETE}$: $O(n)$. 1 2 3 4 5 6 7 LIST - DELETE '' ( L , x ) prev = L . nil while prev . next != x if prev . next == L . nil error \"element not exist\" prev = prev . next prev . next = x . next $\\text{SEARCH}$: $O(n)$. 1 2 3 4 5 LIST - SEARCH '' ( L , k ) x = L . nil . next while x != L . nil and x . key != k x = x . next return x","title":"10.2-5"},{"location":"Chap10/10.2/#102-6","text":"The dynamic-set operation $\\text{UNION}$ takes two disjoint sets $S_1$ and $S_2$ as input, and it returns a set $S = S_1 \\cup S_2$ consisting of all the elements of $S_1$ and $S_2$. The sets $S_1$ and $S_2$ are usually destroyed by the operation. Show how to support $\\text{UNION}$ in $O(1)$ time using a suitable list data structure. If both sets are a doubly linked lists, we just point link the last element of the first list to the first element in the second. If the implementation uses sentinels, we need to destroy one of them. 1 2 3 4 5 LIST - UNION ( L [ 1 ], L [ 2 ]) L [ 2 ]. nil . next . prev = L [ 1 ]. nil . prev L [ 1 ]. nil . prev . next = L [ 2 ]. nil . next L [ 2 ]. nil . prev . next = L [ 1 ]. nil L [ 1 ]. nil . prev = L [ 2 ]. nil . prev","title":"10.2-6"},{"location":"Chap10/10.2/#102-7","text":"Give a $\\Theta(n)$-time nonrecursive procedure that reverses a singly linked list of $n$ elements. The procedure should use no more than constant storage beyond that needed for the list itself. 1 2 3 4 5 6 7 8 9 LIST - REVERSE ( L ) p [ 1 ] = NIL p [ 2 ] = L . head while p [ 2 ] != NIL p [ 3 ] = p [ 2 ]. next p [ 2 ]. next = p [ 1 ] p [ 1 ] = p [ 2 ] p [ 2 ] = p [ 3 ] L . head = p [ 1 ]","title":"10.2-7"},{"location":"Chap10/10.2/#102-8-star","text":"Explain how to implement doubly linked lists using only one pointer value $x.np$ per item instead of the usual two ($next$ and $prev$). Assume all pointer values can be interpreted as $k$-bit integers, and define $x.np$ to be $x.np = x.next \\text{ XOR } x.prev$, the $k$-bit \"exclusive-or\" of $x.next$ and $x.prev$. (The value $\\text{NIL}$ is represented by $0$.) Be sure to describe what information you need to access the head of the list. Show how to implement the $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ operations on such a list. Also show how to reverse such a list in $O(1)$ time. 1 2 3 4 5 6 7 8 LIST - SEARCH ( L , k ) prev = NIL x = L . head while x != NIL and x . key != k next = prev XOR x . np prev = x x = next return x 1 2 3 4 5 6 7 LIST - INSERT ( L , x ) x . np = NIL XOR L . tail if L . tail != NIL L . tail . np = ( L . tail . np XOR NIL ) XOR x // tail.prev XOR x if L . head == NIL L . head = x L . tail = x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 LIST - DELETE ( L , x ) y = L . head prev = NIL while y != NIL next = prev XOR y . np if y != x prev = y y = next else if prev != NIL prev . np = ( prev . np XOR y ) XOR next // prev.prev XOR next else L . head = next if next != NIL next . np = prev XOR ( y XOR next . np ) // prev XOR next.next else L . tail = prev 1 2 3 4 LIST - REVERSE ( L ) tmp = L . head L . head = L . tail L . tail = tmp","title":"10.2-8 $\\star$"},{"location":"Chap10/10.3/","text":"10.3-1 Draw a picture of the sequence $\\langle 13, 4, 8, 19, 5, 11 \\rangle$ stored as a doubly linked list using the multiple-array representation. Do the same for the single-array representation. A multiple array version could be $L = 2$, $$ \\begin{array}{ccccccc} / & 3 & 4 & 5 & 6 & 7 & / \\\\ & 12 & 4 & 8 & 19 & 5 & 11 \\\\ & & 2 & 3 & 4 & 5 & 6 \\end{array} $$ A single array version could be $L = 4$, $$ \\begin{array}{cccccccccccccccccc} 12 & 7 & / & 4 & 10 & 4 & 8 & 13 & 7 & 19 & 16 & 10 & 5 & 19 & 13 & 11 & / & 16 \\end{array} $$ 10.3-2 Write the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ for a homogeneous collection of objects implemented by the single-array representation. 1 2 3 4 5 6 ALLOCATE - OBJECT () if free == NIL error \"out of space\" else x = free free = A [ x + 1 ] return x 1 2 3 FREE - OBJECT ( x ) A [ x + 1 ] = free free = x 10.3-3 Why don't we need to set or reset the $prev$ attributes of objects in the implementation of the $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures? We implement $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ in the hope of managing the storage of currently non-used object in the free list so that one can be allocated for reusing. As the free list acts like a stack, to maintain this stack-like collection, we merely remember its first pointer and set the $next$ attribute of objects. There is no need to worry the $prev$ attribute, for it hardly has any impact on the resulting free list. 10.3-4 It is often desirable to keep all elements of a doubly linked list compact in storage, using, for example, the first $m$ index locations in the multiple-array representation. (This is the case in a paged, virtual-memory computing environment.) Explain how to implement the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ so that the representation is compact. Assume that there are no pointers to elements of the linked list outside the list itself. ($\\textit{Hint:}$ Use the array implementation of a stack.) 1 2 3 4 5 ALLOCATE - OBJECT () if STACK - EMPTY ( F ) error \"out of space\" else x = POP ( F ) return x 1 2 3 4 5 6 7 8 FREE - OBJECT ( x ) p = F . top - 1 p . prev . next = x p . next . prev = x x . key = p . key x . prev = p . prev x . next = p . next PUSH ( F , p ) 10.3-5 Let $L$ be a doubly linked list of length $n$ stored in arrays $key$, $prev$, and $next$ of length $m$. Suppose that these arrays are managed by $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures that keep a doubly linked free list $F$. Suppose further that of the $m$ items, exactly $n$ are on list $L$ and $m - n$ are on the free list. Write a procedure $\\text{COMPACTIFY-LIST}(L, F)$ that, given the list $L$ and the free list $F$, moves the items in $L$ so that they occupy array positions $1, 2, \\ldots, n$ and adjusts the free list $F$ so that it remains correct, occupying array positions $n + 1, n + 2, \\ldots, m$. The running time of your procedure should be $\\Theta(n)$, and it should use only a constant amount of extra space. Argue that your procedure is correct. We represent the combination of arrays $key$, $prev$, and $next$ by a multible-array $A$. Each object of $A$'s is either in list $L$ or in the free list $F$, but not in both. The procedure $\\text{COMPACTIFY-LIST}$ transposes the first object in $L$ with the first object in $A$, the second objects until the list $L$ is exhausted. 1 2 3 4 5 6 7 8 9 10 11 12 13 COMPACTIFY - LIST ( L , F ) TRANSPOSE ( A [ L . head ], A [ 1 ]) if F . head == 1 F . head = L . head L . head = 1 l = A [ L . head ]. next i = 2 while l != NIL TRANSPOSE ( A [ l ], A [ i ]) if F == i F = l l = A [ l ]. next i = i + 1 1 2 3 4 5 TRANSPOSE ( a , b ) SWAP ( a . prev . next , b . prev . next ) SWAP ( a . prev , b . prev ) SWAP ( a . next . prev , b . next . prev ) SWAP ( a . next , b . next )","title":"10.3 Implementing pointers and objects"},{"location":"Chap10/10.3/#103-1","text":"Draw a picture of the sequence $\\langle 13, 4, 8, 19, 5, 11 \\rangle$ stored as a doubly linked list using the multiple-array representation. Do the same for the single-array representation. A multiple array version could be $L = 2$, $$ \\begin{array}{ccccccc} / & 3 & 4 & 5 & 6 & 7 & / \\\\ & 12 & 4 & 8 & 19 & 5 & 11 \\\\ & & 2 & 3 & 4 & 5 & 6 \\end{array} $$ A single array version could be $L = 4$, $$ \\begin{array}{cccccccccccccccccc} 12 & 7 & / & 4 & 10 & 4 & 8 & 13 & 7 & 19 & 16 & 10 & 5 & 19 & 13 & 11 & / & 16 \\end{array} $$","title":"10.3-1"},{"location":"Chap10/10.3/#103-2","text":"Write the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ for a homogeneous collection of objects implemented by the single-array representation. 1 2 3 4 5 6 ALLOCATE - OBJECT () if free == NIL error \"out of space\" else x = free free = A [ x + 1 ] return x 1 2 3 FREE - OBJECT ( x ) A [ x + 1 ] = free free = x","title":"10.3-2"},{"location":"Chap10/10.3/#103-3","text":"Why don't we need to set or reset the $prev$ attributes of objects in the implementation of the $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures? We implement $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ in the hope of managing the storage of currently non-used object in the free list so that one can be allocated for reusing. As the free list acts like a stack, to maintain this stack-like collection, we merely remember its first pointer and set the $next$ attribute of objects. There is no need to worry the $prev$ attribute, for it hardly has any impact on the resulting free list.","title":"10.3-3"},{"location":"Chap10/10.3/#103-4","text":"It is often desirable to keep all elements of a doubly linked list compact in storage, using, for example, the first $m$ index locations in the multiple-array representation. (This is the case in a paged, virtual-memory computing environment.) Explain how to implement the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ so that the representation is compact. Assume that there are no pointers to elements of the linked list outside the list itself. ($\\textit{Hint:}$ Use the array implementation of a stack.) 1 2 3 4 5 ALLOCATE - OBJECT () if STACK - EMPTY ( F ) error \"out of space\" else x = POP ( F ) return x 1 2 3 4 5 6 7 8 FREE - OBJECT ( x ) p = F . top - 1 p . prev . next = x p . next . prev = x x . key = p . key x . prev = p . prev x . next = p . next PUSH ( F , p )","title":"10.3-4"},{"location":"Chap10/10.3/#103-5","text":"Let $L$ be a doubly linked list of length $n$ stored in arrays $key$, $prev$, and $next$ of length $m$. Suppose that these arrays are managed by $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures that keep a doubly linked free list $F$. Suppose further that of the $m$ items, exactly $n$ are on list $L$ and $m - n$ are on the free list. Write a procedure $\\text{COMPACTIFY-LIST}(L, F)$ that, given the list $L$ and the free list $F$, moves the items in $L$ so that they occupy array positions $1, 2, \\ldots, n$ and adjusts the free list $F$ so that it remains correct, occupying array positions $n + 1, n + 2, \\ldots, m$. The running time of your procedure should be $\\Theta(n)$, and it should use only a constant amount of extra space. Argue that your procedure is correct. We represent the combination of arrays $key$, $prev$, and $next$ by a multible-array $A$. Each object of $A$'s is either in list $L$ or in the free list $F$, but not in both. The procedure $\\text{COMPACTIFY-LIST}$ transposes the first object in $L$ with the first object in $A$, the second objects until the list $L$ is exhausted. 1 2 3 4 5 6 7 8 9 10 11 12 13 COMPACTIFY - LIST ( L , F ) TRANSPOSE ( A [ L . head ], A [ 1 ]) if F . head == 1 F . head = L . head L . head = 1 l = A [ L . head ]. next i = 2 while l != NIL TRANSPOSE ( A [ l ], A [ i ]) if F == i F = l l = A [ l ]. next i = i + 1 1 2 3 4 5 TRANSPOSE ( a , b ) SWAP ( a . prev . next , b . prev . next ) SWAP ( a . prev , b . prev ) SWAP ( a . next . prev , b . next . prev ) SWAP ( a . next , b . next )","title":"10.3-5"},{"location":"Chap10/10.4/","text":"10.4-1 Draw the binary tree rooted at index $6$ that is represented by the following attributes: $$ \\begin{array}{cccc} \\text{index} & key & left & right \\\\ \\hline 1 & 12 & 7 & 3 \\\\ 2 & 15 & 8 & \\text{NIL} \\\\ 3 & 4 & 10 & \\text{NIL} \\\\ 4 & 10 & 5 & 9 \\\\ 5 & 2 & \\text{NIL} & \\text{NIL} \\\\ 6 & 18 & 1 & 4 \\\\ 7 & 7 & \\text{NIL} & \\text{NIL} \\\\ 8 & 14 & 6 & 2 \\\\ 9 & 21 & \\text{NIL} & \\text{NIL} \\\\ 10 & 5 & \\text{NIL} & \\text{NIL} \\end{array} $$ 10.4-2 Write an $O(n)$-time recursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. 1 2 3 4 5 6 PRINT - BINARY - TREE ( T ) x = T . root if x != NIL PRINT - BINARY - TREE ( x . left ) print x . key PRINT - BINARY - TREE ( x . right ) 10.4-3 Write an O$(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. Use a stack as an auxiliary data structure. 1 2 3 4 5 6 7 8 9 10 11 12 PRINT - BINARY - TREE ( T , S ) PUSH ( S , T . root ) while ! STACK - EMPTY ( S ) x = S [ S . top ] while x != NIL // store all nodes on the path towards the leftmost leaf PUSH ( S , x . left ) x = S [ S . top ] POP ( S ) // S has NIL on its top, so pop it if ! STACK - EMPTY ( S ) // print this nodes, leap to its in-order successor x = POP ( S ) print x . key PUSH ( S , x . right ) 10.4-4 Write an $O(n)$-time procedure that prints all the keys of an arbitrary rooted tree with $n$ nodes, where the tree is stored using the left-child, right-sibling representation. 1 2 3 4 5 6 7 8 9 10 11 PRINT - LCRS - TREE ( T ) x = T . root if x != NIL print x . key lc = x . left - child if lc != NIL PRINT - LCRS - TREE ( lc ) rs = lc . right - sibling while rs != NIL PRINT - LCRS - TREE ( rs ) rs = rs . right - sibling 10.4-5 $\\star$ Write an $O(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node. Use no more than constant extra space outside of the tree itself and do not modify the tree, even temporarily, during the procedure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 PRINT - KEY ( T ) prev = NIL x = T . root while x != NIL if prev = x . parent print x . key if x == x . left x . left else if x . right x . right else x . parent else if prev == x . left and x . left != NIL prev = x x = x . right else prev = x x = x . parent 10.4-6 $\\star$ The left-child, right-sibling representation of an arbitrary rooted tree uses three pointers in each node: left-child , right-sibling , and parent . From any node, its parent can be reached and identified in constant time and all its children can be reached and identified in time linear in the number of children. Show how to use only two pointers and one boolean value in each node so that the parent of a node or all of its children can be reached and identified in time linear in the number of children. Use boolean to identify the last sibling, and the last sibling's right-sibling points to the parent.","title":"10.4 Representing rooted trees"},{"location":"Chap10/10.4/#104-1","text":"Draw the binary tree rooted at index $6$ that is represented by the following attributes: $$ \\begin{array}{cccc} \\text{index} & key & left & right \\\\ \\hline 1 & 12 & 7 & 3 \\\\ 2 & 15 & 8 & \\text{NIL} \\\\ 3 & 4 & 10 & \\text{NIL} \\\\ 4 & 10 & 5 & 9 \\\\ 5 & 2 & \\text{NIL} & \\text{NIL} \\\\ 6 & 18 & 1 & 4 \\\\ 7 & 7 & \\text{NIL} & \\text{NIL} \\\\ 8 & 14 & 6 & 2 \\\\ 9 & 21 & \\text{NIL} & \\text{NIL} \\\\ 10 & 5 & \\text{NIL} & \\text{NIL} \\end{array} $$","title":"10.4-1"},{"location":"Chap10/10.4/#104-2","text":"Write an $O(n)$-time recursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. 1 2 3 4 5 6 PRINT - BINARY - TREE ( T ) x = T . root if x != NIL PRINT - BINARY - TREE ( x . left ) print x . key PRINT - BINARY - TREE ( x . right )","title":"10.4-2"},{"location":"Chap10/10.4/#104-3","text":"Write an O$(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. Use a stack as an auxiliary data structure. 1 2 3 4 5 6 7 8 9 10 11 12 PRINT - BINARY - TREE ( T , S ) PUSH ( S , T . root ) while ! STACK - EMPTY ( S ) x = S [ S . top ] while x != NIL // store all nodes on the path towards the leftmost leaf PUSH ( S , x . left ) x = S [ S . top ] POP ( S ) // S has NIL on its top, so pop it if ! STACK - EMPTY ( S ) // print this nodes, leap to its in-order successor x = POP ( S ) print x . key PUSH ( S , x . right )","title":"10.4-3"},{"location":"Chap10/10.4/#104-4","text":"Write an $O(n)$-time procedure that prints all the keys of an arbitrary rooted tree with $n$ nodes, where the tree is stored using the left-child, right-sibling representation. 1 2 3 4 5 6 7 8 9 10 11 PRINT - LCRS - TREE ( T ) x = T . root if x != NIL print x . key lc = x . left - child if lc != NIL PRINT - LCRS - TREE ( lc ) rs = lc . right - sibling while rs != NIL PRINT - LCRS - TREE ( rs ) rs = rs . right - sibling","title":"10.4-4"},{"location":"Chap10/10.4/#104-5-star","text":"Write an $O(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node. Use no more than constant extra space outside of the tree itself and do not modify the tree, even temporarily, during the procedure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 PRINT - KEY ( T ) prev = NIL x = T . root while x != NIL if prev = x . parent print x . key if x == x . left x . left else if x . right x . right else x . parent else if prev == x . left and x . left != NIL prev = x x = x . right else prev = x x = x . parent","title":"10.4-5 $\\star$"},{"location":"Chap10/10.4/#104-6-star","text":"The left-child, right-sibling representation of an arbitrary rooted tree uses three pointers in each node: left-child , right-sibling , and parent . From any node, its parent can be reached and identified in constant time and all its children can be reached and identified in time linear in the number of children. Show how to use only two pointers and one boolean value in each node so that the parent of a node or all of its children can be reached and identified in time linear in the number of children. Use boolean to identify the last sibling, and the last sibling's right-sibling points to the parent.","title":"10.4-6 $\\star$"},{"location":"Chap10/Problems/10-1/","text":"For each of the four types of lists in the following table, what is the asymptotic worst-case running time for each dynamic-set operation listed? $$ \\begin{array}{l|c|c|c|c|} & \\text{unsorted, singly linked} & \\text{sorted, singly linked} & \\text{unsorted, doubly linked} & \\text{sorted, doubly linked} \\\\ \\hline \\text{SEARCH($L, k$)} & & & & \\\\ \\hline \\text{INSERT($L, x$)} & & & & \\\\ \\hline \\text{DELETE($L, x$)} & & & & \\\\ \\hline \\text{SUCCESSOR($L, x$)} & & & & \\\\ \\hline \\text{PREDECESSOR($L, x$)} & & & & \\\\ \\hline \\text{MINIMUM($L$)} & & & & \\\\ \\hline \\text{MAXIMUM($L$)} & & & & \\\\ \\hline \\end{array} $$ $$ \\begin{array}{l|c|c|c|c|} & \\text{unsorted, singly linked} & \\text{sorted, singly linked} & \\text{unsorted, doubly linked} & \\text{sorted, doubly linked} \\\\ \\hline \\text{SEARCH($L, k$)} & \\Theta(n) & \\Theta(n) & \\Theta(n) & \\Theta(n) \\\\ \\hline \\text{INSERT($L, x$)} & \\Theta(1) & \\Theta(n) & \\Theta(1) & \\Theta(n) \\\\ \\hline \\text{DELETE($L, x$)} & \\Theta(n) & \\Theta(n) & \\Theta(1) & \\Theta(1) \\\\ \\hline \\text{SUCCESSOR($L, x$)} & \\Theta(n) & \\Theta(1) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\text{PREDECESSOR($L, x$)} & \\Theta(n) & \\Theta(n) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\text{MINIMUM($L$)} & \\Theta(n) & \\Theta(1) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\text{MAXIMUM($L$)} & \\Theta(n) & \\Theta(n) & \\Theta(n) & \\Theta(n) \\\\ \\hline \\end{array} $$","title":"10-1 Comparisons among lists"},{"location":"Chap10/Problems/10-2/","text":"A mergeable heap supports the following operations: $\\text{MAKE-HEAP}$ (which creates an empty mergeable heap), $\\text{INSERT}$, $\\text{MINIMUM}$, $\\text{EXTRACT-MIN}$, and $\\text{UNION}$. Show how to implement mergeable heaps using linked lists in each of the following cases. Try to make each operation as efficient as possible. Analyze the running time of each operation in terms of the size of the dynamic set(s) being operated on. a. Lists are sorted. b. Lists are unsorted. c. Lists are unsorted, and dynamic sets to be merged are disjoint. In all three cases, $\\text{MAKE-HEAP}$ simply creates a new list $L$, sets $L.head = \\text{NIL}$, and returns $L$ in constant time. Assume lists are doubly linked. To realize a linked list as a heap, we imagine the usual array implementation of a binary heap, where the children of the $i$th element are $2i$ and $2i + 1$. a. To insert, we perform a linear scan to see where to insert an element such that the list remains sorted. This takes linear time. The first element in the list is the minimum element, and we can find it in constant time. $\\text{EXTRACT-MIN}$ returns the first element of the list, then deletes it. Union performs a merge operation between the two sorted lists, interleaving their entries such that the resulting list is sorted. This takes time linear in the sum of the lengths of the two lists. b. To insert an element $x$ into the heap, begin linearly scanning the list until the first instance of an element $y$ which is strictly larger than $x$. If no such larger element exists, simply insert $x$ at the end of the list. If $y$ does exist, replace $y \\text t$ by $x$. This maintains the min-heap property because $x \\le y$ and $y$ was smaller than each of its children, so $x$ must be as well. Moreover, $x$ is larger than its parent because $y$ was the first element in the list to exceed $x$. Now insert $y$, starting the scan at the node following $x$. Since we check each node at most once, the time is linear in the size of the list. To get the minimum element, return the key of the head of the list in constant time. To extract the minimum element, we first call $\\text{MINIMUM}$. Next, we'll replace the key of the head of the list by the key of the second smallest element $y$ in the list. We'll take the key stored at the end of the list and use it to replace the key of $y$. Finally, we'll delete the last element of the list, and call $\\text{MIN-HEAPIFY}$ on the list. To implement this with linked lists, we need to step through the list to get from element $i$ to element $2i$. We omit this detail from the code, but we'll consider it for runtime analysis. Since the value of $i$ on which $\\text{MIN-HEAPIFY}$ is called is always increasing and we never need to step through elements multiple times, the runtime is linear in the length of the list. 1 2 3 4 5 6 7 8 EXTRACT - MIN ( L ) min = MINIMIM ( L ) linearly scan for the second smallest element , located in position i L . head . key = L [ i ] L [ i ]. key = L [ L . length ]. key DELETE ( L , L [ L . length ]) MIN - HEAPIFY ( L [ i ], i ) return min 1 2 3 4 5 6 7 8 9 10 11 12 MIN - HEAPIFY ( L [ i ], i ) l = L [ 2 i ]. key r = L [ 2 i + 1 ]. key p = L [ i ]. key smallest = i if L [ 2 i ] != NIL and l < p smallest = 2 i if L [ 2 i + 1 ] != NIL and r < L [ smallest ] smallest = 2 i + 1 if smallest != i exchange L [ i ] with L [ smallest ] MIN - HEAPIFY ( L [ smallest ], smallest ]) Union is implemented below, where we assume $A$ and $B$ are the two list representations of heaps to be merged. The runtime is again linear in the lengths of the lists to be merged. 1 2 3 4 5 6 7 8 9 10 11 UNION ( A , B ) if A . head == NIL return B x = A . head while B . head != NIL if B . head . key \u2264 x . key INSERT ( B , x . key ) x . key = B . head . key DELETE ( B , B . head ) x = x . next return A c. Since the algorithms in part (b) didn't depend on the elements being distinct, we can use the same ones.","title":"10-2 Mergeable heaps using linked lists"},{"location":"Chap10/Problems/10-3/","text":"Exercise 10.3-4 asked how we might maintain an $n$-element list compactly in the first $n$ positions of an array. We shall assume that all keys are distinct and that the compact list is also sorted, that is, $key[i] < key[next[i]]$ for all $i = 1, 2, \\ldots, n$ such that $next[i] \\ne \\text{NIL}$. We will also assume that we have a variable $L$ that contains the index of the first element on the list. Under these assumptions, you will show that we can use the following randomized algorithm to search the list in $O(\\sqrt n)$ expected time. 1 2 3 4 5 6 7 8 9 10 11 12 COMPACT - LIST - SEARCH ( L , n , k ) i = L while i != NIL and key [ i ] < k j = RANDOM ( 1 , n ) if key [ i ] < key [ j ] and key [ j ] \u2264 k i = j if key [ i ] == k return i i = next [ i ] if i == NIL or key [ i ] > k return NIL else return i If we ignore lines 3\u20137 of the procedure, we have an ordinary algorithm for searching a sorted linked list, in which index $i$ points to each position of the list in turn. The search terminates once the index $i$ \"falls off\" the end of the list or once $key[i] \\ge k$. In the latter case, if $key[i] = k$, clearly we have found a key with the value $k$. If, however, $key[i] > k$, then we will never find a key with the value $k$, and so terminating the search was the right thing to do. Lines 3\u20137 attempt to skip ahead to a randomly chosen position $j$. Such a skip benefits us if $key[j]$ is larger than $key[i]$ and no larger than $k$; in such a case, $j$ marks a position in the list that $i$ would have to reach during an ordinary list search. Because the list is compact, we know that any choice of $j$ between $1$ and $n$ indexes some object in the list rather than a slot on the free list. Instead of analyzing the performance of $\\text{COMPACT-LIST-SEARCH}$ directly, we shall analyze a related algorithm, $\\text{COMPACT-LIST-SEARCH}'$, which executes two separate loops. This algorithm takes an additional parameter $t$ which determines an upper bound on the number of iterations of the first loop. 1 2 3 4 5 6 7 8 9 10 11 12 13 COMPACT - LIST - SEARCH ' ( L , n , k , t ) i = L for q = 1 to t j = RANDOM ( 1 , n ) if key [ i ] < key [ j ] and key [ j ] \u2264 k i = j if key [ i ] == k return i while i != NIL and key [ i ] < k i = next [ i ] if i == NIL or key [ i ] > k return NIL else return i To compare the execution of the algorithms $\\text{COMPACT-LIST-SEARCH}(L, n, k)$ and $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$, assume that the sequence of integers returned by the calls of $\\text{RANDOM}(1, n)$ is the same for both algorithms. a. Suppose that $\\text{COMPACT-LIST-SEARCH}(L, n, k)$ takes $t$ iterations of the while loop of lines 2\u20138. Argue that $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$ returns the same answer and that the total number of iterations of both the for and while loops within $\\text{COMPACT-LIST-SEARCH}'$ is at least $t$. In the call $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$, let $X_t$ be the random variable that describes the distance in the linked list (that is, through the chain of $next$ pointers) from position $i$ to the desired key $k$ after $t$ iterations of the for loop of lines 2\u20137 have occurred. b. Argue that the expected running time of $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$ is $O(t + \\text E[X_t])$. c. Show that $\\text E[X_t] \\le \\sum_{r = 1}^n (1 - r / n)^t$. ($\\textit{Hint:}$ Use equation $\\text{(C.25)}$.) d. Show that $\\sum_{r = 0}^{n - 1} r^t \\le n^{t + 1} / (t + 1)$. e. Prove that $\\text E[X_t] \\le n / (t + 1)$. f. Show that $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$ runs in $O(t + n / t)$ expected time. g. Conclude that $\\text{COMPACT-LIST-SEARCH}$ runs in $O(\\sqrt n)$ expected time. h. Why do we assume that all keys are distinct in $\\text{COMPACT-LIST-SEARCH}$? Argue that random skips do not necessarily help asymptotically when the list contains repeated key values. a. If the original version of the algorithm takes only $t$ iterations, then, we have that it was only at most t random skips though the list to get to the desired value, since each iteration of the original while loop is a possible random jump followed by a normal step through the linked list. b. The for loop on lines 2\u20137 will get run exactly $t$ times, each of which is constant runtime. After that, the while loop on lines 8\u20139 will be run exactly $X_t$ times. So, the total runtime is $O(t + \\text E[X_t])$. 1 2 3 4 5 6 7 8 9 10 11 12 UNION ( A , B ) if A . head == NIL return B i = 1 x = A . head while B . head != NIL if B . head . key \u2264 x . key insert a node at the end of list B with key = x . key x . key = B . head . key DELETE ( B , B . head ) x = x . next return A c. Using equation $\\text{C.25}$, we have that $\\text E[X_t] = \\sum_{i = 1}^\\infty \\Pr\\{X_t \\ge i\\}$. So, we need to show that $\\Pr\\{X_t \\ge i\\} \\le (1 - i / n)^t$. This can be seen because having $X_t$ being greater than $i$ means that each random choice will result in an element that is either at least $i$ steps before the desired element, or is after the desired element. There are $n - i$ such elements, out of the total $n$ elements that we were pricking from. So, for a single one of the choices to be from such a range, we have a probability of $(n - i) / n = (1 - i / n)$. Since each of the selections was independent, the total probability that all of them were is $(1 - i / n)^t$, as desired. Lastly, we can note that since the linked list has length $n$, the probability that $X_t$ is greater than $n$ is equal to zero. d. Since we have that $t > 0$, we know that the function $f(x) = x^t$ is increasing, so, that means that $\\lfloor x \\rfloor^t \\le f(x)$. So, $$\\sum_{r = 0}^{n - 1} r^t = \\int_0^n \\lfloor r \\rfloor^t dr \\le \\int_0^n f(r)dr = \\frac{n^{t + 1}}{t + 1}.$$ e. $$ \\begin{aligned} \\text E[X_t] & \\le \\sum_{r = 1}^n (1 - r / n)^t \\\\ & = \\sum_{r = 1}^n \\sum_{i = 0}^t \\binom{t}{i} (-r / n)^i \\\\ & = \\sum_{i = 0}^t \\sum_{r = 1}^n \\binom{t}{i} (-r / n)^i \\\\ & = \\sum_{i = 0}^t \\binom{t}{i} (-1)^i \\Bigg(n^i - 1 + \\sum_{r = 0}^{n - 1} r^t\\Bigg) / n \\\\ & \\le \\sum_{i = 0}^t \\binom{t}{i} (-1)^i \\Big(n^i - 1 + \\frac{n^{i + 1}}{i + 1}) / n \\\\ & \\le \\sum_{i = 0}^t \\binom{t}{i} (-1)^i \\frac{n^i}{i + 1} \\\\ & = \\frac{1}{t + 1} \\sum_{i = 0}^t \\binom{t + 1}{i + 1} (-n)^i \\\\ & \\le \\frac{(1 - n)^{t + 1}}{t + 1}. \\end{aligned} $$ f. We just put together parts (b) and (e) to get that it runs in time $O(t + n / (t + 1))$. But, this is the same as $O(t + n / t)$. g. Since we have that for any number of iterations $t$ that the first algorithm takes to find its answer, the second algorithm will return it in time $O(t + n / t)$. In particular, if we just have that $t = \\sqrt n$. The second algorithm takes time only $O(\\sqrt n)$. This means that tihe first list search algorithm is $O(\\sqrt n)$ as well. h. If we don't have distinct key values, then, we may randomly select an element that is further along than we had been before, but not jump to it because it has the same key as what we were currently at. The analysis will break when we try to bound the probability that $X_t \\ge i$.","title":"10-3 Searching a sorted compact list"},{"location":"Chap11/11.1/","text":"11.1-1 Suppose that a dynamic set $S$ is represented by a direct-address table $T$ of length $m$. Describe a procedure that finds the maximum element of $S$. What is the worst-case performance of your procedure? As the dynamic set $S$ is represented by the direct-address table $T$, for each key $k$ in $S$, there is a slot $k$ in $T$ points to it. If no element with key $k$ in $S$, then $T[k] = \\text{NIL}$. Using this property, we can find the maximum element of $S$ by traversing down from the highest slot to seek the first non-$\\text{NIL}$ one. 1 2 MAXIMUM ( S ) return TABLE - MAXIMUM ( T , m - 1 ) 1 2 3 4 5 6 TABLE - MAXIMUM ( T , l ) if l < 0 return NIL else if DIRECT - ADDRESS - SEARCH ( T , l ) != NIL return l else return TABLE - MAXIMUM ( T , l - 1 ) The $\\text{TABLE-MAXIMUM}$ procedure gest down and checks $1$ sloc at a time, linearly approaches the solution. In the worst case where $S$ is empty, $\\text{TABLE-MAXIMUM}$ examines $m$ slots. Therefore, the worst-case performance of $\\text{MAXIMUM}$ is $O(n)$, where $n$ is the number of elements in the set $S$. 11.1-2 A bit vector is simply an array of bits ($0$s and $1$s). A bit vector of length $m$ takes much less space than an array of $m$ pointers. Describe how to use a bit vector to represent a dynamic set of distinct elements with no satellite data. Dictionary operations should run in $O(1)$ time. Using the bit vector data structure, we can represent keys less than $m$ by a string of $m$ bits, denoted by $V[0..m - 1]$, in which each position that occupied by the bit $1$, corresponds to a key in the set $S$. If the set contains no element with key $k$, then $V[k] = 0$. For instance, we can store the set $\\{2, 4, 6, 10, 16\\}$ in a bit vector of length $20$: $$001010100010000010000$$ 1 2 3 4 BITMAP - SEARCH ( V , k ) if V [ k ] != 0 return k else return NIL 1 2 BITMAP - INSERT ( V , x ) V [ x ] = 1 1 2 BITMAP - DELETE ( V , x ) V [ x ] = 0 Each of these operations takes only $O(1)$ time. 11.1-3 Suggest how to implement a direct-address table in which the keys of stored elements do not need to be distinct and the elements can have satellite data. All three dictionary operations ($\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$) should run in $O(1)$ time. (Don't forget that $\\text{DELETE}$ takes as an argument a pointer to an object to be deleted, not a key.) Assuming that fetching an element should return the satellite data of all the stored elements, we can have each key map to a doubly linked list. $\\text{INSERT}$: appends the element to the list in constant time $\\text{DELETE}$: removes the element from the linked list in constant time (the element contains pointers to the previous and next element) $\\text{SEARCH}$: returns the first element, which is a node in a linked list, in constant time 11.1-4 $\\star$ We wish to implement a dictionary by using direct addressing on a huge array. At the start, the array entries may contain garbage, and initializing the entire array is impractical because of its size. Describe a scheme for implementing a direct-address dictionary on a huge array. Each stored object should use $O(1)$ space; the operations $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ should take $O(1)$ time each; and initializing the data structure should take $O(1)$ time. ($\\textit{Hint:}$ Use an additional array, treated somewhat like a stack whose size is the number of keys actually stored in the dictionary, to help determine whether a given entry in the huge array is valid or not.) We denote the huge array by $T$ and, taking the hint from the book, we also have a stack implemented by an array $S$. The size of $S$ equals the number of keys actually stored, so that $S$ should be allocated at the dictionary's maximum size. The stack has an attribute $S.top$, so that only entries $S[1..S.top]$ are valid. The idea of this scheme is that entries of $T$ and $S$ validate each other. If key $k$ is actually stored in $T$, then $T[k]$ contains the index, say $j$, of a valid entry in $S$, and $S[j]$ contains the value $k$. Let us call this situation, in which $1 \\le T[k] \\le S.top$, $S[T[k]] = k$, and $T[S[j]] = j$, a validating cycle . Assuming that we also need to store pointers to objects in our direct-address table, we can store them in an array that is parallel to either $T$ or $S$. Since $S$ is smaller than $T$, we'll use an array $S'$, allocated to be the same size as $S$, for these pointers. Thus, if the dictionary contains an object $x$ with key $k$, then there is a validating cycle and $S'[T[k]]$ points to $x$. The operations on the dictionary work as follows: Initialization: Simply set $S.top = 0$, so that there are no valid entries in the stack. SEARCH: Given key $k$, we check whether we have a validating cycle, i.e., whether $1 \\le T [k] \\le S.top$ and $S[T[k]] = k$. If so, we return $S'[T[k]]$, and otherwise we return $\\text{NIL}$. INSERT: To insert object $x$ with key $k$, assuming that this object is not already in the dictionary, we increment $S.top$, set $S[S.top] = k$, set $S'[S.top] = x$, and set $T[k] = S.top$. DELETE: To delete object $x$ with key $k$, assuming that this object is in the dictionary, we need to break the validating cycle. The trick is to also ensure that we don't leave a \"hole\" in the stack, and we solve this problem by moving the top entry of the stack into the position that we are vacating-and then fixing up that entry's validating cycle. That is, we execute the following sequence of assignments: $$ \\begin{aligned} & S[T[k]] = S[S.top] \\\\ & S'[T[k]] = S'[S.top] \\\\ & T[S[T[k]]] = T[k] \\\\ & T[k] = 0 \\\\ & S.top = S.top - 1 \\end{aligned} $$ Each of these operation - initialization, $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$-takes $O(1)$ time.","title":"11.1 Direct-address tables"},{"location":"Chap11/11.1/#111-1","text":"Suppose that a dynamic set $S$ is represented by a direct-address table $T$ of length $m$. Describe a procedure that finds the maximum element of $S$. What is the worst-case performance of your procedure? As the dynamic set $S$ is represented by the direct-address table $T$, for each key $k$ in $S$, there is a slot $k$ in $T$ points to it. If no element with key $k$ in $S$, then $T[k] = \\text{NIL}$. Using this property, we can find the maximum element of $S$ by traversing down from the highest slot to seek the first non-$\\text{NIL}$ one. 1 2 MAXIMUM ( S ) return TABLE - MAXIMUM ( T , m - 1 ) 1 2 3 4 5 6 TABLE - MAXIMUM ( T , l ) if l < 0 return NIL else if DIRECT - ADDRESS - SEARCH ( T , l ) != NIL return l else return TABLE - MAXIMUM ( T , l - 1 ) The $\\text{TABLE-MAXIMUM}$ procedure gest down and checks $1$ sloc at a time, linearly approaches the solution. In the worst case where $S$ is empty, $\\text{TABLE-MAXIMUM}$ examines $m$ slots. Therefore, the worst-case performance of $\\text{MAXIMUM}$ is $O(n)$, where $n$ is the number of elements in the set $S$.","title":"11.1-1"},{"location":"Chap11/11.1/#111-2","text":"A bit vector is simply an array of bits ($0$s and $1$s). A bit vector of length $m$ takes much less space than an array of $m$ pointers. Describe how to use a bit vector to represent a dynamic set of distinct elements with no satellite data. Dictionary operations should run in $O(1)$ time. Using the bit vector data structure, we can represent keys less than $m$ by a string of $m$ bits, denoted by $V[0..m - 1]$, in which each position that occupied by the bit $1$, corresponds to a key in the set $S$. If the set contains no element with key $k$, then $V[k] = 0$. For instance, we can store the set $\\{2, 4, 6, 10, 16\\}$ in a bit vector of length $20$: $$001010100010000010000$$ 1 2 3 4 BITMAP - SEARCH ( V , k ) if V [ k ] != 0 return k else return NIL 1 2 BITMAP - INSERT ( V , x ) V [ x ] = 1 1 2 BITMAP - DELETE ( V , x ) V [ x ] = 0 Each of these operations takes only $O(1)$ time.","title":"11.1-2"},{"location":"Chap11/11.1/#111-3","text":"Suggest how to implement a direct-address table in which the keys of stored elements do not need to be distinct and the elements can have satellite data. All three dictionary operations ($\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$) should run in $O(1)$ time. (Don't forget that $\\text{DELETE}$ takes as an argument a pointer to an object to be deleted, not a key.) Assuming that fetching an element should return the satellite data of all the stored elements, we can have each key map to a doubly linked list. $\\text{INSERT}$: appends the element to the list in constant time $\\text{DELETE}$: removes the element from the linked list in constant time (the element contains pointers to the previous and next element) $\\text{SEARCH}$: returns the first element, which is a node in a linked list, in constant time","title":"11.1-3"},{"location":"Chap11/11.1/#111-4-star","text":"We wish to implement a dictionary by using direct addressing on a huge array. At the start, the array entries may contain garbage, and initializing the entire array is impractical because of its size. Describe a scheme for implementing a direct-address dictionary on a huge array. Each stored object should use $O(1)$ space; the operations $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ should take $O(1)$ time each; and initializing the data structure should take $O(1)$ time. ($\\textit{Hint:}$ Use an additional array, treated somewhat like a stack whose size is the number of keys actually stored in the dictionary, to help determine whether a given entry in the huge array is valid or not.) We denote the huge array by $T$ and, taking the hint from the book, we also have a stack implemented by an array $S$. The size of $S$ equals the number of keys actually stored, so that $S$ should be allocated at the dictionary's maximum size. The stack has an attribute $S.top$, so that only entries $S[1..S.top]$ are valid. The idea of this scheme is that entries of $T$ and $S$ validate each other. If key $k$ is actually stored in $T$, then $T[k]$ contains the index, say $j$, of a valid entry in $S$, and $S[j]$ contains the value $k$. Let us call this situation, in which $1 \\le T[k] \\le S.top$, $S[T[k]] = k$, and $T[S[j]] = j$, a validating cycle . Assuming that we also need to store pointers to objects in our direct-address table, we can store them in an array that is parallel to either $T$ or $S$. Since $S$ is smaller than $T$, we'll use an array $S'$, allocated to be the same size as $S$, for these pointers. Thus, if the dictionary contains an object $x$ with key $k$, then there is a validating cycle and $S'[T[k]]$ points to $x$. The operations on the dictionary work as follows: Initialization: Simply set $S.top = 0$, so that there are no valid entries in the stack. SEARCH: Given key $k$, we check whether we have a validating cycle, i.e., whether $1 \\le T [k] \\le S.top$ and $S[T[k]] = k$. If so, we return $S'[T[k]]$, and otherwise we return $\\text{NIL}$. INSERT: To insert object $x$ with key $k$, assuming that this object is not already in the dictionary, we increment $S.top$, set $S[S.top] = k$, set $S'[S.top] = x$, and set $T[k] = S.top$. DELETE: To delete object $x$ with key $k$, assuming that this object is in the dictionary, we need to break the validating cycle. The trick is to also ensure that we don't leave a \"hole\" in the stack, and we solve this problem by moving the top entry of the stack into the position that we are vacating-and then fixing up that entry's validating cycle. That is, we execute the following sequence of assignments: $$ \\begin{aligned} & S[T[k]] = S[S.top] \\\\ & S'[T[k]] = S'[S.top] \\\\ & T[S[T[k]]] = T[k] \\\\ & T[k] = 0 \\\\ & S.top = S.top - 1 \\end{aligned} $$ Each of these operation - initialization, $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$-takes $O(1)$ time.","title":"11.1-4 $\\star$"},{"location":"Chap11/11.2/","text":"11.2-1 Suppose we use a hash function $h$ to hash $n$ distinct keys into an array $T$ of length $m$. Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of $\\{\\{k, l\\}: k \\ne l \\text{ and } h(k) = h(l)\\}$? For each pair of keys $k$, $l$, where $k \\ne l$, define the indicator random variable $X_{kl} = \\text I\\{h(k) = h(l)\\}$. Since we assume simple uniform hashing, $\\Pr\\{X_{kl} = 1\\} = \\Pr\\{h(k) = h(l)\\} = 1 / m$, and so $\\text E[X_{kl}] = 1 / m$. Now define the random variable $Y$ to be the total number of collisions, so that $Y = \\sum_{k \\ne l} X_{kl}$. The expected number of collisions is $$ \\begin{aligned} \\text E[Y] & = \\text E\\bigg[\\sum_{k \\ne l} X_{kl}\\bigg] \\\\ & = \\sum_{k \\ne l} \\text E[X_{kl}] & \\text{(linearity of expectation)} \\\\ & = \\binom{n}{2}\\frac{1}{m} \\\\ & = \\frac{n(n - 1)}{2} \\cdot \\frac{1}{m} \\\\ & = \\frac{n(n - 1)}{2m}. \\end{aligned} $$ 11.2-2 Demonstrate what happens when we insert the keys $5, 28, 19, 15, 20, 33, 12, 17, 10$ into a hash table with collisions resolved by chaining. Let the table have $9$ slots, and let the hash function be $h(k) = k \\mod 9$. Let us number our slots $0, 1, \\dots, 8$. Then our resulting hash table will look like following: $$ \\begin{array}{c|l} h(k) & \\text{keys} \\\\ \\hline 0 \\mod 9 & \\\\ 1 \\mod 9 & 28 \\to 19 \\to 10 \\\\ 2 \\mod 9 & 20 \\\\ 3 \\mod 9 & 12 \\\\ 4 \\mod 9 & \\\\ 5 \\mod 9 & 5 \\\\ 6 \\mod 9 & 15 \\to 33 \\\\ 7 \\mod 9 & \\\\ 8 \\mod 9 & 17 \\end{array} $$ 11.2-3 Professor Marley hypothesizes that he can obtain substantial performance gains by modifying the chaining scheme to keep each list in sorted order. How does the professor's modification affect the running time for successful searches, unsuccessful searches, insertions, and deletions? Successful searches: no difference, $\\Theta(1 + \\alpha)$. Unsuccessful searches: faster but still $\\Theta(1 + \\alpha)$. Insertions: same as successful searches, $\\Theta(1 + \\alpha)$. Deletions: same as successful searches, $\\Theta(1 + \\alpha)$. 11.2-4 Suggest how to allocate and deallocate storage for elements within the hash table itself by linking all unused slots into a free list. Assume that one slot can store a flag and either one element plus a pointer or two pointers. All dictionary and free-list operations should run in $O(1)$ expected time. Does the free list need to be doubly linked, or does a singly linked free list suffice? The flag in each slot will indicate whether the slot is free. A free slot is in the free list, a doubly linked list of all free slots in the table. The slot thus contains two pointers. A used slot contains an element and a pointer (possibly $\\text{NIL}$) to the next element that hashes to this slot. (Of course, that pointer points to another slot in the table.) Operations Insertion: If the element hashes to a free slot, just remove the slot from the free list and store the element there (with a $\\text{NIL}$ pointer). The free list must be doubly linked in order for this deletion to run in $O(1)$ time. If the element hashes to a used slot $j$, check whether the element $x$ already there \"belongs\" there (its key also hashes to slot $j$). If so, add the new element to the chain of elements in this slot. To do so, allocate a free slot (e.g., take the head of the free list) for the new element and put this new slot at the head of the list pointed to by the hashed-to slot ($j$). If not, $E$ is part of another slot's chain. Move it to a new slot by allocating one from the free list, copying the old slot's ($j$'s) contents (element $x$ and pointer) to the new slot, and updating the pointer in the slot that pointed to $j$ to point to the new slot. Then insert the new element in the now-empty slot as usual. To update the pointer to $j$, it is necessary to find it by searching the chain of elements starting in the slot $x$ hashes to. Deletion: Let $j$ be the slot the element $x$ to be deleted hashes to. If $x$ is the only element in $j$ ($j$ doesn't point to any other entries), just free the slot, returning it to the head of the free list. If $x$ is in $j$ but there's a pointer to a chain of other elements, move the first pointed-to entry to slot $j$ and free the slot it was in. If $x$ is found by following a pointer from $j$, just free $x$'s slot and splice it out of the chain (i.e., update the slot that pointed to $x$ to point to $x$'s successor). Searching: Check the slot the key hashes to, and if that is not the desired element, follow the chain of pointers from the slot. All the operations take expected $O(1)$ times for the same reason they do with the version in the book: The expected time to search the chains is $O(1 + \\alpha)$ regardless of where the chains are stored, and the fact that all the elements are stored in the table means that $\\alpha \\le 1$. If the free list were singly linked, then operations that involved removing an arbitrary slot from the free list would not run in $O(1)$ time. 11.2-5 Suppose that we are storing a set of $n$ keys into a hash table of size $m$. Show that if the keys are drawn from a universe $U$ with $|U| > nm$, then $U$ has a subset of size $n$ consisting of keys that all hash to the same slot, so that the worst-case searching time for hashing with chaining is $\\Theta(n)$. Suppose the $m - 1$ slots contains at most $n - 1$ elements, then the remaining slot should have $$|U| - (m - 1)(n - 1) > nm - (m - 1)(n - 1) = n + m - 1 \\ge n$$ elements, thus $U$ has a subset of size $n$. 11.2-6 Suppose we have stored $n$ keys in a hash table of size $m$, with collisions resolved by chaining, and that we know the length of each chain, including the length $L$ of the longest chain. Describe a procedure that selects a key uniformly at random from among the keys in the hash table and returns it in expected time $O(L \\cdot (1 + 1 / \\alpha))$. We can view the hash table as if it had $m$ rows and $L$ columns; each row stores one chain. The array has $mL$ entries storing $n$ keys, and $mL - n$ empty values. The procedure picks array positions at random until it finds a key, which it returns. The probability of success on one draw is $n / mL$, so $mL / n = L / \\alpha$ trials are needed. Each trial takes time $O(1)$, since the individual chain sizes are known. The chain for the last draw needs to be scanned to find the desired element, however, costing $O(L)$.","title":"11.2 Hash tables"},{"location":"Chap11/11.2/#112-1","text":"Suppose we use a hash function $h$ to hash $n$ distinct keys into an array $T$ of length $m$. Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of $\\{\\{k, l\\}: k \\ne l \\text{ and } h(k) = h(l)\\}$? For each pair of keys $k$, $l$, where $k \\ne l$, define the indicator random variable $X_{kl} = \\text I\\{h(k) = h(l)\\}$. Since we assume simple uniform hashing, $\\Pr\\{X_{kl} = 1\\} = \\Pr\\{h(k) = h(l)\\} = 1 / m$, and so $\\text E[X_{kl}] = 1 / m$. Now define the random variable $Y$ to be the total number of collisions, so that $Y = \\sum_{k \\ne l} X_{kl}$. The expected number of collisions is $$ \\begin{aligned} \\text E[Y] & = \\text E\\bigg[\\sum_{k \\ne l} X_{kl}\\bigg] \\\\ & = \\sum_{k \\ne l} \\text E[X_{kl}] & \\text{(linearity of expectation)} \\\\ & = \\binom{n}{2}\\frac{1}{m} \\\\ & = \\frac{n(n - 1)}{2} \\cdot \\frac{1}{m} \\\\ & = \\frac{n(n - 1)}{2m}. \\end{aligned} $$","title":"11.2-1"},{"location":"Chap11/11.2/#112-2","text":"Demonstrate what happens when we insert the keys $5, 28, 19, 15, 20, 33, 12, 17, 10$ into a hash table with collisions resolved by chaining. Let the table have $9$ slots, and let the hash function be $h(k) = k \\mod 9$. Let us number our slots $0, 1, \\dots, 8$. Then our resulting hash table will look like following: $$ \\begin{array}{c|l} h(k) & \\text{keys} \\\\ \\hline 0 \\mod 9 & \\\\ 1 \\mod 9 & 28 \\to 19 \\to 10 \\\\ 2 \\mod 9 & 20 \\\\ 3 \\mod 9 & 12 \\\\ 4 \\mod 9 & \\\\ 5 \\mod 9 & 5 \\\\ 6 \\mod 9 & 15 \\to 33 \\\\ 7 \\mod 9 & \\\\ 8 \\mod 9 & 17 \\end{array} $$","title":"11.2-2"},{"location":"Chap11/11.2/#112-3","text":"Professor Marley hypothesizes that he can obtain substantial performance gains by modifying the chaining scheme to keep each list in sorted order. How does the professor's modification affect the running time for successful searches, unsuccessful searches, insertions, and deletions? Successful searches: no difference, $\\Theta(1 + \\alpha)$. Unsuccessful searches: faster but still $\\Theta(1 + \\alpha)$. Insertions: same as successful searches, $\\Theta(1 + \\alpha)$. Deletions: same as successful searches, $\\Theta(1 + \\alpha)$.","title":"11.2-3"},{"location":"Chap11/11.2/#112-4","text":"Suggest how to allocate and deallocate storage for elements within the hash table itself by linking all unused slots into a free list. Assume that one slot can store a flag and either one element plus a pointer or two pointers. All dictionary and free-list operations should run in $O(1)$ expected time. Does the free list need to be doubly linked, or does a singly linked free list suffice? The flag in each slot will indicate whether the slot is free. A free slot is in the free list, a doubly linked list of all free slots in the table. The slot thus contains two pointers. A used slot contains an element and a pointer (possibly $\\text{NIL}$) to the next element that hashes to this slot. (Of course, that pointer points to another slot in the table.) Operations Insertion: If the element hashes to a free slot, just remove the slot from the free list and store the element there (with a $\\text{NIL}$ pointer). The free list must be doubly linked in order for this deletion to run in $O(1)$ time. If the element hashes to a used slot $j$, check whether the element $x$ already there \"belongs\" there (its key also hashes to slot $j$). If so, add the new element to the chain of elements in this slot. To do so, allocate a free slot (e.g., take the head of the free list) for the new element and put this new slot at the head of the list pointed to by the hashed-to slot ($j$). If not, $E$ is part of another slot's chain. Move it to a new slot by allocating one from the free list, copying the old slot's ($j$'s) contents (element $x$ and pointer) to the new slot, and updating the pointer in the slot that pointed to $j$ to point to the new slot. Then insert the new element in the now-empty slot as usual. To update the pointer to $j$, it is necessary to find it by searching the chain of elements starting in the slot $x$ hashes to. Deletion: Let $j$ be the slot the element $x$ to be deleted hashes to. If $x$ is the only element in $j$ ($j$ doesn't point to any other entries), just free the slot, returning it to the head of the free list. If $x$ is in $j$ but there's a pointer to a chain of other elements, move the first pointed-to entry to slot $j$ and free the slot it was in. If $x$ is found by following a pointer from $j$, just free $x$'s slot and splice it out of the chain (i.e., update the slot that pointed to $x$ to point to $x$'s successor). Searching: Check the slot the key hashes to, and if that is not the desired element, follow the chain of pointers from the slot. All the operations take expected $O(1)$ times for the same reason they do with the version in the book: The expected time to search the chains is $O(1 + \\alpha)$ regardless of where the chains are stored, and the fact that all the elements are stored in the table means that $\\alpha \\le 1$. If the free list were singly linked, then operations that involved removing an arbitrary slot from the free list would not run in $O(1)$ time.","title":"11.2-4"},{"location":"Chap11/11.2/#112-5","text":"Suppose that we are storing a set of $n$ keys into a hash table of size $m$. Show that if the keys are drawn from a universe $U$ with $|U| > nm$, then $U$ has a subset of size $n$ consisting of keys that all hash to the same slot, so that the worst-case searching time for hashing with chaining is $\\Theta(n)$. Suppose the $m - 1$ slots contains at most $n - 1$ elements, then the remaining slot should have $$|U| - (m - 1)(n - 1) > nm - (m - 1)(n - 1) = n + m - 1 \\ge n$$ elements, thus $U$ has a subset of size $n$.","title":"11.2-5"},{"location":"Chap11/11.2/#112-6","text":"Suppose we have stored $n$ keys in a hash table of size $m$, with collisions resolved by chaining, and that we know the length of each chain, including the length $L$ of the longest chain. Describe a procedure that selects a key uniformly at random from among the keys in the hash table and returns it in expected time $O(L \\cdot (1 + 1 / \\alpha))$. We can view the hash table as if it had $m$ rows and $L$ columns; each row stores one chain. The array has $mL$ entries storing $n$ keys, and $mL - n$ empty values. The procedure picks array positions at random until it finds a key, which it returns. The probability of success on one draw is $n / mL$, so $mL / n = L / \\alpha$ trials are needed. Each trial takes time $O(1)$, since the individual chain sizes are known. The chain for the last draw needs to be scanned to find the desired element, however, costing $O(L)$.","title":"11.2-6"},{"location":"Chap11/11.3/","text":"11.3-1 Suppose we wish to search a linked list of length $n$, where each element contains a key $k$ along with a hash value $h(k)$. Each key is a long character string. How might we take advantage of the hash values when searching the list for an element with a given key? If every element also contained a hash of the long character string, when we are searching for the desired element, we'll first check if the hashvalue of the node in the linked list, and move on if it disagrees. This can increase the runtime by a factor proportional to the length of the long character strings. 11.3-2 Suppose that we hash a string of $r$ characters into $m$ slots by treating it as a radix-128 number and then using the division method. We can easily represent the number $m$ as a 32-bit computer word, but the string of $r$ characters, treated as a radix-128 number, takes many words. How can we apply the division method to compute the hash value of the character string without using more than a constant number of words of storage outside the string itself? 1 2 3 sum = 0 for i = 1 to r sum = ( sum * 128 + s [ i ]) % m Use sum as the key. 11.3-3 Consider a version of the division method in which $h(k) = k \\mod m$, where $m = 2^p - 1$ and $k$ is a character string interpreted in radix $2^p$. Show that if we can derive string $x$ from string $y$ by permuting its characters, then $x$ and $y$ hash to the same value. Give an example of an application in which this property would be undesirable in a hash function. First, we observe that we can generate any permutation by a sequence of interchanges of pairs of characters. One can prove this property formally, but informally, consider that both heapsort and quicksort work by interchanging pairs of elements and that they have to be able to produce any permutation of their input array. Thus, it suffices to show that if string $x$ can be derived from string $y$ by interchanging a single pair of characters, then $x$ and $y$ hash to the same value. Let us denote the $i$th character in $x$ by $x_i$, and similarly for $y$. The interpretation of $x$ in radix $2^p$ is $\\sum_{i = 0}^{n - 1} x_i 2^{ip}$, and so $h(x) = (\\sum_{i = 0}^{n - 1} x_i 2^{ip}) \\mod (2^p - 1)$. Similarly, $h(y) = (\\sum_{i = 0}^{n - 1} y_i 2^{ip}) \\mod (2^p - 1)$. Suppose that $x$ and $y$ are identical strings of $n$ characters except that the characters in positions $a$ and $b$ are interchanged: $x_a = y_b$ and $y_a = x_b$. Without loss of generality, let $a > b$. We have $$h(x) - h(y) = \\Big(\\sum_{i = 0}^{n - 1} x_i 2^{ip}\\Big) \\mod (2^p - 1) - \\Big(\\sum_{i = 0}^{n - 1} y_i 2^{ip}\\Big) \\mod (2^p - 1).$$ Since $0 \\le h(x)$, $h(y) < 2^p - 1$, we have that $-(2^p - 1) < h(x) - h(y) < 2^p - 1$. If we show that $(h(x) - h(y)) \\mod (2^p - 1) = 0$, then $h(x) = h(y)$. Since the sums in the hash functions are the same except for indices $a$ and $b$, we have $$ \\begin{aligned} (h(x) - h(y)) \\mod (2^p - 1) & = ((x_a 2^{ap} + x_b 2^{bp}) - (y_a 2^{ap} + y_b 2^{bp})) \\mod (2^p - 1) \\\\ & = ((x_a 2^{ap} + x_b 2^{bp}) - (x_b 2^{ap} + x_a 2^{bp})) \\mod (2^p - 1) \\\\ & = ((x_a - x_b)2^{ap} - (x_a - x_b) 2^{bp}) \\mod (2^p - 1) \\\\ & = ((x_a - x_b)(2^{ap} - 2^{bp})) \\mod (2^p - 1) \\\\ & = ((x_a - x_b)2^{bp}(2^{(a - b)p} - 1)) \\mod (2^p - 1). \\end{aligned} $$ By equation $\\text{(A.5)}$, $$\\sum_{i = 0}^{a - b - 1} 2^{pi} = \\frac{2^{(a - b)p} - 1}{2^p - 1},$$ and multiplying both sides by $s^p - 1$, we get $2^{(a - b)p} - 1 = \\big(\\sum_{i = 0}^{a - b - 1} 2^{pi}\\big)(2^p - 1)$. Thus, $$ \\begin{aligned} (h(x) - h(y))\\mod(2^p - 1) & = \\Bigg((x_a - x_b)2^{bp}\\Bigg(\\sum_{i = 0}^{a - b - 1} 2^{pi}\\Bigg)(2^p - 1)\\Bigg) \\mod (2^p - 1) \\\\ & = 0, \\end{aligned} $$ since one of the factors is $2^p - 1$. We have shown that $(h(x) - h(y)) \\mod (2^p - 1) = 0$, and so $h(x) = h(y)$. 11.3-4 Consider a hash table of size $m = 1000$ and a corresponding hash function $h(k) = \\lfloor m (kA \\mod 1) \\rfloor$ for $A = (\\sqrt 5 - 1) / 2$. Compute the locations to which the keys $61$, $62$, $63$, $64$, and $65$ are mapped. $h(61) = \\lfloor 1000(61 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 700$. $h(62) = \\lfloor 1000(62 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 318$. $h(63) = \\lfloor 1000(63 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 936$. $h(64) = \\lfloor 1000(64 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 554$. $h(65) = \\lfloor 1000(65 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 172$. 11.3-5 $\\star$ Define a family $\\mathcal H$ of hash functions from a finite set $U$ to a finite set $B$ to be $\\epsilon$-universal if for all pairs of distinct elements $k$ and $l$ in $U$, $$\\Pr\\{h(k) = h(l)\\} \\le \\epsilon,$$ where the probability is over the choice of the hash function $h$ drawn at random from the family $\\mathcal H$. Show that an $\\epsilon$-universal family of hash functions must have $$\\epsilon \\ge \\frac{1}{|B|} - \\frac{1}{|U|}.$$ Let $b = |B|$ and $u = |U|$. We start by showing that the total number of collisions is minimized by a hash function that maps $u / b$ elements of $U$ to each of the $b$ values in $B$. For a given hash function, let $u_j$ be the number of elements that map to $j \\in B$. We have $u = \\sum_{j \\in B} u_j$. We also have that the number of collisions for a given value of $j \\in B$ is $\\binom{u_j}{2} = u_j(u_j - 1) / 2$. Lemma The total number of collisions is minimized when $u_j = u / b$ for each $j \\in B$. Proof If $u_j \\le u / b$, let us call $j$ underloaded , and if $u_j \\ge u / b$, let us call $j$ overloaded . Consider an unbalanced situation in which $u_j \\ne u / b$ for at least one value $j \\in B$. We can think of converting a balanced situation in which all $u_j$ equal $u / b$ into the unbalanced situation by repeatedly moving an element that maps to an underloaded value to map instead to an overloaded value. (If you think of the values of $B$ as representing buckets, we are repeatedly moving elements from buckets containing at most $u / b$ elements to buckets containing at least $u / b$ elements.) We now show that each such move increases the number of collisions, so that all the moves together must increase the number of collisions. Suppose that we move an element from an underloaded value $j$ to an overloaded value $k$, and we leave all other elements alone. Because $j$ is underloaded and $k$ is overloaded, $u_j \\le u / b\\le u_k$. Considering just the collisions for values $j$ and $k$, we have $u_j(u_j - 1) / 2 + u_k(u_k - 1) / 2$ collisions before the move and $(u_j - 1)(u_j - 2) / 2 + (u_k + 1)u_k / 2$ collisions afterward. We wish to show that $$u_j(u_j - 1) / 2 + u_k(u_k - 1) / 2 < (u_j - 1)(u_j - 2) / 2 + (u_k + 1)u_k / 2.$$ We have the following sequence of equivalent inequalities: $$ \\begin{aligned} u_j & < u_k + 1 \\\\ 2u_j & < 2u_k + 2 \\\\ -u_k & < u_k - 2u_j + 2 \\\\ u_j^2 - u_j + u_k^2 - u_k & < u_j^2 - 3u_j + 2 + u_k^2 + u_k \\\\ u_j(u_j - 1) + u_k(u_k - 1) & < (u_j - 1)(u_j - 2) + (u_k + 1)u_k \\\\ u_j(u_j - 1) / 2 + u_k(u_k - 1) / 2 & < (u_j - 1)(u_j - 2) / 2 + (u_k + 1)u_k / 2. \\end{aligned} $$ Thus, each move increases the number of collisions. We conclude that the number of collisions is minimized when $u_j = u / b$ for each $j \\in B$. By the above lemma, for any hash function, the total number of collisions must be at least $b(u / b)(u / b - 1) / 2$. The number of pairs of distinct elements is $\\binom{u}{2} = u(u - 1) / 2$. Thus, the number of collisions per pair of distinct elements must be at least $$ \\begin{aligned} \\frac{b(u / b)(u / b - 1) / 2}{u(u - 1) / 2} & = \\frac{u / b - 1}{u - 1} \\\\ & > \\frac{u / b - 1}{u} \\\\ & = \\frac{1}{b} - \\frac{1}{u}. \\end{aligned} $$ Thus, the bound on the probability of a collision for any pair of distinct elements can be no less than $1 / b - 1 / u = 1 / |B| - 1 / |U|$. 11.3-6 $\\star$ Let $U$ be the set of $n$-tuples of values drawn from $\\mathbb Z_p$, and let $B = \\mathbb Z_p$, where $p$ is prime. Define the hash function $h_b: U \\rightarrow B$ for $b \\in \\mathbb Z_p$ on an input $n$-tuple $\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle$ from $U$ as $$h_b(\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle) = \\Bigg(\\sum_{j = 0}^{n - 1} a_jb^j \\Bigg) \\mod p,$$ and let $\\mathcal{H} = \\{h_b : b \\in \\mathbb Z_p\\}$. Argue that $\\mathcal H$ is $((n - 1) / p)$-universal according to the definition of $\\epsilon$-universal in Exercise 11.3-5. ($\\textit{Hint:}$ See Exercise 31.4-4.) Fix $b \\in \\mathbb Z_p$. By exercise 31.4-4, $h_b(x)$ collides with $h_b(y)$ for at most $n - 1$ other $y \\in U$. Since there are a total of $p$ possible values that $h_b$ takes on, the probability that $h_b(x) = h_b(y)$ is bounded from above by $\\frac{n - 1}{p}$, since this holds for any value of $b$, $\\mathcal H$ is $((n - 1 ) /p)$-universal.","title":"11.3 Hash functions"},{"location":"Chap11/11.3/#113-1","text":"Suppose we wish to search a linked list of length $n$, where each element contains a key $k$ along with a hash value $h(k)$. Each key is a long character string. How might we take advantage of the hash values when searching the list for an element with a given key? If every element also contained a hash of the long character string, when we are searching for the desired element, we'll first check if the hashvalue of the node in the linked list, and move on if it disagrees. This can increase the runtime by a factor proportional to the length of the long character strings.","title":"11.3-1"},{"location":"Chap11/11.3/#113-2","text":"Suppose that we hash a string of $r$ characters into $m$ slots by treating it as a radix-128 number and then using the division method. We can easily represent the number $m$ as a 32-bit computer word, but the string of $r$ characters, treated as a radix-128 number, takes many words. How can we apply the division method to compute the hash value of the character string without using more than a constant number of words of storage outside the string itself? 1 2 3 sum = 0 for i = 1 to r sum = ( sum * 128 + s [ i ]) % m Use sum as the key.","title":"11.3-2"},{"location":"Chap11/11.3/#113-3","text":"Consider a version of the division method in which $h(k) = k \\mod m$, where $m = 2^p - 1$ and $k$ is a character string interpreted in radix $2^p$. Show that if we can derive string $x$ from string $y$ by permuting its characters, then $x$ and $y$ hash to the same value. Give an example of an application in which this property would be undesirable in a hash function. First, we observe that we can generate any permutation by a sequence of interchanges of pairs of characters. One can prove this property formally, but informally, consider that both heapsort and quicksort work by interchanging pairs of elements and that they have to be able to produce any permutation of their input array. Thus, it suffices to show that if string $x$ can be derived from string $y$ by interchanging a single pair of characters, then $x$ and $y$ hash to the same value. Let us denote the $i$th character in $x$ by $x_i$, and similarly for $y$. The interpretation of $x$ in radix $2^p$ is $\\sum_{i = 0}^{n - 1} x_i 2^{ip}$, and so $h(x) = (\\sum_{i = 0}^{n - 1} x_i 2^{ip}) \\mod (2^p - 1)$. Similarly, $h(y) = (\\sum_{i = 0}^{n - 1} y_i 2^{ip}) \\mod (2^p - 1)$. Suppose that $x$ and $y$ are identical strings of $n$ characters except that the characters in positions $a$ and $b$ are interchanged: $x_a = y_b$ and $y_a = x_b$. Without loss of generality, let $a > b$. We have $$h(x) - h(y) = \\Big(\\sum_{i = 0}^{n - 1} x_i 2^{ip}\\Big) \\mod (2^p - 1) - \\Big(\\sum_{i = 0}^{n - 1} y_i 2^{ip}\\Big) \\mod (2^p - 1).$$ Since $0 \\le h(x)$, $h(y) < 2^p - 1$, we have that $-(2^p - 1) < h(x) - h(y) < 2^p - 1$. If we show that $(h(x) - h(y)) \\mod (2^p - 1) = 0$, then $h(x) = h(y)$. Since the sums in the hash functions are the same except for indices $a$ and $b$, we have $$ \\begin{aligned} (h(x) - h(y)) \\mod (2^p - 1) & = ((x_a 2^{ap} + x_b 2^{bp}) - (y_a 2^{ap} + y_b 2^{bp})) \\mod (2^p - 1) \\\\ & = ((x_a 2^{ap} + x_b 2^{bp}) - (x_b 2^{ap} + x_a 2^{bp})) \\mod (2^p - 1) \\\\ & = ((x_a - x_b)2^{ap} - (x_a - x_b) 2^{bp}) \\mod (2^p - 1) \\\\ & = ((x_a - x_b)(2^{ap} - 2^{bp})) \\mod (2^p - 1) \\\\ & = ((x_a - x_b)2^{bp}(2^{(a - b)p} - 1)) \\mod (2^p - 1). \\end{aligned} $$ By equation $\\text{(A.5)}$, $$\\sum_{i = 0}^{a - b - 1} 2^{pi} = \\frac{2^{(a - b)p} - 1}{2^p - 1},$$ and multiplying both sides by $s^p - 1$, we get $2^{(a - b)p} - 1 = \\big(\\sum_{i = 0}^{a - b - 1} 2^{pi}\\big)(2^p - 1)$. Thus, $$ \\begin{aligned} (h(x) - h(y))\\mod(2^p - 1) & = \\Bigg((x_a - x_b)2^{bp}\\Bigg(\\sum_{i = 0}^{a - b - 1} 2^{pi}\\Bigg)(2^p - 1)\\Bigg) \\mod (2^p - 1) \\\\ & = 0, \\end{aligned} $$ since one of the factors is $2^p - 1$. We have shown that $(h(x) - h(y)) \\mod (2^p - 1) = 0$, and so $h(x) = h(y)$.","title":"11.3-3"},{"location":"Chap11/11.3/#113-4","text":"Consider a hash table of size $m = 1000$ and a corresponding hash function $h(k) = \\lfloor m (kA \\mod 1) \\rfloor$ for $A = (\\sqrt 5 - 1) / 2$. Compute the locations to which the keys $61$, $62$, $63$, $64$, and $65$ are mapped. $h(61) = \\lfloor 1000(61 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 700$. $h(62) = \\lfloor 1000(62 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 318$. $h(63) = \\lfloor 1000(63 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 936$. $h(64) = \\lfloor 1000(64 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 554$. $h(65) = \\lfloor 1000(65 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 172$.","title":"11.3-4"},{"location":"Chap11/11.3/#113-5-star","text":"Define a family $\\mathcal H$ of hash functions from a finite set $U$ to a finite set $B$ to be $\\epsilon$-universal if for all pairs of distinct elements $k$ and $l$ in $U$, $$\\Pr\\{h(k) = h(l)\\} \\le \\epsilon,$$ where the probability is over the choice of the hash function $h$ drawn at random from the family $\\mathcal H$. Show that an $\\epsilon$-universal family of hash functions must have $$\\epsilon \\ge \\frac{1}{|B|} - \\frac{1}{|U|}.$$ Let $b = |B|$ and $u = |U|$. We start by showing that the total number of collisions is minimized by a hash function that maps $u / b$ elements of $U$ to each of the $b$ values in $B$. For a given hash function, let $u_j$ be the number of elements that map to $j \\in B$. We have $u = \\sum_{j \\in B} u_j$. We also have that the number of collisions for a given value of $j \\in B$ is $\\binom{u_j}{2} = u_j(u_j - 1) / 2$. Lemma The total number of collisions is minimized when $u_j = u / b$ for each $j \\in B$. Proof If $u_j \\le u / b$, let us call $j$ underloaded , and if $u_j \\ge u / b$, let us call $j$ overloaded . Consider an unbalanced situation in which $u_j \\ne u / b$ for at least one value $j \\in B$. We can think of converting a balanced situation in which all $u_j$ equal $u / b$ into the unbalanced situation by repeatedly moving an element that maps to an underloaded value to map instead to an overloaded value. (If you think of the values of $B$ as representing buckets, we are repeatedly moving elements from buckets containing at most $u / b$ elements to buckets containing at least $u / b$ elements.) We now show that each such move increases the number of collisions, so that all the moves together must increase the number of collisions. Suppose that we move an element from an underloaded value $j$ to an overloaded value $k$, and we leave all other elements alone. Because $j$ is underloaded and $k$ is overloaded, $u_j \\le u / b\\le u_k$. Considering just the collisions for values $j$ and $k$, we have $u_j(u_j - 1) / 2 + u_k(u_k - 1) / 2$ collisions before the move and $(u_j - 1)(u_j - 2) / 2 + (u_k + 1)u_k / 2$ collisions afterward. We wish to show that $$u_j(u_j - 1) / 2 + u_k(u_k - 1) / 2 < (u_j - 1)(u_j - 2) / 2 + (u_k + 1)u_k / 2.$$ We have the following sequence of equivalent inequalities: $$ \\begin{aligned} u_j & < u_k + 1 \\\\ 2u_j & < 2u_k + 2 \\\\ -u_k & < u_k - 2u_j + 2 \\\\ u_j^2 - u_j + u_k^2 - u_k & < u_j^2 - 3u_j + 2 + u_k^2 + u_k \\\\ u_j(u_j - 1) + u_k(u_k - 1) & < (u_j - 1)(u_j - 2) + (u_k + 1)u_k \\\\ u_j(u_j - 1) / 2 + u_k(u_k - 1) / 2 & < (u_j - 1)(u_j - 2) / 2 + (u_k + 1)u_k / 2. \\end{aligned} $$ Thus, each move increases the number of collisions. We conclude that the number of collisions is minimized when $u_j = u / b$ for each $j \\in B$. By the above lemma, for any hash function, the total number of collisions must be at least $b(u / b)(u / b - 1) / 2$. The number of pairs of distinct elements is $\\binom{u}{2} = u(u - 1) / 2$. Thus, the number of collisions per pair of distinct elements must be at least $$ \\begin{aligned} \\frac{b(u / b)(u / b - 1) / 2}{u(u - 1) / 2} & = \\frac{u / b - 1}{u - 1} \\\\ & > \\frac{u / b - 1}{u} \\\\ & = \\frac{1}{b} - \\frac{1}{u}. \\end{aligned} $$ Thus, the bound on the probability of a collision for any pair of distinct elements can be no less than $1 / b - 1 / u = 1 / |B| - 1 / |U|$.","title":"11.3-5 $\\star$"},{"location":"Chap11/11.3/#113-6-star","text":"Let $U$ be the set of $n$-tuples of values drawn from $\\mathbb Z_p$, and let $B = \\mathbb Z_p$, where $p$ is prime. Define the hash function $h_b: U \\rightarrow B$ for $b \\in \\mathbb Z_p$ on an input $n$-tuple $\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle$ from $U$ as $$h_b(\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle) = \\Bigg(\\sum_{j = 0}^{n - 1} a_jb^j \\Bigg) \\mod p,$$ and let $\\mathcal{H} = \\{h_b : b \\in \\mathbb Z_p\\}$. Argue that $\\mathcal H$ is $((n - 1) / p)$-universal according to the definition of $\\epsilon$-universal in Exercise 11.3-5. ($\\textit{Hint:}$ See Exercise 31.4-4.) Fix $b \\in \\mathbb Z_p$. By exercise 31.4-4, $h_b(x)$ collides with $h_b(y)$ for at most $n - 1$ other $y \\in U$. Since there are a total of $p$ possible values that $h_b$ takes on, the probability that $h_b(x) = h_b(y)$ is bounded from above by $\\frac{n - 1}{p}$, since this holds for any value of $b$, $\\mathcal H$ is $((n - 1 ) /p)$-universal.","title":"11.3-6 $\\star$"},{"location":"Chap11/11.4/","text":"11.4-1 Consider inserting the keys $10, 22, 31, 4, 15, 28, 17, 88, 59$ into a hash table of length $m = 11$ using open addressing with the auxiliary hash function $h'(k) = k$. Illustrate the result of inserting these keys using linear probing, using quadratic probing with $c_1 = 1$ and $c_2 = 3$, and using double hashing with $h_1(k) = k$ and $h_2(k) = 1 + (k \\mod (m - 1))$. We use $T_t$ to represent each time stamp $t$ starting with $i = 0$, and if encountering a collision, then we iterate $i$ from $i = 1$ to $i = m - 1 = 10$ until there is no collision. Linear probing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & 88 & 88 \\\\ 2 \\mod 11 & & & & & & & & & \\\\ 3 \\mod 11 & & & & & & & & & \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 8 \\mod 11 & & & & & & & & & 59 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Quadradic probing , it will look identical until there is a collision on inserting the fifth element: $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i + 3i^2) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & 88 & 88 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & & & & & \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & & 59 \\\\ 8 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Note that there is no way to insert the element $59$ now, because the offsets coming from $c_1 = 1$ and $c_2 = 3$ can only be even, and an odd offset would be required to insert $59$ because $59 \\mod 11 = 4$ and all the empty positions are at odd indices. Double hashing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i(1 + k \\mod 10)) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & & 59 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & 88 & 88 \\\\ 8 \\mod 11 & & & & & & & & & \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ 11.4-2 Write pseudocode for $\\text{HASH-DELETE}$ as outlined in the text, and modify $\\text{HASH-INSERT}$ to handle the special value $\\text{DELETED}$. 1 2 3 4 5 6 7 8 9 10 HASH - DELETE ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == k T [ j ] = DELETE return j else i = i + 1 until T [ j ] == NIL or i == m error \"element not exist\" By implementing $\\text{HASH-DELETE}$ in this way, the $\\text{HASH-INSERT}$ need to be modified to treat $\\text{NIL}$ slots as empty ones. 1 2 3 4 5 6 7 8 9 10 HASH - INSERT ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == NIL or T [ j ] == DELETE T [ j ] = k return j else i = i + 1 until i == m error \"hash table overflow\" 11.4-3 Consider an open-address hash table with uniform hashing. Give upper bounds on the expected number of probes in an unsuccessful search and on the expected number of probes in a successful search when the load factor is $3 / 4$ and when it is $7 / 8$. $\\alpha = 3 / 4$, unsuccessful: $\\frac{1}{1 - \\frac{3}{4}} = 4$ probes, successful: $\\frac{1}{\\frac{3}{4}} \\ln\\frac{1}{1-\\frac{3}{4}} \\approx 1.848$ probes. $\\alpha = 7 / 8$, unsuccessful: $\\frac{1}{1 - \\frac{7}{8}} = 8$ probes, successful: $\\frac{1}{\\frac{7}{8}} \\ln\\frac{1}{1 - \\frac{7}{8}} \\approx 2.377$ probes. 11.4-4 $\\star$ Suppose that we use double hashing to resolve collisions\u2014that is, we use the hash function $h(k, i) = (h_1(k) + ih_2(k)) \\mod m$. Show that if $m$ and $h_2(k)$ have greatest common divisor $d \\ge 1$ for some key $k$, then an unsuccessful search for key $k$ examines $(1/d)$th of the hash table before returning to slot $h_1(k)$. Thus, when $d = 1$, so that $m$ and $h_2(k)$ are relatively prime, the search may examine the entire hash table. ($\\textit{Hint:}$ See Chapter 31.) Suppose $d = \\gcd(m, h_2(k))$, the $\\text{LCM}$ $l = m \\cdot h_2(k) / d$. Since $d | h_2(k)$, then $m \\cdot h_2(k) / d \\mod m = 0 \\cdot (h_2(k) / d \\mod m) = 0$, therefore $(l + ih_2(k)) \\mod m = ih_2(k) \\mod m$, which means $ih_2(k) \\mod m$ has a period of $m / d$. 11.4-5 $\\star$ Consider an open-address hash table with a load factor $\\alpha$. Find the nonzero value $\\alpha$ for which the expected number of probes in an unsuccessful search equals twice the expected number of probes in a successful search. Use the upper bounds given by Theorems 11.6 and 11.8 for these expected numbers of probes. $$ \\begin{aligned} \\frac{1}{1 - \\alpha} & = 2 \\cdot \\frac{1}{\\alpha} \\ln\\frac{1}{1 - \\alpha} \\\\ \\alpha & = 0.71533. \\end{aligned} $$","title":"11.4 Open addressing"},{"location":"Chap11/11.4/#114-1","text":"Consider inserting the keys $10, 22, 31, 4, 15, 28, 17, 88, 59$ into a hash table of length $m = 11$ using open addressing with the auxiliary hash function $h'(k) = k$. Illustrate the result of inserting these keys using linear probing, using quadratic probing with $c_1 = 1$ and $c_2 = 3$, and using double hashing with $h_1(k) = k$ and $h_2(k) = 1 + (k \\mod (m - 1))$. We use $T_t$ to represent each time stamp $t$ starting with $i = 0$, and if encountering a collision, then we iterate $i$ from $i = 1$ to $i = m - 1 = 10$ until there is no collision. Linear probing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & 88 & 88 \\\\ 2 \\mod 11 & & & & & & & & & \\\\ 3 \\mod 11 & & & & & & & & & \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 8 \\mod 11 & & & & & & & & & 59 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Quadradic probing , it will look identical until there is a collision on inserting the fifth element: $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i + 3i^2) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & 88 & 88 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & & & & & \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & & 59 \\\\ 8 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Note that there is no way to insert the element $59$ now, because the offsets coming from $c_1 = 1$ and $c_2 = 3$ can only be even, and an odd offset would be required to insert $59$ because $59 \\mod 11 = 4$ and all the empty positions are at odd indices. Double hashing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i(1 + k \\mod 10)) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & & 59 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & 88 & 88 \\\\ 8 \\mod 11 & & & & & & & & & \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$","title":"11.4-1"},{"location":"Chap11/11.4/#114-2","text":"Write pseudocode for $\\text{HASH-DELETE}$ as outlined in the text, and modify $\\text{HASH-INSERT}$ to handle the special value $\\text{DELETED}$. 1 2 3 4 5 6 7 8 9 10 HASH - DELETE ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == k T [ j ] = DELETE return j else i = i + 1 until T [ j ] == NIL or i == m error \"element not exist\" By implementing $\\text{HASH-DELETE}$ in this way, the $\\text{HASH-INSERT}$ need to be modified to treat $\\text{NIL}$ slots as empty ones. 1 2 3 4 5 6 7 8 9 10 HASH - INSERT ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == NIL or T [ j ] == DELETE T [ j ] = k return j else i = i + 1 until i == m error \"hash table overflow\"","title":"11.4-2"},{"location":"Chap11/11.4/#114-3","text":"Consider an open-address hash table with uniform hashing. Give upper bounds on the expected number of probes in an unsuccessful search and on the expected number of probes in a successful search when the load factor is $3 / 4$ and when it is $7 / 8$. $\\alpha = 3 / 4$, unsuccessful: $\\frac{1}{1 - \\frac{3}{4}} = 4$ probes, successful: $\\frac{1}{\\frac{3}{4}} \\ln\\frac{1}{1-\\frac{3}{4}} \\approx 1.848$ probes. $\\alpha = 7 / 8$, unsuccessful: $\\frac{1}{1 - \\frac{7}{8}} = 8$ probes, successful: $\\frac{1}{\\frac{7}{8}} \\ln\\frac{1}{1 - \\frac{7}{8}} \\approx 2.377$ probes.","title":"11.4-3"},{"location":"Chap11/11.4/#114-4-star","text":"Suppose that we use double hashing to resolve collisions\u2014that is, we use the hash function $h(k, i) = (h_1(k) + ih_2(k)) \\mod m$. Show that if $m$ and $h_2(k)$ have greatest common divisor $d \\ge 1$ for some key $k$, then an unsuccessful search for key $k$ examines $(1/d)$th of the hash table before returning to slot $h_1(k)$. Thus, when $d = 1$, so that $m$ and $h_2(k)$ are relatively prime, the search may examine the entire hash table. ($\\textit{Hint:}$ See Chapter 31.) Suppose $d = \\gcd(m, h_2(k))$, the $\\text{LCM}$ $l = m \\cdot h_2(k) / d$. Since $d | h_2(k)$, then $m \\cdot h_2(k) / d \\mod m = 0 \\cdot (h_2(k) / d \\mod m) = 0$, therefore $(l + ih_2(k)) \\mod m = ih_2(k) \\mod m$, which means $ih_2(k) \\mod m$ has a period of $m / d$.","title":"11.4-4 $\\star$"},{"location":"Chap11/11.4/#114-5-star","text":"Consider an open-address hash table with a load factor $\\alpha$. Find the nonzero value $\\alpha$ for which the expected number of probes in an unsuccessful search equals twice the expected number of probes in a successful search. Use the upper bounds given by Theorems 11.6 and 11.8 for these expected numbers of probes. $$ \\begin{aligned} \\frac{1}{1 - \\alpha} & = 2 \\cdot \\frac{1}{\\alpha} \\ln\\frac{1}{1 - \\alpha} \\\\ \\alpha & = 0.71533. \\end{aligned} $$","title":"11.4-5 $\\star$"},{"location":"Chap11/11.5/","text":"11.5-1 $\\star$ Suppose that we insert $n$ keys into a hash table of size $m$ using open addressing and uniform hashing. Let $p(n, m)$ be the probability that no collisions occur. Show that $p(n, m) \\le e^{-n(n - 1) / 2m}$. ($\\textit{Hint:}$ See equation $\\text{(3.12)}$.) Argue that when $n$ exceeds $\\sqrt m$, the probability of avoiding collisions goes rapidly to zero. $$ \\begin{aligned} p(n, m) & = \\frac{m}{m} \\cdot \\frac{m - 1}{m} \\cdots \\frac{m - n + 1}{m} \\\\ & = \\frac{m \\cdot (m - 1) \\cdots (m - n + 1)}{m^n}. \\end{aligned} $$ $$ \\begin{aligned} (m - i) \\cdot (m - n + i) & = (m - \\frac{n}{2} + \\frac{n}{2} - i) \\cdot (m - \\frac{n}{2} - \\frac{n}{2} + i) \\\\ & = (m - \\frac{n}{2})^2 - (i - \\frac{n}{2})^2 \\\\ & \\le (m - \\frac{n}{2})^2. \\end{aligned} $$ $$ \\begin{aligned} p(n, m) & \\le \\frac{m \\cdot (m - \\frac{n}{2})^{n - 1}}{m^n} \\\\ & = (1 - \\frac{n}{2m}) ^ {n - 1}. \\end{aligned} $$ Based on equation $\\text{(3.12)}$, $e^x \\ge 1 + x$, $$ \\begin{aligned} p(n, m) & \\le (e^{-n / 2m})^{n - 1} \\\\ & = e^{-n(n - 1) / 2m}. \\end{aligned} $$","title":"11.5 Perfect hashing"},{"location":"Chap11/11.5/#115-1-star","text":"Suppose that we insert $n$ keys into a hash table of size $m$ using open addressing and uniform hashing. Let $p(n, m)$ be the probability that no collisions occur. Show that $p(n, m) \\le e^{-n(n - 1) / 2m}$. ($\\textit{Hint:}$ See equation $\\text{(3.12)}$.) Argue that when $n$ exceeds $\\sqrt m$, the probability of avoiding collisions goes rapidly to zero. $$ \\begin{aligned} p(n, m) & = \\frac{m}{m} \\cdot \\frac{m - 1}{m} \\cdots \\frac{m - n + 1}{m} \\\\ & = \\frac{m \\cdot (m - 1) \\cdots (m - n + 1)}{m^n}. \\end{aligned} $$ $$ \\begin{aligned} (m - i) \\cdot (m - n + i) & = (m - \\frac{n}{2} + \\frac{n}{2} - i) \\cdot (m - \\frac{n}{2} - \\frac{n}{2} + i) \\\\ & = (m - \\frac{n}{2})^2 - (i - \\frac{n}{2})^2 \\\\ & \\le (m - \\frac{n}{2})^2. \\end{aligned} $$ $$ \\begin{aligned} p(n, m) & \\le \\frac{m \\cdot (m - \\frac{n}{2})^{n - 1}}{m^n} \\\\ & = (1 - \\frac{n}{2m}) ^ {n - 1}. \\end{aligned} $$ Based on equation $\\text{(3.12)}$, $e^x \\ge 1 + x$, $$ \\begin{aligned} p(n, m) & \\le (e^{-n / 2m})^{n - 1} \\\\ & = e^{-n(n - 1) / 2m}. \\end{aligned} $$","title":"11.5-1 $\\star$"},{"location":"Chap11/Problems/11-1/","text":"Suppose that we use an open-addressed hash table of size $m$ to store $n \\le m / 2$ items. a. Assuming uniform hashing, show that for $i = 1, 2, \\ldots, n$, the probability is at most $2^{-k}$ that the $i$th insertion requires strictly more than $k$ probes. b. Show that for $i = 1, 2, \\ldots, n$, the probability is $O(1 / n^2)$ that the $i$th insertion requires more than $2\\lg n$ probes. Let the random variable $X_i$ denote the number of probes required by the $i$th insertion. You have shown in part (b) that $\\Pr\\{X_i > 2\\lg n\\} = O(1 / n^2)$. Let the random variable $X = \\max_{1 \\le i \\le n} X_i$ denote the maximum number of probes required by any of the $n$ insertions. c. Show that $\\Pr\\{X > 2\\lg n\\} = O(1 / n)$. d. Show that the expected length $\\text E[X]$ of the longest probe sequence is $O(\\lg n)$. a. Since we assume uniform hashing, we can use the same observation as is used in Corollary 11.7: that inserting a key entails an unsuccessful search followed by placing the key into the first empty slot found. As in the proof of Theorem 11.6, if we let $X$ be the random variable denoting the number of probes in an unsuccessful search, then $\\Pr\\{X \\ge i\\} \\le \\alpha^{i - 1}$. Since $n \\le m / 2$, we have $\\alpha \\le 1 / 2$. Letting $i = k + 1$, we have $\\Pr\\{X > k\\} = \\Pr\\{X \\ge k + 1\\} \\le (1 / 2)^{(k + 1) - 1} = 2^{-k}$. b. Substituting $k = 2\\lg n$ into the statement of part (a) yields that the probability that the $i$th insertion requires more than $k = 2\\lg n$ probes is at most $2^{-2\\lg n} = (2^{\\lg n})^{-2} = n^{-2} = 1 / n^2$. We must deal with the possibility that $2\\lg n$ is not an integer, however. Then the event that the $i$th insertion requires more than $2\\lg n$ probes is the same as the event that the $i$th insertion requires more than $\\lfloor 2\\lg n \\rfloor$ probes. Since $\\lfloor 2\\lg n \\rfloor > 2\\lg n - 1$, we have that the probability of this event is at most $2^{-\\lfloor 2\\lg n \\rfloor} < 2^{-(2\\lg n - 1)} = 2 / n^2 = O(1 / n^2)$. c. Let the event $A$ be $X > 2\\lg n$, and for $i = 1, 2, \\ldots, n$, let the event $A_i$ be $X_i > 2\\lg n$. In part (b), we showed that $\\Pr\\{A_i\\} = O(1 / n^2)$ for $i = 1, 2, \\ldots, n$. From how we defined these events, $A = A_1 \\cup A_2 \\cup \\cdots \\cup A_n$. Using Boole's inequality, $\\text{(C.19)}$, we have $$ \\begin{aligned} \\Pr\\{A\\} & \\le \\Pr\\{A_1\\} + \\Pr\\{A_1\\} + \\cdots + \\Pr\\{A_n\\} \\\\ & \\le n \\cdot O(1 / n^2) \\\\ & = O(1 / n). \\end{aligned} $$ d. We use the definition of expectation and break the sum into two parts: $$ \\begin{aligned} \\text E[X] & = \\sum_{k = 1}^n k \\cdot \\Pr\\{X = k\\} \\\\ & = \\sum_{k = 1}^{\\lceil 2\\lg n \\rceil} k \\cdot \\Pr\\{X = k\\} + \\sum_{\\lceil 2\\lg n \\rceil + 1}^n k \\cdot \\Pr\\{X = k\\} \\\\ & \\le \\sum_{k = 1}^{\\lceil 2\\lg n \\rceil} \\lceil 2\\lg n \\rceil \\cdot \\Pr\\{X = k\\} + \\sum_{\\lceil 2\\lg n \\rceil + 1}^n n \\cdot \\Pr\\{X = k\\} \\\\ & = \\lceil 2\\lg n \\rceil \\sum_{k = 1}^{\\lceil 2\\lg n \\rceil} \\Pr\\{X = k\\} + n \\sum_{\\lceil 2\\lg n \\rceil + 1}^n \\Pr\\{X = k\\}. \\end{aligned} $$ Since $X$ takes on exactly one value, we have that $\\sum_{k = 1}^{\\lceil 2\\lg n \\rceil} \\Pr\\{X = k\\} = \\Pr\\{X \\le \\lceil 2\\lg n \\rceil\\} \\le 1$ and $\\sum_{k = \\lceil 2\\lg n \\rceil + 1}^n \\Pr\\{X = k\\} \\le \\Pr\\{X > 2\\lg n\\} = O(1 / n)$, by part (c). Therefore, $$ \\begin{aligned} \\text E[X] & \\le \\lceil 2\\lg n \\rceil \\cdot 1 + n \\cdot O(1 / n) \\\\ & = \\lceil 2\\lg n \\rceil + O(1) \\\\ & = O(\\lg n). \\end{aligned} $$","title":"11-1 Longest-probe bound for hashing"},{"location":"Chap11/Problems/11-2/","text":"Suppose that we have a hash table with $n$ slots, with collisions resolved by chaining, and suppose that $n$ keys are inserted into the table. Each key is equally likely to be hashed to each slot. Let $M$ be the maximum number of keys in any slot after all the keys have been inserted. Your mission is to prove an $O(\\lg n / \\lg\\lg n)$ upper bound on $\\text E[M]$, the expected value of $M$. a. Argue that the probability $Q_k$ that exactly $k$ keys hash to a particular slot is given by $$Q_k = \\bigg(\\frac{1}{n} \\bigg)^k \\bigg(1 - \\frac{1}{n} \\bigg)^{n - k} \\binom{n}{k}.$$ b. Let $P_k$ be the probability that $M = k$, that is, the probability that the slot containing the most keys contains $k$ keys. Show that $P_k \\le n Q_k$. c. Use Stirling's approximation, equation $\\text{(3.18)}$, to show that $Q_k < e^k / k^k$. d. Show that there exists a constant $c > 1$ such that $Q_{k_0} < 1 / n^3$ for $k_0 = c\\lg n / \\lg\\lg n$. Conclude that $P_k < 1 / n^2$ for $k \\ge k_0 = c\\lg n / \\lg\\lg n$. e. Argue that $$\\text E[M] \\le \\Pr\\bigg\\{M > \\frac{c\\lg n}{\\lg\\lg n}\\bigg\\} \\cdot n + \\Pr\\bigg\\{M \\le \\frac{c\\lg n}{\\lg\\lg n}\\bigg\\} \\cdot \\frac{c\\lg n}{\\lg\\lg n}.$$ Conclude that $\\text E[M] = O(\\lg n / \\lg\\lg n)$. a. A particular key is hashed to a particular slot with probability $1 / n$. Suppose we select a specific set of $k$ keys. The probability that these $k$ keys are inserted into the slot in question and that all other keys are inserted elsewhere is $$\\Big(\\frac{1}{n}\\Big)^k \\Big(1 - \\frac{1}{n}\\Big)^{n - k}.$$ Since there are $\\binom{n}{k}$ ways to choose our $k$ keys, we get $$Q_k = \\Big(\\frac{1}{n}\\Big)^k \\Big(1 - \\frac{1}{n}\\Big)^{n - k} \\binom{n}{k}.$$ b. For $i = 1, 2, \\ldots, n$, let $X_i$ be a random variable denoting the number of keys that hash to slot $i$, and let $A_i$ be the event that $X_i = k$, i.e., that exactly $k$ keys hash to slot $i$. From part (a), we have $\\Pr\\{A\\} = Q_k$. Then, $$ \\begin{aligned} P_k & = \\Pr\\{M = k\\} \\\\ & = \\Pr\\Big\\{\\Big(\\max_{1 \\le i \\le n} X_i\\Big) = k\\Big\\} \\\\ & = \\Pr\\{\\text{there exists $i$ such that $X_i = k$ and that $X_i\\le k$ for $i = 1, 2, \\ldots, n$}\\} \\\\ & \\le \\Pr\\{\\text{there exists $i$ such that $X_i = k$}\\} \\\\ & = \\Pr\\{A_1 \\cup A_2 \\cup \\cdots \\cup A_n\\} \\\\ & \\le \\Pr\\{A_1\\} + \\Pr\\{A_2\\} + \\cdots + \\Pr\\{A_n\\} \\qquad \\text{(by inequality (C.19))} \\\\ & = nQ_k. \\end{aligned} $$ c. We start by showing two facts. First, $1 - 1 / n < 1$, which implies $(1 - 1 / n)^{n - k} < 1$. Second, $n! / (n - k)! = n \\cdot (n - 1) \\cdot (n - 2) \\cdots (n - k + 1) < n^k$. Using these facts, along with the simplification $k! > (k / e)^k$ of equation $\\text{(3.18)}$, we have $$ \\begin{aligned} Q_k & = \\Big(\\frac{1}{n}\\Big)^k \\Big(1 - \\frac{1}{n}\\Big)^{n - k} \\frac{n!}{k!(n - k)!} \\\\ & < \\frac{n!}{n^k k! (n - k)!} & ((1 - 1 / n)^{n - k} < 1) \\\\ & < \\frac{1}{k!} & (n! / (n - k)! < n^k) \\\\ & < \\frac{e^k}{k^k}. & (k! > (k / e)^k) \\end{aligned} $$ d. Notice that when $n = 2$, $\\lg\\lg n = 0$, so to be precise, we need to assume that $n \\ge 3$. In part (c), we showed that $Q_k < e^k / k^k$ for any $k$; in particular, this inequality holds for $k_0$. Thus, it suffices to show that $e^{k_0} / k_0^{k_0} < 1 / n^3$ or, equivalently, that $n^3 < k_0^{k_0} / e^{k_0}$. Taking logarithms of both sides gives an equivalent condition: $$ \\begin{aligned} 3\\lg n & < k_0(\\lg k_0 - \\lg e) \\\\ & = \\frac{c\\lg n}{\\lg\\lg n}(\\lg c + \\lg\\lg n - \\lg\\lg\\lg n - \\lg e). \\end{aligned} $$ Dividing both sides by $\\lg n$ gives the condition $$ \\begin{aligned} 3 & < \\frac{c}{\\lg\\lg n} (\\lg c + \\lg\\lg n - \\lg\\lg\\lg n - \\lg e) \\\\ & = c \\Big(1 + \\frac{\\lg c - \\lg e}{\\lg\\lg n} - \\frac{\\lg\\lg\\lg n}{\\lg\\lg n}\\Big). \\end{aligned} $$ Let $x$ be the last expression in parentheses: $$x = \\Big(1 + \\frac{\\lg c - \\lg e}{\\lg\\lg n} - \\frac{\\lg\\lg\\lg n}{\\lg\\lg n}\\Big).$$ We need to show that there exists a constant $c > 1$ such that $3 < cx$. Noting that $\\lim_{n \\to \\infty} x = 1$, we see that there exists $n_0$ such that $x \\ge 1 / 2$ for all $n \\ge n_0$. Thus, any constant $c > 6$ works for $n \\ge n_0$. We handle smaller values of $n$\u2014in particular, $3 \\le n < n_0$\u2014as follows. Since $n$ is constrained to be an integer, there are a finite number of n in the range $3 \\le n < n_0$. We can evaluate the expression $x$ for each such value of $n$ and determine a value of $c$ for which $3 < cx$ for all values of $n$. The final value of $c$ that we use is the larger of $6$, which works for all $n \\ge n_0$, and $\\max_{3 \\le n \\le n_0}\\{c: 3 < cx\\}$, i.e., the largest value of $c$ that we chose for the range $3 \\le n < n_0$. Thus, we have shown that $Q_{k_0} < 1 / n^3$, as desired. To see that $P_k < 1 / n^2$ for $k \\ge k_0$, we observe that by part (b), $P_k \\le nQ_k$ for all $k$. Choosing $k = k_0$ gives $P_{k_0} \\le nQ_{k_0} < n \\cdot (1 / n^3) = 1 / n^2$. For $k > k_0$, we will show that we can pick the constant $c$ such that $Q_k < 1 / n^3$ for all $k \\ge k_0$, and thus conclude that $P_k < 1 / n^2$ for all $k \\ge k_0$. To pick $c$ as required, we let $c$ be large enough that $k_0 > 3 > e$. Then $e / k < 1$ for all $k \\ge k_0$, and so $e^k / k^k$ decreases as $k$ increases. Thus, $$ \\begin{aligned} Q_k & < e^k / k^k \\\\ & \\le e^{k_0} / k^{k_0} \\\\ & < 1 / n^3 \\end{aligned} $$ for $k \\ge k_0$. e. The expectation of $M$ is $$ \\begin{aligned} \\text E[M] & = \\sum_{k = 0}^n k \\cdot \\Pr\\{M = k\\} \\\\ & = \\sum_{k = 0}^{k_0} k \\cdot \\Pr\\{M = k\\} + \\sum_{k = k_0 + 1}^n k \\cdot \\Pr\\{M = k\\} \\\\ & \\le \\sum_{k = 0}^{k_0} k_0 \\cdot \\Pr\\{M = k\\} + \\sum_{k = k_0 + 1}^n n \\cdot \\Pr\\{M = k\\} \\\\ & \\le k_0 \\sum_{k = 0}^{k_0} \\Pr\\{M = k\\} + n \\sum_{k = k_0 + 1}^n \\Pr\\{M = k\\} \\\\ & = k_0 \\cdot \\Pr\\{M \\le k_0\\} + n \\cdot \\Pr\\{M > k_0\\}, \\end{aligned} $$ which is what we needed to show, since $k_0 = c \\lg n / \\lg\\lg n$. To show that $\\text E[M] = O(\\lg n / \\lg\\lg n)$, note that $\\Pr\\{M \\le k_0\\} \\le 1$ and $$ \\begin{aligned} \\Pr\\{M > k_0\\} & = \\sum_{k = k_0 + 1}^n \\Pr\\{M = k\\} \\\\ & = \\sum_{k = k_0 + 1}^n P_k \\\\ & < \\sum_{k = k_0 + 1}^n 1 / n^2 & \\text{(by part (d))} \\\\ & < n \\cdot (1 / n^2) \\\\ & = 1 / n. \\end{aligned} $$ We conclude that $$ \\begin{aligned} \\text E[M] & \\le k_0 \\cdot 1 + n \\cdot (1 / n) \\\\ & = k_0 + 1 \\\\ & = O(\\lg n / \\lg\\lg n). \\end{aligned} $$","title":"11-2 Slot-size bound for chaining"},{"location":"Chap11/Problems/11-3/","text":"Suppose that we are given a key $k$ to search for in a hash table with positions $0, 1, \\ldots, m - 1$, and suppose that we have a hash function $h$ mapping the key space into the set $\\{0, 1, \\ldots, m - 1\\}$. The search scheme is as follows: Compute the value $j = h(k)$, and set $i = 0$. Probe in position $j$ for the desired key $k$. If you find it, or if this position is empty, terminate the search. Set $i = i + 1$. If $i$ now equals $m$, the table is full, so terminate the search. Otherwise, set $j = (i + j) \\mod m$, and return to step 2. Assume that $m$ is a power of $2$. a. Show that this scheme is an instance of the general \"quadratic probing\" scheme by exhibiting the appropriate constants $c_1$ and $c_2$ for equation $\\text{(11.5)}$. b. Prove that this algorithm examines every table position in the worst case. a. From how the probe-sequence computation is specified, it is easy to see that the probe sequence is $$\\langle h(k), h(k) + 1, h(k) + 1 + 2, h(k) + 1 + 2 + 3, \\ldots, h(k) + 1 + 2 + 3 + \\cdots + i, \\ldots \\rangle,$$ where all arithmetic is modulo $m$. Starting the probe numbers from $0$, the $i$th probe is offset (modulo $m$) from $h(k)$ by $$\\sum_{j = 0}^i j = \\frac{i(i + 1)}{2} = \\frac{1}{2}i^2 + \\frac{1}{2}i.$$ Thus, we can write the probe sequence as $$h'(k, i) = \\Big(h(k) + \\frac{1}{2} i + \\frac{1}{2} i^2 \\Big) \\mod m,$$ which demonstrates that this scheme is a special case of quadratic probing. b. Let $h'(k, i)$ denote the ith probe of our scheme. We saw in part (a) that $h'(k, i) = (h(k) + i(i + 1) / 2) \\mod m$. To show that our algorithm examines every table position in the worst case, we show that for a given key, each of the first $m$ probes hashes to a distinct value. That is, for any key $k$ and for any probe numbers $i$ and $j$ such that $0 \\le i < j < m$, we have $h'(k, i) \\ne h'(k, j)$. We do so by showing that $h'(k, i) = h'(k, j)$ yields a contradiction. Let us assume that there exists a key $k$ and probe numbers $i$ and $j$ satsifying $0 \\le i < j < m$ for which $h'(k, i) = h'(k, j)$. Then $$h(k) + i(i + 1) / 2 = h(k) + j(j + 1) / 2 \\mod m,$$ which in turn implies that $$i(i + 1) / 2 = j(j + 1) / 2 \\mod m,$$ or $$j(j + 1) / 2 - i(i + 1) / 2 = 0 \\mod m.$$ Since $j(j + 1) / 2 - i(i + 1) / 2 = (j - i)(j + i + 1) / 2$, we have $$(j - i)(j + i + 1) / 2 = 0 \\mod m.$$ The factors $j - i$ and $j + i + 1$ must have different parities, i.e., $j - i$ is even if and only if $j + i + 1$ is odd. (Work out the various cases in which $i$ and $j$ are even and odd.) Since $(j - i)(j + i + 1) / 2 = 0 \\mod m$, we have $(j - i)(j + i + 1) / 2 = rm$ for some integer $r$ or, equivalently, $(j - i)(j + i + 1) = r \\cdot 2m$. Using the assumption that $m$ is a power of $2$, let $m = 2^p$ for some nonnegative integer $p$, so that now we have $(j - i)(j + i + 1) = r \\cdot 2^{p + 1}$. Because exactly one of the factors $j - i$ and $j + i + 1$ is even, $2^{p + 1}$ must divide one of the factors. It cannot be $j - i$, since $j - i < m < 2^{p + 1}$. But it also cannot be $j + i + 1$, since $j + i + 1 \\le (m - 1) + (m - 2) + 1 = 2m - 2 < 2^{p + 1}$. Thus we have derived the contradiction that $2^{p + 1}$ divides neither of the factors $j - i$ and $j + i + 1$. We conclude that $h'(k, i) \\ne h'(k, j)$.","title":"11-3 Quadratic probing"},{"location":"Chap11/Problems/11-4/","text":"Let $\\mathcal H$ be a class of hash functions in which each hash function $h \\in \\mathcal H$ maps the universe $U$ of keys to $\\{0, 1, \\ldots, m - 1 \\}$. We say that $\\mathcal H$ is k-universal if, for every fixed sequence of $k$ distinct keys $\\langle x^{(1)}, x^{(2)}, \\ldots, x^{(k)} \\rangle$ and for any $h$ chosen at random from $\\mathcal H$, the sequence $\\langle h(x^{(1)}), h(x^{(2)}), \\ldots, h(x^{(k)}) \\rangle$ is equally likely to be any of the $m^k$ sequences of length $k$ with elements drawn from $\\{0, 1, \\ldots, m - 1 \\}$. a. Show that if the family $\\mathcal H$ of hash functions is $2$-universal, then it is universal. b. Suppose that the universe $U$ is the set of $n$-tuples of values drawn from $\\mathbb Z_p = \\{0, 1, \\ldots, p - 1\\}$, where $p$ is prime. Consider an element $x = \\langle x_0, x_1, \\ldots, x_{n - 1} \\rangle \\in U$. For any $n$-tuple $a = \\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle \\in U$, define the hash function $h_a$ by $$h_a(x) = \\Bigg(\\sum_{j = 0}^{n - 1} a_j x_j \\Bigg) \\mod p.$$ Let $\\mathcal H = \\{h_a\\}$. Show that $\\mathcal H$ is universal, but not $2$-universal. ($\\textit{Hint:}$ Find a key for which all hash functions in $\\mathcal H$ produce the same value.) c. Suppose that we modify $\\mathcal H$ slightly from part (b): for any $a \\in U$ and for any $b \\in \\mathbb Z_p$, define $$h'_{ab}(x) = \\Bigg(\\sum_{j = 0}^{n - 1} a_j x_j + b \\Bigg) \\mod p$$ and $\\mathcal h' = \\{h'_{ab}\\}$. Argue that $\\mathcal H'$ is $2$-universal. ($\\textit{Hint:}$ Consider fixed $n$-tuples $x \\in U$ and $y \\in U$, with $x_i \\ne y_i$ for some $i$. What happens to $h'_{ab}(x)$ and $h'_{ab}(y)$ as $a_i$ and $b$ range over $\\mathbb Z_p$?) d. Suppose that Alice and Bob secretly agree on a hash function $h$ form $2$-universal family $\\mathcal H$ of hash functions. Each $h \\in \\mathcal H$ maps from a universe of keys $u$ to $\\mathbb Z_p$, where $p$ is aprime. Later, Alice sends a message $m$ to Bob over the Internet, where $m \\in U$. She authenticates this message to Bob by also sending an authentication tag $t = h(m)$, and Bob checks that the pair $(m, t)$ he receives indeed satisfies $t = h(m)$. Suppose that an adversary intercepts $(m, t)$ en route and tries to fool Bob by replacing the pair $(m, t)$ with a different pair $(m', t')$. Argue that the probability that the adversary succeeds in fooling Bob into accepting $(m', t')$ is at most $1 / p$, no matter how much computing power the adversary has, and even if the adversary knows the family $\\mathcal H$ of hash functions used. a. The number of hash functions for which $h(k) = h(l)$ is $\\frac{m}{m^2}|\\mathcal H| = \\frac{1}{m}|\\mathcal H|$, therefore the family is universal. b. For $x = \\langle 0, 0, \\ldots, 0 \\rangle$, $\\mathcal H$ could not be $2$-universal. c. Let $x, y \\in U$ be fixed, distinct $n$-tuples. As $a_i$ and $b$ range over $\\mathbb Z_p, h'_{ab}(x)$ is equally likely to achieve every value from $1$ to $p$ since for any sequence $a$, we can let $b$ vary from $1$ to $p - 1$. Thus, $\\langle h'_{ab}(x), h'_{ab}(y) \\rangle$ is equally likely to be any of the $p^2$ sequences, so $\\mathcal H$ is $2$-universal. d. Since $\\mathcal H$ is $2$-universal, every pair of $\\langle t, t' \\rangle$ is equally likely to appear, thus $t'$ could be any value from $\\mathbb Z_p$. Even the adversary knows $\\mathcal H$, since $\\mathcal H$ is $2$-universal, then $\\mathcal H$ is universal, the probability of choosing a hash function that $h(k) = h(l)$ is at most $1 / p$, therefore the probability is at most $1 / p$.","title":"11-4 Hashing and authentication"},{"location":"Chap12/12.1/","text":"12.1-1 For the set of $\\{ 1, 4, 5, 10, 16, 17, 21 \\}$ of keys, draw binary search trees of heights $2$, $3$, $4$, $5$, and $6$. $height = 2$: $height = 3$: $height = 4$: $height = 5$: $height = 6$: 12.1-2 What is the difference between the binary-search-tree property and the min-heap property (see page 153)? Can the min-heap property be used to print out the keys of an $n$-node tree in sorted order in $O(n)$ time? Show how, or explain why not. In a min-heap, a node's key is $\\le$ both of its children's keys. In a binary-search-tree, a node's key is $\\ge$ its left child's key, but $\\le$ its right child's key. The min-heap property, unlike the binary-searth-tree property, doesn't help print the nodes in sorted order because it doesn't tell which subtree of a node contains the element to print after that node. In a min-heap, the smallest element larger than the node could be in either subtree. Note that if the min-heap property could be used to print the keys in sorted order in $O(n)$ time, we would have an $O(n)$-time algorithm for sorting, because building the heap takes only $O(n)$ time. But we know (Chapter 8) that a comparison sort must take \u007f$\\Omega(n\\lg n)$ time. 12.1-3 Give a nonrecursive algorithm that performs an inorder tree walk. ($\\textit{Hint:}$ An easy solution uses a stack as an auxiliary data structure. A more complicated, but elegant, solution uses no stack but assumes that we can test two pointers for equality.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 INORDER - TREE - WALK ( T ) let S be an empty stack current = T . root done = 0 while ! done if current != NIL PUSH ( S , current ) current = current . left else if ! S . EMPTY () current = POP ( S ) print current current = current . right else done = 1 12.1-4 Give recursive algorithms that perform preorder and postorder tree walks in $\\Theta(n)$ time on a tree of $n$ nodes. 1 2 3 4 5 PREORDER - TREE - WALK ( x ) if x != NIL print x . key PREORDER - TREE - WALK ( x . left ) PREORDER - TREE - WALK ( x . right ) 1 2 3 4 5 POSTORDER - TREE - WALK ( x ) if x != NIL POSTORDER - TREE - WALK ( x . left ) POSTORDER - TREE - WALK ( x . right ) print x . key 12.1-5 Argue that since sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case in the comparison model, any comparison-based algorithm for constructing a binary search tree from an arbitrary list of $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case. Assume, for the sake of contradiction, that we can construct the binary search tree by comparison-based algorithm using less than $\\Omega(n\\lg n)$ time, since the inorder tree walk is $\\Theta(n)$, then we can get the sorted elements in less than $\\Omega(n\\lg n)$ time, which contradicts the fact that sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case.","title":"12.1 What is a binary search tree?"},{"location":"Chap12/12.1/#121-1","text":"For the set of $\\{ 1, 4, 5, 10, 16, 17, 21 \\}$ of keys, draw binary search trees of heights $2$, $3$, $4$, $5$, and $6$. $height = 2$: $height = 3$: $height = 4$: $height = 5$: $height = 6$:","title":"12.1-1"},{"location":"Chap12/12.1/#121-2","text":"What is the difference between the binary-search-tree property and the min-heap property (see page 153)? Can the min-heap property be used to print out the keys of an $n$-node tree in sorted order in $O(n)$ time? Show how, or explain why not. In a min-heap, a node's key is $\\le$ both of its children's keys. In a binary-search-tree, a node's key is $\\ge$ its left child's key, but $\\le$ its right child's key. The min-heap property, unlike the binary-searth-tree property, doesn't help print the nodes in sorted order because it doesn't tell which subtree of a node contains the element to print after that node. In a min-heap, the smallest element larger than the node could be in either subtree. Note that if the min-heap property could be used to print the keys in sorted order in $O(n)$ time, we would have an $O(n)$-time algorithm for sorting, because building the heap takes only $O(n)$ time. But we know (Chapter 8) that a comparison sort must take \u007f$\\Omega(n\\lg n)$ time.","title":"12.1-2"},{"location":"Chap12/12.1/#121-3","text":"Give a nonrecursive algorithm that performs an inorder tree walk. ($\\textit{Hint:}$ An easy solution uses a stack as an auxiliary data structure. A more complicated, but elegant, solution uses no stack but assumes that we can test two pointers for equality.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 INORDER - TREE - WALK ( T ) let S be an empty stack current = T . root done = 0 while ! done if current != NIL PUSH ( S , current ) current = current . left else if ! S . EMPTY () current = POP ( S ) print current current = current . right else done = 1","title":"12.1-3"},{"location":"Chap12/12.1/#121-4","text":"Give recursive algorithms that perform preorder and postorder tree walks in $\\Theta(n)$ time on a tree of $n$ nodes. 1 2 3 4 5 PREORDER - TREE - WALK ( x ) if x != NIL print x . key PREORDER - TREE - WALK ( x . left ) PREORDER - TREE - WALK ( x . right ) 1 2 3 4 5 POSTORDER - TREE - WALK ( x ) if x != NIL POSTORDER - TREE - WALK ( x . left ) POSTORDER - TREE - WALK ( x . right ) print x . key","title":"12.1-4"},{"location":"Chap12/12.1/#121-5","text":"Argue that since sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case in the comparison model, any comparison-based algorithm for constructing a binary search tree from an arbitrary list of $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case. Assume, for the sake of contradiction, that we can construct the binary search tree by comparison-based algorithm using less than $\\Omega(n\\lg n)$ time, since the inorder tree walk is $\\Theta(n)$, then we can get the sorted elements in less than $\\Omega(n\\lg n)$ time, which contradicts the fact that sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case.","title":"12.1-5"},{"location":"Chap12/12.2/","text":"12.2-1 Suppose that we have numbers between $1$ and $1000$ in a binary search tree, and we want to search for the number $363$. Which of the following sequences could not be the sequence of nodes examined? a. $2, 252, 401, 398, 330, 344, 397, 363$. b. $924, 220, 911, 244, 898, 258, 362, 363$. c. $925, 202, 911, 240, 912, 245, 363$. d. $2, 399, 387, 219, 266, 382, 381, 278, 363$. e. $935, 278, 347, 621, 299, 392, 358, 363$. c. could not be the sequence of nodes explored because we take the left child from the $911$ node, and yet somehow manage to get to the $912$ node which cannot belong the left subtree of $911$ because it is greater. e. is also impossible because we take the right subtree on the $347$ node and yet later come across the $299$ node. 12.2-2 Write recursive versions of $\\text{TREE-MINIMUM}$ and $\\text{TREE-MAXIMUM}$. 1 2 3 4 TREE - MINIMUM ( x ) if x . left != NIL return TREE - MINIMUM ( x . left ) else return x 1 2 3 4 TREE - MAXIMUM ( x ) if x . right != NIL return TREE - MAXIMUM ( x . right ) else return x 12.2-3 Write the $\\text{TREE-PREDECESSOR}$ procedure. 1 2 3 4 5 6 7 8 TREE - PREDECESSOR ( x ) if x . left != NIL return TREE - MAXIMUM ( x . left ) y = x . p while y != NIL and x == y . left x = y y = y . p return y 12.2-4 Professor Bunyan thinks he has discovered a remarkable property of binary search trees. Suppose that the search for key $k$ in a binary search tree ends up in a leaf. Consider three sets: $A$, the keys to the left of the search path; $B$, the keys on the search path; and $C$, the keys to the right of the search path. Professor Bunyan claims that any three keys $a \\in A$, $b \\in B$, and $c \\in C$ must satisfy $a \\le b \\le c$. Give a smallest possible counterexample to the professor's claim. Search for $9$ in this tree. Then $A = \\{7\\}$, $B = \\{5, 8, 9\\}$ and $C = \\{\\}$. So, since $7 > 5$ it breaks professor's claim. 12.2-5 Show that if a node in a binary search tree has two children, then its successor has no left child and its predecessor has no right child. Let $x$ be a node with two children. In an inorder tree walk, the nodes in $x$'s left subtree immediately precede $x$ and the nodes in $x$'s right subtree immediately follow $x$. Thus, $x$'s predecessor is in its left subtree, and its successor is in its right subtree. Let $s$ be $x$'s successor. Then s cannot have a left child, for a left child of $s$ would come between $x$ and $s$ in the inorder walk. (It's after $x$ because it's in $x$'s right subtree, and it's before s because it's in $s$'s left subtree.) If any node were to come between $x$ and $s$ in an inorder walk, then s would not be $x$'s successor, as we had supposed. Symmetrically, $x$'s predecessor has no right child. 12.2-6 Consider a binary search tree $T$ whose keys are distinct. Show that if the right subtree of a node $x$ in $T$ is empty and $x$ has a successor $y$, then $y$ is the lowest ancestor of $x$ whose left child is also an ancestor of $x$. (Recall that every node is its own ancestor.) First we establish that $y$ must be an ancestor of $x$. If $y$ weren't an ancestor of $x$, then let $z$ denote the first common ancestor of $x$ and $y$. By the binary-search-tree property, $x < z < y$, so $y$ cannot be the successor of $x$. Next observe that $y.left$ must be an ancestor of $x$ because if it weren't, then $y.right$ would be an ancestor of $x$, implying that $x > y$. Finally, suppose that $y$ is not the lowest ancestor of $x$ whose left child is also an ancestor of $x$. Let $z$ denote this lowest ancestor. Then $z$ must be in the left subtree of $y$, which implies $z < y$, contradicting the fact that $y$ is the successor if $x$. 12.2-7 An alternative method of performing an inorder tree walk of an $n$-node binary search tree finds the minimum element in the tree by calling $\\text{TREE-MINIMUM}$ and then making $n - 1$ calls to $\\text{TREE-SUCCESSOR}$. Prove that this algorithm runs in $\\Theta(n)$ time. Note that a call to $\\text{TREE-MINIMUM}$ followed by $n - 1$ calls to $\\text{TREE-SUCCESSOR}$ performs exactly the same inorder walk of the tree as does the procedure $\\text{INORDER-TREE-WALK}$. $\\text{INORDER-TREE-WALK}$ prints the $\\text{TREE-MINIMUM}$ first, and by definition, the $\\text{TREE-SUCCESSOR}$ of a node is the next node in the sorted order determined by an inorder tree walk. This algorithm runs in $\\Theta(n)$ time because: It requires \u007f$O(n)$ time to do the n procedure calls. It traverses each of the $n - 1$ tree edges at most twice, which takes $O(n)$ time. To see that each edge is traversed at most twice (once going down the tree and once going up), consider the edge between any node $u$ and either of its children, node $v$. By starting at the root, we must traverse $(u, v)$ downward from $u$ to $v$, before traversing it upward from $v$ to $u$. The only time the tree is traversed downward is in code of $\\text{TREE-MINIMUM}$, and the only time the tree is traversed upward is in code of $\\text{TREE-SUCCESSOR}$ when we look for the successor of a node that has no right subtree. Suppose that $v$ is $u$'s left child. Before printing $u$, we must print all the nodes in its left subtree, which is rooted at $v$, guaranteeing the downward traversal of edge $(u, v)$. After all nodes in $u$'s left subtree are printed, $u$ must be printed next. Procedure $\\text{TREE-SUCCESSOR}$ traverses an upward path to $u$ from the maximum element (which has no right subtree) in the subtree rooted at $v$. This path clearly includes edge $(u, v)$, and since all nodes in $u$'s left subtree are printed, edge $(u, v)$ is never traversed again. Now suppose that $v$ is $u$'s right child. After $u$ is printed, $\\text{TREE-SUCCESSOR}(u)$ is called. To get to the minimum element in $u$'s right subtree (whose root is $v$), the edge $(u, v)$ must be traversed downward. After all values in $u$'s right subtree are printed, $\\text{TREE-SUCCESSOR}$ is called on the maximum element (again, which has no right subtree) in the subtree rooted at $v$. $\\text{TREE-SUCCESSOR}$ traverses a path up the tree to an element after $u$, since $u$ was already printed. Edge $(u, v)$ must be traversed upward on this path, and since all nodes in $u$'s right subtree have been printed, edge $(u, v)$ is never traversed again. Hence, no edge is traversed twice in the same direction. Therefore, this algorithm runs in $\\Theta(n)$ time. 12.2-8 Prove that no matter what node we start at in a height-$h$ binary search tree, $k$ successive calls to $\\text{TREE-SUCCESSOR}$ take $O(k + h)$ time. Suppose $x$ is the starting node and $y$ is the ending node. The distance between $x$ and $y$ is at most $2h$, and all the edges connecting the $k$ nodes are visited twice, therefore it takes $O(k + h)$ time. 12.2-9 Let $T$ be a binary search tree whose keys are distinct, let $x$ be a leaf node, and let $y$ be its parent. Show that $y.key$ is either the smallest key in $T$ larger than $x.key$ or the largest key in $T$ smaller than $x.key$. If $x = y.left$, then calling successor on $x$ will result in no iterations of the while loop, and so will return $y$. If $x = y.right$, the while loop for calling predecessor (see exercise 3) will be run no times, and so $y$ will be returned.","title":"12.2 Querying a binary search tree"},{"location":"Chap12/12.2/#122-1","text":"Suppose that we have numbers between $1$ and $1000$ in a binary search tree, and we want to search for the number $363$. Which of the following sequences could not be the sequence of nodes examined? a. $2, 252, 401, 398, 330, 344, 397, 363$. b. $924, 220, 911, 244, 898, 258, 362, 363$. c. $925, 202, 911, 240, 912, 245, 363$. d. $2, 399, 387, 219, 266, 382, 381, 278, 363$. e. $935, 278, 347, 621, 299, 392, 358, 363$. c. could not be the sequence of nodes explored because we take the left child from the $911$ node, and yet somehow manage to get to the $912$ node which cannot belong the left subtree of $911$ because it is greater. e. is also impossible because we take the right subtree on the $347$ node and yet later come across the $299$ node.","title":"12.2-1"},{"location":"Chap12/12.2/#122-2","text":"Write recursive versions of $\\text{TREE-MINIMUM}$ and $\\text{TREE-MAXIMUM}$. 1 2 3 4 TREE - MINIMUM ( x ) if x . left != NIL return TREE - MINIMUM ( x . left ) else return x 1 2 3 4 TREE - MAXIMUM ( x ) if x . right != NIL return TREE - MAXIMUM ( x . right ) else return x","title":"12.2-2"},{"location":"Chap12/12.2/#122-3","text":"Write the $\\text{TREE-PREDECESSOR}$ procedure. 1 2 3 4 5 6 7 8 TREE - PREDECESSOR ( x ) if x . left != NIL return TREE - MAXIMUM ( x . left ) y = x . p while y != NIL and x == y . left x = y y = y . p return y","title":"12.2-3"},{"location":"Chap12/12.2/#122-4","text":"Professor Bunyan thinks he has discovered a remarkable property of binary search trees. Suppose that the search for key $k$ in a binary search tree ends up in a leaf. Consider three sets: $A$, the keys to the left of the search path; $B$, the keys on the search path; and $C$, the keys to the right of the search path. Professor Bunyan claims that any three keys $a \\in A$, $b \\in B$, and $c \\in C$ must satisfy $a \\le b \\le c$. Give a smallest possible counterexample to the professor's claim. Search for $9$ in this tree. Then $A = \\{7\\}$, $B = \\{5, 8, 9\\}$ and $C = \\{\\}$. So, since $7 > 5$ it breaks professor's claim.","title":"12.2-4"},{"location":"Chap12/12.2/#122-5","text":"Show that if a node in a binary search tree has two children, then its successor has no left child and its predecessor has no right child. Let $x$ be a node with two children. In an inorder tree walk, the nodes in $x$'s left subtree immediately precede $x$ and the nodes in $x$'s right subtree immediately follow $x$. Thus, $x$'s predecessor is in its left subtree, and its successor is in its right subtree. Let $s$ be $x$'s successor. Then s cannot have a left child, for a left child of $s$ would come between $x$ and $s$ in the inorder walk. (It's after $x$ because it's in $x$'s right subtree, and it's before s because it's in $s$'s left subtree.) If any node were to come between $x$ and $s$ in an inorder walk, then s would not be $x$'s successor, as we had supposed. Symmetrically, $x$'s predecessor has no right child.","title":"12.2-5"},{"location":"Chap12/12.2/#122-6","text":"Consider a binary search tree $T$ whose keys are distinct. Show that if the right subtree of a node $x$ in $T$ is empty and $x$ has a successor $y$, then $y$ is the lowest ancestor of $x$ whose left child is also an ancestor of $x$. (Recall that every node is its own ancestor.) First we establish that $y$ must be an ancestor of $x$. If $y$ weren't an ancestor of $x$, then let $z$ denote the first common ancestor of $x$ and $y$. By the binary-search-tree property, $x < z < y$, so $y$ cannot be the successor of $x$. Next observe that $y.left$ must be an ancestor of $x$ because if it weren't, then $y.right$ would be an ancestor of $x$, implying that $x > y$. Finally, suppose that $y$ is not the lowest ancestor of $x$ whose left child is also an ancestor of $x$. Let $z$ denote this lowest ancestor. Then $z$ must be in the left subtree of $y$, which implies $z < y$, contradicting the fact that $y$ is the successor if $x$.","title":"12.2-6"},{"location":"Chap12/12.2/#122-7","text":"An alternative method of performing an inorder tree walk of an $n$-node binary search tree finds the minimum element in the tree by calling $\\text{TREE-MINIMUM}$ and then making $n - 1$ calls to $\\text{TREE-SUCCESSOR}$. Prove that this algorithm runs in $\\Theta(n)$ time. Note that a call to $\\text{TREE-MINIMUM}$ followed by $n - 1$ calls to $\\text{TREE-SUCCESSOR}$ performs exactly the same inorder walk of the tree as does the procedure $\\text{INORDER-TREE-WALK}$. $\\text{INORDER-TREE-WALK}$ prints the $\\text{TREE-MINIMUM}$ first, and by definition, the $\\text{TREE-SUCCESSOR}$ of a node is the next node in the sorted order determined by an inorder tree walk. This algorithm runs in $\\Theta(n)$ time because: It requires \u007f$O(n)$ time to do the n procedure calls. It traverses each of the $n - 1$ tree edges at most twice, which takes $O(n)$ time. To see that each edge is traversed at most twice (once going down the tree and once going up), consider the edge between any node $u$ and either of its children, node $v$. By starting at the root, we must traverse $(u, v)$ downward from $u$ to $v$, before traversing it upward from $v$ to $u$. The only time the tree is traversed downward is in code of $\\text{TREE-MINIMUM}$, and the only time the tree is traversed upward is in code of $\\text{TREE-SUCCESSOR}$ when we look for the successor of a node that has no right subtree. Suppose that $v$ is $u$'s left child. Before printing $u$, we must print all the nodes in its left subtree, which is rooted at $v$, guaranteeing the downward traversal of edge $(u, v)$. After all nodes in $u$'s left subtree are printed, $u$ must be printed next. Procedure $\\text{TREE-SUCCESSOR}$ traverses an upward path to $u$ from the maximum element (which has no right subtree) in the subtree rooted at $v$. This path clearly includes edge $(u, v)$, and since all nodes in $u$'s left subtree are printed, edge $(u, v)$ is never traversed again. Now suppose that $v$ is $u$'s right child. After $u$ is printed, $\\text{TREE-SUCCESSOR}(u)$ is called. To get to the minimum element in $u$'s right subtree (whose root is $v$), the edge $(u, v)$ must be traversed downward. After all values in $u$'s right subtree are printed, $\\text{TREE-SUCCESSOR}$ is called on the maximum element (again, which has no right subtree) in the subtree rooted at $v$. $\\text{TREE-SUCCESSOR}$ traverses a path up the tree to an element after $u$, since $u$ was already printed. Edge $(u, v)$ must be traversed upward on this path, and since all nodes in $u$'s right subtree have been printed, edge $(u, v)$ is never traversed again. Hence, no edge is traversed twice in the same direction. Therefore, this algorithm runs in $\\Theta(n)$ time.","title":"12.2-7"},{"location":"Chap12/12.2/#122-8","text":"Prove that no matter what node we start at in a height-$h$ binary search tree, $k$ successive calls to $\\text{TREE-SUCCESSOR}$ take $O(k + h)$ time. Suppose $x$ is the starting node and $y$ is the ending node. The distance between $x$ and $y$ is at most $2h$, and all the edges connecting the $k$ nodes are visited twice, therefore it takes $O(k + h)$ time.","title":"12.2-8"},{"location":"Chap12/12.2/#122-9","text":"Let $T$ be a binary search tree whose keys are distinct, let $x$ be a leaf node, and let $y$ be its parent. Show that $y.key$ is either the smallest key in $T$ larger than $x.key$ or the largest key in $T$ smaller than $x.key$. If $x = y.left$, then calling successor on $x$ will result in no iterations of the while loop, and so will return $y$. If $x = y.right$, the while loop for calling predecessor (see exercise 3) will be run no times, and so $y$ will be returned.","title":"12.2-9"},{"location":"Chap12/12.3/","text":"12.3-1 Give a recursive version of the $\\text{TREE-INSERT}$ procedure. 1 2 3 4 RECURSIVE - TREE - INSERT ( T , z ) if T . root == NIL T . root = z else INSERT ( NIL , T . root , z ) 1 2 3 4 5 6 7 8 9 INSERT ( p , x , z ) if x == NIL z . p = p if z . key < p . key p . left = z else p . right = z else if z . key < x . key INSERT ( x , x . left , z ) else INSERT ( x , x . right , z ) 12.3-2 Suppose that we construct a binary search tree by repeatedly inserting distinct values into the tree. Argue that the number of nodes examined in searching for a value in the tree is one plus the number of nodes examined when the value was first inserted into the tree. Number of nodes examined while searching also includes the node which is searched for, which isn't the case when we inserted it. 12.3-3 We can sort a given set of $n$ numbers by first building a binary search tree containing these numbers (using $\\text{TREE-INSERT}$ repeatedly to insert the numbers one by one) and then printing the numbers by an inorder tree walk. What are the worst-case and best-case running times for this sorting algorithm? Here's the algorithm: 1 2 3 4 5 TREE - SORT ( A ) let T be an empty binary search tree for i = 1 to n TREE - INSERT ( T , A [ i ]) INORDER - TREE - WALK ( T . root ) Worst case: $\\Theta(n^2)$\u2014occurs when a linear chain of nodes results from the repeated $\\text{TREE-INSERT}$ operations. Best case: $\\Theta(n\\lg n)$\u2014occurs when a binary tree of height $\\Theta(\\lg n)$ results from the repeated $\\text{TREE-INSERT}$ operations. 12.3-4 Is the operation of deletion \"commutative\" in the sense that deleting $x$ and then $y$ from a binary search tree leaves the same tree as deleting $y$ and then $x$? Argue why it is or give a counterexample. No, giving the following courterexample. Delete $A$ first, then delete $B$: 1 2 3 4 5 A C C / \\ / \\ \\ B D B D D / C Delete $B$ first, then delete $A$: 1 2 3 4 5 A A D / \\ \\ / B D D C / / C C 12.3-5 Suppose that instead of each node $x$ keeping the attribute $x.p$, pointing to $x$'s parent, it keeps $x.succ$, pointing to $x$'s successor. Give pseudocode for $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ on a binary search tree $T$ using this representation. These procedures should operate in time $O(h)$, where $h$ is the height of the tree $T$. ($\\textit{Hint:}$ You may wish to implement a subroutine that returns the parent of a node.) In $\\text{SEARCH}$ and $\\text{INSERT}$, we do not need to know the parent of $x$. 1 2 3 4 5 6 7 8 9 PARENT ( z ) if z == NIL m = T . root else m = z . left while m != x p = m m = m . right return p 1 2 3 4 5 6 7 8 9 10 11 12 13 14 GET - PARENT ( T , x ) if x . right == NIL if x == x . succ . left x . p = x . succ else z = x . succ x . p = PARENT ( z ) else y = TREE - MAXIMUM ( x . right ) if x == y . succ . left x . p = y . succ else z = y . succ x . p = PARENT ( z ) Therefore we can find $x$'s parent in $O(h)$, $\\text{DELETE}$ is $O(h + h) = O(h)$. 12.3-6 When node $z$ in $\\text{TREE-DELETE}$ has two children, we could choose node $y$ as its predecessor rather than its successor. What other changes to $\\text{TREE-DELETE}$ would be necessary if we did so? Some have argued that a fair strategy, giving equal priority to predecessor and successor, yields better empirical performance. How might $\\text{TREE-DELETE}$ be changed to implement such a fair strategy? Update line 5 so that $y$ is set equal to $\\text{TREE-MAXIMUM}(z.left)$. To implement the fair strategy, we could randomly decide each time $\\text{TREE-DELETE}$ is called whether or not to use the predecessor or successor.","title":"12.3 Insertion and deletion"},{"location":"Chap12/12.3/#123-1","text":"Give a recursive version of the $\\text{TREE-INSERT}$ procedure. 1 2 3 4 RECURSIVE - TREE - INSERT ( T , z ) if T . root == NIL T . root = z else INSERT ( NIL , T . root , z ) 1 2 3 4 5 6 7 8 9 INSERT ( p , x , z ) if x == NIL z . p = p if z . key < p . key p . left = z else p . right = z else if z . key < x . key INSERT ( x , x . left , z ) else INSERT ( x , x . right , z )","title":"12.3-1"},{"location":"Chap12/12.3/#123-2","text":"Suppose that we construct a binary search tree by repeatedly inserting distinct values into the tree. Argue that the number of nodes examined in searching for a value in the tree is one plus the number of nodes examined when the value was first inserted into the tree. Number of nodes examined while searching also includes the node which is searched for, which isn't the case when we inserted it.","title":"12.3-2"},{"location":"Chap12/12.3/#123-3","text":"We can sort a given set of $n$ numbers by first building a binary search tree containing these numbers (using $\\text{TREE-INSERT}$ repeatedly to insert the numbers one by one) and then printing the numbers by an inorder tree walk. What are the worst-case and best-case running times for this sorting algorithm? Here's the algorithm: 1 2 3 4 5 TREE - SORT ( A ) let T be an empty binary search tree for i = 1 to n TREE - INSERT ( T , A [ i ]) INORDER - TREE - WALK ( T . root ) Worst case: $\\Theta(n^2)$\u2014occurs when a linear chain of nodes results from the repeated $\\text{TREE-INSERT}$ operations. Best case: $\\Theta(n\\lg n)$\u2014occurs when a binary tree of height $\\Theta(\\lg n)$ results from the repeated $\\text{TREE-INSERT}$ operations.","title":"12.3-3"},{"location":"Chap12/12.3/#123-4","text":"Is the operation of deletion \"commutative\" in the sense that deleting $x$ and then $y$ from a binary search tree leaves the same tree as deleting $y$ and then $x$? Argue why it is or give a counterexample. No, giving the following courterexample. Delete $A$ first, then delete $B$: 1 2 3 4 5 A C C / \\ / \\ \\ B D B D D / C Delete $B$ first, then delete $A$: 1 2 3 4 5 A A D / \\ \\ / B D D C / / C C","title":"12.3-4"},{"location":"Chap12/12.3/#123-5","text":"Suppose that instead of each node $x$ keeping the attribute $x.p$, pointing to $x$'s parent, it keeps $x.succ$, pointing to $x$'s successor. Give pseudocode for $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ on a binary search tree $T$ using this representation. These procedures should operate in time $O(h)$, where $h$ is the height of the tree $T$. ($\\textit{Hint:}$ You may wish to implement a subroutine that returns the parent of a node.) In $\\text{SEARCH}$ and $\\text{INSERT}$, we do not need to know the parent of $x$. 1 2 3 4 5 6 7 8 9 PARENT ( z ) if z == NIL m = T . root else m = z . left while m != x p = m m = m . right return p 1 2 3 4 5 6 7 8 9 10 11 12 13 14 GET - PARENT ( T , x ) if x . right == NIL if x == x . succ . left x . p = x . succ else z = x . succ x . p = PARENT ( z ) else y = TREE - MAXIMUM ( x . right ) if x == y . succ . left x . p = y . succ else z = y . succ x . p = PARENT ( z ) Therefore we can find $x$'s parent in $O(h)$, $\\text{DELETE}$ is $O(h + h) = O(h)$.","title":"12.3-5"},{"location":"Chap12/12.3/#123-6","text":"When node $z$ in $\\text{TREE-DELETE}$ has two children, we could choose node $y$ as its predecessor rather than its successor. What other changes to $\\text{TREE-DELETE}$ would be necessary if we did so? Some have argued that a fair strategy, giving equal priority to predecessor and successor, yields better empirical performance. How might $\\text{TREE-DELETE}$ be changed to implement such a fair strategy? Update line 5 so that $y$ is set equal to $\\text{TREE-MAXIMUM}(z.left)$. To implement the fair strategy, we could randomly decide each time $\\text{TREE-DELETE}$ is called whether or not to use the predecessor or successor.","title":"12.3-6"},{"location":"Chap12/12.4/","text":"12.4-1 Prove equation $\\text{(12.3)}$. The equation is $$\\sum_{i = 0}^{n - 1} \\binom{i + 3}{3} = \\binom{n + 3}{4}. \\tag{12.3}$$ $$ \\begin{aligned} \\sum_{i = 0}^{n - 1} \\binom{i + 3}{3} & = \\sum_{i = 0}^{n - 1} \\frac{(i + 3)(i + 2)(i + 1)}{6} \\\\ & = \\frac{1}{6} \\sum_{i = 0}^{n - 1} i^3 + 6i^2 + 11i + 6 \\\\ & = \\frac{1}{6} (\\frac{(n - 1)^2 n^2}{4} + \\frac{6(n - 1)n(2n - 1)}{6} + \\frac{11n(n - 1)}{2} + 6n) \\\\ & = \\frac{n(n + 1)(n + 2)(n + 3)}{24} \\\\ & = \\binom{n + 3}{4}. \\end{aligned} $$ 12.4-2 Describe a binary search tree on n nodes such that the average depth of a node in the tree is $\\Theta(\\lg n)$ but the height of the tree is $\\omega(\\lg n)$. Give an asymptotic upper bound on the height of an $n$-node binary search tree in which the average depth of a node is $\\Theta(\\lg n)$. We will answer the second part first. We shall show that if the average depth of a p node is $\\Theta(\\lg n)$, then the height of the tree is $O(n\\lg n)$. Then we will answer the first part by exhibiting that this bound is tight: there is a binary search tree with p average node depth $\\Theta(\\lg n)$ and height $\\Theta(\\sqrt{n\\lg n}) = \\omega(\\lg n)$. Lemma If the average depth of p a node in an $n$-node binary search tree is $\\Theta(\\lg n)$, then the height of the tree is $O(\\sqrt{n\\lg n})$. Proof Suppose that an $n$-node binary search tree has average depth $\\Theta(\\lg n)$ and height $h$. Then there exists a path from the root to a node at depth $h$, and the depths of the nodes on this path are $0, 1, \\ldots, h$. Let $P$ be the set of nodes on this path and $Q$ be all other nodes. Then the average depth of a node is $$ \\begin{aligned} \\frac{1}{n} \\Big(\\sum_{x \\in P} \\text{depth($x$)} + \\sum_{y \\in Q} \\text{depth($y$)}\\Big) & \\ge \\frac{1}{n} \\sum_{x \\in P} \\text{depth($x$)} \\\\ & = \\frac{1}{n} \\sum_{d = 0}^h d \\\\ & = \\frac{1}{n} \\cdot \\Theta(h^2). \\end{aligned} $$ For the purpose of contradiction, suppose that $h$ is not $O(\\sqrt{n\\lg n})$, so that $h = \\omega(\\sqrt{n\\lg n})$. Then we have $$ \\begin{aligned} \\frac{1}{n} \\cdot \\Theta(h^2) & = \\frac{1}{n} \\cdot \\omega(n\\lg n) \\\\ & = \\omega(\\lg n), \\end{aligned} $$ which contradicts the assumption that the average depth is $\\Theta(\\lg n)$. Thus, the height is $O(\\sqrt{n\\lg n})$. Here is an example of an $n$-node binary search tree with average node depth $\\Theta(\\lg n)$ but height $\\omega(\\lg n)$: In this tree, $n - \\sqrt{n\\lg n}$ nodes are a complete binary tree, and the other $\\sqrt{n\\lg n}$ nodes protrude from below as a single chain. This tree has height $$ \\begin{aligned} \\Theta(\\lg(n - \\sqrt{n\\lg n})) + \\sqrt{n\\lg n} & = \\Theta(\\sqrt{n\\lg n}) \\\\ & = \\omega(\\lg n). \\end{aligned} $$ To compute an upper bound on the average depth of a node, we use $O(\\lg n)$ as an upper bound on the depth p of each of the $n - \\sqrt{n\\lg n}$ nodes in the complete binary tree part and $O(\\lg n + \\sqrt{n\\lg n})$ as an upper bound on the depth of each of the $\\sqrt{n\\lg n}$ nodes in the protruding chain. Thus, the average depth of a node is bounded from above by $$ \\begin{aligned} \\frac{1}{n} \\cdot O(\\sqrt{n\\lg n}(\\lg n + \\sqrt{n\\lg n}) + (n - \\sqrt{n\\lg n})\\lg n) & = \\frac{1}{n} \\cdot O(n\\lg n) \\\\ & = O(\\lg n). \\end{aligned} $$ To bound the average depth of a node from below, observe that the bottommost level of the complete binary tree part has $\\Theta(n - \\sqrt{n - \\lg n})$ nodes, and each of these nodes has depth $\\Theta(\\lg n)$. Thus, the average node depth is at least $$ \\begin{aligned} \\frac{1}{n} \\cdot \\Theta((n - \\sqrt{n - \\lg n})\\lg n) & = \\frac{1}{n} \\cdot \\Omega(n\\lg n) \\\\ & = \\Omega(\\lg n). \\end{aligned} $$ Because the average node depth is both $O(\\lg n)$ and $\\Omega(\\lg n)$, it is $\\Theta(\\lg n)$. 12.4-3 Show that the notion of a randomly chosen binary search tree on $n$ keys, where each binary search tree of $n$ keys is equally likely to be chosen, is different from the notion of a randomly built binary search tree given in this section. ($\\textit{Hint:}$ List the possibilities when $n = 3$.) For $n = 3$, there are $5$ binary search trees. However, if we build the trees will a random permutation, the first tree will built twice. 12.4-4 Show that the function $f(x) = 2^x$ is convex. We'll go one better than showing that the function $2^x$ is convex. Instead, we'll show that the function $c^x$ is convex, for any positive constant $c$. According to the definition of convexity on page 1199 of the text, a function $f(x)$ is convex if for all $x$ and $y$ and for all $0 \\le \\lambda \\le 1$, we have $f(\\lambda x + (1 - \\lambda)y) \\le \\lambda f(x) + (1 - \\lambda)f(y)$. Thus, we need to show that for all $0 \\le \\lambda \\le 1$, we have $c^{\\lambda x + (1 - \\lambda)y} \\le \\lambda c^x + (1 - \\lambda)c^y$. We start by proving the following lemma. Lemma For any real numbers $a$ and $b$ and any positive real number $c$, $$c^a \\ge c^b + (a - b)c^b\\ln c.$$ Proof We first show that for all real $r$, we have $c^r \\ge 1 + r\\ln c$. By equation $\\text{(3.12)}$ from the text, we have $e^x \\ge 1 + x$ for all real $x$. Let $x = r\\ln c$, so that $e^x = e^{r\\ln c} = (e^{\\ln c})^r = c^r$. Then we have $c^r = e^{r\\ln c} \\ge 1 + r\\ln c$. Substituting $a - b$ for $r$ in the above inequality, we have $c^{a - b} \\ge 1 + (a - b)\\ln c$. Multiplying both sides by $c^b$ gives $c^a \\ge c^b + (a - b)c^b\\ln c$. (lemma) Now we can show that $c^{\\lambda x + (1 - \\lambda)y} \\le \\lambda c^x + (1 - \\lambda)c^y$ for all $0 \\le \\lambda \\le 1$. For convenience, let $z = \\lambda x + (1 - \\lambda)y$. In the inequality given by the lemma, substitute $x$ for $a$ and $z$ for $b$, giving $$c^x \\ge c^z + (x - z)c^z\\ln c.$$ Also substitute $y$ for $a$ and $z$ for $b$, giving $$c^y \\ge c^z + (y - z)c^z\\ln c.$$ If we multiply the first inequality by $\\lambda$ and the second by $1 - \\lambda$ and then add the resulting inequalities, we get $$ \\begin{aligned} \\lambda c^x + (1 - \\lambda)c^y & \\ge \\lambda(c^z + (x - z)c^z\\ln c) + (1 - \\lambda)(c^z + (y - z)c^z\\ln c) \\\\ & = \\lambda c^z + \\lambda x c^z\\ln c - \\lambda z c^z\\ln c + (1 - \\lambda)c^z + (1 - \\lambda)yc^z\\ln c - (1 - \\lambda)zc^z\\ln c \\\\ & = (\\lambda + (1 - \\lambda))c^z + (\\lambda x + (1 - \\lambda)y)c^z\\ln c - (\\lambda + (1 - \\lambda))zc^z\\ln c \\\\ & = c^z + zc^z\\ln c - zc^z\\ln c \\\\ & = c^z \\\\ & = c^{\\lambda x + (1 - \\lambda)y}, \\end{aligned} $$ as we wished to show. 12.4-5 $\\star$ Consider $\\text{RANDOMIZED-QUICKSORT}$ operating on a sequence of $n$ distinct input numbers. Prove that for any constant $k > 0$, all but $O(1 / n^k)$ of the $n!$ input permutations yield an $O(n\\lg n)$ running time. Let $A(n)$ denote the probability that when quicksorting a list of length $n$, some pivot is selected to not be in the middle $n^{1 - k / 2}$ of the numberes. This doesn't happen with probability $\\frac{1}{n^{k / 2}}$. Then, we have that the two subproblems are of size $n_1, n_2$ with $n_1 + n_2 = n - 1$, then $$A(n) \\le \\frac{1}{n^{k / 2}} + T(n_1)+T(n_2).$$ Since we bounded the depth by $O(1 / \\lg n)$ let $\\{a_{i, j}\\}_i$ be all the subproblem sizes left at depth $j$, $$A(n) \\le \\frac{1}{n^{k / 2}} \\sum_j\\sum_i \\frac{1}{a}.$$","title":"12.4 Randomly built binary search trees"},{"location":"Chap12/12.4/#124-1","text":"Prove equation $\\text{(12.3)}$. The equation is $$\\sum_{i = 0}^{n - 1} \\binom{i + 3}{3} = \\binom{n + 3}{4}. \\tag{12.3}$$ $$ \\begin{aligned} \\sum_{i = 0}^{n - 1} \\binom{i + 3}{3} & = \\sum_{i = 0}^{n - 1} \\frac{(i + 3)(i + 2)(i + 1)}{6} \\\\ & = \\frac{1}{6} \\sum_{i = 0}^{n - 1} i^3 + 6i^2 + 11i + 6 \\\\ & = \\frac{1}{6} (\\frac{(n - 1)^2 n^2}{4} + \\frac{6(n - 1)n(2n - 1)}{6} + \\frac{11n(n - 1)}{2} + 6n) \\\\ & = \\frac{n(n + 1)(n + 2)(n + 3)}{24} \\\\ & = \\binom{n + 3}{4}. \\end{aligned} $$","title":"12.4-1"},{"location":"Chap12/12.4/#124-2","text":"Describe a binary search tree on n nodes such that the average depth of a node in the tree is $\\Theta(\\lg n)$ but the height of the tree is $\\omega(\\lg n)$. Give an asymptotic upper bound on the height of an $n$-node binary search tree in which the average depth of a node is $\\Theta(\\lg n)$. We will answer the second part first. We shall show that if the average depth of a p node is $\\Theta(\\lg n)$, then the height of the tree is $O(n\\lg n)$. Then we will answer the first part by exhibiting that this bound is tight: there is a binary search tree with p average node depth $\\Theta(\\lg n)$ and height $\\Theta(\\sqrt{n\\lg n}) = \\omega(\\lg n)$. Lemma If the average depth of p a node in an $n$-node binary search tree is $\\Theta(\\lg n)$, then the height of the tree is $O(\\sqrt{n\\lg n})$. Proof Suppose that an $n$-node binary search tree has average depth $\\Theta(\\lg n)$ and height $h$. Then there exists a path from the root to a node at depth $h$, and the depths of the nodes on this path are $0, 1, \\ldots, h$. Let $P$ be the set of nodes on this path and $Q$ be all other nodes. Then the average depth of a node is $$ \\begin{aligned} \\frac{1}{n} \\Big(\\sum_{x \\in P} \\text{depth($x$)} + \\sum_{y \\in Q} \\text{depth($y$)}\\Big) & \\ge \\frac{1}{n} \\sum_{x \\in P} \\text{depth($x$)} \\\\ & = \\frac{1}{n} \\sum_{d = 0}^h d \\\\ & = \\frac{1}{n} \\cdot \\Theta(h^2). \\end{aligned} $$ For the purpose of contradiction, suppose that $h$ is not $O(\\sqrt{n\\lg n})$, so that $h = \\omega(\\sqrt{n\\lg n})$. Then we have $$ \\begin{aligned} \\frac{1}{n} \\cdot \\Theta(h^2) & = \\frac{1}{n} \\cdot \\omega(n\\lg n) \\\\ & = \\omega(\\lg n), \\end{aligned} $$ which contradicts the assumption that the average depth is $\\Theta(\\lg n)$. Thus, the height is $O(\\sqrt{n\\lg n})$. Here is an example of an $n$-node binary search tree with average node depth $\\Theta(\\lg n)$ but height $\\omega(\\lg n)$: In this tree, $n - \\sqrt{n\\lg n}$ nodes are a complete binary tree, and the other $\\sqrt{n\\lg n}$ nodes protrude from below as a single chain. This tree has height $$ \\begin{aligned} \\Theta(\\lg(n - \\sqrt{n\\lg n})) + \\sqrt{n\\lg n} & = \\Theta(\\sqrt{n\\lg n}) \\\\ & = \\omega(\\lg n). \\end{aligned} $$ To compute an upper bound on the average depth of a node, we use $O(\\lg n)$ as an upper bound on the depth p of each of the $n - \\sqrt{n\\lg n}$ nodes in the complete binary tree part and $O(\\lg n + \\sqrt{n\\lg n})$ as an upper bound on the depth of each of the $\\sqrt{n\\lg n}$ nodes in the protruding chain. Thus, the average depth of a node is bounded from above by $$ \\begin{aligned} \\frac{1}{n} \\cdot O(\\sqrt{n\\lg n}(\\lg n + \\sqrt{n\\lg n}) + (n - \\sqrt{n\\lg n})\\lg n) & = \\frac{1}{n} \\cdot O(n\\lg n) \\\\ & = O(\\lg n). \\end{aligned} $$ To bound the average depth of a node from below, observe that the bottommost level of the complete binary tree part has $\\Theta(n - \\sqrt{n - \\lg n})$ nodes, and each of these nodes has depth $\\Theta(\\lg n)$. Thus, the average node depth is at least $$ \\begin{aligned} \\frac{1}{n} \\cdot \\Theta((n - \\sqrt{n - \\lg n})\\lg n) & = \\frac{1}{n} \\cdot \\Omega(n\\lg n) \\\\ & = \\Omega(\\lg n). \\end{aligned} $$ Because the average node depth is both $O(\\lg n)$ and $\\Omega(\\lg n)$, it is $\\Theta(\\lg n)$.","title":"12.4-2"},{"location":"Chap12/12.4/#124-3","text":"Show that the notion of a randomly chosen binary search tree on $n$ keys, where each binary search tree of $n$ keys is equally likely to be chosen, is different from the notion of a randomly built binary search tree given in this section. ($\\textit{Hint:}$ List the possibilities when $n = 3$.) For $n = 3$, there are $5$ binary search trees. However, if we build the trees will a random permutation, the first tree will built twice.","title":"12.4-3"},{"location":"Chap12/12.4/#124-4","text":"Show that the function $f(x) = 2^x$ is convex. We'll go one better than showing that the function $2^x$ is convex. Instead, we'll show that the function $c^x$ is convex, for any positive constant $c$. According to the definition of convexity on page 1199 of the text, a function $f(x)$ is convex if for all $x$ and $y$ and for all $0 \\le \\lambda \\le 1$, we have $f(\\lambda x + (1 - \\lambda)y) \\le \\lambda f(x) + (1 - \\lambda)f(y)$. Thus, we need to show that for all $0 \\le \\lambda \\le 1$, we have $c^{\\lambda x + (1 - \\lambda)y} \\le \\lambda c^x + (1 - \\lambda)c^y$. We start by proving the following lemma. Lemma For any real numbers $a$ and $b$ and any positive real number $c$, $$c^a \\ge c^b + (a - b)c^b\\ln c.$$ Proof We first show that for all real $r$, we have $c^r \\ge 1 + r\\ln c$. By equation $\\text{(3.12)}$ from the text, we have $e^x \\ge 1 + x$ for all real $x$. Let $x = r\\ln c$, so that $e^x = e^{r\\ln c} = (e^{\\ln c})^r = c^r$. Then we have $c^r = e^{r\\ln c} \\ge 1 + r\\ln c$. Substituting $a - b$ for $r$ in the above inequality, we have $c^{a - b} \\ge 1 + (a - b)\\ln c$. Multiplying both sides by $c^b$ gives $c^a \\ge c^b + (a - b)c^b\\ln c$. (lemma) Now we can show that $c^{\\lambda x + (1 - \\lambda)y} \\le \\lambda c^x + (1 - \\lambda)c^y$ for all $0 \\le \\lambda \\le 1$. For convenience, let $z = \\lambda x + (1 - \\lambda)y$. In the inequality given by the lemma, substitute $x$ for $a$ and $z$ for $b$, giving $$c^x \\ge c^z + (x - z)c^z\\ln c.$$ Also substitute $y$ for $a$ and $z$ for $b$, giving $$c^y \\ge c^z + (y - z)c^z\\ln c.$$ If we multiply the first inequality by $\\lambda$ and the second by $1 - \\lambda$ and then add the resulting inequalities, we get $$ \\begin{aligned} \\lambda c^x + (1 - \\lambda)c^y & \\ge \\lambda(c^z + (x - z)c^z\\ln c) + (1 - \\lambda)(c^z + (y - z)c^z\\ln c) \\\\ & = \\lambda c^z + \\lambda x c^z\\ln c - \\lambda z c^z\\ln c + (1 - \\lambda)c^z + (1 - \\lambda)yc^z\\ln c - (1 - \\lambda)zc^z\\ln c \\\\ & = (\\lambda + (1 - \\lambda))c^z + (\\lambda x + (1 - \\lambda)y)c^z\\ln c - (\\lambda + (1 - \\lambda))zc^z\\ln c \\\\ & = c^z + zc^z\\ln c - zc^z\\ln c \\\\ & = c^z \\\\ & = c^{\\lambda x + (1 - \\lambda)y}, \\end{aligned} $$ as we wished to show.","title":"12.4-4"},{"location":"Chap12/12.4/#124-5-star","text":"Consider $\\text{RANDOMIZED-QUICKSORT}$ operating on a sequence of $n$ distinct input numbers. Prove that for any constant $k > 0$, all but $O(1 / n^k)$ of the $n!$ input permutations yield an $O(n\\lg n)$ running time. Let $A(n)$ denote the probability that when quicksorting a list of length $n$, some pivot is selected to not be in the middle $n^{1 - k / 2}$ of the numberes. This doesn't happen with probability $\\frac{1}{n^{k / 2}}$. Then, we have that the two subproblems are of size $n_1, n_2$ with $n_1 + n_2 = n - 1$, then $$A(n) \\le \\frac{1}{n^{k / 2}} + T(n_1)+T(n_2).$$ Since we bounded the depth by $O(1 / \\lg n)$ let $\\{a_{i, j}\\}_i$ be all the subproblem sizes left at depth $j$, $$A(n) \\le \\frac{1}{n^{k / 2}} \\sum_j\\sum_i \\frac{1}{a}.$$","title":"12.4-5 $\\star$"},{"location":"Chap12/Problems/12-1/","text":"Equal keys pose a problem for the implementation of binary search trees. a. What is the asymptotic performance of $\\text{TREE-INSERT}$ when used to insert $n$ items with identical keys into an initially empty binary search tree? We propose to improve $\\text{TREE-INSERT}$ by testing before line 5 to determine whether $z.key = x.key$ and by testing before line 11 to determine whether $z.key = y.key$. If equality holds, we implement one of the following strategies. For each strategy, find the asymptotic performance of inserting $n$ items with identical keys into an initially empty binary search tree. (The strategies are described for line 5, in which we compare the keys of $z$ and $x$. Substitute $y$ for $x$ to arrive at the strategies for line 11.) b. Keep a boolean flag $x.b$ at node $x$, and set $x$ to either $x.left$ or $x.right$ based on the value of $x.b$, which alternates between $\\text{FALSE}$ and $\\text{TRUE}$ each time we visit $x$ while inserting a node with the same key as $x$. c. Keep a list of nodes with equal keys at $x$, and insert $z$ into the list. d. Randomly set $x$ to either $x.left$ or $x.right$. (Give the worst-case performance and informally derive the expected running time.) a. Each insertion will add the element to the right of the rightmost leaf because the inequality on line 11 will always evaluate to false. This will result in the runtime being $\\sum_{i = 1}^n i \\in \\Theta(n^2)$. b. This strategy will result in each of the two children subtrees having a difference in size at most one. This means that the height will be $\\Theta(\\lg n)$. So, the total runtime will be $\\sum_{i = 1}^n \\lg n \\in \\Theta(n\\lg n)$. c. This will only take linear time since the tree itself will be height $0$, and a single insertion into a list can be done in constant time. d. Worst-case: every random choice is to the right (or all to the left) this will result in the same behavior as in the first part of this problem, $\\Theta(n^2)$. Expected running time: notice that when randomly choosing, we will pick left roughly half the time, so, the tree will be roughly balanced, so, we have that the depth is roughly $\\lg(n)$, $\\Theta(n\\lg n)$.","title":"12-1 Binary search trees with equal keys"},{"location":"Chap12/Problems/12-2/","text":"Given two strings $a = a_0a_1 \\ldots a_p$ and $b = b_0b_1 \\ldots b_q$, where each $a_i$ and each $b_j$ is in some ordered set of characters, we say that string $a$ is lexicographically less than string $b$ if either there exists an integer $j$, where $0 \\le j \\le \\min(p, q)$, such that $a_i = b_i$ for all $i = 0, 1, \\ldots j - 1$ and $a_j < b_j$, or $p < q$ and $a_i = b_i$ for all $i = 0, 1, \\ldots, p$. For example, if $a$ and $b$ are bit strings, then $10100 < 10110$ by rule 1 (letting $j = 3$) and $10100 < 101000$ by rule 2. This ordering is similar to that used in English-language dictionaries. The radix tree data structure shown in Figure 12.5 stores the bit strings $1011, 10, 011, 100$, and $0$. When searching for a key $a = a_0a_1 \\ldots a_p$, we go left at a node of depth $i$ if $a_i = 0$ and right if $a_i = 1$. Let $S$ be a set of distinct bit strings whose lengths sum to $n$. Show how to use a radix tree to sort $S$ lexicographically in $\\Theta(n)$ time. For the example in Figure 12.5, the output of the sort should be the sequence $0, 011, 10, 100, 1011$. To sort the strings of $S$, we first insert them into a radix tree, and then use a preorder tree walk to extract them in lexicographically sorted order. The tree walk outputs strings only for nodes that indicate the existence of a string (i.e., those that are lightly shaded in Figure 12.5 of the text). Correctness: The preorder ordering is the correct order because: Any node's string is a prefix of all its descendants' strings and hence belongs before them in the sorted order (rule 2). A node's left descendants belong before its right descendants because the corresponding strings are identical up to that parent node, and in the next position the left subtree's strings have $0$ whereas the right subtree's strings have $1$ (rule 1). Time: $\\Theta(n)$. Insertion takes $\\Theta(n)$ time, since the insertion of each string takes time proportional to its length (traversing a path through the tree whose length is the length of the string), and the sum of all the string lengths is $n$. The preorder tree walk takes $O(n)$ time. It is just like $\\text{INORDER-TREE-WALK}$ (it prints the current node and calls itself recursively on the left and right subtrees), so it takes time proportional to the number of nodes in the tree. The number of nodes is at most $1$ plus the sum $(n)$ of the lengths of the binary strings in the tree, because a length-$i$ string corresponds to a path through the root and $i$ other nodes, but a single node may be shared among many string paths.","title":"12-2 Radix trees"},{"location":"Chap12/Problems/12-3/","text":"In this problem, we prove that the average depth of a node in a randomly built binary search tree with $n$ nodes is $O(\\lg n)$. Although this result is weaker than that of Theorem 12.4, the technique we shall use reveals a surprising similarity between the building of a binary search tree and the execution of $\\text{RANDOMIZED-QUICKSORT}$ from Section 7.3. We define the total path length $P(T)$ of a binary tree $T$ as the sum, over all nodes $x$ in $T$, of the depth of node $x$, which we denote by $d(x, T)$. a. Argue that the average depth of a node in $T$ is $$\\frac{1}{n} \\sum_{x \\in T} d(x, T) = \\frac{1}{n} P(T).$$ Thus, we wish to show that the expected value of $P(T)$ is $O(n\\lg n)$. b. Let $T_L$ and $T_R$ denote the left and right subtrees of tree $T$, respectively. Argue that if $T$ has $n$ nodes, then $$P(T) = P(T_L) + P(T_R) + n - 1.$$ c. Let $P(n)$ denote the average total path length of a randomly built binary search tree with n nodes. Show that $$P(n) = \\frac{1}{n} \\sum_{i = 0}^{n - 1} (P(i) + P(n - i - 1) + n - 1).$$ d. Show how to rewrite $P(n)$ as $$P(n) = \\frac{2}{n} \\sum_{k = 1}^{n - 1} P(k) + \\Theta(n).$$ e. Recalling the alternative analysis of the randomized version of quicksort given in Problem 7-3, conclude that $P(n) = O(n\\lg n)$. At each recursive invocation of quicksort, we choose a random pivot element to partition the set of elements being sorted. Each node of a binary search tree partitions the set of elements that fall into the subtree rooted at that node. f. Describe an implementation of quicksort in which the comparisons to sort a set of elements are exactly the same as the comparisons to insert the elements into a binary search tree. (The order in which comparisons are made may differ, but the same comparisons must occur.) a. The total path length $P(T)$ is defined as $\\sum_{x \\in T} d(x, T)$. Dividing both quantities by $n$ gives the desired equation. b. For any node $x$ in $T_L$, we have $d(x, T_L) = d(x, T) - 1$, since the distance to the root of $T_L$ is one less than the distance to the root of $T$. Similarly, for any node $x$ in $T_R$, we have $d(x, T_R) = d(x, T) - 1$. Thus if $T$ has $n$ nodes, we have $$P(T) = P(T_L) + P(T_R) + n - 1,$$ since each of the $n$ nodes of $T$ (except the root) is in either $T_L$ or $T_R$. c. If $T$ is a randomly built binary search tree, then the root is equally likely to be any of the $n$ elements in the tree, since the root is the first element inserted. It follows that the number of nodes in subtree $T_L$ is eqaully likely to be any integer in the set $\\{0, 1, \\ldots, n - 1\\}$. The definition of $P(n)$ as the average total path length of a randomly built binary search tree, along with part (b), gives us the recurrence $$P(n) = \\frac{1}{n} \\sum_{i = 0}^{n - 1} (P(i) + P(n - i - 1) + n - 1).$$ d. Since $P(0) = 0$, and since for $k = 1, 2, \\ldots, n - 1$, each term $P(k)$ in the summation appears once as $P(i)$ and once as $P(n - i - 1)$, we can rewrite the equation from part (c) as $$P(n) = \\frac{2}{n} \\sum_{k = 1}^{n - 1} P(k) + \\Theta(n).$$ e. Observe that if, in the recurrence $\\text{(7.6)}$ in part (c) Problem 7-3, we replace $\\text E[T(\\cdot)]$ by $P(\\cdot)$ and we replace $q$ by $k$, we get almost the same recurrence as in part (d) of Problem 12-3. The remaining difference is that in Problem 12-3(d), the summation starts at $1$ rather than $2$. Observe, however, that a binary tree with just one node has a total path length of $0$, so that $P(1) = 0$. Thus, we can rewrite the recurrence in Problem 12-3(d) as $$P(n) = \\frac{2}{n} \\sum_{k = 2}^{n - 1} P(k) + \\Theta(n)$$ and use the same technique as was used in Problem 7-3 to solve it. We start by solving part (d) of Problem 7-3: showing that $$\\sum_{k = 2}^{n - 1} k\\lg k \\le \\frac{1}{2}n^2\\lg n - \\frac{1}{8}n^2.$$ Following the hint in Problem 7-3(d), we split the summation into two parts: $$\\sum_{k = 2}^{n - 1} k\\lg k = \\sum_{k = 2}^{\\lceil n / 2\\rceil - 1} k\\lg k + \\sum_{k = \\lceil n / 2\\rceil}^{n - 1} k\\lg k.$$ The $\\lg k$ in the first summation on the right is less than $\\lg(n / 2) = \\lg n - 1$, and the $\\lg k$ in the second summation is less than $\\lg n$. Thus, $$ \\begin{aligned} \\sum_{k = 2}^{n - 1} k\\lg k & < (\\lg n - 1) \\sum_{k = 2}^{\\lceil n / 2\\rceil - 1} k + \\lg n \\sum_{k = \\lceil n / 2\\rceil}^{n - 1} k \\\\ & = \\lg n \\sum_{k = 2}^{n - 1} k - \\sum_{k = 2}^{\\lceil n / 2 \\rceil - 1} k \\\\ & \\le \\frac{1}{2} n(n - 1)\\lg n - \\frac{1}{2}\\Big(\\frac{n}{1} - 1\\Big) \\frac{n}{2} \\\\ & \\le \\frac{1}{2} n^2\\lg n - \\frac{1}{8} n^2 \\end{aligned} $$ if $n \\ge 2$. Now we show that the recurrence $$P(n) = \\frac{2}{n} \\sum_{k = 2}^{n - 1} P(k) + \\Theta(n)$$ has the solution $P(n) = O(n\\lg n)$. We use the substitution method. Assume inductively that $P(n) \\le an\\lg n + b$ for some positive constants $a$ and $b$ to be determined. We can pick $a$ and $b$ sufficiently large so that $an\\lg n + b \\ge P(1)$. Then, for $n > 1$, we have by substitution $$ \\begin{aligned} P(n) & = \\frac{2}{n} \\sum_{k = 2}^{n - 1} P(k) + \\Theta(n) \\\\ & \\le \\frac{2}{n} \\sum_{k = 2}^{n - 1} (ak\\lg k + b) + \\Theta(n) \\\\ & = \\frac{2a}{n} \\sum_{k = 2}^{n - 1} k\\lg k + \\frac{2b}{n} (n - 2) + \\Theta(n) \\\\ & \\le \\frac{2a}{n} \\Big(\\frac{1}{2} n^2\\lg n - \\frac{1}{8}n^2\\Big) + \\frac{2b}{n}(n - 2) + \\Theta(n) \\\\ & \\le an\\lg n - \\frac{a}{4}n + 2b + \\Theta(n) \\\\ & = an\\lg n + b + \\Big(\\Theta(n) + b - \\frac{a}{4}n\\Big) \\\\ & \\le an\\lg n + b, \\end{aligned} $$ since we can choose $a$ large enough so that $\\frac{a}{4}n$ dominates $\\Theta(n) + b$. Thus, $P(n) = O(n\\lg n)$. f. We draw an analogy between inserting an element into a subtree of a binary search tree and sorting a subarray in quicksort. Observe that once an element $x$ is chosen as the root of a subtree $T$, all elements that will be inserted after $x$ into $T$ will be compared to $x$. Similarly, observe that once an element $y$ is chosen as the pivot in a subarray $S$, all other elements in $S$ will be compared to $y$. Therefore, the quicksort implementation in which the comparisons are the same as those made when inserting into a binary search tree is simply to consider the pivots in the same order as the order in which the elements are inserted into the tree.","title":"12-3 Average node depth in a randomly built binary search tree"},{"location":"Chap12/Problems/12-4/","text":"Let $b_n$ denote the number of different binary trees with $n$ nodes. In this problem, you will find a formula for $b_n$, as well as an asymptotic estimate. a. Show that $b_0 = 1$ and that, for $n \\ge 1$, $$b_n = \\sum_{k = 0}^{n - 1} b_k b_{n - 1 - k}.$$ b. Referring to Problem 4-4 for the definition of a generating function, let $B(x)$ be the generating function $$B(x) = \\sum_{n = 0}^\\infty b_n x^n.$$ Show that $B(x) = xB(x)^2 + 1$, and hence one way to express $B(x)$ in closed form is $$B(x) = \\frac{1}{2x} (1 - \\sqrt{1 - 4x}).$$ The Taylor expansion of $f(x)$ around the point $x = a$ is given by $$f(x) = \\sum_{k = 0}^\\infty \\frac{f^{(k)}(a)}{k!} (x - a)^k,$$ where $f^{(k)}(x)$ is the $k$th derivative of $f$ evaluated at $x$. c. Show that $$b_n = \\frac{1}{n + 1} \\binom{2n}{n}$$ (the $n$th Catalan number ) by using the Taylor expansion of $\\sqrt{1 - 4x}$ around $x = 0$. (If you wish, instead of using the Taylor expansion, you may use the generalization of the binomial expansion (C.4) to nonintegral exponents $n$, where for any real number $n$ and for any integer $k$, we interpret $\\binom{n}{k}$ to be $n(n - 1) \\cdots (n - k + 1) / k!$ if $k \\ge 0$, and $0$ otherwise.) d. Show that $$b_n = \\frac{4^n}{\\sqrt{\\pi}n^{3 / 2}} (1 + O(1 / n)).$$ a. A root with two subtree. b. $$ \\begin{aligned} B(x)^2 & = (b_0 x^0 + b_1 x^1 + b_2 x^2 + \\cdots)^2 \\\\ & = b_0^2 x^0 + (b_0 b_1 + b_1 b_0) x^1 + (b_0 b_2 + b_1 b_1 + b_2 b_0) x^2 + \\cdots \\\\ & = \\sum_{k = 0}^0 b_k b_{0 - k} x^0 + \\sum_{k = 0}^1 b_k b_{1 - k} x^1 + \\sum_{k = 0}^2 b_k b_{2 - k} x^2 + \\cdots \\end{aligned} $$ $$ \\begin{aligned} xB(x)^2 + 1 & = 1 + \\sum_{k = 0}^0 b_k b_{1 - 1 - k} x^1 + \\sum_{k = 0}^2 b_k b_{2-1 - k} x^3 + \\sum_{k = 0}^2 b_k b_{3-1 - k} x^2 + \\cdots \\\\ & = 1 + b_1 x^1 + b_2 x^2 + b_3 x^3 + \\cdots \\\\ & = b_0 x^0 + b_1 x^1 + b_2 x^2 + b_3 x^3 + \\cdots \\\\ & = \\sum_{n = 0}^\\infty b_n x^n \\\\ & = B(x). \\end{aligned} $$ $$ \\begin{aligned} x B(x)^2 + 1 & = x \\cdot \\frac{1}{4x^2} (1 + 1 - 4x - 2\\sqrt{1 - 4x}) + 1 \\\\ & = \\frac{1}{4x} (2 - 2\\sqrt{1 - 4x}) - 1 + 1 \\\\ & = \\frac{1}{2x} (1 - \\sqrt{1 - 4x}) \\\\ & = B(x). \\end{aligned} $$ c. Let $f(x) = \\sqrt{1 - 4x}$, the numerator of the derivative is $$ \\begin{aligned} 2 \\cdot (1 \\cdot 2) \\cdot (3 \\cdot 2) \\cdot (5 \\cdot 2) \\cdots & = 2^k \\cdot \\prod_{i = 0}^{k - 2} (2k + 1) \\\\ & = 2^k \\cdot \\frac{(2(k - 1))!}{2^{k - 1}(k - 1)!} \\\\ & = \\frac{2(2(k - 1))!}{(k - 1)!}. \\end{aligned} $$ $$f(x) = 1 - 2x - 2x^2 - 4 x^3 - 10x^4 - 28x^5 - \\cdots.$$ The coefficient is $\\frac{2(2(k - 1))!}{k!(k - 1)!}$. $$ \\begin{aligned} B(x) & = \\frac{1}{2x}(1 - f(x)) \\\\ & = 1 + x + 2x^2 + 5x^3 + 14x^4 + \\cdots \\\\ & = \\sum_{n = 0}^\\infty \\frac{(2n)!}{(n + 1)!n!} x \\\\ & = \\sum_{n = 0}^\\infty \\frac{1}{n + 1} \\frac{(2n)!}{n!n!} x \\\\ & = \\sum_{n = 0}^\\infty \\frac{1}{n + 1} \\binom{2n}{n} x. \\end{aligned} $$ $$b_n = \\frac{1}{n + 1} \\binom{2n}{n}.$$ d. $$ \\begin{aligned} b_n & = \\frac{1}{n + 1} \\frac{(2n)!}{n!n!} \\\\ & \\approx \\frac{1}{n + 1} \\frac{\\sqrt{4 \\pi n}(2n / e)^{2n}}{2 \\pi n (n / e)^{2n}} \\\\ & = \\frac{1}{n + 1} \\frac{4^n}{\\sqrt{\\pi n} } \\\\ & = (\\frac{1}{n} + (\\frac{1}{n + 1} - \\frac{1}{n})) \\frac{4^n}{\\sqrt{\\pi n}} \\\\ & = (\\frac{1}{n} - \\frac{1}{n^2 + n}) \\frac{4^n}{\\sqrt{\\pi n}} \\\\ & = \\frac{1}{n} (1 - \\frac{1}{n + 1}) \\frac{4^n}{\\sqrt{\\pi n}} \\\\ & = \\frac{4^n}{\\sqrt{\\pi}n^{3 / 2}} (1 + O(1 / n)). \\end{aligned} $$","title":"12-4 Number of different binary trees"},{"location":"Chap13/13.1/","text":"13.1-1 In the style of Figure 13.1(a), draw the complete binary search tree of height $3$ on the keys $\\{1, 2, \\ldots, 15\\}$. Add the $\\text{NIL}$ leaves and color the nodes in three different ways such that the black-heights of the resulting red-black trees are $2$, $3$, and $4$. Complete binary tree of $height = 3$: Red-black tree of $black\\text-heights = 2$: Red-black tree of $black\\text-heights = 3$: Red-black tree of $black\\text-heights = 4$: 13.1-2 Draw the red-black tree that results after $\\text{TREE-INSERT}$ is called on the tree in Figure 13.1 with key $36$. If the inserted node is colored red, is the resulting tree a red-black tree? What if it is colored black? If the inserted node is colored red, the tree doesn't satisfy property 4 because $35$ will be the parent of $36$, which is also colored red. If the inserted node is colored black, the tree doesn't satisfy property 5 because there will be two paths from node $38$ to $T.nil$ which contain different numbers of black nodes. We don't draw the wrong red-black tree; however, we draw the adjusted correct tree: 13.1-3 Let us define a relaxed red-black tree as a binary search tree that satisfies red-black properties 1, 3, 4, and 5. In other words, the root may be either red or black. Consider a relaxed red-black tree $T$ whose root is red. If we color the root of $T$ black but make no other changes to $T$, is the resulting tree a red-black tree? If we color the root of a relaxed red-black tree black but make no other changes, the resulting tree is a red-black tree. Not even any black-heights change. 13.1-4 Suppose that we \"absorb\" every red node in a red-black tree into its black parent, so that the children of the red node become children of the black parent. (Ignore what happens to the keys.) What are the possible degrees of a black node after all its red children are absorbed? What can you say about the depths of the leaves of the resulting tree? After absorbing each red node into its black parent, the degree of each node black node is $2$, if both children were already black, $3$, if one child was black and one was red, or $4$, if both children were red. All leaves of the resulting tree have the same depth. 13.1-5 Show that the longest simple path from a node $x$ in a red-black tree to a descendant leaf has length at most twice that of the shortest simple path from node $x$ to a descendant leaf. In the longest path, at least every other node is black. In the shortest path, at most every node is black. Since the two paths contain equal numbers of black nodes, the length of the longest path is at most twice the length of the shortest path. We can say this more precisely, as follows: Since every path contains $\\text{bh}(x)$ black nodes, even the shortest path from $x$ to a descendant leaf has length at least $\\text{bh}(x)$. By definition, the longest path from $x$ to a descendant leaf has length $\\text{height}(x)$. Since the longest path has $\\text{bh}(x)$ black nodes and at least half the nodes on the longest path are black (by property 4), $\\text{bh}(x) \\ge \\text{height}(x) / 2$, so length of longest path $= \\text{height}(x) \\le 2 \\cdot \\text{bh}(x) \\le$ twice length of shortest path. 13.1-6 What is the largest possible number of internal nodes in a red-black tree with black-height $k$? What is the smallest possible number? The largest is a path with half black nodes and half red nodes, which has $2^{2k} - 1$ internal nodes. The smallest is a path with all black nodes, which has $2^k - 1$ internal nodes. 13.1-7 Describe a red-black tree on $n$ keys that realizes the largest possible ratio of red internal nodes to black internal nodes. What is this ratio? What tree has the smallest possible ratio, and what is the ratio? The largest ratio is $2$, each black node has two red children. The smallest ratio is $0$.","title":"13.1 Properties of red-black trees"},{"location":"Chap13/13.1/#131-1","text":"In the style of Figure 13.1(a), draw the complete binary search tree of height $3$ on the keys $\\{1, 2, \\ldots, 15\\}$. Add the $\\text{NIL}$ leaves and color the nodes in three different ways such that the black-heights of the resulting red-black trees are $2$, $3$, and $4$. Complete binary tree of $height = 3$: Red-black tree of $black\\text-heights = 2$: Red-black tree of $black\\text-heights = 3$: Red-black tree of $black\\text-heights = 4$:","title":"13.1-1"},{"location":"Chap13/13.1/#131-2","text":"Draw the red-black tree that results after $\\text{TREE-INSERT}$ is called on the tree in Figure 13.1 with key $36$. If the inserted node is colored red, is the resulting tree a red-black tree? What if it is colored black? If the inserted node is colored red, the tree doesn't satisfy property 4 because $35$ will be the parent of $36$, which is also colored red. If the inserted node is colored black, the tree doesn't satisfy property 5 because there will be two paths from node $38$ to $T.nil$ which contain different numbers of black nodes. We don't draw the wrong red-black tree; however, we draw the adjusted correct tree:","title":"13.1-2"},{"location":"Chap13/13.1/#131-3","text":"Let us define a relaxed red-black tree as a binary search tree that satisfies red-black properties 1, 3, 4, and 5. In other words, the root may be either red or black. Consider a relaxed red-black tree $T$ whose root is red. If we color the root of $T$ black but make no other changes to $T$, is the resulting tree a red-black tree? If we color the root of a relaxed red-black tree black but make no other changes, the resulting tree is a red-black tree. Not even any black-heights change.","title":"13.1-3"},{"location":"Chap13/13.1/#131-4","text":"Suppose that we \"absorb\" every red node in a red-black tree into its black parent, so that the children of the red node become children of the black parent. (Ignore what happens to the keys.) What are the possible degrees of a black node after all its red children are absorbed? What can you say about the depths of the leaves of the resulting tree? After absorbing each red node into its black parent, the degree of each node black node is $2$, if both children were already black, $3$, if one child was black and one was red, or $4$, if both children were red. All leaves of the resulting tree have the same depth.","title":"13.1-4"},{"location":"Chap13/13.1/#131-5","text":"Show that the longest simple path from a node $x$ in a red-black tree to a descendant leaf has length at most twice that of the shortest simple path from node $x$ to a descendant leaf. In the longest path, at least every other node is black. In the shortest path, at most every node is black. Since the two paths contain equal numbers of black nodes, the length of the longest path is at most twice the length of the shortest path. We can say this more precisely, as follows: Since every path contains $\\text{bh}(x)$ black nodes, even the shortest path from $x$ to a descendant leaf has length at least $\\text{bh}(x)$. By definition, the longest path from $x$ to a descendant leaf has length $\\text{height}(x)$. Since the longest path has $\\text{bh}(x)$ black nodes and at least half the nodes on the longest path are black (by property 4), $\\text{bh}(x) \\ge \\text{height}(x) / 2$, so length of longest path $= \\text{height}(x) \\le 2 \\cdot \\text{bh}(x) \\le$ twice length of shortest path.","title":"13.1-5"},{"location":"Chap13/13.1/#131-6","text":"What is the largest possible number of internal nodes in a red-black tree with black-height $k$? What is the smallest possible number? The largest is a path with half black nodes and half red nodes, which has $2^{2k} - 1$ internal nodes. The smallest is a path with all black nodes, which has $2^k - 1$ internal nodes.","title":"13.1-6"},{"location":"Chap13/13.1/#131-7","text":"Describe a red-black tree on $n$ keys that realizes the largest possible ratio of red internal nodes to black internal nodes. What is this ratio? What tree has the smallest possible ratio, and what is the ratio? The largest ratio is $2$, each black node has two red children. The smallest ratio is $0$.","title":"13.1-7"},{"location":"Chap13/13.2/","text":"13.2-1 Write pseudocode for $\\text{RIGHT-ROTATE}$. 1 2 3 4 5 6 7 8 9 10 11 12 13 RIGHT - ROTATE ( T , y ) x = y . left y . left = x . right if x . right != T . nil x . right . p = y x . p = y . p if y . p == T . nil T . root = x else if y == y . p . right y . p . right = x else y . p . left = x x . right = y y . p = x 13.2-2 Argue that in every $n$-node binary search tree, there are exactly $n - 1$ possible rotations. Every node can rotate with its parent, only the root does not have a parent, therefore there are $n - 1$ possible rotations. 13.2-3 Let $a$, $b$, and $c$ be arbitrary nodes in subtrees $\\alpha$, $\\beta$, and $\\gamma$, respectively, in the left tree of Figure 13.2. How do the depths of $a$, $b$, and $c$ change when a left rotation is performed on node $x$ in the figure? $a$: increase by $1$. $b$: unchanged. $c$: decrease by $1$. 13.2-4 Show that any arbitrary $n$-node binary search tree can be transformed into any other arbitrary $n$-node binary search tree using $O(n)$ rotations. ($\\textit{Hint:}$ First show that at most $n - 1$ right rotations suffice to transform the tree into a right-going chain.) Since the exercise asks about binary search trees rather than the more specific redblack trees, we assume here that leaves are full-fledged nodes, and we ignore the sentinels. Taking the book's hint, we start by showing that with at most $n - 1$ right rotations, we can convert any binary search tree into one that is just a right-going chain. The idea is simple. Let us define the right spine as the root and all descendants of the root that are reachable by following only right pointers from the root. A binary search tree that is just a right-going chain has all n nodes in the right spine. As long as the tree is not just a right spine, repeatedly find some node $y$ on the right spine that has a non-leaf left child $x$ and then perform a right rotation on $y$: (In the above figure, note that any of $\\alpha$, $\\beta$, and $\\gamma$ can be an empty subtree.) Observe that this right rotation adds $x$ to the right spine, and no other nodes leave the right spine. Thus, this right rotation increases the number of nodes in the right spine by $1$. Any binary search tree starts out with at least one node\u2014the root\u2014in the right spine. Moreover, if there are any nodes not on the right spine, then at least one such node has a parent on the right spine. Thus, at most $n - 1$ right rotations are needed to put all nodes in the right spine, so that the tree consists of a single right-going chain. If we knew the sequence of right rotations that transforms an arbitrary binary search tree $T$ to a single right-going chain $T'$, then we could perform this sequence in reverse\u2014turning each right rotation into its inverse left rotation\u2014to transform $T'$ back into $T$. Therefore, here is how we can transform any binary search tree $T_1$ into any other binary search tree $T_2$. Let $T'$ be the unique right-going chain consisting of the nodes of $T_1$ (which is the same as the nodes of $T_2$). Let $r = \\langle r_1, r_2, \\ldots, r_k \\rangle$ be a sequence of right rotations that transforms $T_1$ to $T'$, and let $r' = \\langle r_1', r_2', \\ldots, r_{k'}' \\rangle$ be a sequence of right rotations that transforms $T_2$ to $T'$. We know that there exist sequences $r$ and $r'$ with $k$, $k' \\le n - 1$. For each right rotation $r_i'$, let $l_i'$ be the corresponding inverse left rotation. Then the sequence $\\langle r_1, r_2, \\ldots, r_k, l_{k'}', l_{k' - 1}', \\ldots, l_2', l_1' \\rangle$ transforms $T_1$ to $T_2$ in at most $2n - 2$ rotations. 13.2-5 $\\star$ We say that a binary search tree $T_1$ can be right-converted to binary search tree $T_2$ if it is possible to obtain $T_2$ from $T_1$ via a series of calls to $\\text{RIGHT-ROTATE}$. Give an example of two trees $T_1$ and $T_2$ such that $T_1$ cannot be right-converted to $T_2$. Then, show that if a tree $T_1$ can be right-converted to $T_2$, it can be right-converted using $O(n^2)$ calls to $\\text{RIGHT-ROTATE}$. We can use $O(n)$ calls to rotate the node which is the root in $T_2$ to $T_1$'s root, then use the same operation in the two subtrees. There are $n$ nodes, therefore the upper bound is $O(n^2)$.","title":"13.2 Rotations"},{"location":"Chap13/13.2/#132-1","text":"Write pseudocode for $\\text{RIGHT-ROTATE}$. 1 2 3 4 5 6 7 8 9 10 11 12 13 RIGHT - ROTATE ( T , y ) x = y . left y . left = x . right if x . right != T . nil x . right . p = y x . p = y . p if y . p == T . nil T . root = x else if y == y . p . right y . p . right = x else y . p . left = x x . right = y y . p = x","title":"13.2-1"},{"location":"Chap13/13.2/#132-2","text":"Argue that in every $n$-node binary search tree, there are exactly $n - 1$ possible rotations. Every node can rotate with its parent, only the root does not have a parent, therefore there are $n - 1$ possible rotations.","title":"13.2-2"},{"location":"Chap13/13.2/#132-3","text":"Let $a$, $b$, and $c$ be arbitrary nodes in subtrees $\\alpha$, $\\beta$, and $\\gamma$, respectively, in the left tree of Figure 13.2. How do the depths of $a$, $b$, and $c$ change when a left rotation is performed on node $x$ in the figure? $a$: increase by $1$. $b$: unchanged. $c$: decrease by $1$.","title":"13.2-3"},{"location":"Chap13/13.2/#132-4","text":"Show that any arbitrary $n$-node binary search tree can be transformed into any other arbitrary $n$-node binary search tree using $O(n)$ rotations. ($\\textit{Hint:}$ First show that at most $n - 1$ right rotations suffice to transform the tree into a right-going chain.) Since the exercise asks about binary search trees rather than the more specific redblack trees, we assume here that leaves are full-fledged nodes, and we ignore the sentinels. Taking the book's hint, we start by showing that with at most $n - 1$ right rotations, we can convert any binary search tree into one that is just a right-going chain. The idea is simple. Let us define the right spine as the root and all descendants of the root that are reachable by following only right pointers from the root. A binary search tree that is just a right-going chain has all n nodes in the right spine. As long as the tree is not just a right spine, repeatedly find some node $y$ on the right spine that has a non-leaf left child $x$ and then perform a right rotation on $y$: (In the above figure, note that any of $\\alpha$, $\\beta$, and $\\gamma$ can be an empty subtree.) Observe that this right rotation adds $x$ to the right spine, and no other nodes leave the right spine. Thus, this right rotation increases the number of nodes in the right spine by $1$. Any binary search tree starts out with at least one node\u2014the root\u2014in the right spine. Moreover, if there are any nodes not on the right spine, then at least one such node has a parent on the right spine. Thus, at most $n - 1$ right rotations are needed to put all nodes in the right spine, so that the tree consists of a single right-going chain. If we knew the sequence of right rotations that transforms an arbitrary binary search tree $T$ to a single right-going chain $T'$, then we could perform this sequence in reverse\u2014turning each right rotation into its inverse left rotation\u2014to transform $T'$ back into $T$. Therefore, here is how we can transform any binary search tree $T_1$ into any other binary search tree $T_2$. Let $T'$ be the unique right-going chain consisting of the nodes of $T_1$ (which is the same as the nodes of $T_2$). Let $r = \\langle r_1, r_2, \\ldots, r_k \\rangle$ be a sequence of right rotations that transforms $T_1$ to $T'$, and let $r' = \\langle r_1', r_2', \\ldots, r_{k'}' \\rangle$ be a sequence of right rotations that transforms $T_2$ to $T'$. We know that there exist sequences $r$ and $r'$ with $k$, $k' \\le n - 1$. For each right rotation $r_i'$, let $l_i'$ be the corresponding inverse left rotation. Then the sequence $\\langle r_1, r_2, \\ldots, r_k, l_{k'}', l_{k' - 1}', \\ldots, l_2', l_1' \\rangle$ transforms $T_1$ to $T_2$ in at most $2n - 2$ rotations.","title":"13.2-4"},{"location":"Chap13/13.2/#132-5-star","text":"We say that a binary search tree $T_1$ can be right-converted to binary search tree $T_2$ if it is possible to obtain $T_2$ from $T_1$ via a series of calls to $\\text{RIGHT-ROTATE}$. Give an example of two trees $T_1$ and $T_2$ such that $T_1$ cannot be right-converted to $T_2$. Then, show that if a tree $T_1$ can be right-converted to $T_2$, it can be right-converted using $O(n^2)$ calls to $\\text{RIGHT-ROTATE}$. We can use $O(n)$ calls to rotate the node which is the root in $T_2$ to $T_1$'s root, then use the same operation in the two subtrees. There are $n$ nodes, therefore the upper bound is $O(n^2)$.","title":"13.2-5 $\\star$"},{"location":"Chap13/13.3/","text":"13.3-1 In line 16 of $\\text{RB-INSERT}$, we set the color of the newly inserted node $z$ to red. Observe that if we had chosen to set $z$'s color to black, then property 4 of a red-black tree would not be violated. Why didn't we choose to set $z$'s color to black? If we chose to set the color of $z$ to black then we would be violating property 5 of being a red-black tree. Because any path from the root to a leaf under $z$ would have one more black node than the paths to the other leaves. 13.3-2 Show the red-black trees that result after successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty red-black tree. insert $41$: insert $38$: insert $31$: insert $12$: insert $19$: insert $8$: 13.3-3 Suppose that the black-height of each of the subtrees $\\alpha, \\beta, \\gamma, \\delta, \\epsilon$ in Figures 13.5 and 13.6 is $k$. Label each node in each figure with its black-height to verify that the indicated transformation preserves property 5. In Figure 13.5, nodes $A$, $B$, and $D$ have black-height $k + 1$ in all cases, because each of their subtrees has black-height $k$ and a black root. Node $C$ has black-height $k + 1$ on the left (because its red children have black-height $k + 1$) and black-height $k + 2$ on the right (because its black children have black-height $k + 1$). In Figure 13.6, nodes $A$, $B$, and $C$ have black-height $k + 1$ in all cases. At left and in the middle, each of $A$'s and $B$'s subtrees has black-height $k$ and a black root, while $C$ has one such subtree and a red child with black-height $k + 1$. At the right, each of $A$'s and $C$'s subtrees has black-height $k$ and a black root, while $B$'s red children each have black-height $k + 1$. Property 5 is preserved by the transformations. We have shown above that the black-height is well-defined within the subtrees pictured, so property 5 is preserved within those subtrees. Property 5 is preserved for the tree containing the subtrees pictured, because every path through these subtrees to a leaf contributes $k + 2$ black nodes. 13.3-4 Professor Teach is concerned that $\\text{RB-INSERT-FIXUP}$ might set $T.nil.color$ to $\\text{RED}$, in which case the test in line 1 would not cause the loop to terminate when $z$ is the root. Show that the professor's concern is unfounded by arguing that $\\text{RB-INSERT-FIXUP}$ never sets $T.nil.color$ to $\\text{RED}$. Colors are set to red only in cases 1 and 3, and in both situations, it is $z.p.p$ that is reddened. If $z.p.p$ is the sentinel, then $z.p$ is the root. By part (b) of the loop invariant and line 1 of $\\text{RB-INSERT-FIXUP}$, if $z.p$ is the root, then we have dropped out of the loop. The only subtlety is in case 2, where we set $z = z.p$ before coloring $z.p.p$ red. Because we rotate before the recoloring, the identity of $z.p.p$ is the same before and after case 2, so there's no problem. 13.3-5 Consider a red-black tree formed by inserting $n$ nodes with $\\text{RB-INSERT}$. Argue that if $n > 1$, the tree has at least one red node. Case 1: $z$ and $z.p.p$ are $\\text{RED}$, if the loop terminates, then $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Case 2: $z$ and $z.p$ are $\\text{RED}$, and after the rotation $z.p$ could not be the root, thus $z.p$ is $\\text{RED}$ after the fix up. Case 3: $z$ is $\\text{RED}$ and $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Therefore, there is always at least one red node. 13.3-6 Suggest how to implement $\\text{RB-INSERT}$ efficiently if the representation for red-black trees includes no storage for parent pointers. Use stack to record the path to the inserted node, then parent is the top element in the stack. Case 1: we pop $z.p$ and $z.p.p$. Case 2: we pop $z.p$ and $z.p.p$, then push $z.p.p$ and $z$. Case 3: we pop $z.p$, $z.p.p$ and $z.p.p.p$, then push $z.p$.","title":"13.3 Insertion"},{"location":"Chap13/13.3/#133-1","text":"In line 16 of $\\text{RB-INSERT}$, we set the color of the newly inserted node $z$ to red. Observe that if we had chosen to set $z$'s color to black, then property 4 of a red-black tree would not be violated. Why didn't we choose to set $z$'s color to black? If we chose to set the color of $z$ to black then we would be violating property 5 of being a red-black tree. Because any path from the root to a leaf under $z$ would have one more black node than the paths to the other leaves.","title":"13.3-1"},{"location":"Chap13/13.3/#133-2","text":"Show the red-black trees that result after successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty red-black tree. insert $41$: insert $38$: insert $31$: insert $12$: insert $19$: insert $8$:","title":"13.3-2"},{"location":"Chap13/13.3/#133-3","text":"Suppose that the black-height of each of the subtrees $\\alpha, \\beta, \\gamma, \\delta, \\epsilon$ in Figures 13.5 and 13.6 is $k$. Label each node in each figure with its black-height to verify that the indicated transformation preserves property 5. In Figure 13.5, nodes $A$, $B$, and $D$ have black-height $k + 1$ in all cases, because each of their subtrees has black-height $k$ and a black root. Node $C$ has black-height $k + 1$ on the left (because its red children have black-height $k + 1$) and black-height $k + 2$ on the right (because its black children have black-height $k + 1$). In Figure 13.6, nodes $A$, $B$, and $C$ have black-height $k + 1$ in all cases. At left and in the middle, each of $A$'s and $B$'s subtrees has black-height $k$ and a black root, while $C$ has one such subtree and a red child with black-height $k + 1$. At the right, each of $A$'s and $C$'s subtrees has black-height $k$ and a black root, while $B$'s red children each have black-height $k + 1$. Property 5 is preserved by the transformations. We have shown above that the black-height is well-defined within the subtrees pictured, so property 5 is preserved within those subtrees. Property 5 is preserved for the tree containing the subtrees pictured, because every path through these subtrees to a leaf contributes $k + 2$ black nodes.","title":"13.3-3"},{"location":"Chap13/13.3/#133-4","text":"Professor Teach is concerned that $\\text{RB-INSERT-FIXUP}$ might set $T.nil.color$ to $\\text{RED}$, in which case the test in line 1 would not cause the loop to terminate when $z$ is the root. Show that the professor's concern is unfounded by arguing that $\\text{RB-INSERT-FIXUP}$ never sets $T.nil.color$ to $\\text{RED}$. Colors are set to red only in cases 1 and 3, and in both situations, it is $z.p.p$ that is reddened. If $z.p.p$ is the sentinel, then $z.p$ is the root. By part (b) of the loop invariant and line 1 of $\\text{RB-INSERT-FIXUP}$, if $z.p$ is the root, then we have dropped out of the loop. The only subtlety is in case 2, where we set $z = z.p$ before coloring $z.p.p$ red. Because we rotate before the recoloring, the identity of $z.p.p$ is the same before and after case 2, so there's no problem.","title":"13.3-4"},{"location":"Chap13/13.3/#133-5","text":"Consider a red-black tree formed by inserting $n$ nodes with $\\text{RB-INSERT}$. Argue that if $n > 1$, the tree has at least one red node. Case 1: $z$ and $z.p.p$ are $\\text{RED}$, if the loop terminates, then $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Case 2: $z$ and $z.p$ are $\\text{RED}$, and after the rotation $z.p$ could not be the root, thus $z.p$ is $\\text{RED}$ after the fix up. Case 3: $z$ is $\\text{RED}$ and $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Therefore, there is always at least one red node.","title":"13.3-5"},{"location":"Chap13/13.3/#133-6","text":"Suggest how to implement $\\text{RB-INSERT}$ efficiently if the representation for red-black trees includes no storage for parent pointers. Use stack to record the path to the inserted node, then parent is the top element in the stack. Case 1: we pop $z.p$ and $z.p.p$. Case 2: we pop $z.p$ and $z.p.p$, then push $z.p.p$ and $z$. Case 3: we pop $z.p$, $z.p.p$ and $z.p.p.p$, then push $z.p$.","title":"13.3-6"},{"location":"Chap13/13.4/","text":"13.4-1 Argue that after executing $\\text{RB-DELETE-FIXUP}$, the root of the tree must be black. Case 1: transform to 2, 3, 4. Case 2: if terminates, the root of the subtree (the new $x$) is set to black. Case 3: transform to 4. Case 4: the root (the new $x$) is set to black. 13.4-2 Argue that if in $\\text{RB-DELETE}$ both $x$ and $x.p$ are red, then property 4 is restored by the call to $\\text{RB-DELETE-FIXUP}(T, x)$. Suppose that both $x$ and $x.p$ are red in $\\text{RB-DELETE}$. This can only happen in the else-case of line 9. Since we are deleting from a red-black tree, the other child of y.p which becomes $x$'s sibling in the call to $\\text{RB-TRANSPLANT}$ on line 14 must be black, so $x$ is the only child of $x.p$ which is red. The while-loop condition of $\\text{RB-DELETE-FIXUP}(T, x)$ is immediately violated so we simply set $x.color = black$, restoring property 4. 13.4-3 In Exercise 13.3-2, you found the red-black tree that results from successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty tree. Now show the red-black trees that result from the successive deletion of the keys in the order $8, 12, 19, 31, 38, 41$. initial: delete $8$: delete $12$: delete $19$: delete $31$: delete $38$: delete $41$: 13.4-4 In which lines of the code for $\\text{RB-DELETE-FIXUP}$ might we examine or modify the sentinel $T.nil$? Since it is possible that $w$ is $T.nil$, any line of $\\text{RB-DELETE-FIXUP}(T, x)$ which examines or modifies w must be included. However, as described on page 317, $x$ will never be $T.nil$, so we need not include those lines. 13.4-5 In each of the cases of Figure 13.7, give the count of black nodes from the root of the subtree shown to each of the subtrees $\\alpha, \\beta, \\ldots, \\zeta$, and verify that each count remains the same after the transformation. When a node has a $color$ attribute $c$ or $c'$, use the notation $\\text{count}(c)$ or $\\text{count}(c')$ symbolically in your count. Our count will include the root (if it is black). Case 1: For each subtree, it is $2$ both before and after. Case 2: For $\\alpha$ and $\\beta$, it is $1 + \\text{count}(c)$ in both cases. For the rest of the subtrees, it is from $2 + \\text{count}(c)$ to $1 + \\text{count}(c)$. This decrease in the count for the other subtreese is handled by then having $x$ represent an additional black. Case 3: For $\\epsilon$ and $\\zeta$, it is $2+\\text{count}(c)$ both before and after. For all the other subtrees, it is $1+\\text{count}(c)$ both before and after. Case 4: For $\\alpha$ and $\\beta$, it is from $1 + \\text{count}(c)$ to $2 + \\text{count}(c)$. For $\\gamma$ and $\\delta$, it is $1 + \\text{count}(c) + \\text{count}(c')$ both before and after. For $\\epsilon$ and $\\zeta$, it is $1 + \\text{count}(c)$ both before and after. This increase in the count for $\\alpha$ and $\\beta$ is because $x$ before indicated an extra black. 13.4-6 Professors Skelton and Baron are concerned that at the start of case 1 of $\\text{RB-DELETE-FIXUP}$, the node $x.p$ might not be black. If the professors are correct, then lines 5\u20136 are wrong. Show that $x.p$ must be black at the start of case 1, so that the professors have nothing to worry about. Case 1 occurs only if $x$'s sibling $w$ is red. If $x.p$ were red, then there would be two reds in a row, namely $x.p$ (which is also $w.p$) and $w$, and we would have had these two reds in a row even before calling $\\text{RB-DELETE}$. 13.4-7 Suppose that a node $x$ is inserted into a red-black tree with $\\text{RB-INSERT}$ and then is immediately deleted with $\\text{RB-DELETE}$. Is the resulting red-black tree the same as the initial red-black tree? Justify your answer. No, the red-black tree will not necessarily be the same. Here are two examples: one in which the tree's shape changes, and one in which the shape remains the same but the node colors change.","title":"13.4 Deletion"},{"location":"Chap13/13.4/#134-1","text":"Argue that after executing $\\text{RB-DELETE-FIXUP}$, the root of the tree must be black. Case 1: transform to 2, 3, 4. Case 2: if terminates, the root of the subtree (the new $x$) is set to black. Case 3: transform to 4. Case 4: the root (the new $x$) is set to black.","title":"13.4-1"},{"location":"Chap13/13.4/#134-2","text":"Argue that if in $\\text{RB-DELETE}$ both $x$ and $x.p$ are red, then property 4 is restored by the call to $\\text{RB-DELETE-FIXUP}(T, x)$. Suppose that both $x$ and $x.p$ are red in $\\text{RB-DELETE}$. This can only happen in the else-case of line 9. Since we are deleting from a red-black tree, the other child of y.p which becomes $x$'s sibling in the call to $\\text{RB-TRANSPLANT}$ on line 14 must be black, so $x$ is the only child of $x.p$ which is red. The while-loop condition of $\\text{RB-DELETE-FIXUP}(T, x)$ is immediately violated so we simply set $x.color = black$, restoring property 4.","title":"13.4-2"},{"location":"Chap13/13.4/#134-3","text":"In Exercise 13.3-2, you found the red-black tree that results from successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty tree. Now show the red-black trees that result from the successive deletion of the keys in the order $8, 12, 19, 31, 38, 41$. initial: delete $8$: delete $12$: delete $19$: delete $31$: delete $38$: delete $41$:","title":"13.4-3"},{"location":"Chap13/13.4/#134-4","text":"In which lines of the code for $\\text{RB-DELETE-FIXUP}$ might we examine or modify the sentinel $T.nil$? Since it is possible that $w$ is $T.nil$, any line of $\\text{RB-DELETE-FIXUP}(T, x)$ which examines or modifies w must be included. However, as described on page 317, $x$ will never be $T.nil$, so we need not include those lines.","title":"13.4-4"},{"location":"Chap13/13.4/#134-5","text":"In each of the cases of Figure 13.7, give the count of black nodes from the root of the subtree shown to each of the subtrees $\\alpha, \\beta, \\ldots, \\zeta$, and verify that each count remains the same after the transformation. When a node has a $color$ attribute $c$ or $c'$, use the notation $\\text{count}(c)$ or $\\text{count}(c')$ symbolically in your count. Our count will include the root (if it is black). Case 1: For each subtree, it is $2$ both before and after. Case 2: For $\\alpha$ and $\\beta$, it is $1 + \\text{count}(c)$ in both cases. For the rest of the subtrees, it is from $2 + \\text{count}(c)$ to $1 + \\text{count}(c)$. This decrease in the count for the other subtreese is handled by then having $x$ represent an additional black. Case 3: For $\\epsilon$ and $\\zeta$, it is $2+\\text{count}(c)$ both before and after. For all the other subtrees, it is $1+\\text{count}(c)$ both before and after. Case 4: For $\\alpha$ and $\\beta$, it is from $1 + \\text{count}(c)$ to $2 + \\text{count}(c)$. For $\\gamma$ and $\\delta$, it is $1 + \\text{count}(c) + \\text{count}(c')$ both before and after. For $\\epsilon$ and $\\zeta$, it is $1 + \\text{count}(c)$ both before and after. This increase in the count for $\\alpha$ and $\\beta$ is because $x$ before indicated an extra black.","title":"13.4-5"},{"location":"Chap13/13.4/#134-6","text":"Professors Skelton and Baron are concerned that at the start of case 1 of $\\text{RB-DELETE-FIXUP}$, the node $x.p$ might not be black. If the professors are correct, then lines 5\u20136 are wrong. Show that $x.p$ must be black at the start of case 1, so that the professors have nothing to worry about. Case 1 occurs only if $x$'s sibling $w$ is red. If $x.p$ were red, then there would be two reds in a row, namely $x.p$ (which is also $w.p$) and $w$, and we would have had these two reds in a row even before calling $\\text{RB-DELETE}$.","title":"13.4-6"},{"location":"Chap13/13.4/#134-7","text":"Suppose that a node $x$ is inserted into a red-black tree with $\\text{RB-INSERT}$ and then is immediately deleted with $\\text{RB-DELETE}$. Is the resulting red-black tree the same as the initial red-black tree? Justify your answer. No, the red-black tree will not necessarily be the same. Here are two examples: one in which the tree's shape changes, and one in which the shape remains the same but the node colors change.","title":"13.4-7"},{"location":"Chap13/Problems/13-1/","text":"During the course of an algorithm, we sometimes find that we need to maintain past versions of a dynamic set as it is updated. We call such a set persistent . One way to implement a persistent set is to copy the entire set whenever it is modified, but this approach can slow down a program and also consume much space. Sometimes, we can do much better. Consider a persistent set $S$ with the operations $\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$, which we implement using binary search trees as shown in Figure 13.8(a). We maintain a separate root for every version of the set. In order to insert the key $5$ into the set, we create a new node with key $5$. This node becomes the left child of a new node with key $7$, since we cannot modify the existing node with key $7$. Similarly, the new node with key $7$ becomes the left child of a new node with key $8$ whose right child is the existing node with key $10$. The new node with key $8$ becomes, in turn, the right child of a new root $r'$ with key $4$ whose left child is the existing node with key $3$. We thus copy only part of the tree and share some of the nodes with the original tree, as shown in Figure 13.8(b). Assume that each tree node has the attributes $key$, $left$, and $right$ but no parent. (See also Exercise 13.3-6.) a. For a general persistent binary search tree, identify the nodes that we need to change to insert a key $k$ or delete a node $y$. b. Write a procedure $\\text{PERSISTENT-TREE-INSERT}$ that, given a persistent tree $T$ and a key $k$ to insert, returns a new persistent tree $T'$ that is the result of inserting $k$ into $T$. c. If the height of the persistent binary search tree $T$ is $h$, what are the time and space requirements of your implementation of $\\text{PERSISTENT-TREE-INSERT}$? (The space requirement is proportional to the number of new nodes allocated.) d. Suppose that we had included the parent attribute in each node. In this case, $\\text{PERSISTENT-TREE-INSERT}$ would need to perform additional copying. Prove that $\\text{PERSISTENT-TREE-INSERT}$ would then require $\\Omega(n)$ time and space, where $n$ is the number of nodes in the tree. e. Show how to use red-black trees to guarantee that the worst-case running time and space are $O(\\lg n)$ per insertion or deletion. a. When inserting key $k$, all nodes on the path from the root to the added node (a new leaf) must change, since the need for a new child pointer propagates up from the new node to all of its ancestors. When deleting a node, let $y$ be the node actually removed and $z$ be the node given to the delete procedure. If $z$ has at most one child, it will be spliced out, so that all ancestors of $z$ swill be changed. (As with insertion, the need for a new child pointer propagates up from the removed node.) If $z$ has two children, then its successor $y$ will be spliced out and moved to $z$'s position. Therefore all ancestors of both $z$and $y$ must be changed. Because $z$is an ancestor of $y$, we can just say that all ancestors of $y$ must be changed. In either case, $y$'s children (if any) are unchanged, because we have assumed that there is no parent attribute. b. We assume that we can call two procedures: $\\text{MAKE-NEW-NODE}(k)$ creates a new node whose $key$ attribute has value $k$ and with $left$ and $right$ attributes $\\text{NIL}$, and it returns a pointer to the new node. $\\text{COPY-NODE}(x)$ creates a new node whose $key$, $left$, and $right$ attributes have the same values as those of node $x$, and it returns a pointer to the new node. Here are two ways to write $\\text{PERSISTENT-TREE-INSERT}$. The first is a version of $\\text{TREE-INSERT}$, modified to create new nodes along the path to where the new node will go, and to not use parent attributes. It returns the root of the new tree. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 PERSISTENT - TREE - INSERT ( T , k ) z = MAKE - NEW - NODE ( k ) new - root = COPY - NODE ( T . root ) y = NIL x = new - root while x != NIL y = x if z . key < x . key x = COPY - NODE ( x . left ) y . left = x else x = COPY - NODE ( x . right ) y . right = x if y == NIL new - root = z else if z . key < y . key y . left = z else y . right = z return new - root The second is a rather elegant recursive procedure. The initial call should have $T.root$ as its first argument. It returns the root of the new tree. 1 2 3 4 5 6 7 8 PERSISTENT - TREE - INSERT ( r , k ) if r == NIL x = MAKE - NEW - NODE ( k ) else x = COPY - NODE ( r ) if k < r . key x . left = PERSISTENT - TREE - INSERT ( r . left , k ) else x . right = PERSISTENT - TREE - INSERT ( r . right , k ) return x c. Like $\\text{TREE-INSERT}$, $\\text{PERSISTENT-TREE-INSERT}$ does a constant amount of work at each node along the path from the root to the new node. Since the length of the path is at most $h$, it takes $O(h)$ time. Since it allocates a new node (a constant amount of space) for each ancestor of the inserted node, it also needs $O(h)$ space. d. If there were parent attributes, then because of the new root, every node of the tree would have to be copied when a new node is inserted. To see why, observe that the children of the root would change to point to the new root, then their children would change to point to them, and so on. Since there are n nodes, this change would cause insertion to create \u007f$\\Omega(n)$ new nodes and to take \u007f$\\Omega(n)$ time. e. From parts (a) and (c), we know that insertion into a persistent binary search tree of height $h$, like insertion into an ordinary binary search tree, takes worst-case time $O(h)$. A red-black tree has $h = O(\\lg n)$, so insertion into an ordinary red-black tree takes $O(\\lg n)$ time. We need to show that if the red-black tree is persistent, insertion can still be done in $O(\\lg n)$ time. To do this, we will need to show two things: How to still find the parent pointers we need in $O(1)$ time without using a parent attribute. We cannot use a parent attribute because a persistent tree with parent attributes uses $\\Omega(n)$ time for insertion (by part (d)). That the additional node changes made during red-black tree operations (by rotation and recoloring) don't cause more than $O(\\lg n)$ additional nodes to change. Each parent pointer needed during insertion can be found in $O(1)$ time without having a parent attribute as follows: To insert into a red-black tree, we call $\\text{RB-INSERT}$, which in turn calls $\\text{RB-INSERT-FIXUP}$. Make the same changes to $\\text{RB-INSERT}$ as we made to $\\text{TREE-INSERT}$ for persistence. Additionally, as $\\text{RB-INSERT}$ walks down the tree to find the place to insert the new node, have it build a stack of the nodes it traverses and pass this stack to $\\text{RB-INSERT-FIXUP}$. $\\text{RB-INSERT-FIXUP}$ needs parent pointers to walk back up the same path, and at any given time it needs parent pointers only to find the parent and grandparent of the node it is working on. As $\\text{RB-INSERT-FIXUP}$ moves up the stack of parents, it needs only parent pointers that are at known locations a constant distance away in the stack. Thus, the parent information can be found in $O(1)$ time, just as if it were stored in a parent attribute. Rotation and recoloring change nodes as follows: $\\text{RB-INSERT-FIXUP}$ performs at most $2$ rotations, and each rotation changes the child pointers in $3$ nodes (the node around which we rotate, that node's parent, and one of the children of the node around which we rotate). Thus, at most 6 nodes are directly modified by rotation during $\\text{RB-INSERT-FIXUP}$. In a persistent tree, all ancestors of a changed node are copied, so $\\text{RB-INSERT-FIXUP}$'s rotations take $O(\\lg n)$ time to change nodes due to rotation. (Actually, the changed nodes in this case share a single $O(\\lg n)$-length path of ancestors.) $\\text{RB-INSERT-FIXUP}$ recolors some of the inserted node's ancestors, which are being changed anyway in persistent insertion, and some children of ancestors (the \"uncles\" referred to in the algorithm description). There are at most $O(\\lg n)$ ancestors, hence at most $O(\\lg n)$ color changes of uncles. Recoloring uncles doesn't cause any additional node changes due to persistence, because the ancestors of the uncles are the same nodes (ancestors of the inserted node) that are being changed anyway due to persistence. Thus, recoloring does not affect the $O(\\lg n)$ running time, even with persistence. We could show similarly that deletion in a persistent tree also takes worst-case time $O(h)$. We already saw in part (a) that $O(h)$ nodes change. We could write a persistent $\\text{RB-DELETE}$ procedure that runs in $O(h)$ time, analogous to the changes we made for persistence in insertion. But to do so without using parent pointers we need to walk down the tree to the node to be deleted, to build up a stack of parents as discussed above for insertion. This is a little tricky if the set's keys are not distinct, because in order to find the path to the node to delete\u2014a particular node with a given key\u2014we have to make some changes to how we store things in the tree, so that duplicate keys can be distinguished. The easiest way is to have each key take a second part that is unique, and to use this second part as a tiebreaker when comparing keys. Then the problem of showing that deletion needs only $O(\\lg n)$ time in a persistent red-black tree is the same as for insertion. As for insertion, we can show that the parents needed by $\\text{RB-DELETE-FIXUP}$ can be found in $O(1)$ time (using the same technique as for insertion). Also, $\\text{RB-DELETE-FIXUP}$ performs at most 3 rotations, which as discussed above for insertion requires $O(\\lg n)$ time to change nodes due to persistence. It also does $O(\\lg n)$ color changes, which (as for insertion) take only $O(\\lg n)$ time to change ancestors due to persistence, because the number of copied nodes is $O(\\lg n)$.","title":"13-1 Persistent dynamic sets"},{"location":"Chap13/Problems/13-2/","text":"The join operation takes two dynamic sets $S_1$ and $S_2$ and an element $x$ such that for any $x_1 \\in S_1$ and $x_2 \\in S_2$, we have $x_1.key \\le x.key \\le x_2.key$. It returns a set $S = S_1 \\cup \\{x\\} \\cup S_2$. In this problem, we investigate how to implement the join operation on red-black trees. a. Given a red-black tree $T$, let us store its black-height as the new attribute $T.bh$. Argue that $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ can maintain the $bh$ attribute without requiring extra storage in the nodes of the tree and without increasing the asymptotic running times. Show that while descending through $T$, we can determine the black-height of each node we visit in $O(1)$ time per node visited. We wish to implement the operation $\\text{RB-JOIN}(T_1, x, T_2)$, which destroys $T_1$ and $T_2$ and returns a red-black tree $T = T_1 \\cup \\{x\\} \\cup T_2$. Let $n$ be the total number of nodes in $T_1$ and $T_2$. b. Assume that $T_1.bh \\ge T_2.bh$. Describe an $O(\\lg n)$-time algorithm that finds a black node $y$ in $T_1$ with the largest key from among those nodes whose black-height is $T_2.bh$. c. Let $T_y$ be the subtree rooted at $y$. Describe how $T_y \\cup \\{x\\} \\cup T_2$ can replace $T_y$ in $O(1)$ time without destroying the binary-search-tree property. d. What color should we make $x$ so that red-black properties 1, 3, and 5 are maintained? Describe how to enforce properties 2 and 4 in $O(\\lg n)$ time. e. Argue that no generality is lost by making the assumption in part (b). Describe the symmetric situation that arises when $T_1.bh \\le T_2.bh$. f. Argue that the running time of $\\text{RB-JOIN}$ is $O(\\lg n)$. a. Initialize: $bh = 0$. $\\text{RB-INSERT}$: if in the last step the root is red, we increase $bh$ by $1$. $\\text{RB-DELETE}$: if $x$ is root, we decrease $bh$ by $1$. Each node: in the simple path, decrease $bh$ by $1$ each time we find a black node. b. Move to the right child if the node has a right child, otherwise move to the left child. If the node is black, we decease $bh$ by $1$. Repeat the step until $bh = T_2.bh$. c. The time complexity is $O(1)$. 1 2 3 4 5 6 7 8 RB - JOIN ' ( T [ y ], x , T [ 2 ]) z . left = T [ y ] z . right = T [ 2 ] z . parent = T [ y ]. parent z . key = x if T [ y ]. parent . left = T [ y ] T [ y ]. parent . left = z else T [ y ]. parent . right = z d. Red, because if both the colors of the roots of $T_y$ and $T_2$ are black and the color of $x.parent$ is red, then the color of $x$ is black and it'll change both the colors of the roots of $T_y$ and $T_2$ and recursively adjust to ensure that $\\text{BLACK-HEIGHT}$ doesn't change. The time complexity is $O(\\lg n)$. e. Same, if $T_1.bh\\le T_2.bh$, then we can use the above algorithm symmetrically. f. $O(1) + O(\\lg n) = O(\\lg n)$.","title":"13-2 Join operation on red-black trees"},{"location":"Chap13/Problems/13-3/","text":"An AVL tree is a binary search tree that is height balanced : for each node $x$, the heights of the left and right subtrees of $x$ differ by at most $1$. To implement an AVL tree, we maintain an extra attribute in each node: $x.h$ is the height of node $x$. As for any other binary search tree $T$, we assume that $T.root$ points to the root node. a. Prove that an AVL tree with $n$ nodes has height $O(\\lg n)$. ($\\textit{Hint:}$ Prove that an AVL tree of height $h$ has at least $F_h$ nodes, where $F_h$ is the $h$th Fibonacci number.) b. To insert into an AVL tree, we first place a node into the appropriate place in binary search tree order. Afterward, the tree might no longer be height balanced. Specifically, the heights of the left and right children of some node might differ by $2$. Describe a procedure $\\text{BALANCE}(x)$, which takes a subtree rooted at $x$ whose left and right children are height balanced and have heights that differ by at most $2$, i.e., $|x.right.h - x.left.h| \\le 2$, and alters the subtree rooted at $x$ to be height balanced. ($\\textit{Hint:}$ Use rotations.) c. Using part (b), describe a recursive procedure $\\text{AVL-INSERT}(x, z)$ that takes a node $x$ within an AVL tree and a newly created node $z$ (whose key has already been filled in), and adds $z$ to the subtree rooted at $x$, maintaining the property that $x$ is the root of an AVL tree. As in $\\text{TREE-INSERT}$ from Section 12.3, assume that $z.key$ has already been filled in and that $z.left = \\text{NIL}$ and $z.right = \\text{NIL}$; also assume that $z.h = 0$. Thus, to insert the node $z$ into the AVL tree $T$, we call $\\text{AVL-INSERT}(T.root, z)$. d. Show that $\\text{AVL-INSERT}$, run on an $n$-node AVL tree, takes $O(\\lg n)$ time and performs $O(1)$ rotations. a. Let $T(h)$ denote the minimum size of an AVL tree of height $h$. Since it is height $h$, it must have the max of it's children's heights is equal to $h - 1$. Since we are trying to get as few notes total as possible, suppose that the other child has as small of a height as is allowed. Because of the restriction of AVL trees, we have that the smaller child must be at least one less than the larger one, so, we have that $$T(h) \\ge T(h - 1) + T(h - 2) + 1,$$ where the $+1$ is coming from counting the root node. We can get inequality in the opposite direction by simply taking a tree that achieves the minimum number of number of nodes on height $h - 1$ and on $h - 2$ and join them together under another node. So, we have that $$T(h) = T(h - 1) + T(h - 2) + 1, \\text{ where } T(0) = 0, T(1) = 1.$$ This is both the same recurrence and initial conditions as the Fibonacci numbers. So, recalling equation $\\text{(3.25)}$, we have that $$T(h) = \\Big\\lfloor \\frac{\\phi^h}{\\sqrt 5} + \\frac{1}{2} \\Big\\rfloor \\le n.$$ Rearranging for $h$, we have $$ \\begin{aligned} \\frac{\\phi^h}{\\sqrt 5} - \\frac{1}{2} & \\le n \\\\ \\phi^h & \\le \\sqrt 5(n + \\frac{1}{2}) \\\\ h & \\le \\frac{\\lg \\sqrt 5 + \\lg(n + \\frac{1}{2})}{\\lg\\phi} \\in O(\\lg n). \\end{aligned} $$ b. Let $\\text{UNBAL}(x)$ denote $x.left.h - x.right.h$. Then, the algorithm $\\text{BALANCE}$ does what is desired. Note that because we are only rotating a single element at a time, the value of $\\text{UNBAL}(x)$ can only change by at most $2$ in each step. Also, it must eventually start to change as the tree that was shorter becomes saturated with elements. We also fix any breaking of the AVL property that rotating may of caused by our recursive calls to the children. 1 2 3 4 5 6 7 8 BALANCE ( x ) while | UNBAL ( x ) | > 1 if UNBAL ( x ) > 0 RIGHT - ROTATE ( T , x ) else LEFT - ROTATE ( T , x ) BALANCE ( x . left ) BALANCE ( x . right ) c. For the given algorithm $\\text{AVL-INSERT}(x, z)$, it correctly maintains the fact that it is a BST by the way we search for the correct spot to insert $z$. Also, we can see that it maintains the property of being AVL, because after inserting the element, it checks all of the parents for the AVL property, since those are the only places it could of broken. It then fixes it and also updates the height attribute for any of the nodes for which it may of changed. d. Both for loops only run for $O(h) = O(\\lg(n))$ iterations. Also, only a single rotation will occur in the second while loop because when we do it, we will be decreasing the height of the subtree rooted there, which means that it's back down to what it was before, so all of it's ancestors will have unchanged heights, so, no further balancing will be required. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 AVL - INSERT ( x , z ) w = x while w != NIL y = w if z . key > y . key w = w . right else w = w . left if z . key > y . key y . right = z if y . left = NIL y . h = 1 else y . left = z if y . right = NIL y . h = 1 while y != x y . h = 1 + max ( y . left . h , y . right . h ) if y . left . h > y . right . h + 1 RIGHT - ROTATE ( T , y ) if y . right . h > y . left . h + 1 LEFT - ROTATE ( T , y ) y = y . p","title":"13-3 AVL trees"},{"location":"Chap13/Problems/13-4/","text":"If we insert a set of $n$ items into a binary search tree, the resulting tree may be horribly unbalanced, leading to long search times. As we saw in Section 12.4, however, randomly built binary search trees tend to be balanced. Therefore, one strategy that, on average, builds a balanced tree for a fixed set of items would be to randomly permute the items and then insert them in that order into the tree. What if we do not have all the items at once? If we receive the items one at a time, can we still randomly build a binary search tree out of them? We will examine a data structure that answers this question in the affirmative. A treap is a binary search tree with a modified way of ordering the nodes. Figure 13.9 shows an example. As usual, each node $x$ in the tree has a key value $x.key$. In addition, we assign $x.priority$, which is a random number chosen independently for each node. We assume that all priorities are distinct and also that all keys are distinct. The nodes of the treap are ordered so that the keys obey the binary-search-tree property and the priorities obey the min-heap order property: If $v$ is a left child of $u$, then $v.key < u.key$. If $v$ is a right child of $u$, then $v.key > u.key$. If $v$ is a child of $u$, then $v.priority > u.priority$. (This combination of properties is why the tree is called a \"treap\": it has features of both a binary search tree and a heap.) It helps to think of treaps in the following way. Suppose that we insert nodes $x_1, x_2, \\ldots,x_n$, with associated keys, into a treap. Then the resulting treap is the tree that would have been formed if the nodes had been inserted into a normal binary search tree in the order given by their (randomly chosen) priorities, i.e., $x_i.priority < x_j.priority$ means that we had inserted $x_i$ before $x_j$. a. Show that given a set of nodes $x_1, x_2, \\ldots, x_n$, with associated keys and priorities, all distinct, the treap associated with these nodes is unique. b. Show that the expected height of a treap is $\\Theta(\\lg n)$, and hence the expected time to search for a value in the treap is $\\Theta(\\lg n)$. Let us see how to insert a new node into an existing treap. The first thing we do is assign to the new node a random priority. Then we call the insertion algorithm, which we call $\\text{TREAP-INSERT}$, whose operation is illustrated in Figure 13.10. c. Explain how $\\text{TREAP-INSERT}$ works. Explain the idea in English and give pseudocode. ($\\textit{Hint:}$ Execute the usual binary-search-tree insertion procedure and then perform rotations to restore the min-heap order property.) d. Show that the expected running time of $\\text{TREAP-INSERT}$ is $\\Theta(\\lg n)$. $\\text{TREAP-INSERT}$ performs a search and then a sequence of rotations. Although these two operations have the same expected running time, they have different costs in practice. A search reads information from the treap without modifying it. In contrast, a rotation changes parent and child pointers within the treap. On most computers, read operations are much faster than write operations. Thus we would like $\\text{TREAP-INSERT}$ to perform few rotations. We will show that the expected number of rotations performed is bounded by a constant. In order to do so, we will need some definitions, which Figure 13.11 depicts. The left spine of a binary search tree $T$ is the simple path from the root to the node with the smallest key. In other words, the left spine is the simple path from the root that consists of only left edges. Symmetrically, the right spine of $T$ is the simple path from the root consisting of only right edges. The length of a spine is the number of nodes it contains. e. Consider the treap $T$ immediately after $\\text{TREAP-INSERT}$ has inserted node $x$. Let $C$ be the length of the right spine of the left subtree of $x$. Let $D$ be the length of the left spine of the right subtree of $x$. Prove that the total number of rotations that were performed during the insertion of $x$ is equal to $C + D$. We will now calculate the expected values of $C$ and $D$. Without loss of generality, we assume that the keys are $1, 2, \\ldots, n$ since we are comparing them only to one another. For nodes $x$ and $y$ in treap $T$, where $y \\ne x$, let $k = x.key$ and $i = y.key$. We define indicator random variables $$X_{ik} = \\text{I\\{\\$y\\$ is in the right spine of the left subtree of \\$x\\$\\}}.$$ f. Show that $X_{ik} = 1$ if and only if $y.priority > x.priority$, $y.key < x.key$, and, for every $z$ such that $y.key < z.key < x.key$, we have $y.priority < z.priority$. g. Show that $$ \\begin{aligned} \\Pr\\{X_{ik} = 1\\} & = \\frac{(k - i - 1)!}{(k - i + 1)!} \\\\ & = \\frac{1}{(k - i + 1)(k - i)}. \\\\ \\end{aligned} $$ h. Show that $$ \\begin{aligned} \\text E[C] & = \\sum_{j = 1}^{k - 1} \\frac{1}{j(j + 1)} \\\\ & = 1 - \\frac{1}{k}. \\end{aligned} $$ i. Use a symmetry argument to show that $$\\text E[D] = 1 - \\frac{1}{n - k + 1}.$$ j. Conclude that the expected number of rotations performed when inserting a node into a treap is less than $2$. a. The root is the node with smallest priority, the root divides the sets into two subsets based on the key. In each subset, the node with smallest priority is selected as the root, thus we can uniquely determine a treap with a specific input. b. For the priority of all nodes, each permutation corresponds to exactly one treap, that is, all nodes forms a BST in priority, since the priority of all nodes is spontaneous, treap is, essentially, randomly built binary search tress. Therefore, the expected height of a treap is $\\Theta(\\lg n)$. c. First insert a node as usual using the binary-search-tree insertion procedure. Then perform left and right rotations until the parent of the inserted node no longer has larger priority. d. Rotation is $\\Theta(1)$, at most $h$ rotations, therefore the expected running time is $\\Theta(\\lg n)$. e. Left rotation increase $C$ by $1$, right rotation increase $D$ by $1$. f. The first two are obvious. The min-heap property will not hold if $y.priority > z.priority$. g. $$\\Pr\\{X_{ik} = 1\\} = \\frac{(k - i - 1)!}{(k - i + 1)!} = \\frac{1}{(k - i + 1)(k - i)}.$$ h. $$ \\begin{aligned} \\text E[C] & = \\sum_{j = 1}^{k - 1} \\frac{1}{(k - i + 1)(k - i)} \\\\ & = \\sum_{j = 1}^{k - 1} (\\frac{1}{k - i} - \\frac{1}{k - i + 1}) \\\\ & = 1 - \\frac{1}{k}. \\end{aligned} $$ i. $$ \\begin{aligned} \\text E[D] & = \\sum_{j = 1}^{n - k} \\frac{1}{(k - i + 1)(k - i)} \\\\ & = 1 - \\frac{1}{n - k + 1}. \\end{aligned} $$ j. By part (e), the number of rotations is $C + D$. By linearity of expectation, $\\text E[C + D] = 2 - \\frac{1}{k} - \\frac{1}{n - k + 1} \\le 2$ for any choice of $k$.","title":"13-4 Treaps"},{"location":"Chap14/14.1/","text":"14.1-1 Show how $\\text{OS-SELECT}(T.root, 10)$ operates on the red-black tree $T$ of Figure 14.1. $26: r = 13, i = 10$, go left. $17: r = 8, i = 10$, go right. $21: r = 3, i = 2$, go left. $19: r = 1, i = 2$, go right. $20: r = 1, i = 1$, choose $20$. 14.1-2 Show how $\\text{OS-RANK}(T, x)$ operates on the red-black tree $T$ of Figure 14.1 and the node $x$ with $x.key = 35$. $35: r = 1$. $38: r = 1$. $30: r = r + 2 = 3$. $41: r = 3$. $26: r = r + 13 = 16$. 14.1-3 Write a nonrecursive version of $\\text{OS-SELECT}$. 1 2 3 4 5 6 7 8 9 OS - SELECT ( x , i ) r = x . left . size + 1 while r != i if i < r x = x . left else x = x . right i = i - r r = x . left + 1 return x 14.1-4 Write a recursive procedure $\\text{OS-KEY-RANK}(T, k)$ that takes as input an order-statistic tree $T$ and a key $k$ and returns the rank of $k$ in the dynamic set represented by $T$. Assume that the keys of $T$ are distinct. 1 2 3 4 5 6 OS - KEY - RANK ( T , k ) if k == T . root . key return T . root . left . size + 1 else if T . root . key > k return OS - KEY - RANK ( T . left , k ) else return T . root . left . size + 1 + OS - KEY - RANK ( T . right , k ) 14.1-5 Given an element $x$ in an $n$-node order-statistic tree and a natural number $i$, how can we determine the $i$th successor of $x$ in the linear order of the tree in $O(\\lg n)$ time? Given an element $x$ in an $n$-node order-statistic tree $T$ and a natural number $i$, the following procedure retrieves the $i$th successor of $x$ in the linear order of $T$: 1 2 3 4 OS - SUCCESSOR ( T , x , i ) r = OS - RANK ( T , x ) s = r + i return OS - SELECT ( T . root , s ) Since $\\text{OS-RANK}$ and $\\text{OS-SELECT}$ each take $O(\\lg n)$ time, so does the procedure $\\text{OS-SUCCESSOR}$. 14.1-6 Observe that whenever we reference the size attribute of a node in either $\\text{OS-SELECT}$ or $\\text{OS-RANK}$, we use it only to compute a rank. Accordingly, suppose we store in each node its rank in the subtree of which it is the root. Show how to maintain this information during insertion and deletion. (Remember that these two operations can cause rotations.) When inserting node $z$, we search down the tree for the proper place for $z$. For each node $x$ on this path, add $1$ to $x.rank$ if $z$ is inserted within $x$'s left subtree, and leave $x.rank$ unchanged if $z$ is inserted within $x$'s right subtree. Similarly when deleting, subtract $1$ from $x.rank$ whenever the spliced-out node had been in $x$'s left subtree. We also need to handle the rotations that occur during the fixup procedures for insertion and deletion. Consider a left rotation on node $x$, where the pre-rotation right child of $x$ is $y$ (so that $x$ becomes $y$'s left child after the left rotation). We leave $x.rank$ unchanged, and letting $r = y.rank$ before the rotation, we set $y.rank = r + x.rank$. Right rotations are handled in an analogous manner. 14.1-7 Show how to use an order-statistic tree to count the number of inversions (see Problem 2-4) in an array of size $n$ in time $O(n\\lg n)$. Let $A[1..n]$ be the array of $n$ distinct numbers. One way to count the inversions is to add up, for each element, the number of larger elements that precede it in the array: $$\\text{\\# of inversions} = \\sum_{j = 1}^n |Inv(j)|,$$ where $Inv(j) = \\{i: i < j \\text{ and } A[i] > A[j]\\}$. Note that $|Inv(j)|$ is related to $A[j]$'s rank in the subarray $A[1..j]$ because the elements in $Inv(j)$ are the reason that $A[j]$ is not positioned according to its rank. Let $r(j)$ be the rank of $A[j]$ in $A[1..j]$. Then $j = r(j) + |Inv(j)|$, so we can compute $$|Inv(j)| = j - r(j)$$ by inserting $A[1], \\ldots, A[n]$ into an order-statistic tree and using $\\text{OS-RANK}$ to find the rank of each $A[j]$ in the tree immediately after it is inserted into the tree. (This $\\text{OS-RANK}$ value is $r(j)$.) Insertion and $\\text{OS-RANK}$ each take $O(\\lg n)$ time, and so the total time for $n$ elements is $O(n\\lg n)$. 14.1-8 $\\star$ Consider $n$ chords on a circle, each defined by its endpoints. Describe an $O(n\\lg n)$-time algorithm to determine the number of pairs of chords that intersect inside the circle. (For example, if the $n$ chords are all diameters that meet at the center, then the correct answer is $\\binom{n}{2}$.) Assume that no two chords share an endpoint. Sort the vertices in clock-wise order, and assign a unique value to each vertex. For each chord its two vertices are $u_i$, $v_i$ and $u_i < v_i$. Add the vertices one by one in clock-wise order, if we meet a $u_i$, we add it to the order-statistic tree, if we meet a $v_i$, we calculate how many nodes are larger than $u_i$ (which is the number of intersects with chord $i$), and remove $u_i$.","title":"14.1 Dynamic order statistics"},{"location":"Chap14/14.1/#141-1","text":"Show how $\\text{OS-SELECT}(T.root, 10)$ operates on the red-black tree $T$ of Figure 14.1. $26: r = 13, i = 10$, go left. $17: r = 8, i = 10$, go right. $21: r = 3, i = 2$, go left. $19: r = 1, i = 2$, go right. $20: r = 1, i = 1$, choose $20$.","title":"14.1-1"},{"location":"Chap14/14.1/#141-2","text":"Show how $\\text{OS-RANK}(T, x)$ operates on the red-black tree $T$ of Figure 14.1 and the node $x$ with $x.key = 35$. $35: r = 1$. $38: r = 1$. $30: r = r + 2 = 3$. $41: r = 3$. $26: r = r + 13 = 16$.","title":"14.1-2"},{"location":"Chap14/14.1/#141-3","text":"Write a nonrecursive version of $\\text{OS-SELECT}$. 1 2 3 4 5 6 7 8 9 OS - SELECT ( x , i ) r = x . left . size + 1 while r != i if i < r x = x . left else x = x . right i = i - r r = x . left + 1 return x","title":"14.1-3"},{"location":"Chap14/14.1/#141-4","text":"Write a recursive procedure $\\text{OS-KEY-RANK}(T, k)$ that takes as input an order-statistic tree $T$ and a key $k$ and returns the rank of $k$ in the dynamic set represented by $T$. Assume that the keys of $T$ are distinct. 1 2 3 4 5 6 OS - KEY - RANK ( T , k ) if k == T . root . key return T . root . left . size + 1 else if T . root . key > k return OS - KEY - RANK ( T . left , k ) else return T . root . left . size + 1 + OS - KEY - RANK ( T . right , k )","title":"14.1-4"},{"location":"Chap14/14.1/#141-5","text":"Given an element $x$ in an $n$-node order-statistic tree and a natural number $i$, how can we determine the $i$th successor of $x$ in the linear order of the tree in $O(\\lg n)$ time? Given an element $x$ in an $n$-node order-statistic tree $T$ and a natural number $i$, the following procedure retrieves the $i$th successor of $x$ in the linear order of $T$: 1 2 3 4 OS - SUCCESSOR ( T , x , i ) r = OS - RANK ( T , x ) s = r + i return OS - SELECT ( T . root , s ) Since $\\text{OS-RANK}$ and $\\text{OS-SELECT}$ each take $O(\\lg n)$ time, so does the procedure $\\text{OS-SUCCESSOR}$.","title":"14.1-5"},{"location":"Chap14/14.1/#141-6","text":"Observe that whenever we reference the size attribute of a node in either $\\text{OS-SELECT}$ or $\\text{OS-RANK}$, we use it only to compute a rank. Accordingly, suppose we store in each node its rank in the subtree of which it is the root. Show how to maintain this information during insertion and deletion. (Remember that these two operations can cause rotations.) When inserting node $z$, we search down the tree for the proper place for $z$. For each node $x$ on this path, add $1$ to $x.rank$ if $z$ is inserted within $x$'s left subtree, and leave $x.rank$ unchanged if $z$ is inserted within $x$'s right subtree. Similarly when deleting, subtract $1$ from $x.rank$ whenever the spliced-out node had been in $x$'s left subtree. We also need to handle the rotations that occur during the fixup procedures for insertion and deletion. Consider a left rotation on node $x$, where the pre-rotation right child of $x$ is $y$ (so that $x$ becomes $y$'s left child after the left rotation). We leave $x.rank$ unchanged, and letting $r = y.rank$ before the rotation, we set $y.rank = r + x.rank$. Right rotations are handled in an analogous manner.","title":"14.1-6"},{"location":"Chap14/14.1/#141-7","text":"Show how to use an order-statistic tree to count the number of inversions (see Problem 2-4) in an array of size $n$ in time $O(n\\lg n)$. Let $A[1..n]$ be the array of $n$ distinct numbers. One way to count the inversions is to add up, for each element, the number of larger elements that precede it in the array: $$\\text{\\# of inversions} = \\sum_{j = 1}^n |Inv(j)|,$$ where $Inv(j) = \\{i: i < j \\text{ and } A[i] > A[j]\\}$. Note that $|Inv(j)|$ is related to $A[j]$'s rank in the subarray $A[1..j]$ because the elements in $Inv(j)$ are the reason that $A[j]$ is not positioned according to its rank. Let $r(j)$ be the rank of $A[j]$ in $A[1..j]$. Then $j = r(j) + |Inv(j)|$, so we can compute $$|Inv(j)| = j - r(j)$$ by inserting $A[1], \\ldots, A[n]$ into an order-statistic tree and using $\\text{OS-RANK}$ to find the rank of each $A[j]$ in the tree immediately after it is inserted into the tree. (This $\\text{OS-RANK}$ value is $r(j)$.) Insertion and $\\text{OS-RANK}$ each take $O(\\lg n)$ time, and so the total time for $n$ elements is $O(n\\lg n)$.","title":"14.1-7"},{"location":"Chap14/14.1/#141-8-star","text":"Consider $n$ chords on a circle, each defined by its endpoints. Describe an $O(n\\lg n)$-time algorithm to determine the number of pairs of chords that intersect inside the circle. (For example, if the $n$ chords are all diameters that meet at the center, then the correct answer is $\\binom{n}{2}$.) Assume that no two chords share an endpoint. Sort the vertices in clock-wise order, and assign a unique value to each vertex. For each chord its two vertices are $u_i$, $v_i$ and $u_i < v_i$. Add the vertices one by one in clock-wise order, if we meet a $u_i$, we add it to the order-statistic tree, if we meet a $v_i$, we calculate how many nodes are larger than $u_i$ (which is the number of intersects with chord $i$), and remove $u_i$.","title":"14.1-8 $\\star$"},{"location":"Chap14/14.2/","text":"14.2-1 Show, by adding pointers to the nodes, how to support each of the dynamic-set queries $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, and $\\text{PREDECESSOR}$ in $O(1)$worstcase time on an augmented order-statistic tree. The asymptotic performance of other operations on order-statistic trees should not be affected. MINIMUM: A pointer points to the minimum node, if the node is being deleted, move the pointer to its successor. MAXIMUM: Similar to $\\text{MINIMUM}$. SUCCESSOR: Every node records its successor, the insertion and deletion is similar to that in linked list. PREDECESSOR: Similar to $\\text{MAXIMUM}$. 14.2-2 Can we maintain the black-heights of nodes in a red-black tree as attributes in the nodes of the tree without affecting the asymptotic performance of any of the redblack tree operations? Show how, or argue why not. How about maintaining the depths of nodes? Yes, we can maintain black-heights as attributes in the nodes of a red-black tree without affecting the asymptotic performance of the red-black tree operations. We appeal to Theorem 14.1, because the black-height of a node can be computed from the information at the node and its two children. Actually, the black-height can be computed from just one child's information: the black-height of a node is the black-height of a red child, or the black height of a black child plus one. The second child does not need to be checked because of property 5 of red-black trees. Within the $\\text{RB-INSERT-FIXUP}$ and $\\text{RB-DELETE-FIXUP}$ procedures are color changes, each of which potentially cause $O(\\lg n)$ black-height changes. Let us show that the color changes of the fixup procedures cause only local black-height changes and thus are constant-time operations. Assume that the black-height of each node $x$ is kept in the attribute $x.bh$. For $\\text{RB-INSERT-FIXUP}$, there are 3 cases to examine. Case 1: $z$'s uncle is red. Before color changes, suppose that all subtrees $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\epsilon$ have the same black-height $k$ with a black root, so that nodes $A$, $B$, $C$, and $D$ have blackheights of $k + 1$. After color changes, the only node whose black-height changed is node $C$. To fix that, add $z.p.p.bh = z.p.p.bh + 1$ after line 7 in $\\text{RB-INSERT-FIXUP}$. Since the number of black nodes between $z.p.p$ and $z$ remains the same, nodes above $z.p.p$ are not affected by the color change. Case 2: $z$'s uncle $y$ is black, and $z$ is a right child. Case 3: $z$'s uncle $y$ is black, and $z$ is a left child. With subtrees $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\epsilon$ of black-height $k$, we see that even with color changes and rotations, the black-heights of nodes $A$, $B$, and $C$ remain the same $(k + 1)$. Thus, $\\text{RB-INSERT-FIXUP}$ maintains its original $O(\\lg n)$ time. For $\\text{RB-DELETE-FIXUP}$, there are 4 cases to examine. Case 1: $x$'s sibling $w$ is red. Even though case 1 changes colors of nodes and does a rotation, blackheights are not changed. Case 1 changes the structure of the tree, but waits for cases 2, 3, and 4 to deal with the \"extra black\" on $x$. Case 2: $x$'s sibling $w$ is black, and both of $w$'s children are black. $w$ is colored red, and $x$'s \"extra\" black is moved up to $x.p$. Now we can add $x.p.bh = x.bh$ after line 10 in $\\text{RB-DELETE-FIXUP}$. This is a constant-time update. Then, keep looping to deal with the extra black on $x.p$. Case 3: $x$'s sibling w is black, $w$'s left child is red, and $w$'s right child is black. Regardless of the color changes and rotation of this case, the black-heights don't change. Case 3 just sets up the structure of the tree, so it can fall correctly into case 4. Case 4: $x$'s sibling $w$ is black, and $w$'s right child is red. Nodes $A$, $C$, and $E$ keep the same subtrees, so their black-heights don't change. Add these two constant-time assignments in $\\text{RB-DELETE-FIXUP}$ after line 20: $$ \\begin{aligned} x.p.bh & = x.bh + 1 \\\\ x.p.p.bh & = x.p.bh + 1 \\end{aligned} $$ The extra black is taken care of. Loop terminates. Thus, $\\text{RB-DELETE-FIXUP}$ maintains its original $O(\\lg n)$ time. Therefore, we conclude that black-heights of nodes can be maintained as attributes in red-black trees without affecting the asymptotic performance of red-black tree operations. For the second part of the question, no, we cannot maintain node depths without affecting the asymptotic performance of red-black tree operations. The depth of a node depends on the depth of its parent. When the depth of a node changes, the depths of all nodes below it in the tree must be updated. Updating the root node causes $n - 1$ other nodes to be updated, which would mean that operations on the tree that change node depths might not run in $O(n\\lg n)$ time. 14.2-3 $\\star$ Let $\\otimes$ be an associative binary operator, and let $a$ be an attribute maintained in each node of a red-black tree. Suppose that we want to include in each node $x$ an additional attribute $f$ such that $x.f = x_1.a \\otimes x_2.a \\otimes \\cdots \\otimes x_m.a$, where $x_1, x_2, \\ldots ,x_m$ is the inorder listing of nodes in the subtree rooted at $x$. Show how to update the $f$ attributes in $O(1)$ time after a rotation. Modify your argument slightly to apply it to the $size$ attributes in order-statistic trees. $x.f = x.left.f \\otimes x.a \\otimes x.right.f$. 14.2-4 $\\star$ We wish to augment red-black trees with an operation $\\text{RB-ENUMERATE}(x, a, b)$ that outputs all the keys $k$ such that $a \\le k \\le b$ in a red-black tree rooted at $x$. Describe how to implement $\\text{RB-ENUMERATE}$ in $\\Theta(m+\\lg n)$ time, where $m$ is the number of keys that are output and $n$ is the number of internal nodes in the tree. ($\\textit{Hint:}$ You do not need to add new attributes to the red-black tree.) $\\Theta(\\lg n)$: Find the smallest key that larger than or equal to $a$. $\\Theta(m)$: Based on Exercise 14.2-1, find the $m$ successor.","title":"14.2 How to augment a data structure"},{"location":"Chap14/14.2/#142-1","text":"Show, by adding pointers to the nodes, how to support each of the dynamic-set queries $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, and $\\text{PREDECESSOR}$ in $O(1)$worstcase time on an augmented order-statistic tree. The asymptotic performance of other operations on order-statistic trees should not be affected. MINIMUM: A pointer points to the minimum node, if the node is being deleted, move the pointer to its successor. MAXIMUM: Similar to $\\text{MINIMUM}$. SUCCESSOR: Every node records its successor, the insertion and deletion is similar to that in linked list. PREDECESSOR: Similar to $\\text{MAXIMUM}$.","title":"14.2-1"},{"location":"Chap14/14.2/#142-2","text":"Can we maintain the black-heights of nodes in a red-black tree as attributes in the nodes of the tree without affecting the asymptotic performance of any of the redblack tree operations? Show how, or argue why not. How about maintaining the depths of nodes? Yes, we can maintain black-heights as attributes in the nodes of a red-black tree without affecting the asymptotic performance of the red-black tree operations. We appeal to Theorem 14.1, because the black-height of a node can be computed from the information at the node and its two children. Actually, the black-height can be computed from just one child's information: the black-height of a node is the black-height of a red child, or the black height of a black child plus one. The second child does not need to be checked because of property 5 of red-black trees. Within the $\\text{RB-INSERT-FIXUP}$ and $\\text{RB-DELETE-FIXUP}$ procedures are color changes, each of which potentially cause $O(\\lg n)$ black-height changes. Let us show that the color changes of the fixup procedures cause only local black-height changes and thus are constant-time operations. Assume that the black-height of each node $x$ is kept in the attribute $x.bh$. For $\\text{RB-INSERT-FIXUP}$, there are 3 cases to examine. Case 1: $z$'s uncle is red. Before color changes, suppose that all subtrees $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\epsilon$ have the same black-height $k$ with a black root, so that nodes $A$, $B$, $C$, and $D$ have blackheights of $k + 1$. After color changes, the only node whose black-height changed is node $C$. To fix that, add $z.p.p.bh = z.p.p.bh + 1$ after line 7 in $\\text{RB-INSERT-FIXUP}$. Since the number of black nodes between $z.p.p$ and $z$ remains the same, nodes above $z.p.p$ are not affected by the color change. Case 2: $z$'s uncle $y$ is black, and $z$ is a right child. Case 3: $z$'s uncle $y$ is black, and $z$ is a left child. With subtrees $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\epsilon$ of black-height $k$, we see that even with color changes and rotations, the black-heights of nodes $A$, $B$, and $C$ remain the same $(k + 1)$. Thus, $\\text{RB-INSERT-FIXUP}$ maintains its original $O(\\lg n)$ time. For $\\text{RB-DELETE-FIXUP}$, there are 4 cases to examine. Case 1: $x$'s sibling $w$ is red. Even though case 1 changes colors of nodes and does a rotation, blackheights are not changed. Case 1 changes the structure of the tree, but waits for cases 2, 3, and 4 to deal with the \"extra black\" on $x$. Case 2: $x$'s sibling $w$ is black, and both of $w$'s children are black. $w$ is colored red, and $x$'s \"extra\" black is moved up to $x.p$. Now we can add $x.p.bh = x.bh$ after line 10 in $\\text{RB-DELETE-FIXUP}$. This is a constant-time update. Then, keep looping to deal with the extra black on $x.p$. Case 3: $x$'s sibling w is black, $w$'s left child is red, and $w$'s right child is black. Regardless of the color changes and rotation of this case, the black-heights don't change. Case 3 just sets up the structure of the tree, so it can fall correctly into case 4. Case 4: $x$'s sibling $w$ is black, and $w$'s right child is red. Nodes $A$, $C$, and $E$ keep the same subtrees, so their black-heights don't change. Add these two constant-time assignments in $\\text{RB-DELETE-FIXUP}$ after line 20: $$ \\begin{aligned} x.p.bh & = x.bh + 1 \\\\ x.p.p.bh & = x.p.bh + 1 \\end{aligned} $$ The extra black is taken care of. Loop terminates. Thus, $\\text{RB-DELETE-FIXUP}$ maintains its original $O(\\lg n)$ time. Therefore, we conclude that black-heights of nodes can be maintained as attributes in red-black trees without affecting the asymptotic performance of red-black tree operations. For the second part of the question, no, we cannot maintain node depths without affecting the asymptotic performance of red-black tree operations. The depth of a node depends on the depth of its parent. When the depth of a node changes, the depths of all nodes below it in the tree must be updated. Updating the root node causes $n - 1$ other nodes to be updated, which would mean that operations on the tree that change node depths might not run in $O(n\\lg n)$ time.","title":"14.2-2"},{"location":"Chap14/14.2/#142-3-star","text":"Let $\\otimes$ be an associative binary operator, and let $a$ be an attribute maintained in each node of a red-black tree. Suppose that we want to include in each node $x$ an additional attribute $f$ such that $x.f = x_1.a \\otimes x_2.a \\otimes \\cdots \\otimes x_m.a$, where $x_1, x_2, \\ldots ,x_m$ is the inorder listing of nodes in the subtree rooted at $x$. Show how to update the $f$ attributes in $O(1)$ time after a rotation. Modify your argument slightly to apply it to the $size$ attributes in order-statistic trees. $x.f = x.left.f \\otimes x.a \\otimes x.right.f$.","title":"14.2-3 $\\star$"},{"location":"Chap14/14.2/#142-4-star","text":"We wish to augment red-black trees with an operation $\\text{RB-ENUMERATE}(x, a, b)$ that outputs all the keys $k$ such that $a \\le k \\le b$ in a red-black tree rooted at $x$. Describe how to implement $\\text{RB-ENUMERATE}$ in $\\Theta(m+\\lg n)$ time, where $m$ is the number of keys that are output and $n$ is the number of internal nodes in the tree. ($\\textit{Hint:}$ You do not need to add new attributes to the red-black tree.) $\\Theta(\\lg n)$: Find the smallest key that larger than or equal to $a$. $\\Theta(m)$: Based on Exercise 14.2-1, find the $m$ successor.","title":"14.2-4 $\\star$"},{"location":"Chap14/14.3/","text":"14.3-1 Write pseudocode for $\\text{LEFT-ROTATE}$ that operates on nodes in an interval tree and updates the $max$ attributes in $O(1)$ time. Add 2 lines in $\\text{LEFT-ROTATE}$ in 13.2 1 2 y . max = x . max x . max = max ( x . high , x . left . max , x . right . max ) 14.3-2 Rewrite the code for $\\text{INTERVAL-SEARCH}$ so that it works properly when all intervals are open. 1 2 3 4 5 6 7 INTERVAL - SEARCH ( T , i ) x = T . root while x != T . nil and i does not overlap x . int if x . left != T . nil and x . left . max > i . low x = x . left else x = x . right return x 14.3-3 Describe an efficient algorithm that, given an interval $i$ , returns an interval overlapping $i$ that has the minimum low endpoint, or $T.nil$ if no such interval exists. As it travels down the tree, $\\text{INTERVAL-SEARCH}$ first checks whether current node $x$ overlaps the query interval $i$ and, if it does not, goes down to either the left or right child. If node $x$ overlaps $i$, and some node in the right subtree overlaps $i$, but no node in the left subtree overlaps $i$, then because the keys are low endpoints, this order of checking (first $x$, then one child) will return the overlapping interval with the minimum low endpoint. On the other hand, if there is an interval that overlaps $i$ in the left subtree of $x$, then checking $x$ before the left subtree might cause the procedure to return an interval whose low endpoint is not the minimum of those that overlap $i$. Therefore, if there is a possibility that the left subtree might contain an interval that overlaps $i$, we need to check the left subtree first. If there is no overlap in the left subtree but node $x$ overlaps $i$, then we return $x$. We check the right subtree under the same conditions as in $\\text{INTERVAL-SEARCH}$: the left subtree cannot contain an interval that overlaps $i$, and node $x$ does not overlap $i$, either. Because we might search the left subtree first, it is easier to write the pseudocode to use a recursive procedure $\\text{MIN-INTERVAL-SEARCH-FROM}(T, x, i)$, which returns the node overlapping $i$ with the minimum low endpoint in the subtree rooted at $x$, or $T.nil$ if there is no such node. 1 2 MIN - INTERVAL - SEARCH ( T , i ) return MIN - INTERVAL - SEARCH - FROM ( T , T . root , i ) 1 2 3 4 5 6 7 8 9 10 11 MIN - INTERVAL - SEARCH - FROM ( T , x , i ) if x . left != T . nil and x . left . max \u2265 i . low y = MIN - INTERVAL - SEARCH - FROM ( T , x . left , i ) if y != T . nil return y else if i overlaps x . int return x else return T . nil else if i overlaps x . int return x else return MIN - INTERVAL - SEARCH - FROM ( T , x . right , i ) The call $\\text{MIN-INTERVAL-SEARCH}(T, i)$ takes $O(\\lg n)$ time, since each recursive call of $\\text{MIN-INTERVAL-SEARCH-FROM}$ goes one node lower in the tree, and the height of the tree is $O(\\lg n)$. 14.3-4 Given an interval tree $T$ and an interval $i$, describe how to list all intervals in $T$ that overlap $i$ in $O(\\min(n, k \\lg n))$ time, where $k$ is the number of intervals in the output list. ($\\textit{Hint:}$ One simple method makes several queries, modifying the tree between queries. A slightly more complicated method does not modify the tree.) 1 2 3 4 5 6 7 8 9 INTERVALS - SEARCH ( T , x , i ) let list be an empty array if i overlaps x . int list . APPEND ( x ) if x . left != T . nil and x . left . max > i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . left , i )) if x . right != T . nil and x . int . low \u2264 i . high and x . right . max \u2265 i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . right , i )) return list 14.3-5 Suggest modifications to the interval-tree procedures to support the new operation $\\text{INTERVAL-SEARCH-EXACTLY}(T, i)$, where $T$ is an interval tree and $i$ is an interval. The operation should return a pointer to a node $x$ in $T$ such that $x.int.low = i.low$ and $x.int.high = i.high$, or $T.nil$ if $T$ contains no such node. All operations, including $\\text{INTERVAL-SEARCH-EXACTLY}$, should run in $O(\\lg n)$ time on an $n$-node interval tree. Search for nodes which has exactly the same low value. 1 2 3 4 5 6 7 8 9 10 11 INTERVAL - SEARCH - EXACTLY ( T , i ) x = T . root while x != T . nil and i not exactly overlap x if i . high > x . max x = T . nil else if i . low < x . low x = x . left else if i . low > x . low x = x . right else x = T . nil return x 14.3-6 Show how to maintain a dynamic set $Q$ of numbers that supports the operation $\\text{MIN-GAP}$, which gives the magnitude of the difference of the two closest numbers in $Q$. For example, if $Q = \\{1, 5, 9, 15, 18, 22 \\}$, then $\\text{MIN-GAP}(Q)$ returns $18 - 15 = 3$, since $15$ and $18$ are the two closest numbers in $Q$. Make the operations $\\text{INSERT}$, $\\text{DELETE}$, $\\text{SEARCH}$, and $\\text{MIN-GAP}$ as efficient as possible, and analyze their running times. Underlying data structure: A red-black tree in which the numbers in the set are stored simply as the keys of the nodes. $\\text{SEARCH}$ is then just the ordinary $\\text{TREE-SEARCH}$ for binary search trees, which runs in $O(\\lg n)$ time on red-black trees. Additional information: The red-black tree is augmented by the following attributes in each node $x$: $x.min\\text-gap$ contains the minimum gap in the subtree rooted at $x$. It has the magnitude of the difference of the two closest numbers in the subtree rooted at $x$. If $x$ is a leaf (its children are all $T.nil$), let $x.min\\text-gap = \\infty$. $x.min\\text-val$ contains the minimum value ($key$) in the subtree rooted at $x$. $x.max\\text-val$ contains the maximum value ($key$) in the subtree rooted at $x$. Maintaining the information: The three attributes added to the tree can each be computed from information in the node and its children. Hence by Theorem 14.1, they can be maintained during insertion and deletion without affecting the $O(\\lg n)$ running time: $$ x.min\\text-val = \\begin{cases} x.left.min\\text-val & \\text{if there is a left subtree}, \\\\ x.key & \\text{otherwise}, \\end{cases} $$ $$ x.max\\text-val = \\begin{cases} x.right.max\\text-val & \\text{if there is a right subtree}, \\\\ x.key & \\text{otherwise}, \\end{cases} $$ $$ x.min\\text-gap = \\min \\begin{cases} x.left.min\\text-gap & \\text{($\\infty$ if no left subtree)}, \\\\ x.right.min\\text-gap & \\text{($\\infty$ if no right subtree)}, \\\\ x.key - x.left.max\\text-val & \\text{($\\infty$ if no left subtree)}, \\\\ x.right.min\\text-val - x.key & \\text{($\\infty$ if no right subtree)}. \\end{cases} $$ In fact, the reason for defining the $min\\text-val$ and $min\\text-val$ attributes is to make it possible to compute $min\\text-gap$ from information at the node and its children. New operation: $\\text{MIN-GAP}$ simply returns the $min\\text-gap$ stored at the tree root. Thus, its running time is $O(1)$. Note that in addition (not asked for in the exercise), it is possible to find the two closest numbers in $O(\\lg n)$ time. Starting from the root, look for where the minimum gap (the one stored at the root) came from. At each node $x$, simulate the computation of $x.min\\text-gap$ to figure out where $x.min\\text-gap$ came from. If it came from a subtree's $min\\text-gap$ attribute, continue the search in that subtree. If it came from a computation with $x$'s key, then $x$ and that other number are the closest numbers. 14.3-7 $\\star$ VLSI databases commonly represent an integrated circuit as a list of rectangles. Assume that each rectangle is rectilinearly oriented (sides parallel to the $x$- and $y$-axes), so that we represent a rectangle by its minimum and maximum $x$ and $y$-coordinates. Give an $O(n\\lg n)$-time algorithm to decide whether or not a set of $n$ rectangles so represented contains two rectangles that overlap. Your algorithm need not report all intersecting pairs, but it must report that an overlap exists if one rectangle entirely covers another, even if the boundary lines do not intersect. ($\\textit{Hint:}$ Move a \"sweep\" line across the set of rectangles.) General idea: Move a sweep line from left to right, while maintaining the set of rectangles currently intersected by the line in an interval tree. The interval tree will organize all rectangles whose $x$ interval includes the current position of the sweep line, and it will be based on the $y$ intervals of the rectangles, so that any overlapping $y$ intervals in the interval tree correspond to overlapping rectangles. Details: Sort the rectangles by their $x$-coordinates. (Actually, each rectangle must appear twice in the sorted list\u2014once for its left $x$-coordinate and once for its right $x$-coordinate.) Scan the sorted list (from lowest to highest $x$-coordinate). When an $x$-coordinate of a left edge is found, check whether the rectangle's $y$-coordinate interval overlaps an interval in the tree, and insert the rectangle (keyed on its $y$-coordinate interval) into the tree. When an $x$-coordinate of a right edge is found, delete the rectangle from the interval tree. The interval tree always contains the set of \"open\" rectangles intersected by the sweep line. If an overlap is ever found in the interval tree, there are overlapping rectangles. Time: $O(n\\lg n)$. $O(n\\lg n)$ to sort the rectangles (we can use merge sort or heap sort). $O(n\\lg n)$ for interval-tree operations (insert, delete, and check for overlap).","title":"14.3 Interval trees"},{"location":"Chap14/14.3/#143-1","text":"Write pseudocode for $\\text{LEFT-ROTATE}$ that operates on nodes in an interval tree and updates the $max$ attributes in $O(1)$ time. Add 2 lines in $\\text{LEFT-ROTATE}$ in 13.2 1 2 y . max = x . max x . max = max ( x . high , x . left . max , x . right . max )","title":"14.3-1"},{"location":"Chap14/14.3/#143-2","text":"Rewrite the code for $\\text{INTERVAL-SEARCH}$ so that it works properly when all intervals are open. 1 2 3 4 5 6 7 INTERVAL - SEARCH ( T , i ) x = T . root while x != T . nil and i does not overlap x . int if x . left != T . nil and x . left . max > i . low x = x . left else x = x . right return x","title":"14.3-2"},{"location":"Chap14/14.3/#143-3","text":"Describe an efficient algorithm that, given an interval $i$ , returns an interval overlapping $i$ that has the minimum low endpoint, or $T.nil$ if no such interval exists. As it travels down the tree, $\\text{INTERVAL-SEARCH}$ first checks whether current node $x$ overlaps the query interval $i$ and, if it does not, goes down to either the left or right child. If node $x$ overlaps $i$, and some node in the right subtree overlaps $i$, but no node in the left subtree overlaps $i$, then because the keys are low endpoints, this order of checking (first $x$, then one child) will return the overlapping interval with the minimum low endpoint. On the other hand, if there is an interval that overlaps $i$ in the left subtree of $x$, then checking $x$ before the left subtree might cause the procedure to return an interval whose low endpoint is not the minimum of those that overlap $i$. Therefore, if there is a possibility that the left subtree might contain an interval that overlaps $i$, we need to check the left subtree first. If there is no overlap in the left subtree but node $x$ overlaps $i$, then we return $x$. We check the right subtree under the same conditions as in $\\text{INTERVAL-SEARCH}$: the left subtree cannot contain an interval that overlaps $i$, and node $x$ does not overlap $i$, either. Because we might search the left subtree first, it is easier to write the pseudocode to use a recursive procedure $\\text{MIN-INTERVAL-SEARCH-FROM}(T, x, i)$, which returns the node overlapping $i$ with the minimum low endpoint in the subtree rooted at $x$, or $T.nil$ if there is no such node. 1 2 MIN - INTERVAL - SEARCH ( T , i ) return MIN - INTERVAL - SEARCH - FROM ( T , T . root , i ) 1 2 3 4 5 6 7 8 9 10 11 MIN - INTERVAL - SEARCH - FROM ( T , x , i ) if x . left != T . nil and x . left . max \u2265 i . low y = MIN - INTERVAL - SEARCH - FROM ( T , x . left , i ) if y != T . nil return y else if i overlaps x . int return x else return T . nil else if i overlaps x . int return x else return MIN - INTERVAL - SEARCH - FROM ( T , x . right , i ) The call $\\text{MIN-INTERVAL-SEARCH}(T, i)$ takes $O(\\lg n)$ time, since each recursive call of $\\text{MIN-INTERVAL-SEARCH-FROM}$ goes one node lower in the tree, and the height of the tree is $O(\\lg n)$.","title":"14.3-3"},{"location":"Chap14/14.3/#143-4","text":"Given an interval tree $T$ and an interval $i$, describe how to list all intervals in $T$ that overlap $i$ in $O(\\min(n, k \\lg n))$ time, where $k$ is the number of intervals in the output list. ($\\textit{Hint:}$ One simple method makes several queries, modifying the tree between queries. A slightly more complicated method does not modify the tree.) 1 2 3 4 5 6 7 8 9 INTERVALS - SEARCH ( T , x , i ) let list be an empty array if i overlaps x . int list . APPEND ( x ) if x . left != T . nil and x . left . max > i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . left , i )) if x . right != T . nil and x . int . low \u2264 i . high and x . right . max \u2265 i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . right , i )) return list","title":"14.3-4"},{"location":"Chap14/14.3/#143-5","text":"Suggest modifications to the interval-tree procedures to support the new operation $\\text{INTERVAL-SEARCH-EXACTLY}(T, i)$, where $T$ is an interval tree and $i$ is an interval. The operation should return a pointer to a node $x$ in $T$ such that $x.int.low = i.low$ and $x.int.high = i.high$, or $T.nil$ if $T$ contains no such node. All operations, including $\\text{INTERVAL-SEARCH-EXACTLY}$, should run in $O(\\lg n)$ time on an $n$-node interval tree. Search for nodes which has exactly the same low value. 1 2 3 4 5 6 7 8 9 10 11 INTERVAL - SEARCH - EXACTLY ( T , i ) x = T . root while x != T . nil and i not exactly overlap x if i . high > x . max x = T . nil else if i . low < x . low x = x . left else if i . low > x . low x = x . right else x = T . nil return x","title":"14.3-5"},{"location":"Chap14/14.3/#143-6","text":"Show how to maintain a dynamic set $Q$ of numbers that supports the operation $\\text{MIN-GAP}$, which gives the magnitude of the difference of the two closest numbers in $Q$. For example, if $Q = \\{1, 5, 9, 15, 18, 22 \\}$, then $\\text{MIN-GAP}(Q)$ returns $18 - 15 = 3$, since $15$ and $18$ are the two closest numbers in $Q$. Make the operations $\\text{INSERT}$, $\\text{DELETE}$, $\\text{SEARCH}$, and $\\text{MIN-GAP}$ as efficient as possible, and analyze their running times. Underlying data structure: A red-black tree in which the numbers in the set are stored simply as the keys of the nodes. $\\text{SEARCH}$ is then just the ordinary $\\text{TREE-SEARCH}$ for binary search trees, which runs in $O(\\lg n)$ time on red-black trees. Additional information: The red-black tree is augmented by the following attributes in each node $x$: $x.min\\text-gap$ contains the minimum gap in the subtree rooted at $x$. It has the magnitude of the difference of the two closest numbers in the subtree rooted at $x$. If $x$ is a leaf (its children are all $T.nil$), let $x.min\\text-gap = \\infty$. $x.min\\text-val$ contains the minimum value ($key$) in the subtree rooted at $x$. $x.max\\text-val$ contains the maximum value ($key$) in the subtree rooted at $x$. Maintaining the information: The three attributes added to the tree can each be computed from information in the node and its children. Hence by Theorem 14.1, they can be maintained during insertion and deletion without affecting the $O(\\lg n)$ running time: $$ x.min\\text-val = \\begin{cases} x.left.min\\text-val & \\text{if there is a left subtree}, \\\\ x.key & \\text{otherwise}, \\end{cases} $$ $$ x.max\\text-val = \\begin{cases} x.right.max\\text-val & \\text{if there is a right subtree}, \\\\ x.key & \\text{otherwise}, \\end{cases} $$ $$ x.min\\text-gap = \\min \\begin{cases} x.left.min\\text-gap & \\text{($\\infty$ if no left subtree)}, \\\\ x.right.min\\text-gap & \\text{($\\infty$ if no right subtree)}, \\\\ x.key - x.left.max\\text-val & \\text{($\\infty$ if no left subtree)}, \\\\ x.right.min\\text-val - x.key & \\text{($\\infty$ if no right subtree)}. \\end{cases} $$ In fact, the reason for defining the $min\\text-val$ and $min\\text-val$ attributes is to make it possible to compute $min\\text-gap$ from information at the node and its children. New operation: $\\text{MIN-GAP}$ simply returns the $min\\text-gap$ stored at the tree root. Thus, its running time is $O(1)$. Note that in addition (not asked for in the exercise), it is possible to find the two closest numbers in $O(\\lg n)$ time. Starting from the root, look for where the minimum gap (the one stored at the root) came from. At each node $x$, simulate the computation of $x.min\\text-gap$ to figure out where $x.min\\text-gap$ came from. If it came from a subtree's $min\\text-gap$ attribute, continue the search in that subtree. If it came from a computation with $x$'s key, then $x$ and that other number are the closest numbers.","title":"14.3-6"},{"location":"Chap14/14.3/#143-7-star","text":"VLSI databases commonly represent an integrated circuit as a list of rectangles. Assume that each rectangle is rectilinearly oriented (sides parallel to the $x$- and $y$-axes), so that we represent a rectangle by its minimum and maximum $x$ and $y$-coordinates. Give an $O(n\\lg n)$-time algorithm to decide whether or not a set of $n$ rectangles so represented contains two rectangles that overlap. Your algorithm need not report all intersecting pairs, but it must report that an overlap exists if one rectangle entirely covers another, even if the boundary lines do not intersect. ($\\textit{Hint:}$ Move a \"sweep\" line across the set of rectangles.) General idea: Move a sweep line from left to right, while maintaining the set of rectangles currently intersected by the line in an interval tree. The interval tree will organize all rectangles whose $x$ interval includes the current position of the sweep line, and it will be based on the $y$ intervals of the rectangles, so that any overlapping $y$ intervals in the interval tree correspond to overlapping rectangles. Details: Sort the rectangles by their $x$-coordinates. (Actually, each rectangle must appear twice in the sorted list\u2014once for its left $x$-coordinate and once for its right $x$-coordinate.) Scan the sorted list (from lowest to highest $x$-coordinate). When an $x$-coordinate of a left edge is found, check whether the rectangle's $y$-coordinate interval overlaps an interval in the tree, and insert the rectangle (keyed on its $y$-coordinate interval) into the tree. When an $x$-coordinate of a right edge is found, delete the rectangle from the interval tree. The interval tree always contains the set of \"open\" rectangles intersected by the sweep line. If an overlap is ever found in the interval tree, there are overlapping rectangles. Time: $O(n\\lg n)$. $O(n\\lg n)$ to sort the rectangles (we can use merge sort or heap sort). $O(n\\lg n)$ for interval-tree operations (insert, delete, and check for overlap).","title":"14.3-7 $\\star$"},{"location":"Chap14/Problems/14-1/","text":"Suppose that we wish to keep track of a point of maximum overlap in a set of intervals\u2014a point with the largest number of intervals in the set that overlap it. a. Show that there will always be a point of maximum overlap that is an endpoint of one of the segments. b. Design a data structure that efficiently supports the operations $\\text{INTERVAL-INSERT}$, $\\text{INTERVAL-DELETE}$, and $\\text{FIND-POM}$, which returns a point of maximum overlap. ($\\textit{Hint:}$ Keep a red-black tree of all the endpoints. Associate a value of $+1$ with each left endpoint, and associate a value of $-1$ with each right endpoint. Augment each node of the tree with some extra information to maintain the point of maximum overlap.) a. Assume for the purpose of contradiction that there is no point of maximum overlap in an endpoint of a segment. The maximum overlap point $p$ is in the interior of $m$ segments. Actually, $p$ is in the interior of the intersection of those $m$ segments. Now look at one of the endpoints $p'$ of the intersection of the $m$ segments. Point $p'$ has the same overlap as $p$ because it is in the same intersection of $m$ segments, and so $p'$ is also a point of maximum overlap. Moreover, $p'$ is in the endpoint of a segment (otherwise the intersection would not end there), which contradicts our assumption that there is no point of maximum overlap in an endpoint of a segment. Thus, there is always a point of maximum overlap which is an endpoint of one of the segments. b. Keep a balanced binary search tree of the endpoints. That is, to insert an interval, we insert its endpoints separately. With each left endpoint $e$, associate a value $p(e) = +1$ (increasing the overlap by $1$). With each right endpoint $e$ associate a value $p(e) = -1$ (decreasing the overlap by $1$). When multiple endpoints have the same value, insert all the left endpoints with that value before inserting any of the right endpoints with that value. Here's some intuition. Let $e_1, e_2, \\ldots, e_n$ be the sorted sequence of endpoints corresponding to our intervals. Let $s(i, j)$ denote the sum $p(e_i) + p(e_{i + 1}) + \\cdots + p(e_j)$ for $1 \\le i \\le j \\le n$. We wish to find an $i$ maximizing $s(1, i)$. For each node $x$ in the tree, let $l(x)$ and $r(x)$ be the indices in the sorted order of the leftmost and rightmost endpoints, respectively, in the subtree rooted at $x$. Then the subtree rooted at $x$ contains the endpoints $e_{l(x)}, e_{l(x) + 1}, \\ldots, e_{r(x)}$. Each node $x$ stores three new attributes. We store $x.v = s(l(x), r(x))$, the sum of the values of all nodes in the subtree rooted at $x$. We also store $x.m$, the maximum value obtained by the expression $s(l(x), i)$ for any $i$ in $\\{l(x), l(x) + 1, \\ldots, r(x)\\}$. Finally, we store $x.o$ as the value of $i$ for which $x.m$ achieves its maximum. For the sentinel, we define $T.nil.v = T.nil.m = 0$. We can compute these attributes in a bottom-up fashion to satisfy the requirements of Theorem 14.1: $$x.v = x.left.v + p(x) + x.right.v,$$ $$ x.m = \\max \\begin{cases} x.left.m & \\text{(max is in $x$'s left subtree)}, \\\\ x.left.v + p(x) & \\text{(max is at $x$)}, \\\\ x.left.v + p(x) + x.right.m & \\text{(max is in $x$'s right subtree)}. \\end{cases} $$ Computing $x.v$ is straightforward. Computing $x.m$ bears further explanation. Recall that it is the maximum value of the sum of the $p$ values for the nodes in the subtree rooted at $x$, starting at the node for $e_{l(x)}$, which is the leftmost endpoint in $x$'s subtree, and ending at any node for $e_i$ in $x$'s subtree. The endpoint $e_i$ that maximizes this sum\u2014let's call it $e_{i^*}$\u2014corresponds to either a node in $x$'s left subtree, $x$ itself, or a node in $x$'s right subtree. If $e_{i^*}$ corresponds to a node in $x$'s left subtree, then $x.left.m$ represents a sum starting at the node for $e_{l(x)}$ and ending at a node in $x$'s left subtree, and hence $x.m = x.left.m$. If $e_i$ corresponds to $x$ itself, then $x.m$ represents the sum of all $p$ values in $x$'s left subtree, plus $p(x)$, so that $x.m = x.left.v + p(x)$. Finally, if $e_{i^*}$ corresponds to a node in $x$'s right subtree, then $x.m$ represents the sum of all $p$ values in $x$'s left subtree, plus $p(x)$, plus the sum of some subset of $p$ values in $x$'s right subtree. Moreover, the values taken from $x$'s right subtree must start from the leftmost endpoint stored in the right subtree. To maximize this sum, we need to maximize the sum from the right subtree, and that value is precisely $x.right.m$. Hence, in this case, $x.m = x.left.v + p(x) + x.right.m$. Once we understand how to compute $x.m$, it is straightforward to compute $x.o$ from the information in $x$ and its two children. Thus, we can implement the operations as follows: $\\text{INTERVAL-INSERT}$: insert two nodes, one for each endpoint of the interval. $\\text{FIND-POM}$: return the interval whose endpoint is represented by $T.root.o$. (Note that because we are building a binary search tree of all the endpoints and then determining $T.root.o$, we have no need to delete any nodes from the tree.) Because of how we have defined the new attributes, Theorem 14.1 says that each operation runs in $O(\\lg n)$ time. In fact, $\\text{FIND-POM}$ takes only $O(1)$ time.","title":"14-1 Point of maximum overlap"},{"location":"Chap14/Problems/14-2/","text":"We define the Josephus problem as follows. Suppose that $n$ people form a circle and that we are given a positive integer $m \\le n$. Beginning with a designated first person, we proceed around the circle, removing every $m$th person. After each person is removed, counting continues around the circle that remains. This process continues until we have removed all $n$ people. The order in which the people are removed from the circle defines the $(n, m)$-Josephus permutation of the integers $1, 2, \\ldots, n$. For example, the $(7, 3)$-Josephus permutation is $\\langle 3, 6, 2, 7, 5, 1, 4 \\rangle$. a. Suppose that $m$ is a constant. Describe an $O(n)$-time algorithm that, given an integer $n$, outputs the $(n, m)$-Josephus permutation. b. Suppose that $m$ is not a constant. Describe an $O(n\\lg n)$-time algorithm that, given integers $n$ and $m$, outputs the $(n, m)$-Josephus permutation. a. We use a circular list in which each element has two attributes, $key$ and $next$. At the beginning, we initialize the list to contain the keys $1, 2, \\ldots, n$ in that order. This initialization takes $O(n)$ time, since there is only a constant amount of work per element (i.e., setting its $key$ and its $next$ attributes). We make the list circular by letting the $next$ attribute of the last element point to the first element. We then start scanning the list from the beginning. We output and then delete every $m$th element, until the list becomes empty. The output sequence is the $(n, m)$-Josephus permutation. This process takes $O(m)$ time per element, for a total time of $O(mn)$. Since m is a constant, we get $O(mn) = O(n)$ time, as required. b. We can use an order-statistic tree, straight out of Section 14.1. Why? Suppose that we are at a particular spot in the permutation, and let's say that it's the $j$th largest remaining person. Suppose that there are $k \\le n$ people remaining. Then we will remove person $j$, decrement $k$ to reflect having removed this person, and then go on to the $(j + m - 1)$ largest remaining person (subtract $1$ because we have just removed the $j$th largest). But that assumes that $j + m \\le k$. If not, then we use a little modular arithmetic, as shown below. In detail, we use an order-statistic tree $T$, and we call the procedures $\\text{OS-INSERT}$, $\\text{OS-DELETE}$, $\\text{OS-RANK}$, and $\\text{OS-SELECT}$: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 JOSEPHUS ( n , m ) initialize T to be empty for j = 1 to n create a node x with x . key == j OS - INSERT ( T , x ) k = n j = m while k > 2 x = OS - SELECT ( T . root , j ) print x . key OS - DELETE ( T , x ) k = k - 1 j = (( j + m - 2 ) mod k ) + 1 print OS - SELECT ( T . root , 1 ). key The above procedure is easier to understand. Here's a streamlined version: 1 2 3 4 5 6 7 8 9 10 11 JOSEPHUS ( n , m ) initialize T to be empty for j = 1 to n create a node x with x . key == j OS - INSERT ( T , x ) j = 1 for k = n downto 1 j = (( j + m - 2 ) mod k ) + 1 x = OS - SELECT ( T . root , j ) print x . key OS - DELETE ( T , x ) Either way, it takes $O(n\\lg n)$ time to build up the order-statistic tree $T$, and then we make $O(n)$ calls to the order-statistic-tree procedures, each of which takes $O(\\lg n)$ time. Thus, the total time is $O(n\\lg n)$.","title":"14-2 Josephus permutation"},{"location":"Chap15/15.1/","text":"15.1-1 Show that equation $\\text{(15.4)}$ follows from equation $\\text{(15.3)}$ and the initial condition $T(0) = 1$. We can verify that $T(n) = 2^n$ is a solution to the given recurrence by the substitution method. We note that for $n = 0$, the formula is true since $2^0 = 1$. For $n > 0$, substituting into the recurrence and using the formula for summing a geometric series yields $$ \\begin{aligned} T(n) & = 1 + \\sum_{j = 0}^{n - 1} 2^j \\\\ & = 1 + (2^n - 1) \\\\ & = 2^n. \\end{aligned} $$ 15.1-2 Show, by means of a counterexample, that the following \"greedy\" strategy does not always determine an optimal way to cut rods. Define the density of a rod of length $i$ to be $p_i / i$, that is, its value per inch. The greedy strategy for a rod of length $n$ cuts off a first piece of length $i$, where $1 \\le i \\le n$, having maximum density. It then continues by applying the greedy strategy to the remaining piece of length $n - i$. Here is a counterexample for the \"greedy\" strategy: $$ \\begin{array}{c|cccc} \\text{length $i$} & 1 & 2 & 3 & 4 \\\\ \\hline \\text{price $p_i$} & 1 & 20 & 33 & 36 \\\\ p_i / i & 1 & 10 & 11 & 1 \\end{array} $$ Let the given rod length be $4$. According to a greedy strategy, we first cut out a rod of length $3$ for a price of $33$, which leaves us with a rod of length $1$ of price $1$. The total price for the rod is $34$. The optimal way is to cut it into two rods of length $2$ each fetching us $40$ dollars. 15.1-3 Consider a modification of the rod-cutting problem in which, in addition to a price $p_i$ for each rod, each cut incurs a fixed cost of $c$. The revenue associated with a solution is now the sum of the prices of the pieces minus the costs of making the cuts. Give a dynamic-programming algorithm to solve this modified problem. 1 2 3 4 5 6 7 8 9 MODIFIED - CUT - ROD ( p , n , c ) let r [ 0. . n ] be a new array r [ 0 ] = 0 for j = 1 to n q = p [ j ] for i = 1 to j - 1 q = max ( q , p [ i ] + r [ j - i ] - c ) r [ j ] = q return r [ n ] The major modification required is in the body of the inner for loop, which now reads $q = \\max(q, p[i] + r[j - i] - c)$. This change re\ufb02ects the fixed cost of making the cut, which is deducted from the revenue. We also have to handle the case in which we make no cuts (when $i$ equals $j$); the total revenue in this case is simply $p[j]$. Thus, we modify the inner for loop to run from $i$ to $j - 1$ instead of to $j$. The assignment $q = p[j]$ takes care of the case of no cuts. If we did not make these modifications, then even in the case of no cuts, we would be deducting $c$ from the total revenue. 15.1-4 Modify $\\text{MEMOIZED-CUT-ROD}$ to return not only the value but the actual solution, too. 1 2 3 4 5 6 7 8 9 10 MEMOIZED - CUT - ROD ( p , n ) let r [ 0. . n ] and s [ 0. . n ] be new arrays for i = 0 to n r [ i ] = - \u221e ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) print \"The optimal value is\" val \"and the cuts are at\" j = n while j > 0 print s [ j ] j = j - s [ j ] 1 2 3 4 5 6 7 8 9 10 11 12 13 MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) if r [ n ] \u2265 0 return r [ n ] if n == 0 q = 0 else q = - \u221e for i = 1 to n ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n - i , r , s ) if q < p [ i ] + val q = p [ i ] + val s [ n ] = i r [ n ] = q return ( q , s ) $\\text{PRINT-CUT-ROD-SOLUTION}$ constructs the actual lengths where a cut should happen. Array entry $s[i]$ contains the value $j$ indicating that an optimal cut for a rod of length $i$ is $j$ inches. The next cut is given by $s[i - j]$, and so on. 15.1-5 The Fibonacci numbers are defined by recurrence $\\text{(3.22)}$. Give an $O(n)$-time dynamic-programming algorithm to compute the nth Fibonacci number. Draw the subproblem graph. How many vertices and edges are in the graph? 1 2 3 4 5 6 FIBONACCI ( n ) let fib [ 0. . n ] be a new array fib [ 0 ] = fib [ 1 ] = 1 for i = 2 to n fib [ i ] = fib [ i - 1 ] + fib [ i - 2 ] return fib [ n ] $\\text{FIBONACCI}$ directly implements the recurrence relation of the Fibonacci sequence. Each number in the sequence is the sum of the two previous numbers in the sequence. The running time is clearly $O(n)$. The subproblem graph consists of $n + 1$ vertices, $v_0, v_1, \\ldots, v_n$. For $i = 2, 3, \\ldots, n$, vertex $v_i$ has two leaving edges: to vertex $v_{i - 1}$ and to vertex $v_{i - 2}$. No edges leave vertices $v_0$ or $v_1$. Thus, the subproblem graph has $2n - 2$ edges.","title":"15.1 Rod cutting"},{"location":"Chap15/15.1/#151-1","text":"Show that equation $\\text{(15.4)}$ follows from equation $\\text{(15.3)}$ and the initial condition $T(0) = 1$. We can verify that $T(n) = 2^n$ is a solution to the given recurrence by the substitution method. We note that for $n = 0$, the formula is true since $2^0 = 1$. For $n > 0$, substituting into the recurrence and using the formula for summing a geometric series yields $$ \\begin{aligned} T(n) & = 1 + \\sum_{j = 0}^{n - 1} 2^j \\\\ & = 1 + (2^n - 1) \\\\ & = 2^n. \\end{aligned} $$","title":"15.1-1"},{"location":"Chap15/15.1/#151-2","text":"Show, by means of a counterexample, that the following \"greedy\" strategy does not always determine an optimal way to cut rods. Define the density of a rod of length $i$ to be $p_i / i$, that is, its value per inch. The greedy strategy for a rod of length $n$ cuts off a first piece of length $i$, where $1 \\le i \\le n$, having maximum density. It then continues by applying the greedy strategy to the remaining piece of length $n - i$. Here is a counterexample for the \"greedy\" strategy: $$ \\begin{array}{c|cccc} \\text{length $i$} & 1 & 2 & 3 & 4 \\\\ \\hline \\text{price $p_i$} & 1 & 20 & 33 & 36 \\\\ p_i / i & 1 & 10 & 11 & 1 \\end{array} $$ Let the given rod length be $4$. According to a greedy strategy, we first cut out a rod of length $3$ for a price of $33$, which leaves us with a rod of length $1$ of price $1$. The total price for the rod is $34$. The optimal way is to cut it into two rods of length $2$ each fetching us $40$ dollars.","title":"15.1-2"},{"location":"Chap15/15.1/#151-3","text":"Consider a modification of the rod-cutting problem in which, in addition to a price $p_i$ for each rod, each cut incurs a fixed cost of $c$. The revenue associated with a solution is now the sum of the prices of the pieces minus the costs of making the cuts. Give a dynamic-programming algorithm to solve this modified problem. 1 2 3 4 5 6 7 8 9 MODIFIED - CUT - ROD ( p , n , c ) let r [ 0. . n ] be a new array r [ 0 ] = 0 for j = 1 to n q = p [ j ] for i = 1 to j - 1 q = max ( q , p [ i ] + r [ j - i ] - c ) r [ j ] = q return r [ n ] The major modification required is in the body of the inner for loop, which now reads $q = \\max(q, p[i] + r[j - i] - c)$. This change re\ufb02ects the fixed cost of making the cut, which is deducted from the revenue. We also have to handle the case in which we make no cuts (when $i$ equals $j$); the total revenue in this case is simply $p[j]$. Thus, we modify the inner for loop to run from $i$ to $j - 1$ instead of to $j$. The assignment $q = p[j]$ takes care of the case of no cuts. If we did not make these modifications, then even in the case of no cuts, we would be deducting $c$ from the total revenue.","title":"15.1-3"},{"location":"Chap15/15.1/#151-4","text":"Modify $\\text{MEMOIZED-CUT-ROD}$ to return not only the value but the actual solution, too. 1 2 3 4 5 6 7 8 9 10 MEMOIZED - CUT - ROD ( p , n ) let r [ 0. . n ] and s [ 0. . n ] be new arrays for i = 0 to n r [ i ] = - \u221e ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) print \"The optimal value is\" val \"and the cuts are at\" j = n while j > 0 print s [ j ] j = j - s [ j ] 1 2 3 4 5 6 7 8 9 10 11 12 13 MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) if r [ n ] \u2265 0 return r [ n ] if n == 0 q = 0 else q = - \u221e for i = 1 to n ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n - i , r , s ) if q < p [ i ] + val q = p [ i ] + val s [ n ] = i r [ n ] = q return ( q , s ) $\\text{PRINT-CUT-ROD-SOLUTION}$ constructs the actual lengths where a cut should happen. Array entry $s[i]$ contains the value $j$ indicating that an optimal cut for a rod of length $i$ is $j$ inches. The next cut is given by $s[i - j]$, and so on.","title":"15.1-4"},{"location":"Chap15/15.1/#151-5","text":"The Fibonacci numbers are defined by recurrence $\\text{(3.22)}$. Give an $O(n)$-time dynamic-programming algorithm to compute the nth Fibonacci number. Draw the subproblem graph. How many vertices and edges are in the graph? 1 2 3 4 5 6 FIBONACCI ( n ) let fib [ 0. . n ] be a new array fib [ 0 ] = fib [ 1 ] = 1 for i = 2 to n fib [ i ] = fib [ i - 1 ] + fib [ i - 2 ] return fib [ n ] $\\text{FIBONACCI}$ directly implements the recurrence relation of the Fibonacci sequence. Each number in the sequence is the sum of the two previous numbers in the sequence. The running time is clearly $O(n)$. The subproblem graph consists of $n + 1$ vertices, $v_0, v_1, \\ldots, v_n$. For $i = 2, 3, \\ldots, n$, vertex $v_i$ has two leaving edges: to vertex $v_{i - 1}$ and to vertex $v_{i - 2}$. No edges leave vertices $v_0$ or $v_1$. Thus, the subproblem graph has $2n - 2$ edges.","title":"15.1-5"},{"location":"Chap15/15.2/","text":"15.2-1 Find an optimal parenthesization of a matrix-chain product whose sequence of dimensions is $\\langle 5, 10, 3, 12, 5, 50, 6 \\rangle$. $$((5 \\times 10)(10 \\times 3))(((3 \\times 12)(12 \\times 5))((5 \\times 50)(50 \\times 6))).$$ 15.2-2 Give a recursive algorithm $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j)$ that actually performs the optimal matrix-chain multiplication, given the sequence of matrices $\\langle A_1, A_2, \\ldots ,A_n \\rangle$, the $s$ table computed by $\\text{MATRIX-CHAIN-ORDER}$, and the indices $i$ and $j$. (The initial call would be $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)$.) 1 2 3 4 5 6 7 8 MATRIX - CHAIN - MULTIPLY ( A , s , i , j ) if i == j return A [ i ] if i + 1 == j return A [ i ] * A [ j ] b = MATRIX - CHAIN - MULTIPLY ( A , s , i , s [ i , j ]) c = MATRIX - CHAIN - MULTIPLY ( A , s , s [ i , j ] + 1 , j ) return b * c 15.2-3 Use the substitution method to show that the solution to the recurrence $\\text{(15.6)}$ is $\\Omega(2^n)$. Suppose $P(n) \\ge c2^n$, $$ \\begin{aligned} P(n) & \\ge \\sum_{k = 1}^{n - 1} c2^k \\cdot c2^{n - k} \\\\ & = \\sum_{k = 1}^{n - 1} c^2 2^n \\\\ & = c^2 (n - 1) 2^n \\\\ & \\ge c^2 2^n & (n > 1) \\\\ & \\ge c 2^n. & (c \\ge 1) \\end{aligned} $$ 15.2-4 Describe the subproblem graph for matrix-chain multiplication with an input chain of length $n$. How many vertices does it have? How many edges does it have, and which edges are they? The vertices of the subproblem graph are the ordered pairs $v_{ij}$, where $i \\le j$. If $i = j$, then there are no edges out of $v_{ij}$. If $i < j$, then for every $k$ such that $i \\le k < j$, the subproblem graph contains edges $(v_{ij}, v_{jk})$ and $(v_{ij}, v_{k + 1, j})$. These edges indicate that to solve the subproblem of optimally parenthesizing the product $A_i \\cdots A_j$, we need to solve subproblems of optimally parenthesizing the products $A_i \\cdots A_k$ and $A_{k + 1} \\cdots A_j$. The number of vertices is $$\\sum_{i = 1}^n\\sum_{j = i}^n 1 = \\frac{n(n + 1)}{2},$$ and the number of edges is $$ \\begin{aligned} \\sum_{i = 1}^n\\sum_{j = i}^n (j - i) & = \\sum_{i = 1}^n\\sum_{t = 0}^{n - i} t & \\text{(substituting $t = j - i$)} \\\\ & = \\sum_{i = 1}^n \\frac{(n - i)(n - i + 1)}{2}. \\end{aligned} $$ Substituting $r = n - i$ and reversing the order of summation, we obtain $$ \\begin{aligned} \\sum_{i = 1}^n \\frac{(n - i)(n - i + 1)}{2} & = \\frac{1}{2} \\sum_{r = 0}^{n - 1} (r^2 + r) \\\\ & = \\frac{1}{2} \\bigg(\\frac{(n - 1)n(2n - 1)}{6} + \\frac{(n - 1)n}{2}\\bigg) & \\text{(by equations (A.3) and (A.1))} \\\\ & = \\frac{(n - 1)n(n + 1)}{6}. \\end{aligned} $$ Thus, the subproblem graph has $\\Theta(n^2)$ vertices and $\\Theta(n^3)$ edges. 15.2-5 Let $R(i, j)$ be the number of times that table entry $m[i, j]$ is referenced while computing other table entries in a call of $\\text{MATRIX-CHAIN-ORDER}$. Show that the total number of references for the entire table is $$\\sum_{i = 1}^n \\sum_{j = i}^n R(i, j) = \\frac{n^3 - n}{3}.$$ ($\\textit{Hint:}$ You may find equation $\\text{(A.3)}$ useful.) Each time the $l$-loop executes, the $i$-loop executes $n - l + 1$ times. Each time the $i$-loop executes, the $k$-loop executes $j - i = l - 1$ times, each time referencing $m$ twice. Thus the total number of times that an entry of $m$ is referenced while computing other entries is $\\sum_{l = 2}^n (n - l + 1)(l - 1)2$. Thus, $$ \\begin{aligned} \\sum_{i = 1}^n\\sum_{j = 1}^n R(i, j) & = \\sum_{l = 2}^n (n - l + 1)(l - 1)2 \\\\ & = 2 \\sum_{l = 1}^{n - 1} (n - l)l \\\\ & = 2 \\sum_{l = 1}^{n - 1} nl - 2 \\sum_{l = 1}^{n - 1} l^2 \\\\ & = 2 \\frac{n(n - 1)n}{2} - 2\\frac{(n - 1)n(2n - 1)}{6} \\\\ & = n^3 - n^2 - \\frac{2n^3 - 3n^2 + n}{3} \\\\ & = \\frac{n^3 - n}{3}. \\end{aligned} $$ 15.2-6 Show that a full parenthesization of an $n$-element expression has exactly $n - 1$ pairs of parentheses. $n - 1$ multiplications.","title":"15.2 Matrix-chain multiplication"},{"location":"Chap15/15.2/#152-1","text":"Find an optimal parenthesization of a matrix-chain product whose sequence of dimensions is $\\langle 5, 10, 3, 12, 5, 50, 6 \\rangle$. $$((5 \\times 10)(10 \\times 3))(((3 \\times 12)(12 \\times 5))((5 \\times 50)(50 \\times 6))).$$","title":"15.2-1"},{"location":"Chap15/15.2/#152-2","text":"Give a recursive algorithm $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j)$ that actually performs the optimal matrix-chain multiplication, given the sequence of matrices $\\langle A_1, A_2, \\ldots ,A_n \\rangle$, the $s$ table computed by $\\text{MATRIX-CHAIN-ORDER}$, and the indices $i$ and $j$. (The initial call would be $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)$.) 1 2 3 4 5 6 7 8 MATRIX - CHAIN - MULTIPLY ( A , s , i , j ) if i == j return A [ i ] if i + 1 == j return A [ i ] * A [ j ] b = MATRIX - CHAIN - MULTIPLY ( A , s , i , s [ i , j ]) c = MATRIX - CHAIN - MULTIPLY ( A , s , s [ i , j ] + 1 , j ) return b * c","title":"15.2-2"},{"location":"Chap15/15.2/#152-3","text":"Use the substitution method to show that the solution to the recurrence $\\text{(15.6)}$ is $\\Omega(2^n)$. Suppose $P(n) \\ge c2^n$, $$ \\begin{aligned} P(n) & \\ge \\sum_{k = 1}^{n - 1} c2^k \\cdot c2^{n - k} \\\\ & = \\sum_{k = 1}^{n - 1} c^2 2^n \\\\ & = c^2 (n - 1) 2^n \\\\ & \\ge c^2 2^n & (n > 1) \\\\ & \\ge c 2^n. & (c \\ge 1) \\end{aligned} $$","title":"15.2-3"},{"location":"Chap15/15.2/#152-4","text":"Describe the subproblem graph for matrix-chain multiplication with an input chain of length $n$. How many vertices does it have? How many edges does it have, and which edges are they? The vertices of the subproblem graph are the ordered pairs $v_{ij}$, where $i \\le j$. If $i = j$, then there are no edges out of $v_{ij}$. If $i < j$, then for every $k$ such that $i \\le k < j$, the subproblem graph contains edges $(v_{ij}, v_{jk})$ and $(v_{ij}, v_{k + 1, j})$. These edges indicate that to solve the subproblem of optimally parenthesizing the product $A_i \\cdots A_j$, we need to solve subproblems of optimally parenthesizing the products $A_i \\cdots A_k$ and $A_{k + 1} \\cdots A_j$. The number of vertices is $$\\sum_{i = 1}^n\\sum_{j = i}^n 1 = \\frac{n(n + 1)}{2},$$ and the number of edges is $$ \\begin{aligned} \\sum_{i = 1}^n\\sum_{j = i}^n (j - i) & = \\sum_{i = 1}^n\\sum_{t = 0}^{n - i} t & \\text{(substituting $t = j - i$)} \\\\ & = \\sum_{i = 1}^n \\frac{(n - i)(n - i + 1)}{2}. \\end{aligned} $$ Substituting $r = n - i$ and reversing the order of summation, we obtain $$ \\begin{aligned} \\sum_{i = 1}^n \\frac{(n - i)(n - i + 1)}{2} & = \\frac{1}{2} \\sum_{r = 0}^{n - 1} (r^2 + r) \\\\ & = \\frac{1}{2} \\bigg(\\frac{(n - 1)n(2n - 1)}{6} + \\frac{(n - 1)n}{2}\\bigg) & \\text{(by equations (A.3) and (A.1))} \\\\ & = \\frac{(n - 1)n(n + 1)}{6}. \\end{aligned} $$ Thus, the subproblem graph has $\\Theta(n^2)$ vertices and $\\Theta(n^3)$ edges.","title":"15.2-4"},{"location":"Chap15/15.2/#152-5","text":"Let $R(i, j)$ be the number of times that table entry $m[i, j]$ is referenced while computing other table entries in a call of $\\text{MATRIX-CHAIN-ORDER}$. Show that the total number of references for the entire table is $$\\sum_{i = 1}^n \\sum_{j = i}^n R(i, j) = \\frac{n^3 - n}{3}.$$ ($\\textit{Hint:}$ You may find equation $\\text{(A.3)}$ useful.) Each time the $l$-loop executes, the $i$-loop executes $n - l + 1$ times. Each time the $i$-loop executes, the $k$-loop executes $j - i = l - 1$ times, each time referencing $m$ twice. Thus the total number of times that an entry of $m$ is referenced while computing other entries is $\\sum_{l = 2}^n (n - l + 1)(l - 1)2$. Thus, $$ \\begin{aligned} \\sum_{i = 1}^n\\sum_{j = 1}^n R(i, j) & = \\sum_{l = 2}^n (n - l + 1)(l - 1)2 \\\\ & = 2 \\sum_{l = 1}^{n - 1} (n - l)l \\\\ & = 2 \\sum_{l = 1}^{n - 1} nl - 2 \\sum_{l = 1}^{n - 1} l^2 \\\\ & = 2 \\frac{n(n - 1)n}{2} - 2\\frac{(n - 1)n(2n - 1)}{6} \\\\ & = n^3 - n^2 - \\frac{2n^3 - 3n^2 + n}{3} \\\\ & = \\frac{n^3 - n}{3}. \\end{aligned} $$","title":"15.2-5"},{"location":"Chap15/15.2/#152-6","text":"Show that a full parenthesization of an $n$-element expression has exactly $n - 1$ pairs of parentheses. $n - 1$ multiplications.","title":"15.2-6"},{"location":"Chap15/15.3/","text":"15.3-1 Which is a more efficient way to determine the optimal number of multiplications in a matrix-chain multiplication problem: enumerating all the ways of parenthesizing the product and computing the number of multiplications for each, or running $\\text{RECURSIVE-MATRIX-CHAIN}$? Justify your answer. Running $\\text{RECURSIVE-MATRIX-CHAIN}$ is asymptotically more efficient than enumerating all the ways of parenthesizing the product and computing the number of multiplications for each. Consider the treatment of subproblems by the two approaches. For each possible place to split the matrix chain, the enumeration approach \u00dends all ways to parenthesize the left half, finds all ways to parenthesize the right half, and looks at all possible combinations of the left half with the right half. The amount of work to look at each combination of left- and right-half subproblem results is thus the product of the number of ways to do the left half and the number of ways to do the right half. For each possible place to split the matrix chain, $\\text{RECURSIVE-MATRIX-CHAIN}$ finds the best way to parenthesize the left half, finds the best way to parenthesize the right half, and combines just those two results. Thus the amount of work to combine the left- and right-half subproblem results is $O(1)$. Section 15.2 argued that the running time for enumeration is $\\Omega(4^n / n^{3 / 2})$. We will show that the running time for $\\text{RECURSIVE-MATRIX-CHAIN}$ is $O(n3^{n - 1})$. To get an upper bound on the running time of $\\text{RECURSIVE-MATRIX-CHAIN}$, we'll use the same approach used in Section 15.2 to get a lower bound: Derive a recurrence of the form $T(n) \\le \\ldots$ and solve it by substitution. For the lower-bound recurrence, the book assumed that the execution of lines 1\u20132 and 6\u20137 each take at least unit time. For the upper-bound recurrence, we'll assume those pairs of lines each take at most constant time $c$. Thus, we have the recurrence $$ T(n) \\le \\begin{cases} c & \\text{if $n = 1$}, \\\\ c + \\sum_{k = 1}^{n - 1} (T(k) + T(n - k) + c) & \\text{if $n \\ge 2$}. \\\\ \\end{cases} $$ This is just like the book's $\\ge$ recurrence except that it has $c$ instead of $1$, and so we can be rewrite it as $$T(n) \\le 2 \\sum_{i = 1}^{n - 1} T(i) + cn.$$ We shall prove that $T(n) = O(n3^{n - 1})$ using the substitution method. (Note: Any upper bound on $T(n)$ that is $o(4^n / n^{3 / 2})$ will suffice. You might prefer to prove one that is easier to think up, such as $T(n) = O(3.5^n)$.) Specifically, we shall show that $T(n) \\le cn3^{n - 1}$ for all $n \\ge 1$. The basis is easy, since $T(1) \\le c = c \\cdot 1 \\cdot 3^{1 - 1}$ . Inductively, for $n \\ge 2$ we have $$ \\begin{aligned} T(n) & \\le 2\\sum_{i = 1}^{n - 1} T(i) + cn \\\\ & \\le 2\\sum_{i = 1}^{n - 1} ci 3^{i - 1} + cn \\\\ & \\le c \\cdot \\bigg(2\\sum_{i = 1}^{n - 1}i 3^{i - 1} + n\\bigg) \\\\ & = c \\cdot \\bigg(2\\cdot\\bigg(\\frac{n 3^{n - 1}}{3 - 1} + \\frac{1 - 3^n}{(3 - 1)^2}\\bigg) + n\\bigg) & \\text{(see below)} \\\\ & = cn 3^{n - 1} + c\\cdot\\bigg(\\frac{1 - 3^n}{2} + n\\bigg) \\\\ & = cn 3^{n - 1} + \\frac{c}{2}(2n + 1 - 3^n) \\\\ & \\le cn 3^{n - 1} & \\text{for all $c > 0, n \\ge 1$}. \\end{aligned} $$ Running $\\text{RECURSIVE-MATRIX-CHAIN}$ takes $O(n3^{n - 1})$ time, and enumerating all parenthesizations takes $(4^n / n^{3 / 2})$ time, and so $\\text{RECURSIVE-MATRIX-CHAIN}$ is more efficient than enumeration. Note: The above substitution uses the fact that $$\\sum_{i = 1}^{n - 1} ix^{i - 1} = \\frac{nx^{n - 1}}{x - 1} + \\frac{1 - x^n}{(x - 1)^2}.$$ This equation can be derived from equation $\\text{(A.5)}$ by taking the derivative. Let $$f(x) = \\sum_{i = 1}^{n - 1} x^i = \\frac{x^n - 1}{x - 1} - 1.$$ Then $$\\sum_{i = 1}^{n - 1} ix^{i - 1} = f'(x) = \\frac{nx^{n - 1}}{x - 1} + \\frac{1 - x^n}{(x - 1)^2}.$$ 15.3-2 Draw the recursion tree for the $\\text{MERGE-SORT}$ procedure from Section 2.3.1 on an array of $16$ elements. Explain why memoization fails to speed up a good divide-and-conquer algorithm such as $\\text{MERGE-SORT}$. Draw a recursion tree. The $\\text{MERGE-SORT}$ procedure performs at most a single call to any pair of indices of the array that is being sorted. In other words, the subproblems do not overlap and therefore memoization will not improve the running time. 15.3-3 Consider a variant of the matrix-chain multiplication problem in which the goal is to parenthesize the sequence of matrices so as to maximize, rather than minimize, the number of scalar multiplications. Does this problem exhibit optimal substructure? Yes, this problem also exhibits optimal substructure. If we know that we need the subproduct $(A_l \\cdot A_r)$, then we should still find the most expensive way to compute it \u2014 otherwise, we could do better by substituting in the most expensive way. 15.3-4 As stated, in dynamic programming we first solve the subproblems and then choose which of them to use in an optimal solution to the problem. Professor Capulet claims that we do not always need to solve all the subproblems in order to find an optimal solution. She suggests that we can find an optimal solution to the matrix-chain multiplication problem by always choosing the matrix $A_k$ at which to split the subproduct $A_i A_{i + 1} \\cdots A_j$ (by selecting $k$ to minimize the quantity $p_{i - 1} p_k p_j$) before solving the subproblems. Find an instance of the matrix-chain multiplication problem for which this greedy approach yields a suboptimal solution. Suppose that we are given matrices $A_1$, $A_2$, $A_3$, and $A_4$ with dimensions such that $$p_0, p_1, p_2, p_3, p_4 = 1000, 100, 20, 10, 1000.$$ Then $p_0 p_k p_4$ is minimized when $k = 3$, so we need to solve the subproblem of multiplying $A_1 A_2 A_3$, and also $A_4$ which is solved automatically. By her algorithm, this is solved by splitting at $k = 2$. Thus, the full parenthesization is $(((A_1A_2)A_3)A_4)$. This requires $$1000 \\cdot 100 \\cdot 20 + 1000 \\cdot 20 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 12200000$$ scalar multiplications. On the other hand, suppose we had fully parenthesized the matrices to multiply as $((A_1(A_2A_3))A_4)$. Then we would only require $$100 \\cdot 20 \\cdot 10 + 1000 \\cdot 100 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 11020000$$ scalar multiplications, which is fewer than Professor Capulet's method. Therefore her greedy approach yields a suboptimal solution. 15.3-5 Suppose that in the rod-cutting problem of Section 15.1, we also had limit $l_i$ on the number of pieces of length $i$ that we are allowed to produce, for $i = 1, 2, \\ldots, n$. Show that the optimal-substructure property described in Section 15.1 no longer holds. We say that a problem exhibits the optimal substructure property when optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently (i.e., they do not share resources). When we impose a limit $l_i$ on the number of pieces of size $i$ that we are permitted to produce, the subproblems can no longer be solved independently . For example, consider a rod of length $4$ with the following prices and limits: $$ \\begin{array}{c|cccc} \\text{length $i$} & 1 & 2 & 3 & 4 \\\\ \\hline \\text{price $p_i$} & 15 & 20 & 33 & 36 \\\\ \\text{limit $l_i$} & 2 & 1 & 1 & 1 \\end{array} $$ This instance has only three solutions that do not violate the limits: length $4$ with price $36$; lengths $1$ and $3$ with price $48$; and lengths $1$, $1$, and $2$ with price $50$. The optimal solution, therefore is to cut into lengths $1$, $1$, and $2$. When we look at the subproblem for length $2$, it has two solutions that do not violate the limits: length $2$ with price $20$, and lengths $1$ and $1$ with price $30$. The optimal solution for length $2$, therefore, is to cut into lengths $1$ and $1$. But we cannot use this optimal solution for the subproblem in the optimal solution for the original problem, because it would result in using four rods of length $1$ to solve the original problem, violating the limit of two length-$1$ rods. 15.3-6 Imagine that you wish to exchange one currency for another. You realize that instead of directly exchanging one currency for another, you might be better off making a series of trades through other currencies, winding up with the currency you want. Suppose that you can trade $n$ different currencies, numbered $1, 2, \\ldots, n$, where you start with currency $1$ and wish to wind up with currency $n$. You are given, for each pair of currencies $i$ and $j$ , an exchange rate $r_{ij}$, meaning that if you start with $d$ units of currency $i$ , you can trade for $dr_{ij}$ units of currency $j$. A sequence of trades may entail a commission, which depends on the number of trades you make. Let $c_k$ be the commission that you are charged when you make $k$ trades. Show that, if $c_k = 0$ for all $k = 1, 2, \\ldots, n$, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ exhibits optimal substructure. Then show that if commissions $c_k$ are arbitrary values, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ does not necessarily exhibit optimal substructure. Any solution must add the additional assumption that no currency can be repeated in a sequence of trades. Without this assumption, if $r_{ij} > 1 / r_{ji}$ for some currencies $i$ and $j$, we could repeatedly exchange $i \\to j \\to i \\to j \\to \\cdot$ and make an unbounded profit. To see that this problem has optimal substructure when $c_k = 0$ for all $k$, observe that the problem of exchanging currency $a$ for currency $b$ is equivalent to finding a sequence of currencies $k_1, k_2, \\ldots, k_m$ such that $k_1 = a$, $k_m = b$ and the product $r_{k_1k_2}r_{k_2k_3}$ is maximized. We use the usual cut-and-paste argument. Suppose that an optimal solution contains a sequence $\\langle k_i, k_{i + 1}, \\ldots, k_j \\rangle$ of currencies, and suppose that there exists a sequence $\\langle k_i', k_{i + 1}', \\ldots, k_j' \\rangle$, such that $k_i' = k_i$, $k_j' = k_j$, and $r_{k_i' k_{i + 1}'} \\cdots r_{k_{j - 1}' k_j'} > r_{k_i k_{i + 1}} \\cdots r_{k_{j - 1}k_j}$. Then we could substitute the sequence $\\langle k_i', k_{i + 1}', \\ldots, k_j' \\rangle$ for the sequence $\\langle k_i, k_{i + 1}, \\ldots, k_j \\rangle$ in the optimal solution to create an even better solution. We show that optimal substructure does not hold when the $c_k$ are arbitrary values by means of an example. Suppose we have four currencies, with the following exchange rates: $$ \\begin{array}{c|cccc} r_{ij} & 1 & 2 & 3 & 4 \\\\ \\hline 1 & 1 & 2 & 5 / 2 & 6 \\\\ 2 & 1 / 2 & 1 & 3 / 2 & 3 \\\\ 3 & 2 / 5 & 2 / 3 & 1 & 3 \\\\ 4 & 1 / 6 & 1 / 3 & 1 / 3 & 1 \\end{array} $$ Let $c_1 = 2$ and $c_2 = c_3 = 3$. Note that this example is not too badly contrived, in that $r_{ji} = 1 / r_{ij}$ for all $i$ and $j$. To see how this example does not exhibit optimal substructure, let's examine an optimal solution for exchanging currency $1$ for currency $4$. There are five possible exchange sequences, with the following costs: $$ \\begin{array}{lll} \\langle 1, 4 \\rangle & : 6 - 2 & = 4, \\\\ \\langle 1, 2, 4 \\rangle & : 2 \\cdot 3 - 3 & = 3, \\\\ \\langle 1, 3, 4 \\rangle & : 5 / 2 \\cdot 3 - 3 & = 9 / 2, \\\\ \\langle 1, 2, 3, 4 \\rangle & : 2 \\cdot 3 / 2 \\cdot 3 - 3 & = 6, \\\\ \\langle 1, 3, 2, 4 \\rangle & : 5 / 2 \\cdot 2 / 3 \\cdot 3 - 3 & = 2. \\end{array} $$ The optimal exchange sequence, $\\langle 1, 2, 3, 4 \\rangle$, appears in boldface. Let's examine the subproblem of exchanging currency $1$ for currency $3$. Allowing currency $4$ to be part of the exchange sequence, there are again five possible exchange sequences with the following costs and the optimal one in boldface: $$ \\begin{array}{lll} \\langle 1, 3 \\rangle & : 5 / 2 - 2 & = 1 / 2, \\\\ \\langle 1, 2, 3 \\rangle & : 2 \\cdot 3 / 2 - 3 & = 0, \\\\ \\langle 1, 4, 3 \\rangle & : 6 \\cdot 1 / 3 - 3 & = -1, \\\\ \\langle 1, 2, 4, 3 \\rangle & : 2 \\cdot 3 \\cdot 1 / 3 - 3 & = -1, \\\\ \\langle 1, 4, 2, 3 \\rangle & : 6 \\cdot 1 / 3 \\cdot 3 / 2 & = 0. \\end{array} $$ We see that the solution to the original problem includes the subproblem of exchanging currency $1$ for currency $3$, yet the solution $\\langle 1, 2, 3 \\rangle$ to the subproblem used in the optimal solution to the original problem is not the optimal solution $\\langle 1, 3 \\rangle$ to the subproblem on its own.","title":"15.3 Elements of dynamic programming"},{"location":"Chap15/15.3/#153-1","text":"Which is a more efficient way to determine the optimal number of multiplications in a matrix-chain multiplication problem: enumerating all the ways of parenthesizing the product and computing the number of multiplications for each, or running $\\text{RECURSIVE-MATRIX-CHAIN}$? Justify your answer. Running $\\text{RECURSIVE-MATRIX-CHAIN}$ is asymptotically more efficient than enumerating all the ways of parenthesizing the product and computing the number of multiplications for each. Consider the treatment of subproblems by the two approaches. For each possible place to split the matrix chain, the enumeration approach \u00dends all ways to parenthesize the left half, finds all ways to parenthesize the right half, and looks at all possible combinations of the left half with the right half. The amount of work to look at each combination of left- and right-half subproblem results is thus the product of the number of ways to do the left half and the number of ways to do the right half. For each possible place to split the matrix chain, $\\text{RECURSIVE-MATRIX-CHAIN}$ finds the best way to parenthesize the left half, finds the best way to parenthesize the right half, and combines just those two results. Thus the amount of work to combine the left- and right-half subproblem results is $O(1)$. Section 15.2 argued that the running time for enumeration is $\\Omega(4^n / n^{3 / 2})$. We will show that the running time for $\\text{RECURSIVE-MATRIX-CHAIN}$ is $O(n3^{n - 1})$. To get an upper bound on the running time of $\\text{RECURSIVE-MATRIX-CHAIN}$, we'll use the same approach used in Section 15.2 to get a lower bound: Derive a recurrence of the form $T(n) \\le \\ldots$ and solve it by substitution. For the lower-bound recurrence, the book assumed that the execution of lines 1\u20132 and 6\u20137 each take at least unit time. For the upper-bound recurrence, we'll assume those pairs of lines each take at most constant time $c$. Thus, we have the recurrence $$ T(n) \\le \\begin{cases} c & \\text{if $n = 1$}, \\\\ c + \\sum_{k = 1}^{n - 1} (T(k) + T(n - k) + c) & \\text{if $n \\ge 2$}. \\\\ \\end{cases} $$ This is just like the book's $\\ge$ recurrence except that it has $c$ instead of $1$, and so we can be rewrite it as $$T(n) \\le 2 \\sum_{i = 1}^{n - 1} T(i) + cn.$$ We shall prove that $T(n) = O(n3^{n - 1})$ using the substitution method. (Note: Any upper bound on $T(n)$ that is $o(4^n / n^{3 / 2})$ will suffice. You might prefer to prove one that is easier to think up, such as $T(n) = O(3.5^n)$.) Specifically, we shall show that $T(n) \\le cn3^{n - 1}$ for all $n \\ge 1$. The basis is easy, since $T(1) \\le c = c \\cdot 1 \\cdot 3^{1 - 1}$ . Inductively, for $n \\ge 2$ we have $$ \\begin{aligned} T(n) & \\le 2\\sum_{i = 1}^{n - 1} T(i) + cn \\\\ & \\le 2\\sum_{i = 1}^{n - 1} ci 3^{i - 1} + cn \\\\ & \\le c \\cdot \\bigg(2\\sum_{i = 1}^{n - 1}i 3^{i - 1} + n\\bigg) \\\\ & = c \\cdot \\bigg(2\\cdot\\bigg(\\frac{n 3^{n - 1}}{3 - 1} + \\frac{1 - 3^n}{(3 - 1)^2}\\bigg) + n\\bigg) & \\text{(see below)} \\\\ & = cn 3^{n - 1} + c\\cdot\\bigg(\\frac{1 - 3^n}{2} + n\\bigg) \\\\ & = cn 3^{n - 1} + \\frac{c}{2}(2n + 1 - 3^n) \\\\ & \\le cn 3^{n - 1} & \\text{for all $c > 0, n \\ge 1$}. \\end{aligned} $$ Running $\\text{RECURSIVE-MATRIX-CHAIN}$ takes $O(n3^{n - 1})$ time, and enumerating all parenthesizations takes $(4^n / n^{3 / 2})$ time, and so $\\text{RECURSIVE-MATRIX-CHAIN}$ is more efficient than enumeration. Note: The above substitution uses the fact that $$\\sum_{i = 1}^{n - 1} ix^{i - 1} = \\frac{nx^{n - 1}}{x - 1} + \\frac{1 - x^n}{(x - 1)^2}.$$ This equation can be derived from equation $\\text{(A.5)}$ by taking the derivative. Let $$f(x) = \\sum_{i = 1}^{n - 1} x^i = \\frac{x^n - 1}{x - 1} - 1.$$ Then $$\\sum_{i = 1}^{n - 1} ix^{i - 1} = f'(x) = \\frac{nx^{n - 1}}{x - 1} + \\frac{1 - x^n}{(x - 1)^2}.$$","title":"15.3-1"},{"location":"Chap15/15.3/#153-2","text":"Draw the recursion tree for the $\\text{MERGE-SORT}$ procedure from Section 2.3.1 on an array of $16$ elements. Explain why memoization fails to speed up a good divide-and-conquer algorithm such as $\\text{MERGE-SORT}$. Draw a recursion tree. The $\\text{MERGE-SORT}$ procedure performs at most a single call to any pair of indices of the array that is being sorted. In other words, the subproblems do not overlap and therefore memoization will not improve the running time.","title":"15.3-2"},{"location":"Chap15/15.3/#153-3","text":"Consider a variant of the matrix-chain multiplication problem in which the goal is to parenthesize the sequence of matrices so as to maximize, rather than minimize, the number of scalar multiplications. Does this problem exhibit optimal substructure? Yes, this problem also exhibits optimal substructure. If we know that we need the subproduct $(A_l \\cdot A_r)$, then we should still find the most expensive way to compute it \u2014 otherwise, we could do better by substituting in the most expensive way.","title":"15.3-3"},{"location":"Chap15/15.3/#153-4","text":"As stated, in dynamic programming we first solve the subproblems and then choose which of them to use in an optimal solution to the problem. Professor Capulet claims that we do not always need to solve all the subproblems in order to find an optimal solution. She suggests that we can find an optimal solution to the matrix-chain multiplication problem by always choosing the matrix $A_k$ at which to split the subproduct $A_i A_{i + 1} \\cdots A_j$ (by selecting $k$ to minimize the quantity $p_{i - 1} p_k p_j$) before solving the subproblems. Find an instance of the matrix-chain multiplication problem for which this greedy approach yields a suboptimal solution. Suppose that we are given matrices $A_1$, $A_2$, $A_3$, and $A_4$ with dimensions such that $$p_0, p_1, p_2, p_3, p_4 = 1000, 100, 20, 10, 1000.$$ Then $p_0 p_k p_4$ is minimized when $k = 3$, so we need to solve the subproblem of multiplying $A_1 A_2 A_3$, and also $A_4$ which is solved automatically. By her algorithm, this is solved by splitting at $k = 2$. Thus, the full parenthesization is $(((A_1A_2)A_3)A_4)$. This requires $$1000 \\cdot 100 \\cdot 20 + 1000 \\cdot 20 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 12200000$$ scalar multiplications. On the other hand, suppose we had fully parenthesized the matrices to multiply as $((A_1(A_2A_3))A_4)$. Then we would only require $$100 \\cdot 20 \\cdot 10 + 1000 \\cdot 100 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 11020000$$ scalar multiplications, which is fewer than Professor Capulet's method. Therefore her greedy approach yields a suboptimal solution.","title":"15.3-4"},{"location":"Chap15/15.3/#153-5","text":"Suppose that in the rod-cutting problem of Section 15.1, we also had limit $l_i$ on the number of pieces of length $i$ that we are allowed to produce, for $i = 1, 2, \\ldots, n$. Show that the optimal-substructure property described in Section 15.1 no longer holds. We say that a problem exhibits the optimal substructure property when optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently (i.e., they do not share resources). When we impose a limit $l_i$ on the number of pieces of size $i$ that we are permitted to produce, the subproblems can no longer be solved independently . For example, consider a rod of length $4$ with the following prices and limits: $$ \\begin{array}{c|cccc} \\text{length $i$} & 1 & 2 & 3 & 4 \\\\ \\hline \\text{price $p_i$} & 15 & 20 & 33 & 36 \\\\ \\text{limit $l_i$} & 2 & 1 & 1 & 1 \\end{array} $$ This instance has only three solutions that do not violate the limits: length $4$ with price $36$; lengths $1$ and $3$ with price $48$; and lengths $1$, $1$, and $2$ with price $50$. The optimal solution, therefore is to cut into lengths $1$, $1$, and $2$. When we look at the subproblem for length $2$, it has two solutions that do not violate the limits: length $2$ with price $20$, and lengths $1$ and $1$ with price $30$. The optimal solution for length $2$, therefore, is to cut into lengths $1$ and $1$. But we cannot use this optimal solution for the subproblem in the optimal solution for the original problem, because it would result in using four rods of length $1$ to solve the original problem, violating the limit of two length-$1$ rods.","title":"15.3-5"},{"location":"Chap15/15.3/#153-6","text":"Imagine that you wish to exchange one currency for another. You realize that instead of directly exchanging one currency for another, you might be better off making a series of trades through other currencies, winding up with the currency you want. Suppose that you can trade $n$ different currencies, numbered $1, 2, \\ldots, n$, where you start with currency $1$ and wish to wind up with currency $n$. You are given, for each pair of currencies $i$ and $j$ , an exchange rate $r_{ij}$, meaning that if you start with $d$ units of currency $i$ , you can trade for $dr_{ij}$ units of currency $j$. A sequence of trades may entail a commission, which depends on the number of trades you make. Let $c_k$ be the commission that you are charged when you make $k$ trades. Show that, if $c_k = 0$ for all $k = 1, 2, \\ldots, n$, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ exhibits optimal substructure. Then show that if commissions $c_k$ are arbitrary values, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ does not necessarily exhibit optimal substructure. Any solution must add the additional assumption that no currency can be repeated in a sequence of trades. Without this assumption, if $r_{ij} > 1 / r_{ji}$ for some currencies $i$ and $j$, we could repeatedly exchange $i \\to j \\to i \\to j \\to \\cdot$ and make an unbounded profit. To see that this problem has optimal substructure when $c_k = 0$ for all $k$, observe that the problem of exchanging currency $a$ for currency $b$ is equivalent to finding a sequence of currencies $k_1, k_2, \\ldots, k_m$ such that $k_1 = a$, $k_m = b$ and the product $r_{k_1k_2}r_{k_2k_3}$ is maximized. We use the usual cut-and-paste argument. Suppose that an optimal solution contains a sequence $\\langle k_i, k_{i + 1}, \\ldots, k_j \\rangle$ of currencies, and suppose that there exists a sequence $\\langle k_i', k_{i + 1}', \\ldots, k_j' \\rangle$, such that $k_i' = k_i$, $k_j' = k_j$, and $r_{k_i' k_{i + 1}'} \\cdots r_{k_{j - 1}' k_j'} > r_{k_i k_{i + 1}} \\cdots r_{k_{j - 1}k_j}$. Then we could substitute the sequence $\\langle k_i', k_{i + 1}', \\ldots, k_j' \\rangle$ for the sequence $\\langle k_i, k_{i + 1}, \\ldots, k_j \\rangle$ in the optimal solution to create an even better solution. We show that optimal substructure does not hold when the $c_k$ are arbitrary values by means of an example. Suppose we have four currencies, with the following exchange rates: $$ \\begin{array}{c|cccc} r_{ij} & 1 & 2 & 3 & 4 \\\\ \\hline 1 & 1 & 2 & 5 / 2 & 6 \\\\ 2 & 1 / 2 & 1 & 3 / 2 & 3 \\\\ 3 & 2 / 5 & 2 / 3 & 1 & 3 \\\\ 4 & 1 / 6 & 1 / 3 & 1 / 3 & 1 \\end{array} $$ Let $c_1 = 2$ and $c_2 = c_3 = 3$. Note that this example is not too badly contrived, in that $r_{ji} = 1 / r_{ij}$ for all $i$ and $j$. To see how this example does not exhibit optimal substructure, let's examine an optimal solution for exchanging currency $1$ for currency $4$. There are five possible exchange sequences, with the following costs: $$ \\begin{array}{lll} \\langle 1, 4 \\rangle & : 6 - 2 & = 4, \\\\ \\langle 1, 2, 4 \\rangle & : 2 \\cdot 3 - 3 & = 3, \\\\ \\langle 1, 3, 4 \\rangle & : 5 / 2 \\cdot 3 - 3 & = 9 / 2, \\\\ \\langle 1, 2, 3, 4 \\rangle & : 2 \\cdot 3 / 2 \\cdot 3 - 3 & = 6, \\\\ \\langle 1, 3, 2, 4 \\rangle & : 5 / 2 \\cdot 2 / 3 \\cdot 3 - 3 & = 2. \\end{array} $$ The optimal exchange sequence, $\\langle 1, 2, 3, 4 \\rangle$, appears in boldface. Let's examine the subproblem of exchanging currency $1$ for currency $3$. Allowing currency $4$ to be part of the exchange sequence, there are again five possible exchange sequences with the following costs and the optimal one in boldface: $$ \\begin{array}{lll} \\langle 1, 3 \\rangle & : 5 / 2 - 2 & = 1 / 2, \\\\ \\langle 1, 2, 3 \\rangle & : 2 \\cdot 3 / 2 - 3 & = 0, \\\\ \\langle 1, 4, 3 \\rangle & : 6 \\cdot 1 / 3 - 3 & = -1, \\\\ \\langle 1, 2, 4, 3 \\rangle & : 2 \\cdot 3 \\cdot 1 / 3 - 3 & = -1, \\\\ \\langle 1, 4, 2, 3 \\rangle & : 6 \\cdot 1 / 3 \\cdot 3 / 2 & = 0. \\end{array} $$ We see that the solution to the original problem includes the subproblem of exchanging currency $1$ for currency $3$, yet the solution $\\langle 1, 2, 3 \\rangle$ to the subproblem used in the optimal solution to the original problem is not the optimal solution $\\langle 1, 3 \\rangle$ to the subproblem on its own.","title":"15.3-6"},{"location":"Chap15/15.4/","text":"15.4-1 Determine an $\\text{LCS}$ of $\\langle 1, 0, 0, 1, 0, 1, 0, 1 \\rangle$ and $\\langle 0, 1, 0, 1, 1, 0, 1, 1, 0 \\rangle$. $\\langle 1, 0, 0, 1, 1, 0 \\rangle$. 15.4-2 Give pseudocode to reconstruct an $\\text{LCS}$ from the completed $c$ table and the original sequences $X = \\langle x_1, x_2, \\ldots, x_m \\rangle$ and $Y = \\langle y_1, y_2, \\ldots, y_n \\rangle$ in $O(m + n)$ time, without using the $b$ table. 1 2 3 4 5 6 7 8 9 10 PRINT - LCS ( c , X , Y , i , j ) if c [ i , j ] == 0 return if X [ i ] == Y [ j ] PRINT - LCS ( c , X , Y , i - 1 , j - 1 ) print X [ i ] else if c [ i - 1 , j ] > c [ i , j - 1 ] PRINT - LCS ( c , X , Y , i - 1 , j ) else PRINT - LCS ( c , X , Y , i , j - 1 ) 15.4-3 Give a memoized version of $\\text{LCS-LENGTH}$ that runs in $O(mn)$ time. 1 2 3 4 5 6 7 8 MEMOIZED - LCS - LENGTH ( X , Y , i , j ) if c [ i , j ] > - 1 return c [ i , j ] if i == 0 or j == 0 return c [ i , j ] = 0 if x [ i ] == y [ j ] return c [ i , j ] = LCS - LENGTH ( X , Y , i - 1 , j - 1 ) + 1 return c [ i , j ] = max ( LCS - LENGTH ( X , Y , i - 1 , j ), LCS - LENGTH ( X , Y , i , j - 1 )) 15.4-4 Show how to compute the length of an $\\text{LCS}$ using only $2 \\cdot \\min(m, n)$ entries in the $c$ table plus $O(1)$ additional space. Then show how to do the same thing, but using $\\min(m, n)$ entries plus $O(1)$ additional space. When computing a particular row of the $c$ table, no rows before the previous row are needed. Thus only two rows\u2014$2\u00b7length[Y]$ entries\u2014need to be kept in memory at a time. (Note: Each row of $c$ actually has $length[Y] + 1$ entries, but we don't need to store the column of $0$'s\u2014instead we can make the program \"know\" that those entries are $0$.) With this idea, we need only $2 \\cdot \\min(m, n)$ entries if we always call $\\text{LCS-LENGTH}$ with the shorter sequence as the $Y$ argument. We can thus do away with the $c$ table as follows: Use two arrays of length $\\min(m, n)$, $previous\\text-row$ and $current\\text-row$, to hold the appropriate rows of $c$. Initialize $previous\\text-row$ to all $0$ and compute $current\\text-row$ from left to right. When $current\\text-row$ is filled, if there are still more rows to compute, copy $current\\text-row$ into $previous\\text-row$ and compute the new $current\\text-row$. Actually only a little more than one row's worth of $c$ entries\u2014$\\min(m, n) + 1$ entries\u2014are needed during the computation. The only entries needed in the table when it is time to compute $c[i, j]$ are $c[i, k]$ for $k \\le j - 1$ (i.e., earlier entries in the current row, which will be needed to compute the next row); and $c[i - 1, k]$ for $k \\ge j - 1$ (i.e., entries in the previous row that are still needed to compute the rest of the current row). This is one entry for each $k$ from $1$ to $\\min(m, n)$ except that there are two entries with $k = j - 1$, hence the additional entry needed besides the one row's worth of entries. We can thus do away with the $c$ table as follows: Use an array a of length $\\min(m, n) + 1$ to hold the appropriate entries of $c$. At the time $c[i, j]$ is to be computed, $a$ will hold the following entries: $a[k] = c[i, k]$ for $1 \\le k < j - 1$ (i.e., earlier entries in the current \"row\"), $a[k] = c[i - 1, k]$ for $k \\ge j - 1$ (i.e., entries in the previous \"row\"), $a[0] = c[i, j - 1]$ (i.e., the previous entry computed, which couldn't be put into the \"right\" place in a without erasing the still-needed $c[i - 1, j - 1]$). Initialize a to all $0$ and compute the entries from left to right. Note that the 3 values needed to compute $c[i, j]$ for $j > 1$ are in $a[0] = c[i, j - 1], a[ j - 1] = c[i - 1, j - 1]$, and $a[ j] = c[i - 1, j]$. When $c[i, j]$ has been computed, move $a[0](c[i, j - 1])$ to its \"correct\" place, $a[j - 1]$, and put $c[i, j]$ in $a[0]$. 15.4-5 Give an $O(n^2)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. Given a list of numbers $L$, make a copy of $L$ called $L'$ and then sort $L'$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PRINT - LCS ( c , X , Y ) n = c [ X . length , Y . length ] let s [ 1. . n ] be a new array i = X . length j = Y . length while i > 0 and j > 0 if x [ i ] == y [ j ] s [ n ] = x [ i ] n = n - 1 i = i - 1 j = j - 1 else if c [ i - 1 , j ] \u2265 c [ i , j - 1 ] i = i - 1 else j = j - 1 for i = 1 to s . length print s [ i ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) m = | X | n = | Y | if c [ m , n ] != 0 or m == 0 or n == 0 return if x [ m ] == y [ n ] b [ m , n ] = \u2196 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y [ 1. . n - 1 ], c , b ) + 1 else if MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) \u2265 MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) b [ m , n ] = \u2191 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) else b [ m , n ] = \u2190 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) 1 2 3 4 MEMO - LCS - LENGTH ( X , Y ) let c [ 1. . | X | , 1. . | Y | ] and b [ 1. . | X | , 1. . | Y | ] be new tables MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) return c and b Then, just run the $\\text{LCS}$ algorithm on these two lists. The longest common subsequence must be monotone increasing because it is a subsequence of $L'$ which is sorted. It is also the longest monotone increasing subsequence because being a subsequence of $L'$ only adds the restriction that the subsequence must be monotone increasing. Since $|L| = |L'| = n$, and sorting $L$ can be done in $o(n^2)$ time, the final running time will be $O(|L||L'|) = O(n^2)$. 15.4-6 $\\star$ Give an $O(n\\lg n)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. ($\\textit{Hint:}$ Observe that the last element of a candidate subsequence of length $i$ is at least as large as the last element of a candidate subsequence of length $i - 1$. Maintain candidate subsequences by linking them through the input sequence.) The algorithm $\\text{LONG-MONOTONIC}(S)$ returns the longest monotonically increasing subsequence of $S$, where $S$ has length $n$. The algorithm works as follows: a new array B will be created such that $B[i]$ contains the last value of a longest monotonically increasing subsequence of length $i$. A new array $C$ will be such that $C[i]$ contains the monotonically increasing subsequence of length $i$ with smallest last element seen so far. To analyze the runtime, observe that the entries of $B$ are in sorted order, so we can execute line 9 in $O(\\lg n)$ time. Since every other line in the for-loop takes constant time, the total run-time is $O(n\\lg n)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 LONG - MONOTONIC ( S ) let B [ 1. . n ] be a new array where every value = \u221e let C [ 1. . n ] be a new array L = 1 for i = 1 to n if A [ i ] < B [ 1 ] B [ 1 ] = A [ i ] C [ 1 ]. head . key = A [ i ] else let j be the largest index of B such that B [ j ] < A [ i ] B [ j + 1 ] = A [ i ] C [ j + 1 ] = C [ j ] INSERT ( C [ j + 1 ], A [ i ]) if j + 1 > L L = L + 1 print C [ L ]","title":"15.4 Longest common subsequence"},{"location":"Chap15/15.4/#154-1","text":"Determine an $\\text{LCS}$ of $\\langle 1, 0, 0, 1, 0, 1, 0, 1 \\rangle$ and $\\langle 0, 1, 0, 1, 1, 0, 1, 1, 0 \\rangle$. $\\langle 1, 0, 0, 1, 1, 0 \\rangle$.","title":"15.4-1"},{"location":"Chap15/15.4/#154-2","text":"Give pseudocode to reconstruct an $\\text{LCS}$ from the completed $c$ table and the original sequences $X = \\langle x_1, x_2, \\ldots, x_m \\rangle$ and $Y = \\langle y_1, y_2, \\ldots, y_n \\rangle$ in $O(m + n)$ time, without using the $b$ table. 1 2 3 4 5 6 7 8 9 10 PRINT - LCS ( c , X , Y , i , j ) if c [ i , j ] == 0 return if X [ i ] == Y [ j ] PRINT - LCS ( c , X , Y , i - 1 , j - 1 ) print X [ i ] else if c [ i - 1 , j ] > c [ i , j - 1 ] PRINT - LCS ( c , X , Y , i - 1 , j ) else PRINT - LCS ( c , X , Y , i , j - 1 )","title":"15.4-2"},{"location":"Chap15/15.4/#154-3","text":"Give a memoized version of $\\text{LCS-LENGTH}$ that runs in $O(mn)$ time. 1 2 3 4 5 6 7 8 MEMOIZED - LCS - LENGTH ( X , Y , i , j ) if c [ i , j ] > - 1 return c [ i , j ] if i == 0 or j == 0 return c [ i , j ] = 0 if x [ i ] == y [ j ] return c [ i , j ] = LCS - LENGTH ( X , Y , i - 1 , j - 1 ) + 1 return c [ i , j ] = max ( LCS - LENGTH ( X , Y , i - 1 , j ), LCS - LENGTH ( X , Y , i , j - 1 ))","title":"15.4-3"},{"location":"Chap15/15.4/#154-4","text":"Show how to compute the length of an $\\text{LCS}$ using only $2 \\cdot \\min(m, n)$ entries in the $c$ table plus $O(1)$ additional space. Then show how to do the same thing, but using $\\min(m, n)$ entries plus $O(1)$ additional space. When computing a particular row of the $c$ table, no rows before the previous row are needed. Thus only two rows\u2014$2\u00b7length[Y]$ entries\u2014need to be kept in memory at a time. (Note: Each row of $c$ actually has $length[Y] + 1$ entries, but we don't need to store the column of $0$'s\u2014instead we can make the program \"know\" that those entries are $0$.) With this idea, we need only $2 \\cdot \\min(m, n)$ entries if we always call $\\text{LCS-LENGTH}$ with the shorter sequence as the $Y$ argument. We can thus do away with the $c$ table as follows: Use two arrays of length $\\min(m, n)$, $previous\\text-row$ and $current\\text-row$, to hold the appropriate rows of $c$. Initialize $previous\\text-row$ to all $0$ and compute $current\\text-row$ from left to right. When $current\\text-row$ is filled, if there are still more rows to compute, copy $current\\text-row$ into $previous\\text-row$ and compute the new $current\\text-row$. Actually only a little more than one row's worth of $c$ entries\u2014$\\min(m, n) + 1$ entries\u2014are needed during the computation. The only entries needed in the table when it is time to compute $c[i, j]$ are $c[i, k]$ for $k \\le j - 1$ (i.e., earlier entries in the current row, which will be needed to compute the next row); and $c[i - 1, k]$ for $k \\ge j - 1$ (i.e., entries in the previous row that are still needed to compute the rest of the current row). This is one entry for each $k$ from $1$ to $\\min(m, n)$ except that there are two entries with $k = j - 1$, hence the additional entry needed besides the one row's worth of entries. We can thus do away with the $c$ table as follows: Use an array a of length $\\min(m, n) + 1$ to hold the appropriate entries of $c$. At the time $c[i, j]$ is to be computed, $a$ will hold the following entries: $a[k] = c[i, k]$ for $1 \\le k < j - 1$ (i.e., earlier entries in the current \"row\"), $a[k] = c[i - 1, k]$ for $k \\ge j - 1$ (i.e., entries in the previous \"row\"), $a[0] = c[i, j - 1]$ (i.e., the previous entry computed, which couldn't be put into the \"right\" place in a without erasing the still-needed $c[i - 1, j - 1]$). Initialize a to all $0$ and compute the entries from left to right. Note that the 3 values needed to compute $c[i, j]$ for $j > 1$ are in $a[0] = c[i, j - 1], a[ j - 1] = c[i - 1, j - 1]$, and $a[ j] = c[i - 1, j]$. When $c[i, j]$ has been computed, move $a[0](c[i, j - 1])$ to its \"correct\" place, $a[j - 1]$, and put $c[i, j]$ in $a[0]$.","title":"15.4-4"},{"location":"Chap15/15.4/#154-5","text":"Give an $O(n^2)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. Given a list of numbers $L$, make a copy of $L$ called $L'$ and then sort $L'$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PRINT - LCS ( c , X , Y ) n = c [ X . length , Y . length ] let s [ 1. . n ] be a new array i = X . length j = Y . length while i > 0 and j > 0 if x [ i ] == y [ j ] s [ n ] = x [ i ] n = n - 1 i = i - 1 j = j - 1 else if c [ i - 1 , j ] \u2265 c [ i , j - 1 ] i = i - 1 else j = j - 1 for i = 1 to s . length print s [ i ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) m = | X | n = | Y | if c [ m , n ] != 0 or m == 0 or n == 0 return if x [ m ] == y [ n ] b [ m , n ] = \u2196 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y [ 1. . n - 1 ], c , b ) + 1 else if MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) \u2265 MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) b [ m , n ] = \u2191 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) else b [ m , n ] = \u2190 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) 1 2 3 4 MEMO - LCS - LENGTH ( X , Y ) let c [ 1. . | X | , 1. . | Y | ] and b [ 1. . | X | , 1. . | Y | ] be new tables MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) return c and b Then, just run the $\\text{LCS}$ algorithm on these two lists. The longest common subsequence must be monotone increasing because it is a subsequence of $L'$ which is sorted. It is also the longest monotone increasing subsequence because being a subsequence of $L'$ only adds the restriction that the subsequence must be monotone increasing. Since $|L| = |L'| = n$, and sorting $L$ can be done in $o(n^2)$ time, the final running time will be $O(|L||L'|) = O(n^2)$.","title":"15.4-5"},{"location":"Chap15/15.4/#154-6-star","text":"Give an $O(n\\lg n)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. ($\\textit{Hint:}$ Observe that the last element of a candidate subsequence of length $i$ is at least as large as the last element of a candidate subsequence of length $i - 1$. Maintain candidate subsequences by linking them through the input sequence.) The algorithm $\\text{LONG-MONOTONIC}(S)$ returns the longest monotonically increasing subsequence of $S$, where $S$ has length $n$. The algorithm works as follows: a new array B will be created such that $B[i]$ contains the last value of a longest monotonically increasing subsequence of length $i$. A new array $C$ will be such that $C[i]$ contains the monotonically increasing subsequence of length $i$ with smallest last element seen so far. To analyze the runtime, observe that the entries of $B$ are in sorted order, so we can execute line 9 in $O(\\lg n)$ time. Since every other line in the for-loop takes constant time, the total run-time is $O(n\\lg n)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 LONG - MONOTONIC ( S ) let B [ 1. . n ] be a new array where every value = \u221e let C [ 1. . n ] be a new array L = 1 for i = 1 to n if A [ i ] < B [ 1 ] B [ 1 ] = A [ i ] C [ 1 ]. head . key = A [ i ] else let j be the largest index of B such that B [ j ] < A [ i ] B [ j + 1 ] = A [ i ] C [ j + 1 ] = C [ j ] INSERT ( C [ j + 1 ], A [ i ]) if j + 1 > L L = L + 1 print C [ L ]","title":"15.4-6 $\\star$"},{"location":"Chap15/15.5/","text":"15.5-1 Write pseudocode for the procedure $\\text{CONSTRUCT-OPTIMAL-BST}(root)$ which, given the table $root$, outputs the structure of an optimal binary search tree. For the example in Figure 15.10, your procedure should print out the structure $$ \\begin{aligned} & \\text{$k_2$ is the root} \\\\ & \\text{$k_1$ is the left child of $k_2$} \\\\ & \\text{$d_0$ is the left child of $k_1$} \\\\ & \\text{$d_1$ is the right child of $k_1$} \\\\ & \\text{$k_5$ is the right child of $k_2$} \\\\ & \\text{$k_4$ is the left child of $k_5$} \\\\ & \\text{$k_3$ is the left child of $k_4$} \\\\ & \\text{$d_2$ is the left child of $k_3$} \\\\ & \\text{$d_3$ is the right child of $k_3$} \\\\ & \\text{$d_4$ is the right child of $k_4$} \\\\ & \\text{$d_5$ is the right child of $k_5$} \\end{aligned} $$ corresponding to the optimal binary search tree shown in Figure 15.9(b). 1 2 3 4 5 6 7 8 9 10 11 CONSTRUCT - OPTIMAL - BST ( root , i , j , last ) if i == j return if last == 0 print root [ i , j ] + \"is the root\" else if j < last : print root [ i , j ] + \"is the left child of\" + last else print root [ i , j ] + \"is the right child of\" + last CONSTRUCT - OPTIMAL - BST ( root , i , root [ i , j ] - 1 , root [ i , j ]) CONSTRUCT - OPTIMAL - BST ( root , root [ i , j ] + 1 , j , root [ i , j ]) 15.5-2 Determine the cost and structure of an optimal binary search tree for a set of $n = 7$ keys with the following probabilities $$ \\begin{array}{c|cccccccc} i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline p_i & & 0.04 & 0.06 & 0.08 & 0.02 & 0.10 & 0.12 & 0.14 \\\\ q_i & 0.06 & 0.06 & 0.06 & 0.06 & 0.05 & 0.05 & 0.05 & 0.05 \\end{array} $$ 15.5-3 Suppose that instead of maintaining the table $w[i, j]$, we computed the value of $w(i, j)$ directly from equation $\\text{(15.12)}$ in line 9 of $\\text{OPTIMAL-BST}$ and used this computed value in line 11. How would this change affect the asymptotic running time of $\\text{OPTIMAL-BST}$? Each of the $\\Theta(n^2)$ values of $w[i, j]$ would require computing those two sums, both of which can be of size $O(n)$, so, the asymptotic runtime would increase to $O(n^3)$. 15.5-4 $\\star$ Knuth [212] has shown that there are always roots of optimal subtrees such that $root[i, j - 1] \\le root[i, j] \\le root[i + 1, j]$ for all $1 \\le i < j \\le n$. Use this fact to modify the $\\text{OPTIMAL-BST}$ procedure to run in $\\Theta(n^2)$ time. Change the for loop of line 10 in $\\text{OPTIMAL-BST}$ to 1 for r = r [ i , j - 1 ] to r [ i + 1 , j ] Knuth's result implies that it is sufficient to only check these values because optimal root found in this range is in fact the optimal root of some binary search tree. The time spent within the for loop of line 6 is now $\\Theta(n)$. This is because the bounds on $r$ in the new for loop of line 10 are nonoverlapping. To see this, suppose we have fixed $l$ and $i$. On one iteration of the for loop of line 6, the upper bound on $r$ is $$r[i + 1, j] = r[i + 1, i + l - 1].$$ When we increment $i$ by $1$ we increase $j$ by $1$. However, the lower bound on $r$ for the next iteration subtracts this, so the lower bound on the next iteration is $$r[i + 1, j + 1 - 1] = r[i + 1, j].$$ Thus, the total time spent in the for loop of line 6 is $\\Theta(n)$. Since we iterate the outer for loop of line 5 $n$ times, the total runtime is $\\Theta(n^2)$.","title":"15.5 Optimal binary search trees"},{"location":"Chap15/15.5/#155-1","text":"Write pseudocode for the procedure $\\text{CONSTRUCT-OPTIMAL-BST}(root)$ which, given the table $root$, outputs the structure of an optimal binary search tree. For the example in Figure 15.10, your procedure should print out the structure $$ \\begin{aligned} & \\text{$k_2$ is the root} \\\\ & \\text{$k_1$ is the left child of $k_2$} \\\\ & \\text{$d_0$ is the left child of $k_1$} \\\\ & \\text{$d_1$ is the right child of $k_1$} \\\\ & \\text{$k_5$ is the right child of $k_2$} \\\\ & \\text{$k_4$ is the left child of $k_5$} \\\\ & \\text{$k_3$ is the left child of $k_4$} \\\\ & \\text{$d_2$ is the left child of $k_3$} \\\\ & \\text{$d_3$ is the right child of $k_3$} \\\\ & \\text{$d_4$ is the right child of $k_4$} \\\\ & \\text{$d_5$ is the right child of $k_5$} \\end{aligned} $$ corresponding to the optimal binary search tree shown in Figure 15.9(b). 1 2 3 4 5 6 7 8 9 10 11 CONSTRUCT - OPTIMAL - BST ( root , i , j , last ) if i == j return if last == 0 print root [ i , j ] + \"is the root\" else if j < last : print root [ i , j ] + \"is the left child of\" + last else print root [ i , j ] + \"is the right child of\" + last CONSTRUCT - OPTIMAL - BST ( root , i , root [ i , j ] - 1 , root [ i , j ]) CONSTRUCT - OPTIMAL - BST ( root , root [ i , j ] + 1 , j , root [ i , j ])","title":"15.5-1"},{"location":"Chap15/15.5/#155-2","text":"Determine the cost and structure of an optimal binary search tree for a set of $n = 7$ keys with the following probabilities $$ \\begin{array}{c|cccccccc} i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline p_i & & 0.04 & 0.06 & 0.08 & 0.02 & 0.10 & 0.12 & 0.14 \\\\ q_i & 0.06 & 0.06 & 0.06 & 0.06 & 0.05 & 0.05 & 0.05 & 0.05 \\end{array} $$","title":"15.5-2"},{"location":"Chap15/15.5/#155-3","text":"Suppose that instead of maintaining the table $w[i, j]$, we computed the value of $w(i, j)$ directly from equation $\\text{(15.12)}$ in line 9 of $\\text{OPTIMAL-BST}$ and used this computed value in line 11. How would this change affect the asymptotic running time of $\\text{OPTIMAL-BST}$? Each of the $\\Theta(n^2)$ values of $w[i, j]$ would require computing those two sums, both of which can be of size $O(n)$, so, the asymptotic runtime would increase to $O(n^3)$.","title":"15.5-3"},{"location":"Chap15/15.5/#155-4-star","text":"Knuth [212] has shown that there are always roots of optimal subtrees such that $root[i, j - 1] \\le root[i, j] \\le root[i + 1, j]$ for all $1 \\le i < j \\le n$. Use this fact to modify the $\\text{OPTIMAL-BST}$ procedure to run in $\\Theta(n^2)$ time. Change the for loop of line 10 in $\\text{OPTIMAL-BST}$ to 1 for r = r [ i , j - 1 ] to r [ i + 1 , j ] Knuth's result implies that it is sufficient to only check these values because optimal root found in this range is in fact the optimal root of some binary search tree. The time spent within the for loop of line 6 is now $\\Theta(n)$. This is because the bounds on $r$ in the new for loop of line 10 are nonoverlapping. To see this, suppose we have fixed $l$ and $i$. On one iteration of the for loop of line 6, the upper bound on $r$ is $$r[i + 1, j] = r[i + 1, i + l - 1].$$ When we increment $i$ by $1$ we increase $j$ by $1$. However, the lower bound on $r$ for the next iteration subtracts this, so the lower bound on the next iteration is $$r[i + 1, j + 1 - 1] = r[i + 1, j].$$ Thus, the total time spent in the for loop of line 6 is $\\Theta(n)$. Since we iterate the outer for loop of line 5 $n$ times, the total runtime is $\\Theta(n^2)$.","title":"15.5-4 $\\star$"},{"location":"Chap15/Problems/15-1/","text":"Suppose that we are given a directed acyclic graph $G = (V, E)$ with real-valued edge weights and two distinguished vertices $s$ and $t$ . Describe a dynamic-programming approach for finding a longest weighted simple path from $s$ to $t$ . What does the subproblem graph look like? What is the efficiency of your algorithm? We will make use of the optimal substructure property of longest paths in acyclic graphs. Let $u$ be some vertex of the graph. If $u = t$, then the longest path from $u$ to $t$ has zero weight. If $u \\ne t$, let $p$ be a longest path from $u$ to $t$. Path $p$ has at least two vertices. Let $v$ be the second vertex on the path. Let $p'$ be the subpath of $p$ from $v$ to $t$ ($p'$ might be a zero-length path). That is, the path $p$ looks like $$u \\to v \\overset{p'}{\\leadsto} t.$$ We claim that $p'$ is a longest path from $v$ to $t$. To prove the claim, we use a cut-and-paste argument. If $p'$ were not a longest path, then there exists a longer path $p''$ from $v$ to $t$. We could cut out $p'$ and paste in $p''$ to produce a path $u \\to v \\overset{p''}{\\leadsto} t$ which is longer than $p$, thus contradicting the assumption that $p$ is a longest path from $u$ to $t$. It is important to note that the graph is acyclic . Because the graph is acyclic, path $p''$ cannot include the vertex u, for otherwise there would be a cycle of the form $u \\to v \\leadsto u$ in the graph. Thus, we can indeed use $p''$ to construct a longer path. The acyclicity requirement ensures that by pasting in path $p''$ , the overall path is still a simple path (there is no cycle in the path). This difference between the cyclic and the acyclic case allows us to use dynamic programming to solve the acyclic case. Let $dist[u]$ denote the weight of a longest path from $u$ to $t$. The optimal substructure property allows us to write a recurrence for $dist[u]$ as $$ dist[u] = \\begin{cases} 0 & \\text{if $u = t$}, \\\\ \\max\\limits_{(u, v)\\in E}{w(u, v) + dist[v]} & \\text{otherwise}. \\end{cases} $$ This recurrence allows us to construct the following procedure: 1 2 3 4 5 6 7 8 9 10 11 12 13 LONGEST - PATH - AUS ( G , u , t , dist , next ) if u == t dist [ u ] = 0 return ( dist , next ) else if next [ u ] \u2265 0 return ( dist , next ) else next [ u ] = 0 for each vertex v \u2208 G . Adj [ u ] ( dist , next ) = LONGEST - PATH - AUX ( G , v , t , dist , next ) if w ( u , v ) + dist [ v ] > dist [ u ] dist [ u ] = w ( u , v ) + dist [ v ] next [ u ] = v return ( dist , next ) (See Section 22.1 for an explanation of the notation $G.Adj[u]$.) $\\text{LONGEST-PATH-AUX}$ is a memoized, recursive procedure, which returns the tuple $(dist, next)$. The array $dist$ is the memoized array that holds the solution to subproblems. That is, after the procedure returns, $dist[u]$ will hold the weight of a longest path from $u$ to $t$. The array $next$ serves two purposes: It holds information necessary for printing out an actual path. Specifically, if $u$ is a vertex on the longest path that the procedure found, then $next[u]$ is the next vertex on the path. The value in $next[u]$ is used to check whether the current subproblem has been solved earlier. A value of at least zero indicates that this subproblem has been solved earlier. The first if condition checks for the base case $u = t$. The second if condition checks whether the current subproblem has already been solved. The for loop iterates over each adjacent edge($u, v)$ and updates the longest distance in $dist[u]$. What is the running time of $\\text{LONGEST-PATH-AUX}$? Each subproblem represented by a vertex $u$ is solved at most once due to the memoization. For each vertex, we examine its adjacent edges. Thus, each edge is examined at most once, and the overall running time is $O(E)$. (Section 22.1 discusses how we achieve $O(E)$ time by representing the graph with adjacency lists.) The $\\text{PRINT-PATH}$ procedure prints out the path using information stored in the next array: 1 2 3 4 5 6 PRINT - PATH ( s , t , next ) u = s print u while u != t print \"\u2192\" next [ u ] u = next [ u ] The $\\text{LONGEST-PATH-MAIN}$ procedure is the main driver. It creates and initializes the $dist$ and the $next$ arrays. It then calls $\\text{LONGEST-PATH-AUX}$ to find a path and $\\text{PRINT-PATH}$ to print out the actual path. 1 2 3 4 5 6 7 8 9 10 11 LONGEST - PATH - MAIN ( G , s , t ) n = | G . V | let dist [ 1. . n ] and next [ 1. . n ] be new arrays for i = 1 to n dist [ i ] = - \u221e next [ i ] = - 1 ( dist , next ) = LONGEST - PATH - AUX ( G , s , t , dist , next ) if dist [ s ] == - \u221e print \"No path exists\" else print \"The weight of the longest path is\" dist [ s ] PRINT - PATH ( s , t , next ) Initializating the dist and next arrays takes $O(V)$ time. Thus the overall running time of $\\text{LONGEST-PATH-MAIN}$ is $O(V + E)$. Alternative solution We can also solve the problem using a bottom-up aproach. To do so, we need to ensure that we solve \"smaller\" subproblems before we solve \"larger\" ones. In our case, we can use a topological sort (see Section 22.4) to obtain a bottom-up procedure, imposing the required ordering on the vertices in $\\Theta(V + E)$ time. 1 2 3 4 5 6 7 8 9 10 11 12 13 LONGEST - PATH2 ( G , s , t ) let dist [ 1. . n ] and next [ 1. . n ] be new arrays topologically sort the vertices of G for i = 1 to | G . V | dist [ i ] = - \u221e dist [ s ] = 0 for each u in topological order , starting from s for each edge ( u , v ) \u2208 G . Adj [ u ] if dist [ u ] + w ( u , v ) > dist [ v ] dist [ v ] = dist [ u ] + w ( u , v ) next [ u ] = v print \"The longest distance is\" dist [ t ] PRINT - PATH ( s , t , next ) The running time of $\\text{LONGEST-PATH2}$ is $\\Theta(V + E)$.","title":"15-1 Longest simple path in a directed acyclic graph"},{"location":"Chap15/Problems/15-10/","text":"Your knowledge of algorithms helps you obtain an exciting job with the Acme Computer Company, along with a $\\$10,000$ signing bonus. You decide to invest this money with the goal of maximizing your return at the end of 10 years. You decide to use the Amalgamated Investment Company to manage your investments. Amalgamated Investments requires you to observe the following rules. It offers $n$ different investments, numbered $1$ through $n$. In each year $j$, investment $i$ provides a return rate of $r_{ij}$ . In other words, if you invest $d$ dollars in investment $i$ in year $j$, then at the end of year $j$ , you have $dr_{ij}$ dollars. The return rates are guaranteed, that is, you are given all the return rates for the next 10 years for each investment. You make investment decisions only once per year. At the end of each year, you can leave the money made in the previous year in the same investments, or you can shift money to other investments, by either shifting money between existing investments or moving money to a new investement. If you do not move your money between two consecutive years, you pay a fee of $f_1$ dollars, whereas if you switch your money, you pay a fee of $f_2$ dollars, where $f_2 > f_1$. a. The problem, as stated, allows you to invest your money inmultiple investments in each year. Prove that there exists an optimal investment strategy that, in each year, puts all the money into a single investment. (Recall that an optimal investment strategy maximizes the amount of money after 10 years and is not concerned with any other objectives, such as minimizing risk.) b. Prove that the problem of planning your optimal investment strategy exhibits optimal substructure. c. Design an algorithm that plans your optimal investment strategy. What is the running time of your algorithm? d. Suppose that Amalgamated Investments imposed the additional restriction that, at any point, you can have no more than $\\$15,000$ in any one investment. Show that the problem of maximizing your income at the end of 10 years no longer exhibits optimal substructure. a. Without loss of generality, suppose that there exists an optimal solution $S$ which involves investing $d_1$ dollars into investment $k$ and $d_2$ dollars into investement $m$ in year $1$. Further, suppose in this optimal solution, you don't move your money for the first $j$ years. If $r_{k1} + r_{k2} + \\ldots + r_{kj} > r_{m1} +r_{m2} + \\ldots + r_{mj}$ then we can perform the usual cut-and-paste maneuver and instead invest $d_1 + d_2$ dollars into investment $k$ for $j$ years. Keeping all other investments the same, this results in a strategy which is at least as profitable as $S$, but has reduced the number of different investments in a given span of years by $1$. Continuing in this way, we can reduce the optimal strategy to consist of only a single investment each year. b. If a particular investment strategy is the year-one-plan for a optimal investment strategy, then we must solve two kinds of optimal suproblem: either we maintain the strategy for an additional year, not incurring the moneymoving fee, or we move the money, which amounts to solving the problem where we ignore all information from year $1$. Thus, the problem exhibits optimal substructure. c. The algorithm works as follows: We build tables $I$ and $R$ of size $10$ such that $I[i]$ tells which investment should be made (with all money) in year $i$, and $R[i]$ gives the total return on the investment strategy in years $i$ through $10$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 INVEST ( d , n ) let I [ 1..10 ] and R [ 1..10 ] be new tables for k = 10 downto 1 q = 1 for i = 1 to n if r [ i , k ] > r [ q , k ] // i now holds the investment which looks best for a given year q = i if R [ k + 1 ] + dr_ { I [ k + 1 ] k } - f [ 1 ] > R [ k + 1 ] + dr [ q , k ] - f [ 2 ] // If revenue is greater when money is not moved R [ k ] = R [ k + 1 ] + dr_ { I [ k + 1 ] k } - f [ 1 ] I [ k ] = I [ k + 1 ] else R [ k ] = R [ k + 1 ] + dr [ q , k ] - f [ 2 ] I [ k ] = q return I as an optimal stategy with return R [ 1 ] d. The previous investment strategy was independent of the amount of money you started with. When there is a cap on the amount you can invest, the amount you have to invest in the next year becomes relevant. If we know the year-one-strategy of an optimal investment, and we know that we need to move money after the first year, we're left with the problem of investing a different initial amount of money, so we'd have to solve a subproblem for every possible initial amount of money. Since there is no bound on the returns, there's also no bound on the number of subproblems we need to solve.","title":"15-10 Planning an investment strategy"},{"location":"Chap15/Problems/15-11/","text":"The Rinky Dink Company makes machines that resurface ice rinks. The demand for such products varies from month to month, and so the company needs to develop a strategy to plan its manufacturing given the fluctuating, but predictable, demand. The company wishes to design a plan for the next $n$ months. For each month $i$, the company knows the demand $d_i$, that is, the number of machines that it will sell. Let $D = \\sum_{i = 1}^n d_i$ be the total demand over the next $n$ months. The company keeps a full-time staff who provide labor to manufacture up to $m$ machines per month. If the company needs to make more than $m$ machines in a given month, it can hire additional, part-time labor, at a cost that works out to $c$ dollars per machine. Furthermore, if, at the end of a month, the company is holding any unsold machines, it must pay inventory costs. The cost for holding $j$ machines is given as a function $h(j)$ for $j = 1, 2, \\ldots, D$, where $h(j) \\ge 0$ for $1 \\le j \\le D$ and $h(j) \\le h(j + 1)$ for $1 \\le j \\le D - 1$. Give an algorithm that calculates a plan for the company that minimizes its costs while fulfilling all the demand. The running time should be polyomial in $n$ and $D$. We state the subproblem $(k, s)$ as \"What is the cheapest way to satisfy all the demands of months $k, \\ldots, n$ when we start with a surplus of s before the $k$th month?\" A plan for the subproblem $(k, s)$ would specify the number of machines to manufacture for each month $k, \\ldots, n$ such that demands are satisfied. In some optimal plan $P$ to $(k, s)$, let $f^*$ machines be maufactured in month $k$. Thus, the surplus $s'$ in month $k + 1$ is $s + f^* - d_k$. Let $P'$ be the part of the plan $P$ for months $k + 1, \\ldots, n$. We claim that $P'$ is an optimal plan for the subproblem $(k + 1, s')$. Why? Suppose $P'$ were not an optimal plan and let $P''$ be an optimal plan for $(k + 1, s')$. If we modify plan $P$ by cutting out $P'$ and pasting in $P''$ (i.e., by using plan $P''$ for months $k + 1, \\ldots, n$), we obtain another plan for $(k, s)$ which is cheaper than plan $P$ . Thus, we obtain a contradiction to the assumption that plan $P$ was optimal. Let $cost[k, s]$ denote the cost of an optimal plan for $(k, s)$, and let $f$ denote the number of machines that can be manufactured in month $k$. The bounds for $f$ are as follows: At least the number of machines so that (along with surplus $s$) there are enough machines to satisfy the current month's demand. Let us denote this lower bound by $L(k, s)$. We have $$L(k, s) = \\max(d_k - s, 0).$$ At most the number of machines such that there are enough machines to satisfy the demands of all the following months. Let us denote this upper bound by $U(k, s)$. We have $$U(k, s) = \\Bigg(\\sum_{i = k}^n d_i \\Bigg) - s.$$ For the last month, we need only manufacture the minimum required number of machines, given by $L(n, s)$. For other months, we examine the costs of manufacturing all feasible numbers of machines and see which choice gives us the cheapest plan. We can now write the recurrence for cost as the following: $$ cost[k, s] = \\begin{cases} c \\cdot \\max(L(n, s) - m, 0) + h(s + L(n, s) - d_n) & \\text{if $k = n$}, \\\\ \\min\\limits_{L(k, s) \\le f \\le U(k, s)} \\Big\\{cost[k + 1, s + f - d_k] + c \\cdot \\max(f - m, 0) + h(s + f - d_k)\\Big\\} & \\text{if $0 < k < n$}. \\end{cases} $$ The recurrence suggests how to build an optimal plan in a bottom-up fashion. We now present the algorithm for constructing an optimal plan. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 INVERTORY - PLANNING ( n , m , c , D , d , h ) let cost [ 1. . n , 0. . D ] and make [ 1. . n , 0. . D ] be new tables // Compute cost[n, 0..D] and make[n, 0..D]. for s = 0 to D f = max ( d [ n ] - s , 0 ) cost [ n , s ] = c * max ( f - m , 0 ) + h ( s + f - d [ n ]) make [ n , s ] = f // Compute cost[1..n - 1, 0..D] and make[1..n - 1, 0..D]. U = d [ n ] for k = n - 1 downto 1 U = U + d [ k ] for s = 0 to D cost [ k , s ] = \u221e for f = max ( d [ k ] - s , 0 ) to U - s val = cost [ k + 1 , s + f - d [ k ]] + c * max ( f - m , 0 ) + h ( s + f - d [ k ]) if val < cost [ k , s ] cost [ k , s ] = val make [ k , s ] = f print cost [ 1 , 0 ] PRINT - PLAN ( make , n , d ) 1 2 3 4 5 PRINT - PLAN ( make , n , d ) s = 0 for k = 1 to n print \"For month\" k \"manufacture\" make [ k , s ] \"machines\" s = s + make [ k , s ] - d [ k ] In $\\text{INVENTORY-PLANNING}$, we build the solution month by month, starting from month $n$, moving backward toward month $1$. First, we solve the subproblem for the last month, for all surpluses. Then, for each month and for each surplus entering that month, we calculate the cheapest way to satisfy demand for that month based on the solved subproblems of the next month. $f$ is the number of machines that we try to manufacture in month $k$. $cost[k, s]$ holds the cheapest way to satisfy demands of months $k, \\ldots, n$, with a net surplus of s left over at the beginning of month $k$. $make[k, s]$ holds the number of machines to manufacture in month $k$ and the surplus $s$ of an optimal plan. We will use this table to reconstruct the optimal plan. We first initialize the base cases, which are the cases for month $n$ starting with surplus $s$, for $s = 0, \\ldots, D$. If $d_n > s$, it suffices to manufacture $d_n - s$ machines, since we need not keep any surplus after month $n$. If $d_n \\le s$, we need not manufacture any machines at all. We then calculate the total cost for month $n$ as the sum of hiring extra labor $c \\cdot \\max(f - m, 0)$ and the inventory costs for leftover surplus $h(s + f - d_n)$, which can be nonzero if we had started out with a large surplus. The outer for loop of the next block of code runs down from month $n - 1$ to $1$, thus ensuring that when we consider month $k$, we have already solved the subproblems of month $k + 1$. The next inner for loop iterates through all possible values of $f$ as described. For every choice of $f$ for a given month $k$, the total cost of $(k, s)$ is given by the cost of extra labor (if any) plus the cost of inventory (if there is a surplus) plus the cost of the subproblem $(k + 1, s + f - d_k)$. This value is checked and updated. Finally, the required answer is the answer to the subproblem $(1, 0)$, which appears in $cost[1, 0]$. That is, it is the cheapest way to satisfy all the demands of months $1, \\ldots, n$ when we start with a surplus of $0$. The running time of $\\text{INVENTORY-PLANNING}$ is clearly $O(nD^2)$. The space requirement is $O(nD)$. We can improve upon the space requirement by noting that we need only store the solution to subproblems of the next month. With this observation, we can construct an algorithm that uses $O(n + D)$ space.","title":"15-11 Inventory planning"},{"location":"Chap15/Problems/15-12/","text":"Suppose that you are the general manager for a major-league baseball team. During the off-season, you need to sign some free-agent players for your team. The team owner has given you a budget of $\\$X$ to spend on free agents. You are allowed to spend less than $\\$X$ altogether, but the owner will fire you if you spend any more than $\\$X$. You are considering $N$ different positions, and for each position, $P$ free-agent players who play that position are available. Because you do not want to overload your roster with too many players at any position, for each position you may sign at most one free agent who plays that position. (If you do not sign any players at a particular position, then you plan to stick with the players you already have at that position.) To determine how valuable a player is going to be, you decide to use a sabermetric statistic known as \"$\\text{VORP}$\", or \"value over replacement player\". A player with a higher $\\text{VORP}$ is more valuable than a player with a lower $\\text{VORP}$. A player with a higher $\\text{VORP}$ is not necessarily more expensive to sign than a player with a lower $\\text{VORP}$, because factors other than a player's value determine how much it costs to sign him. For each available free-agent player, you have three pieces of information: the player's position, the amount of money it will cost to sign the player, and the player's $\\text{VORP}$. Devise an algorithm that maximizes the total $\\text{VORP}$ of the players you sign while spending no more than $\\$X$ altogether. You may assume that each player signs for a multiple of $100,000$. Your algorithm should output the total $\\text{VORP}$ of the players you sign, the total amount of money you spend, and a list of which players you sign. Analyze the running time and space requirement of your algorithm. Let $p.cost$ denote the cost and $p.vorp$ denote the $\\text{VORP}$ of player $p$. We shall assume that all dollar amounts are expressed in units of $\\$100,000$. Since the order of choosing players for the positions does not matter, we may assume that we make our decisions starting from position $1$, moving toward position $N$. For each position, we decide to either sign one player or sign no players. Suppose we decide to sign player $p$, who plays position $1$. Then, we are left with an amount of $X - p.cost$ dollars to sign players at positions $2, \\ldots, N$. This observation guides us in how to frame the subproblems. We define the cost and $\\text{VORP}$ of a set of players as the sum of costs and the sum of $\\text{VORP}$s of all players in that set. Let ($(i, x)$ denote the following subproblem: \"Suppose we consider only positions $i, i + 1, \\ldots, N$ and we can spend at most $x$ dollars. What set of players (with at most one player for each position under consideration) has the maximum $\\text{VORP}$?\" A valid set of players for ($(i, x)$ is one in which each player in the set plays one of the positions $i, i + 1, \\ldots, n$, each position has at most one player, and the cost of the players in the set is at most $x$ dollars. An optimal set of players for ($(i, x)$ is a valid set with the maximum $\\text{VORP}$. We now show that the problem exhibits optimal substructure. Theorem (Optimal substructure of the VORP maximization problem) Let $L = \\{p_1, p_2, \\ldots, p_k\\}$ be a set of players, possibly empty, with maximum $\\text{VORP}$ for the subproblem $(i, x)$. If $i = N$, then $L$ has at most one player. If all players in position $N$ have cost more than $x$, then $L$ has no players. Otherwise, $L = \\{p_1\\}$, where $p_1$ has the maximum $\\text{VORP}$ among players for position $N$ with cost at most $x$. If $i < N$ and $L$ includes player $p$ for position i, then $L' = L - \\{p\\}$ is an optimal set for the subproblem $(i + 1, x - p.cost)$. If $i < N$ and $L$ does not include a player for position $i$, then $L$ is an optimal set for the subproblem $(i + 1, x)$. Proof Property 1. follows trivially from the problem statement. Suppose that $L'$ is not an optimal set for the subproblem $(i + 1, x - p.cost)$. Then, there exists another valid set $L''$ for $(i + 1, x - p.cost)$ that has $\\text{VORP}$ more than $L'$. Let $L^{\\prime\\prime\\prime} = L'' \\cup \\{p\\}$. The cost of $L^{\\prime\\prime\\prime}$ is at most $x$, since $L''$ has a cost at most $x - p.cost$. Moreover, $L^{\\prime\\prime\\prime}$ has at most one player for each position $i, i + 1, \\ldots, N$. Thus, $L^{\\prime\\prime\\prime}$ is a valid set for $(i, x)$. But $L^{\\prime\\prime\\prime}$ has $\\text{VORP}$ more than $L$, thus contradicting the assumption that $L$ had the maximum $\\text{VORP}$ for $(i, x)$. Clearly, any valid set for $(i + 1, x)$ is also a valid set for $(i, x)$. If $L$ were not an optimal set for $(i + 1, x)$, then there exists another valid set $L'$ for $(i + 1, x)$ with $\\text{VORP}$ more than $L$. The set $L'$ would also be a valid set for $(i, x)$, which contradicts the assumption that $L$ had the maximum $\\text{VORP}$ for $(i, x)$. The theorem suggests that when $i < N$, we examine two subproblems and choose the better of the two. Let $v[i, x]$ denote the maximum $\\text{VORP}$ for $(i, x)$. Let $S(i, x)$ be the set of players who play position $i$ and cost at most $x$. In the following recurrence for $v[i, x]$ we assume that the max function returns $-\\infty$ when invoked over an empty set: $$ v[i, x] = \\begin{cases} \\max\\limits_{p \\in S(N, x)} {p.vorp} & \\text{if $i = N$}, \\\\ \\max \\Big\\{v[i + 1, x], \\max\\limits_{p \\in S(i, x)}{p.vorp + v[i + 1, x - p.cost]} \\Big\\} & \\text{if $i < N$}. \\end{cases} $$ This recurrence lends itself to implementation in a straightforward way. Let $p_{ij}$ denote the $j$th player who plays position $i$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 FREE - AGENT - VORP ( p , N , P , X ) let v [ 1. . N , 0. . X ] and who [ 1. . N , 0. . X ] be new tables for x = 0 to X v [ N , x ] = - \u221e who [ N , x ] = 0 for k = 1 to P if p [ N , k ]. cost \u2264 x and p [ N , k ]. vorp > v [ N , x ] v [ N , x ] = p [ N , k ]. vorp who [ N , x ] = k for i = N - 1 downto 1 for x = 0 to X v [ i , x ] = v [ i + 1 , x ] who [ i , x ] = 0 for k = 1 to P if p [ i , k ]. cost \u2264 x and v [ i + 1 , x - p [ i , k ]. cost ] + p [ i , k ]. vorp > v [ i , x ] v [ i , x ] = v [ i + 1 , x - p [ i , k ]. cost ] + p [ i , k ]. vorp who [ i , x ] = k print \"The maximum value of VORP is\" v [ 1 , X ] amt = X for i = 1 to N k = who [ i , amt ] if k != 0 print \"sign player\" p [ i , k ] amt = amt - p [ i , k ]. cost print \"The total money spent is\" X - amt The input to $\\text{FREE-AGENT-VORP}$ is the list of players $p$ and $N$, $P$, and $X$, as given in the problem. The table $v[i, x]$ holds the maximum $\\text{VORP}$ for the subproblem $(i, x)$. The table $who[i, x]$ holds information necessary to reconstruct the actual solution. Specifically, $who[i, x]$ holds the index of player to sign for position $i$, or $0$ if no player should be signed for position $i$. The first set of nested for loops initializes the base cases, in which $i = N$. For every amount $x$, the inner loop simply picks the player with the highest $\\text{VORP}$ who plays position $N$ and whose cost is at most $x$. The next set of three nested for loops represents the main computation. The outermost for loop runs down from position $N - 1$ to $1$. This order ensures that smaller subproblems are solved before larger ones. We initialize $v[i, x]$ as $v[i + 1, x]$. This way, we already take care of the case in which we decide not to sign any player who plays position $i$. The innermost for loop tries to sign each player (if we have enough money) in turn, and it keeps track of the maximum $\\text{VORP}$ possible. The maximum $\\text{VORP}$ for the entire problem ends up in $v[1, X]$. The final for loop uses the information in who table to print out which players to sign. The running time of $\\text{FREE-AGENT-VORP}$ is clearly $\\Theta(NPX)$, and it uses $\\Theta(NX)$ space.","title":"15-12 Signing free-agent baseball players"},{"location":"Chap15/Problems/15-2/","text":"A palindrome is a nonempty string over some alphabet that reads the same forward and backward. Examples of palindromes are all strings of length $1$, $\\text{civic}$, $\\text{racecar}$, and $\\text{aibohphobia}$ (fear of palindromes). Give an efficient algorithm to find the longest palindrome that is a subsequence of a given input string. For example, given the input $\\text{character}$, your algorithm should return $\\text{carac}$. What is the running time of your algorithm? We solve the longest palindrome subsequence ($\\text{LPS}$) problem in a manner similar to how we compute the longest common subsequence in Section 15.4. Step 1: Characterizing a longest palindrome subsequence The $\\text{LPS}$ problem has an optimal-substructure property, where the subproblems correspond to pairs of indices, starting and ending, of the input sequence. For a sequence $X = \\langle x_1, x_2, \\ldots, x_n\\rangle$, we denote the subsequence starting at $x_i$ and ending at $x_j$ by $X_{ij} = \\langle x_i, x_{i + 1}, \\ldots, x_j \\rangle$. Theorem (Optimal substructure of an LPS) Let $X = \\langle x_1, x_2, \\ldots, x_n \\rangle$ be the input sequence, and let $Z = \\langle z_1, z_2, \\ldots, z_m \\rangle$ be any $\\text{LPS}$ of $X$. If $n = 1$, then $m = 1$ and $z_1 = x_1$. If $n = 2$ and $x_1 = x_2$, then $m = 2$ and $z_1 = z_2 = x_1 = x_2$. If $n = 2$ and $x_1 \\ne x_2$, then $m = 1$ and $z_1$ is equal to either $x_1$ or $x_n$. If $n > 2$ and $x_1 = x_n$, then $m > 2$, $z_1 = z_m = x_1 = x_n$, and $Z_{2, m - 1}$ is an $\\text{LPS}$ of $X_{2, n - 1}$. If $n > 2$ and $x_1\\ne x_n$, then $z_1 \\ne x_1$ implies that $Z_{1, m}$ is an $\\text{LPS}$ of $X_{2, n}$. If $n > 2$ and $x_1\\ne x_n$, then $z_m \\ne x_n$ implies that $Z_{1, m}$ is an $\\text{LPS}$ of $X_{1, n - 1}$. Proof Properties 1., 2., and 3. follow trivially from the definition of $\\text{LPS}$. If $n > 2$ and $x_1 = x_n$, then we can choose $x_1$ and $x_n$ as the ends of $Z$ and at least one more element of $X$ as part of $Z$. Thus, it follows that $m > 2$. If $z_1 \\ne x_1$, then we could append $x_1 = x_n$ to the ends of $Z$ to obtain a palindrome subsequence of $X$ with length $m + 2$, contradicting the supposition that $Z$ is a longest palindrome subsequence of $X$. Thus, we must have $z_1 = x_1 (= x_n = zm)$. Now, $Z_{2, m - 1}$ is a length-$(m - 2)$ palindrome subsequence of $X_{2, n - 1}$. We wish to show that it is an $\\text{LPS}$. Suppose for the purpose of contradiction that there exists a palindrome subsequence $W$ of $X_{2, n - 1}$ with length greater than $m - 2$. Then, appending $x_1 = x_n$ to the ends of $W$ produces a palindrome subsequence of $X$ whose length is greater than $m$, which is a contradiction. If $z_1 \\ne x_1$, then $Z$ is a palindrome subsequence of $X_{2, n}$. If there were a palindrome subsequence $W$ of $X_{2, n}$ with length greater than $m$, then $W$ would also be a palindrome subsequence of $X$, contradicting the assumption that $Z$ is an $\\text{LPS}$ of $X$. The proof is symmetric to (2). The way that the theorem characterizes longest palindrome subsequences tells us that an $\\text{LPS}$ of a sequence contains within it an $\\text{LPS}$ of a subsequence of the sequence. Thus, the $\\text{LPS}$ problem has an optimal-substructure property. Step 2: A recursive solution The theorem implies that we should examine either one or two subproblems when finding an $\\text{LPS}$ of $X = \\langle x_1, x_2, \\ldots, x_n \\rangle$, depending on whether $x_1 = x_n$. Let us define $p[i, j]$ to be the length of an $\\text{LPS}$ of the subsequence $X_{ij}$. If $i = j$, the $\\text{LPS}$ has length $1$. If $j = i + 1$, then the $\\text{LPS}$ has length either $1$ or $2$, depending on whether $x_i = x_j$. The optimal substructure of the $\\text{LPS}$ problem gives the following recursive formula: $$ p[i, j] = \\begin{cases} 1 & \\text{if $i = j$}, \\\\ 2 & \\text{if $j = i + 1$ and $x_i = x_j$}, \\\\ 1 & \\text{if $j = i + 1$ and $x_i \\ne x_j$}, \\\\ p[i + 1, j - 1] + 2 & \\text{if $j > i + 1$ and $x_i = x_j$}, \\\\ \\max(p[i, j - 1], p[i + 1, j]) & \\text{if $j > i + 1$ and $x_i \\ne x_j$}. \\end{cases} $$ Step 3: Computing the length of an LPS Procedure $\\text{LONGEST-PALINDROME}$ takes a sequence $X = \\langle x_1, x_2, \\ldots, x_n \\rangle$ as input. The procedure fills cells $p[i, i]$, where $1 \\le i \\le n$, and $p[i, i + 1]$, where $1 \\le i \\le n - 1$, as the base cases. It then starts filling cells $p[i, j]$, where $j > i + 1$. The procedure fills the $p$ table row by row, starting with row $n - 2$ and moving toward row $1$. (Rows $n - 1$ and $n$ are already filled as part of the base cases.) Within each row, the procedure fills the entries from left to right. The procedure also maintains the table $b[1..n, 1..n]$ to help us construct an optimal solution. Intuitively, $b[i, j]$ points to the table entry corresponding to the optimal subproblem solution chosen when computing $p[i, j]$. The procedure returns the $b$ and $p$ tables, $p[1, n]$ contains the length of an $\\text{LPS}$ of $X$. The running time of $\\text{LONGEST-PALINDROME}$ is clearly $\\Theta(n^2)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 LONGEST - PALINDROME ( X ) n = X . length let b [ 1. . n , 1. . n ] and p [ 0. . n , 0. . n ] be new tables for i = 1 to n - 1 p [ i , i ] = 1 j = i + 1 if x [ i ] == x [ j ] p [ i , j ] = 2 b [ i , j ] = \"\u2199\" else p [ i , j ] = 1 b [ i , j ] = \"\u2193\" p [ n , n ] = 1 for i = n - 2 downto 1 for j = i + 2 to n if x [ i ] == x [ j ] p [ i , j ] = p [ i + 1 , j - 1 ] + 2 b [ i , j ] = \"\u2199\" else if p [ i + 1 , j ] \u2265 p [ i , j - 1 ] p [ i , j ] = p [ i + 1 , j ] b [ i , j ] = \"\u2193\" else p [ i , j ] = p [ i , j - 1 ] b [ i , j ] = \"\u2190\" return p and b Step 4: Constructing an LPS The $b$ table returned by $\\text{LONGEST-PALINDROME}$ enables us to quickly construct an $\\text{LPS}$ of $X = \\langle x_1, x_2, \\ldots, x_m\\rangle$. We simply begin at $b[1, n]$ and trace through the table by following the arrows. Whenever we encounter a \"$\\swarrow$\" in entry $b[i, j]$, it implies that $x_i = y_j$ are the first and last elements of the $\\text{LPS}$ that $\\text{LONGEST-PALINDROME}$ found. The following recursive procedure returns a sequence $S$ that contains an $\\text{LPS}$ of $X$. The initial call is $\\text{GENERATE-LPS}(b, X, 1, X.length, \\langle \\rangle)$, where $\\langle\\rangle$ denotes an empty sequence. Within the procedure, the symbol $||$ denotes concatenation of a symbol and a sequence. 1 2 3 4 5 6 7 8 9 10 GENERATE - LPS ( b , X , i , j , S ) if i > j return S else if i == j return S || x [ i ] else if b [ i , j ] == \"\u2199\" return x [ i ] || GENERATE - LPS ( b , X , i + 1 , j - 1 , S ) || x [ i ] else if b [ i , j ] == \"\u2193\" return GENERATE - LPS ( b , X , i + 1 , j , S ) else return GENERATE - LPS ( b , X , i , j - 1 , S )","title":"15-2 Longest palindrome subsequence"},{"location":"Chap15/Problems/15-3/","text":"In the euclidean traveling-salesman problem , we are given a set of $n$ points in the plane, and we wish to find the shortest closed tour that connects all n points. Figure 15.11(a) shows the solution to a $7$-point problem. The general problem is NP-hard, and its solution is therefore believed to require more than polynomial time (see Chapter 34). J. L. Bentley has suggested that we simplify the problem by restricting our attention to bitonic tours , that is, tours that start at the leftmost point, go strictly rightward to the rightmost point, and then go strictly leftward back to the starting point. Figure 15.11(b) shows the shortest bitonic tour of the same $7$ points. In this case, a polynomial-time algorithm is possible. Describe an $O(n^2)$-time algorithm for determining an optimal bitonic tour. You may assume that no two points have the same $x$-coordinate and that all operations on real numbers take unit time. ($\\textit{Hint:}$ Scan left to right, maintaining optimal possibilities for the two parts of the tour.) Taking the book's hint, we sort the points by $x$-coordinate, left to right, in $O(n\\lg n)$ time. Let the sorted points be, left to right, $\\langle p_1, p_2, p_3, \\ldots, p_n \\rangle$. Therefore, $p_1$ is the leftmost point, and $p_n$ is the rightmost. We define as our subproblems paths of the following form, which we call bitonic paths. A bitonic path $P_{i, j}$, where $i \\le j$, includes all points $p_1, p_2, \\ldots, p_j$; it starts at some point $p_i$, goes strictly left to point $p_1$, and then goes strictly right to point $p_j$. By \"going strictly left,\" we mean that each point in the path has a lower $x$-coordinate than the previous point. Looked at another way, the indices of the sorted points form a strictly decreasing sequence. Likewise, \"going strictly right\" means that the indices of the sorted points form a strictly increasing sequence. Moreover, $P_{i, j}$ contains all the points $p_1, p_2, p_3, \\ldots, p_j$ Note that $p_j$ is the rightmost point in $P_{i, j}$ and is on the rightgoing subpath. The leftgoing subpath may be degenerate, consisting of just $p_1$. Let us denote the euclidean distance between any two points $p_i$ and $p_j$ by $|p_i p_j|$. And let us denote by $b[i, j]$, for $1 \\le i \\le j \\le n$, the length of the shortest bitonic path $P_{i, j}$. Since the leftgoing subpath may be degenerate, we can easily compute all values $b[1, j]$. The only value of $b[i, i]$ that we will need is $b[n, n]$, which is the length of the shortest bitonic tour. We have the following formulation of $b[i, j]$ for $1 \\le i \\le j \\le n$: $$ \\begin{aligned} b[1, 2] & = |p_1 p_2|, \\\\ b[i, j] & = b[i, j - 1] + |p_{j - 1} p_j| & \\text{for $i < j - 1$}, \\\\ b[j - 1, j] & = \\min_{1\\le k < j - 1} \\{b[k, j - 1] + |p_k p_j|\\}. \\end{aligned} $$ Why are these formulas correct? Any bitonic path ending at $p_2$ has $p_2$ as its rightmost point, so it consists only of $p_1$ and $p_2$. Its length, therefore, is $|p_1 p_2|$. Now consider a shortest bitonic path $P_{i, j}$. The point $p_{j - 1}$ is somewhere on this path. If it is on the rightgoing subpath, then it immediately preceeds $p_j$ on this subpath. Otherwise, it is on the leftgoing subpath, and it must be the rightmost point on this subpath, so $i = j - 1$. In the first case, the subpath from $p_i$ to $p_{j - 1}$ must be a shortest bitonic path $P_{i, j - 1}$, for otherwise we could use a cut-and-paste argument to come up with a shorter bitonic path than $P_{i, j}$. (This is part of our optimal substructure.) The length of $P_{i, j}$, therefore, is given by $b[i, j - 1] + |p_{j - 1} p_j|$. In the second case, $p_j$ has an immediate predecessor $p_k$, where $k < j - 1$, on the rightgoing subpath. Optimal substructure again applies: the subpath from $p_k$ to $p_{j - 1}$ must be a shortest bitonic path $P_{k, j - 1}$, for otherwise we could use cut-and-paste to come up with a shorter bitonic path than $P_{i, j}$. (We have implicitly relied on paths having the same length regardless of which direction we traverse them.) The length of $P_{i, j}$, therefore, is given by $\\min_{1 \\le k \\le j - 1} \\{b[k, j - 1] + |p_k p_j|\\}$. We need to compute $b[n, n]$. In an optimal bitonic tour, one of the points adjacent to $p_n$ must be $p_{n - 1}$, and so we have $$b[n, n] = b[n - 1, n] + |p_{n - 1} p_n|.$$ To reconstruct the points on the shortest bitonic tour, we define $r[i, j]$ to be the index of the immediate predecessor of $p_j$ on the shortest bitonic path $P_{i, j}$. Because the immediate predecessor of $p_2$ on $P_{1, 2}$ is $p_1$, we know that $r[1, 2]$ must be $1$. The pseudocode below shows how we compute $b[i, j]$ and $r[i, j]$. It fills in only entries $b[i, j]$ where $1 \\le i \\le n - 1$ and $i + 1 \\le j \\le n$, or where $i = j = n$, and only entries $r[i, j]$ where $1 \\le i \\le n - 2$ and $i + 2 \\le j \\le n$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 EUCLIDEAN - TSP ( p ) sort the points so that \u27e8 p [ 1 ], p [ 2 ], p [ 3 ], ..., p [ n ] \u27e9 are in order of increasing x - coordinate let b [ 1. . n , 2. . n ] and r [ 1. . n - 2 , 3. . n ] be new arrays b [ 1 , 2 ] = | p [ 1 ] p [ 2 ] | for j = 3 to n for i = 1 to j - 2 b [ i , j ] = b [ i , j - 1 ] + | p [ j - 1 ] p [ j ] | r [ i , j ] = j - 1 b [ j - 1 , j ] = \u221e for k = 1 to j - 2 q = b [ k , j - 1 ] + | p [ k ] p [ j ] | if q < b [ j - 1 , j ] b [ j - 1 , j ] = q r [ j - 1 , j ] = k b [ n , n ] = b [ n - 1 , n ] + | p [ n - 1 ] p [ n ] | return b and r We print out the tour we found by starting at $p_n$, then a leftgoing subpath that includes $p_{n - 1}$, from right to left, until we hit $p_1$. Then we print right-to-left the remaining subpath, which does not include $p_{n - 1}$. For the example in Figure 15.11(b) on page 405, we wish to print the sequence $p_7, p_6, p_4, p_3, p_1, p_2, p_5$. Our code is recursive. The right-to-left subpath is printed as we go deeper into the recursion, and the left-to-right subpath is printed as we back out. 1 2 3 4 5 6 PRINT - TOUR ( r , n ) print p [ n ] print p [ n - 1 ] k = r [ n - 1 , n ] PRINT - PATH ( r , k , n - 1 ) print p [ k ] 1 2 3 4 5 6 7 8 9 10 11 PRINT - PATH ( r , i , j ) if i < j k = r [ i , j ] if k != i print p [ k ] if k > 1 PRINT - PATH ( r , i , k ) else k = r [ j , i ] if k > 1 PRINT - PATH ( r , k , j ) print p [ k ] The relative values of the parameters $i$ and $j$ in each call of $\\text{PRINT-PATH}$ indicate which subpath we're working on. If $i < j$, we're on the right-to-left subpath, and if $i > j$, we're on the left-to-right subpath. The test for $k \\ne i$ prevents us from printing $p_1$ an extra time, which could occur when we call $\\text{PRINT-PATH}(r, 1, 2)$. The time to run $\\text{EUCLIDEAN-TSP}$ is $O(n^2)$ since the outer loop on $j$ iterates $n - 2$ times and the inner loops on $i$ and $k$ each run at most $n - 2$ times. The sorting step at the beginning takes $O(n\\lg n)$ time, which the loop times dominate. The time to run $\\text{PRINT-TOUR}$ is $O(n)$, since each point is printed just once.","title":"15-3 Bitonic euclidean"},{"location":"Chap15/Problems/15-4/","text":"Consider the problem of neatly printing a paragraph with a monospaced font (all characters having the same width) on a printer. The input text is a sequence of $n$ words of lengths $l_1, l_2, \\ldots, l_n$, measured in characters. We want to print this paragraph neatly on a number of lines that hold a maximum of $M$ characters each. Our criterion of \"neatness\" is as follows. If a given line contains words $i$ through $j$, where $i \\le j$ , and we leave exactly one space between words, the number of extra space characters at the end of the line is $M - j + i - \\sum_{k = i}^j l_k$, which must be nonnegative so that the words fit on the line. We wish to minimize the sum, over all lines except the last, of the cubes of the numbers of extra space characters at the ends of lines. Give a dynamic-programming algorithm to print a paragraph of $n$ words neatly on a printer. Analyze the running time and space requirements of your algorithm. Note: We assume that no word is longer than will fit into a line, i.e., $l_i \\le M$ for all $i$. First, we'll make some definitions so that we can state the problem more uniformly. Special cases about the last line and worries about whether a sequence of words fits in a line will be handled in these definitions, so that we can forget about them when framing our overall strategy. Define $extras[i, j] = M - j + i - \\sum_{k = i}^j l_k$ to be the number of extra spaces at the end of a line containing words $i$ through $j$. Note that $extras$ may be negative. Now define the cost of including a line containing words $i$ through $j$ in the sum we want to minimize: $$ lc[i, j] = \\begin{cases} \\infty & \\text{if $extras[i, j] < 0$ (i.e., words $i, \\ldots, j$ don't fit)}, \\\\ 0 & \\text{if $j = n$ and $extras[i, j] \\ge 0$ (last line costs $0$)}, \\\\ (extras[i, j])^3 & \\text{otherwise}. \\end{cases} $$ By making the line cost infinite when the words don't fit on it, we prevent such an arrangement from being part of a minimal sum, and by making the cost $0$ for the last line (if the words fit), we prevent the arrangement of the last line from influencing the sum being minimized. We want to minimize the sum of $lc$ over all lines of the paragraph. Our subproblems are how to optimally arrange words $1, \\ldots, j$, where $j = 1, \\ldots, n$. Consider an optimal arrangement of words $1, \\ldots, j$. Suppose we know that the last line, which ends in word $j$, begins with word $i$. The preceding lines, therefore, contain words $1, \\ldots, i - 1$. In fact, they must contain an optimal arrangement of words $1, \\ldots, i - 1$. (The usual type of cut-and-paste argument applies.) Let $c[j]$ be the cost of an optimal arrangement of words $1, \\ldots, j$. If we know that the last line contains words $i, \\ldots, j$, then $c[j] = c[i - 1] + lc[i, j]$. As a base case, when we're computing $c[1]$, we need $c[0]$. If we set $c[0] = 0$, then $c[1] = lc[1, 1]$, which is what we want. But of course we have to figure out which word begins the last line for the subproblem of words $1, \\ldots, j$. So we try all possibilities for word $i$, and we pick the one that gives the lowest cost. Here, $i$ ranges from $1$ to $j$. Thus, we can define $c[j]$ recursively by $$ c[j] = \\begin{cases} 0 & \\text{if $j = 0$}, \\\\ \\min\\limits_{1 \\le i \\le j} (c[i - 1] + lc[i, j]) & \\text{if $j > 0$}. \\end{cases} $$ Note that the way we defined $lc$ ensures that all choices made will fit on the line (since an arrangement with $lc = \\infty$ cannot be chosen as the minimum), and the cost of putting words $i, \\ldots, j$ on the last line will not be $0$ unless this really is the last line of the paragraph ($j = n$) or words $i, \\ldots, j$ fill the entire line. We can compute a table of $c$ values from left to right, since each value depends only on earlier values. To keep track of what words go on what lines, we can keep a parallel $p$ table that points to where each $c$ value came from. When $c[j]$ is computed, if $c[j]$ is based on the value of $c[k - 1]$, set $p[j] = k$. Then after $c[n]$ is computed, we can trace the pointers to see where to break the lines. The last line starts at word $p[n]$ and goes through word $n$. The previous line starts at word $p[p[n]]$ and goes through word $p[n] - 1$, et. In pseudocode, here's how we construct the tables: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 PRINT - NEATLY ( l , n , M ) let extras [ 1. . n , 1. . n ], lc [ 1. . n , 1. . n ], and c [ 0. . n ] be new arrays // Compute extras[i, j] for 1 \u2264 i \u2264 j \u2264 n. for i = 1 to n extras [ i , i ] = M - l [ i ] for j = i + 1 to n extras [ i , j ] = extras [ i , j - 1 ] - l [ j ] - 1 // Compute lc[i, j] for 1 \u2264 i \u2264 j \u2264 n. for i = 1 to n for j = i to n if extras [ i , j ] < 0 lc [ i , j ] = \u221e else if j == n and extras [ i , j ] \u2265 0 lc [ i , j ] = 0 else lc [ i , j ] = ( extras [ i , j ]) ^ 3 // Compute c[j] and p[j] for 1 \u2264 j \u2264 n. c [ 0 ] = 0 for j = 1 to n c [ j ] = \u221e for i = 1 to j if c [ i - 1 ] + lc [ i , j ] < c [ j ] c [ j ] = c [ i - 1 ] + lc [ i , j ] p [ j ] = i return c and p Quite clearly, both the time and space are $\\Theta(n^2)$. In fact, we can do a bit better: we can get both the time and space down to $\\Theta(nM)$. The key observation is that at most $\\lceil M / 2 \\rceil$ words can fit on a line. (Each word is at least one character long, and there's a space between words.) Since a line with words $i, \\ldots, j$ contains $j - i + 1$ words, if $j - i + 1 > \\lceil M / 2 \\rceil$ then we know that $lc[i, j] = \\infty$. We need only compute and store $extras[i, j]$ and $lc[i, j]$ for $j - i + 1 \\le \\lceil M / 2 \\rceil$. And the inner for loop header in the computation of $c[j]$ and $p[j]$ can run from $\\max(1, j - \\lceil M / 2 \\rceil + 1)$ to $j$. We can reduce the space even further to $\\Theta(n)$. We do so by not storing the $lc$ and $extras$ tables, and instead computing the value of $lc[i, j]$ as needed in the last loop. The idea is that we could compute $lc[i, j]$ in $O(1)$ time if we knew the value of $extras[i, j]$. And if we scan for the minimum value in descending order of $i$, we can compute that as $extras[i, j] = extras[i + 1, j] - l_i - 1$. (Initially, $extras[j, j] = M - l_j$.) This improvement reduces the space to $\\Theta(n)$, since now the only tables we store are $c$ and $p$. Here's how we print which words are on which line. The printed output of $\\text{GIVE-LINES}(p, j)$ is a sequence of triples $(k, i, j)$, indicating that words $i, \\ldots, j$ are printed on line $k$. The return value is the line number $k$. 1 2 3 4 5 6 7 GIVE - LINES ( p , j ) i = p [ j ] if i == 1 k = 1 else k = GIVE - LINES ( p , i - 1 ) + 1 print ( k , i , j ) return k The initial call is $\\text{GIVE-LINES}(p, n)$. Since the value of $j$ decreases in each recursive call, $\\text{GIVE-LINES}$ takes a total of $O(n)$ time.","title":"15-4 Printing neatly"},{"location":"Chap15/Problems/15-5/","text":"In order to transform one source string of text $x[1..m]$ to a target string $y[1..n]$, we can perform various transformation operations. Our goal is, given $x$ and $y$, to produce a series of transformations that change $x$ to $y$. We use an array $z$\u2014assumed to be large enough to hold all the characters it will need\u2014to hold the intermediate results. Initially, $z$ is empty, and at termination, we should have $z[j] = y[j]$ for $j = 1, 2, \\ldots, n$. We maintain current indices $i$ into $x$ and $j$ into $z$, and the operations are allowed to alter $z$ and these indices. Initially, $i = j = 1$. We are required to examine every character in $x$ during the transformation, which means that at the end of the sequence of transformation operations, we must have $i = m + 1$. We may choose from among six transformation operations: Copy a character from $x$ to $z$ by setting $z[j] = x[i]$ and then incrementing both $i$ and $j$. This operation examines $x[i]$. Replace a character from $x$ by another character $c$, by setting $z[j] = c$, and then incrementing both $i$ and $j$. This operation examines $x[i]$. Delete a character from $x$ by incrementing $i$ but leaving $j$ alone. This operation examines $x[i]$. Insert the character $c$ into $z$ by setting $z[j] = c$ and then incrementing $j$, but leaving $i$ alone. This operation examines no characters of $x$. Twiddle (i.e., exchange) the next two characters by copying them from $x$ to $z$ but in the opposite order; we do so by setting $z[j] = x[i + 1]$ and $z[j + 1] = x[i]$ and then setting $i = i + 2$ and $j = j + 2$. This operation examines $x[i]$ and $x[i + 1]$. Kill the remainder of $x$ by setting $i = m + 1$. This operation examines all characters in $x$ that have not yet been examined. This operation, if performed, must be the final operation. As an example, one way to transform the source string $\\text{algorithm}$ to the target string $\\text{altruistic}$ is to use the following sequence of operations, where the underlined characters are $x[i]$ and $z[j]$ after the operation: $$ \\begin{array}{lll} \\text{Operation} & x & z \\\\ \\hline \\textit{initial strings} & \\underline algorithm & \\text{\\textunderscore} \\\\ \\text{copy} & a\\underline lgorithm & a\\text{\\textunderscore} \\\\ \\text{copy} & al\\underline gorithm & al\\text{\\textunderscore} \\\\ \\text{replace by $t$} & alg\\underline orithm & alt\\text{\\textunderscore} \\\\ \\text{delete} & algo\\underline rithm & alt\\text{\\textunderscore} \\\\ \\text{copy} & algor\\underline ithm & altr\\text{\\textunderscore} \\\\ \\text{insert $u$} & algor\\underline ithm & altru\\text{\\textunderscore} \\\\ \\text{insert $i$} & algor\\underline ithm & altrui\\text{\\textunderscore} \\\\ \\text{insert $s$} & algor\\underline ithm & altruis\\text{\\textunderscore} \\\\ \\text{twiddle} & algorit\\underline hm & altruisti\\text{\\textunderscore} \\\\ \\text{insert $c$} & algorit\\underline hm & altruistic\\text{\\textunderscore} \\\\ \\text{kill} & algorithm\\text{\\textunderscore} & altruistic\\text{\\textunderscore} \\end{array} $$ Note that there are several other sequences of transformation operations that transform $\\text{algorithm}$ to $\\text{altruistic}$. Each of the transformation operations has an associated cost. The cost of an operation depends on the specific application, but we assume that each operation's cost is a constant that is known to us. We also assume that the individual costs of the copy and replace operations are less than the combined costs of the delete and insert operations; otherwise, the copy and replace operations would not be used. The cost of a given sequence of transformation operations is the sum of the costs of the individual operations in the sequence. For the sequence above, the cost of transforming $\\text{algorithm}$ to $\\text{altruistic}$ is $$\\text{($3 \\cdot$ cost(copy)) + cost(replace) + cost(delete) + ($4 \\cdot$ cost(insert)) + cost(twiddle) + cost(kill)}.$$ a. Given two sequences $x[1..m]$ and $y[1..n]$ and set of transformation-operation costs, the edit distance from $x$ to $y$ is the cost of the least expensive operatoin sequence that transforms $x$ to $y$. Describe a dynamic-programming algorithm that finds the edit distance from $x[1..m]$ to $y[1..n]$ and prints an optimal opeartion sequence. Analyze the running time and space requirements of your algorithm. The edit-distance problem generalizes the problem of aligning two DNA sequences (see, for example, Setubal and Meidanis [310, Section 3.2]). There are several methods for measuring the similarity of two DNA sequences by aligning them. One such method to align two sequences $x$ and $y$ consists of inserting spaces at arbitrary locations in the two sequences (including at either end) so that the resulting sequences $x'$ and $y'$ have the same length but do not have a space in the same position (i.e., for no position $j$ are both $x'[j]$ and $y'[j]$ a space). Then we assign a \"score\" to each position. Position $j$ receives a score as follows: $+1$ if $x'[j] = y'[j]$ and neither is a space, $-1$ if $x'[j] \\ne y'[j]$ and neither is a space, $-2$ if either $x'[j]$ or $y'[j]$ is a space. The score for the alignment is the sum of the scores of the individual positions. For example, given the sequences $x = \\text{GATCGGCAT}$ and $y = \\text{CAATGTGAATC}$, one alignment is $$ \\begin{array}{cccccccccccc} \\text G & & \\text A & \\text T & \\text C & \\text G & & \\text G & \\text C & \\text A & \\text T & \\\\ \\text C & \\text A & \\text A & \\text T & & \\text G & \\text T & \\text G & \\text A & \\text A & \\text T & \\text C \\\\ - & * & + & + & * & + & * & + & - & + & + & * \\end{array} $$ A $+$ under a position indicates a score of $+1$ for that position, a $-$ indicates a score of $-1$, and a $*$ indicates a score of $-2$, so that this alignment has a total score of $6 \\cdot -2 \\cdot 1 - 4 \\cdot 2 = -4$. b. Explain how to cast the problem of finding an optimal alignment as an edit distance problem using a subset of the transformation operations copy, replace, delete, insert, twiddle, and kill. a. Dynamic programming is the ticket. This problem is slightly similar to the longest-common-subsequence problem. In fact, we'll define the notational conveniences $X_i$ and $Y_j$ in the similar manner as we did for the LCS problem: $X_i = x[1..i]$ and $Y_j = y[1..j]$. Our subproblems will be determining an optimal sequence of operations that converts $X_i$ to $Y_j$, for $0 \\le i \\le m$ and $0 \\le j \\le n$. We'll call this the \"$X_i \\to Y_j$ problem.\" The original problem is the $X_m \\to Y_n$ problem. Let's suppose for the moment that we know what was the last operation used to convert $X_i$ to $Y_j$. There are six possibilities. We denote by $c[i, j]$ the cost of an optimal solution to the $X_i \\to Y_j$ problem. If the last operation was a copy , then we must have had $x[i] = y[j]$. The subproblem that remains is converting $X_{i - 1}$ to $Y_{j - 1}$. And an optimal solution to the $X_i \\to Y_j$ problem must include an optimal solution to the $X_{i - 1} \\to Y_{j - 1}$ problem. The cut-and-paste argument applies. Thus, assuming that the last operation was a copy, we have $$c[i, j] = c[i - 1, j - 1] + \\text{cost(copy)}.$$ If it was a replace , then we must have had $x[i] \\ne y[j]$. (Here, we assume that we cannot replace a character with itself. It is a straightforward modification if we allow replacement of a character with itself.) We have the same optimal substructure argument as for copy, and assuming that the last operation was a replace, we have $$c[i, j] = c[i - 1, j - 1] + \\text{cost(replace)}.$$ If it was a twiddle , then we must have had both $x[i] = y[j - 1]$ and $x[i - 1] = y[j]$, along with the implicit assumption that $i, j \\ge 2$. Now our subproblem is $X_{i - 2} \\to Y_{j - 2}$ and, assuming that the last operation was a twiddle, we have $$c[i, j] = c[i - 2, j - 2] + \\text{cost(twiddle)}.$$ If it was a delete , then we have no restrictions on $x$ or $y$. Since we can view delete as removing a character from $X_i$ and leaving $Y_j$ alone, our subproblem is $X_{i - 1} \\to Y_j$. Assuming that the last operation was a delete, we have $$c[i, j] = c[i - 1, j] + \\text{cost(delete)}.$$ If it was an insert , then we have no restrictions on $x$ or $y$. Our subproblem is $X_ i \\to Y_{j - 1}$. Assuming that the last operation was an insert, we have $$c[i, j] = c[i, j - 1] + \\text{cost(insert)}.$$ If it was a kill , then we had to have completed converting $X_m$ to $Y_n$, so that the current problem must be the $X_m \\to Y_n$ problem. In other words, we must have $i = m$ and $j = n$. If we think of a kill as a multiple delete, we can get any $X_i \\to Y_n$, where $0 \\le i < m$, as a subproblem. We pick the best one, and so assuming that the last operation was a kill, we have $$c[m, n] = \\min_{0 \\le i < m}{c[i, n]} + \\text{cost(kill)}.$$ We have not handled the base cases, in which $i = 0$ or $j = 0$. These are easy. $X_0$ and $Y_0$ are the empty strings. We convert an empty string into $Y_j$ by a sequence of $j$ inserts, so that $c[0, j] = j \\cdot \\text{cost(insert)}$. Similarly, we convert $X_i$ into $Y_0$ by a sequence of $i$ deletes, so that $c[i, 0] = i \\cdot \\text{cost(delete)}$. When $i = j = 0$, either formula gives us $c[0, 0] = 0$, which makes sense, since there's no cost to convert the empty string to the empty string. For $i, j > 0$, our recursive formulation for $c[i, j]$ applies the above formulas in the situations in which they hold: $$ c[i, j] = \\min \\begin{cases} c[i - 1, j - 1] + \\text{cost(copy)} & \\text{if $x[i] = y[j]$}, \\\\ c[i - 1, j - 1] + \\text{cost(replace)} & \\text{if $x[i] \\ne y[j]$}, \\\\ c[i - 2, j - 2] + \\text{cost(twiddle)} & \\text{if $i, j \\ge 2$}, \\\\ & x[i] = y[j - 1], \\\\ & \\text{and $x[i - 1] = y[j]$}, \\\\ c[i - 1, j] + \\text{cost(delete)} & \\text{always}, \\\\ c[i, j] = c[i, j - 1] + \\text{cost(insert)} & \\text{always}, \\\\ \\min\\limits_{0 \\le i < m} {c[i, n]} + \\text{cost(kill)} & \\text{if $i = m$ and $j = n$}. \\end{cases} $$ Like we did for LCS, our pseudocode fills in the table in row-major order, i.e., row-by-row from top to bottom, and left to right within each row. Columnmajor order (column-by-column from left to right, and top to bottom within each column) would also work. Along with the $c[i, j]$ table, we fill in the table $op[i, j]$, holding which operation was used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 EDIT - DISTANCE ( x , y , m , n ) let c [ 0. . m , 0. . n ] and op [ 0. . m , 0. . n ] be new arrays for i = 0 to m c [ i , 0 ] = i * cost ( delete ) op [ i , 0 ] = DELETE for j = 0 to n c [ 0 , j ] = j * cost ( insert ) op [ 0 , j ] = INSERT for i = 1 to m for j = 1 to n c [ i , j ] = \u221e if x [ i ] == y [ j ] c [ i , j ] = c [ i - 1 , j - 1 ] + cost ( copy ) op [ i , j ] = COPY if x [ i ] != y [ j ] and c [ i - 1 , j - 1 ] + cost ( replace ) < c [ i , j ] c [ i , j ] = c [ i - 1 , j - 1 ] + cost ( replace ) op [ i , j ] = REPLACE ( by y [ j ]) if i \u2265 2 and j \u2265 2 and x [ i ] == y [ j - 1 ] and x [ i - 1 ] == y [ j ] and c [ i - 2 , j - 2 ] + cost ( twiddle ) < c [ i , j ] c [ i , j ] = c [ i - 2 , j - 2 ] + cost ( twiddle ) op [ i , j ] = TWIDDLE if c [ i - 1 , j ] + cost ( delete ) < c [ i , j ] c [ i , j ] = c [ i - 1 , j ] + cost ( delete ) op [ i , j ] = DELETE if c [ i , j - 1 ] + cost ( insert ) < c [ i , j ] c [ i , j ] = c [ i , j - 1 ] + cost ( insert ) op [ i , j ] = INSERT ( y [ j ]) for i = 0 to m - 1 if c [ i , n ] + cost ( kill ) < c [ m , n ] c [ m , n ] = c [ i , n ] + cost ( kill ) op [ m , n ] = KILL i return c and op The time and space are both $\\Theta(mn)$. If we store a $\\text{KILL}$ operation in $op[m, n]$, we also include the index $i$ after which we killed, to help us reconstruct the optimal sequence of operations. (We don't need to store $y[i]$ in the $op$ table for replace or insert operations.) To reconstruct this sequence, we use the $op$ table returned by $\\text{EDIT-DISTANCE}$. The procedure $\\text{OP-SEQUENCE}(op, i, j)$ reconstructs the optimal operation sequence that we found to transform $X_i$ into $Y_j$. The base case is when $i = j = 0$. The first call is $\\text{OP-SEQUENCE}(op, m, n)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 OP - SEQUENCE ( op , i , j ) if i == 0 and j = 0 return if op [ i , j ] == COPY or op [ i , j ] = REPLACE i ' = i - 1 j ' = j - 1 else if op [ i , j ] == TWIDDLE i ' = i - 2 j ' = j - 2 else if op [ i , j ] == DELETE i ' = i - 1 j ' = j else if op [ i , j ] == INSERT // don't care yet what character is inserted i ' = i j ' = j - 1 else // must be KILL, and must have i = m, and j = n let op [ i , j ] == KILL k i ' = k j ' = j OP - SEQUENCE ( op , i ' , j ' ) print op [ i , j ] This procedure determines which subproblem we used, recurses on it, and then prints its own last operation. b. The DNA-alignment problem is just the edit-distance problem, with $$ \\begin{array}{rcl} \\text{cost(copy)} & = & -1, \\\\ \\text{cost(replace)} & = & +1, \\\\ \\text{cost(delete)} & = & +2, \\\\ \\text{cost(insert)} & = & +2, \\end{array} $$ and the twiddle and kill operations are not permitted. The score that we are trying to maximize in the DNA-alignment problem is precisely the negative of the cost we are trying to minimize in the edit-distance problem. The negative cost of copy is not an impediment, since we can only apply the copy operation when the characters are equal.","title":"15-5 Edit distance"},{"location":"Chap15/Problems/15-6/","text":"Professor Stewart is consulting for the president of a corporation that is planning a company party. The company has a hierarchical structure; that is, the supervisor relation forms a tree rooted at the president. The personnel office has ranked each employee with a conviviality rating, which is a real number. In order to make the party fun for all attendees, the president does not want both an employee and his or her immediate supervisor to attend. Professor Stewart is given the tree that describes the structure of the corporation, using the left-child, right-sibling representation described in Section 10.4. Each node of the tree holds, in addition to the pointers, the name of an employee and that employee's conviviality ranking. Describe an algorithm to make up a guest list that maximizes the sum of the conviviality ratings of the guests. Analyze the running time of your algorithm. The problem exhibits optimal substructure in the following way: If the root $r$ is included in an optimal solution, then we must solve the optimal subproblems rooted at the grandchildren of $r$. If $r$ is not included, then we must solve the optimal subproblems on trees rooted at the children of $r$. The dynamic programming algorithm to solve this problem works as follows: We make a table $C$ indexed by vertices which tells us the optimal conviviality ranking of a guest list obtained from the subtree with root at that vertex. We also make a table $G$ such that $G[i]$ tells us the guest list we would use when vertex $i$ is at the root. Let $T$ be the tree of guests. To solve the problem, we need to examine the guest list stored at $G[T.root]$. First solve the problem at each leaf $L$. If the conviviality ranking at $L$ is positive, $G[L] = \\{L\\}$ and $C[L] = L.conviv$. Otherwise $G[L] = \\emptyset$ and $C[L] = 0$. Iteratively solve the subproblems located at parents of nodes at which the subproblem has been solved. In general for a node $x$, $$C[x] = \\min(\\sum_{y\\text{ is a child of } x} C[y], \\sum_{y\\text{ is a grandchild of } x} C[y]).$$ The runtime of the algorithm is $O(n^2)$ where $n$ is the number of vertices, because we solve $n$ subproblems, each in constant time, but the tree traversals required to find the appropriate next node to solve could take linear time.","title":"15-6 Planning a company party"},{"location":"Chap15/Problems/15-7/","text":"We can use dynamic programming on a directed graph $G = (V, E)$ for speech recognition. Each edge $(u, v) \\in E$ is labeled with a sound $\\sigma(u, v)$ from a finite set $\\Sigma$ of sounds. The labeled graph is a formal model of a person speaking a restricted language. Each path in the graph starting from a distinguished vertex $v_0 \\in V$ corresponds to a possible sequence of sounds producted by the model. We define the label of a directed path to be the concatenation of the labels of the edges on that path. a. Describe an efficient algorithm that, given an edge-labeled graph $G$ with distinguished vertex $v_0$ and a sequence $s = \\langle \\sigma_1, \\sigma_2, \\ldots, \\sigma_k \\rangle$ of sounds from $\\Sigma$, returns a path in $G$ that begins at $v_0$ and has $s$ as its label, if any such path exists. Otherwise, the algorithm should return $\\text{NO-SUCH-PATH}$. Analyze the running time of your algorithm. ($\\textit{Hint:}$ You may find concepts from Chapter 22 useful.) Now, suppose that every edge $(u, v) \\in E$ has an associated nonnegatve probability $p(u, v)$ of traversing the edge $(u, v)$ from vertex $u$ and thus producing the corresponding sound. The sum of the probabilities of the edges leaving any vertex equals $1$. The probability of a path is defined to the product of the probabilities of its edges. We can view the probability of a path beginning at $v_0$ as the probability that a \"random walk\" beginning at $v_0$ will follow the specified path, where we randomly choose which edge to take leaving a vertex $u$ according to the probabilities of the available edges leaving $u$. b. Extend your answer to part (a) so that if a path is returned, it is a most probable path starting at $v_0$ and having label $s$. Analyze the running time of your algorithm. a. Our substructure will consist of trying to find suffixes of s of length one less starting at all the edges leaving $v_0$ with label $\\sigma_0$. if any of them have a solution, then, there is a solution. If none do, then there is none. See the algorithm $\\text{VITERBI}$ for details. 1 2 3 4 5 6 7 8 9 VITERBI ( G , s , v [ 0 ]) if s . length = 0 return v [ 0 ] for edges ( v [ 0 ], v [ 1 ]) in V for some v [ 1 ] if sigma ( v [ 0 ], v [ 1 ]) = sigma [ 1 ] res = VITERBI ( G , ( sigma [ 2 ], ..., sigma [ k ]), v [ 1 ]) if res != NO - SUCH - PATH return ( v [ 0 ], res ) return NO - SUCH - PATH Since the subproblems are indexed by a suffix of $s$ (of which there are only $k$) and a vertex in the graph, there are at most $O(k|V|)$ different possible arguments. Since each run may require testing a edge going to every other vertex, and each iteration of the for loop takes at most a constant amount of time other than the call to $\\text{PROB-VITERBI}$, the final runtime is $O(k|V|^2)$. b. For this modification, we will need to try all the possible edges leaving from $v_0$ instead of stopping as soon as we find one that works. The substructure is very similar. We'll make it so that instead of just returning the sequence, we'll have the algorithm also return the probability of that maximum probability sequence, calling the fields seq and prob respectively. See the algorithm $\\text{PROB-VITERBI}$. Since the runtime is indexed by the same things, we have that we will call it with at most $O(k|V|)$ different possible arguments. Since each run may require testing a edge going to every other vertex, and each iteration of the for loop takes at most a constant amount of time other than the call to $\\text{PROB-VITERBI}$, the final runtime is $O(k|V|^2)$. 1 2 3 4 5 6 7 8 9 10 11 PROB - VITERBI ( G , s , v [ 0 ]) if s . length = 0 return v [ 0 ] sols . seq = NO - SUCH - PATH sols . prob = 0 for edges ( v [ 0 ], v [ 1 ]) in V for some v [ 1 ] if sigma ( v [ 0 ], v [ 1 ]) = sigma [ 1 ] res = PROB - VITERBI ( G , ( sigma [ 2 ], ..., sigma [ k ]), v [ 1 ]) if p ( v [ 0 ], v [ 1 ]) * res . prob \u2265 sols . prob sols . prob = p ( v [ 0 ], v [ 1 ]) * res . prob and sols . seq = v [ 0 ], res . seq return sols","title":"15-7 Viterbi algorithm"},{"location":"Chap15/Problems/15-8/","text":"We are given a color picture consisting of an $m \\times n$ array $A[1..m, 1..n]$ of pixels, where each pixel specifies a triple of red, green, and blue (RGB) intensities. Suppose that we wish to compress this picture slightly. Specifically, we wish to remove one pixel from each of the $m$ rows, so that the whole picture becomes one pixel narrower. To avoid disturbing visual effects, however, we require that the pixels removed in two adjacent rows be in the same or adjacent columns; the pixels removed form a \"seam\" from the top row to the bottom row where successive pixels in the seam are adjacent vertically or diagonally. a. Show that the number of such possible seams grows at least exponentially in $m$, assuming that $n > 1$. b. Suppose now that along with each pixel $A[i, j]$, we have calculated a real-valued disruption measure $d[i, j]$, indicating how disruptive it would be to remove pixel $A[i, j]$. Intuitively, the lower a pixel's disruption measure, the more similar the pixel is to its neighbors. Suppose further that we define the disruption measure of a seam to be the sum of the disruption measures of its pixels. Give an algorithm to find a seam with the lowest disruption measure. How efficient is your algorithm? a. Let us set up a recurrence for the number of valid seams as a function of $m$. Suppose we are in the process of carving out a seam row by row, starting from the first row. Let the last pixel carved out be $A[i, j]$. How many choices do we have for the pixel in row $i + 1$ such that the pixel continues the seam? If the last pixel $A[i, j]$ were on the column boundary ($i = 1$ or $i = n$), then there would be two choices for the next pixel. For example, when $i = 1$, the two choices for the next pixel are $A[i + 1, j]$ and $A[i + 1, j + 1]$. Otherwise, there would be three choices for the next pixel: $A[i + 1, j - 1], A[i + 1, j], A[i + 1, j + 1]$. Thus, for a general pixel $A[i, j]$, there are at least two possible choices for a pixel $p$ in the next row such that $p$ continues a seam ending in $A[i, j]$. Let $T(i)$ denote the number of possible seams from row $1$ to row $i$. Then, for $i = 1$, we have $T(i) = n$, and for $i > 1$, $$T(i) \\ge 2T(i - 1).$$ It is easy to guess that $T(i) \\ge n2^{n - 1}$, which we verify by direct substitution. For $i = 1$, we have $T(i) = n \\ge n \\cdot 2^0$. For $i > 1$, we have $$ \\begin{aligned} T(i) & \\ge 2T(i - 1) \\\\ & \\ge 2 \\cdot n 2^{i - 1} \\\\ & = n 2^{i - 1}. \\end{aligned} $$ Thus, the total number $T(m)$ of seams is at least $n2^{m - 1}$. We conclude that the number of seams grows at least exponentially in $m$. b. As proved in the previous part, it is infeasible to systematically check every seam, since the number of possible seams grows exponentially. The structure of the problem allows us to build the solution row by row. Consider a pixel $A[i, j]$. We ask the question: 'If $i$ were the first row of the picture, what is the minimum disruptive measure of seams that start with the pixel $A[i, j]$?' Let $S^*$ be a seam of minimum disruptive measure among all seams that start with pixel $A[i, j]$. Let $A[i + 1, p]$, where $p \\in \\{j - 1, j, j + 1\\}$, be the pixel of $S^*$ in the next row. Let $S'$ be the sub-seam of $S^*$ that starts with $A[i + 1, p]$. We claim that $S'$ has the minimum disruptive measure among seams that start with $A[i + 1, p]$. Why? Suppose there exists another seam $S''$ that starts with $A[i + 1, p]$ and has disruptive measure less than that of $S'$. By using $S''$ as the sub-seam instead of $S'$, we can obtain another seam that starts with $A[i, j]$ and has a disruptive measure which is less than that of $S^*$. Thus, we obtain a contradiction to our assumption that $S^*$ is a seam of minimum disruptive measure. Let $disr[i, j]$ be the value of the minimum disruptive measure among all seams that start with pixel $A[i, j]$. For row $m$, the seam with the minimum disruptive measure consists of just one point. We can now state a recurrence for $disr[i, j]$ as follows. In the base case, $disr[m, j] = d[m, j]$ for $j = 1, 2, \\ldots, n$. In the recursive case, for $j = 1, 2, \\ldots, n$, $$disr[i, j] = d[i, j] + \\min_{k \\in K}{dist[i + i, j + k]},$$ where the set $K$ of index offsets is $$ K = \\begin{cases} {0, 1} & \\text{if $j = 1$}, \\\\ {-1, 0, 1} & \\text{if $1 < j < m$}, \\\\ {-1, 0} & \\text{if $j = n$}. \\end{cases} $$ Since every seam has to start with a pixel of the first row, we simply find the minimum $disr[1, j]$ for pixels in the first row to obtain the minimum disruptive measure. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 COMPRESS - IMAGE ( d ) m = d . rows n = d . columns let disr [ 1. . m , 1. . n ] and next [ 1. . m , 1. . n ] be new tables for j = 1 to n disr [ m , j ] = d [ m , j ] for i = m - 1 downto 1 for j = 1 to n low = max ( - 1 , 1 - j ) high = min ( 1 , n - j ) disr [ i , j ] = \u221e for k = low to high if disr [ i + 1 , j + k ] < disr [ i , j ] disr [ i , j ] = disr [ i + 1 , j + k ] next [ i , j ] = j + k disr [ i , j ] = disr [ i , j ] + d [ i , j ] val = \u221e start = 1 for j = 1 to n if disr [ 1 , j ] < val val = disr [ 1 , j ] start = j print \"The minimum value of the disruptive measure is\" val for i = 1 to m print \"cup point at\" ( i , start ) start = next [ i , start ] The procedure $\\text{COMPRESS-IMAGE}$ is simply an implementation of this recurrence in a bottom-up fashion. We first carry out the initialization of the base cases, which are the cases when row $i = m$. The minimum disruptive measure for the base cases is simply $d[m, j]$. The next for loop runs down from $m - 1$ to $1$. Thus, $disr[i + 1, j]$ is already available before computing $dist[i, j]$ for pixels of row $i$. The assignments to $low$ and $high$ allow the index offset $k$ to range over the correct set $K$ from above. We set $low$ to $0$ when $j = 1$ and to $-1$ when $j > 1$, and we set $high$ to $0$ when $j = n$ and to $1$ when $j < n$. The innermost for loop sets $dist[i, j]$ to the minimum value of $disr[i + 1, j + k]$ for all $k \\in K$, and the line that follows this loop adds in $d[i, j]$. We use the $next$ table to reconstruct the actual seam. For a given pixel, it records which pixel was used as the next pixel. Specifically, for a pixel $A[i, j]$, if $next[i, j] = p$, where $p \\in {j - 1, j, j + 1}$, then the next pixel of the seam is $A[i + 1, p]$. The last line of the for loop adds the disruptive measure of the current pixel to the disruptive measure of the seam. The next for loop finds the minimum disruptive measure of pixels in the first row. We print the minimum disruptive measure as the answer. The rest of the code reconstructs the actual seam, using the information stored in the $next$ array. Noting that the innermost for loop runs over at most three values of $k$, we see that the running time of $\\text{COMPRESS-IMAGE}$ is $O(mn)$. The space requirement is also $O(mn)$. We can improve upon the space requirement by observing that row $i$ of the $disr$ table depends on only row $i + 1$. Therefore, we can store just two rows at any time. Thus, we can improve the space requirement of $\\text{COMPRESS-IMAGE}$ to $O(n)$.","title":"15-8 Image compression by seam carving"},{"location":"Chap15/Problems/15-9/","text":"A certain string-processing language allows a programmer to break a string into two pieces. Because this operation copies the string, it costs $n$ time units to break a string of $n$ characters into two pieces. Suppose a programmer wants to break a string into many pieces. The order in which the breaks occur can affect the total amount of time used. For example, suppose that the programmer wants to break a $20$-character string after characters $2$, $8$, and $10$ (numbering the characters in ascending order from the left-hand end, starting from $1$). If she programs the breaks to occur in left-to-right order, then the first break costs $20$ time units, the second break costs $18$ time units (breaking the string from characters $3$ to $20$ at character $8$), and the third break costs $12$ time units, totaling $50$ time units. If she programs the breaks to occur in right-to-left order, however, then the first break costs $20$ time units, the second break costs $10$ time units, and the third break costs $8$ time units, totaling $38$ time units. In yet another order, she could break first at $8$ (costing $20$), then break the left piece at $2$ (costing $8$), and finally the right piece at $10$ (costing $12$), for a total cost of $40$. Design an algorithm that, given the numbers of characters after which to break, determines a least-cost way to sequence those breaks. More formally, given a string $S$ with $n$ characters and an array $L[1..m]$ containing the break points, com- pute the lowest cost for a sequence of breaks, along with a sequence of breaks that achieves this cost. Our first step will be to identify the subproblems that satisfy the optimalsubstructure property. Before we frame the subproblem, we make two simplifying modifications to the input: We sort $L$ so that the indices in $L$ are in ascending order. We prepend the index $0$ to the beginning of $L$ and append n to the end of $L$. Let $L[i..j]$ denote a subarray of $L$ that starts from index $i$ and ends at index $j$. Define the subproblem denoted by $(i, j)$ as \"What is the cheapest sequence of breaks to break the substring $S[L[i] + 1..L[j]]$?\" Note that the first and last elements of the subarray $L[i..j]$ define the ends of the substring, and we have to worry about only the indices of the subarray $L[i + 1..j - 1]$. For example, let $L = \\langle 20, 17, 14, 11, 25 \\rangle$ and $n = 30$. First, we sort $L$. Then, we prepend $0$ and append $n$ as explained to get $L = \\langle 0, 11, 14, 17, 20, 25, 30 \\rangle$. Now, what is the subproblem $(2, 6)$? We obtain a substring by breaking $S$ after character $L[2] = 11$ and character $L[6] = 25$. We ask \"What is the cheapest sequence of breaks to break the substring $S[12..25]$?\" We have to worry about only indices in the subarray $L[3..5] = \\langle 14, 17, 20 \\rangle$, since the other indices are not present in the substring. At this point, the problem looks similar to matrix-chain multiplication (see Section 15.2). We can make the first break at any element of $L[i + 1..j - 1]$. Suppose that an optimal sequence of breaks $\\sigma$ for subproblem $(i, j)$ makes the first break at $L[k]$, where $i < k < j$. This break gives rise to two subproblems: The \"prefix\" subproblem $(i, k)$, covering the subarray $L[i + 1..k - 1]$, The \"suffix\" subproblem $(k, j)$, covering the subarray $L[k + 1..j - 1]$. The overall cost can be expressed as the sum of the length of the substring, the prefix cost, and the suffix cost. We show optimal substructure by claiming that the sequence of breaks in for the prefix subproblem $(i, k)$ must be an optimal one. Why? If there were a less costly way to break the substring $S[L[i] + 1..L[k]]$ represented by the subproblem $(i, k)$, then substituting that sequence of breaks in $\\sigma$ would produce another sequence of breaks whose cost is lower than that of $\\sigma$, which would be a contradiction. A similar observation holds for the sequence of breaks for the suffix subproblem $(k, j)$: it must be an optimal sequence of breaks. Let $cost[i, j]$ denote the cost of the cheapest solution to subproblem $(i, j)$. We write the recurrence relation for cost as $$ cost[i, j] = \\begin{cases} 0 & \\text{if $j - i \\le 1$}, \\\\ \\min\\limits_{i < k < j} {cost[i, k] + cost[k, j] + (L[j] - L[i])} & \\text{if $j - i > 1$}. \\end{cases} $$ Thus, our approach to solving the subproblem $(i, j)$ will be to try to split the respective substring at all possible values of $k$ and then choosing a break that results in the minimum cost. We need to be careful to solve smaller subproblems before we solve larger subproblems. In particular, we solve subproblems in increasing order of the length $j - 1$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 BREAK - STRING ( n , L ) prepend 0 to the start of L and append n to the end of L m = L . length sort L into increasing order let cost [ 1. . m , 1. . m ] and break [ 1. . m , 1. . m ] be new tables for i = 1 to m - 1 cost [ i , i ] = cost [ i , i + 1 ] = 0 cost [ m , m ] = 0 for len = 3 to m for i = 1 to m - len + 1 j = i + len - 1 cost [ i , j ] = \u221e for k = i + 1 to j - 1 if cost [ i , k ] + cost [ k , j ] < cost [ i , j ] cost [ i , j ] = cost [ i , k ] + cost [ k , j ] break [ i , j ] = k cost [ i , j ] = cost [ i , j ] + L [ j ] - L [ i ] print \"The minimum cost of breaking the string is\" cost [ 1 , m ] PRINT - BREAKS ( L , break , 1 , m ) After sorting $L$, we initialize the base cases, in which $i = j$ or $j = i + 1$. The nested for loops represent the main computation. The outermost for loop runs for $len = 3$ to $m$, which means that we need to consider subarrays of $L$ with length at least $3$, since the first and the last element define the substring, and we need at least one more element to specify a break. The increasing values of $len$ also ensures that we solve subproblems with smaller length before we solve subproblems with greater length. The inner for loop on $i$ runs from $1$ to $m - len + 1$. The upper bound of $m - len + 1$ is the largest value that the start index $i$ can take such that $i + len - 1 \\le m$. In the innermost for loop, we try each possible location k as the place to make the first break for subproblem $(i, j)$. The first such place is $L[i + 1]$, and not $L[i]$, since $L[i]$ represents the start of the substring (and thus not a valid place for a break). Similarly, the last valid place is $L[j - 1]$, because $L[j]$ represents the end of the substring. The if condition tests whether $k$ is the best place for a break found so far, and it updates the best value in $cost[i, j]$ if so. We use $break[i, j]$ to record that the best place for the first break is $k$. Specifically, if $break[i, j] = k$, then an optimal sequence of breaks for $(i, j)$ makes the first break at $L[k]$. Finally, we add the length of the substring $L[j] - L[i]$ to $cost[i, j]$ because, irrespective of what we choose as the first break, it costs us a price equal to the length of the substring to make a break. The lowest cost for the original problem ends up in $cost[1, m]$. By our initialization, $L[1] = 0$ and $L[m] = n$. Thus, $cost[1, m]$ will hold the optimum price of cutting the substring from $L[1] + 1 = 1$ to $L[m] = n$, which is the entire string. The running time is $\\Theta(m^3)$, and it is dictated by the three nested for loops. They fill in the entries above the main diagonal of the two tables, except for entries in which $j = i + 1$. That is, they fill in rows $i = 1, 2, \\ldots, m - 2$, entries $j = i + 2, i + 3, \\ldots, m$. When filling in entry $[i, j]$, we check values of $k$ running from $i + 1$ to $j - 1$, or $j - i - 1$ entries. Thus, the total number of iterations of the innermost for loop is $$ \\begin{aligned} \\sum{i = 1}^{m - 2} \\sum{j = i + 2}^m (j - i - 1) & = \\sum_{i = 1}^{m - 2} \\sum{d = 1}^{m - i - 1} d & \\text{($d = j - i - 1$)} \\\\ & = \\sum_{i = 1}^{m - 2} \\Theta((m - i)^2) & \\text{(equation (A.2))} \\\\ & = \\sum_{h = 2}^{m - 1} \\Theta(h^2) & (h = m - i) \\\\ & = \\Theta(m^3). & \\text{(equation (A.3))} \\end{aligned} $$ Since each iteration of the innermost for loop takes constant time, the total running time is $\\Theta(m^3)$. Note in particular that the running time is independent of the length of the string $n$. 1 2 3 4 5 PRINT - BREAKS ( L , break , i , j ) if j - i \u2265 2 print \"Break at\" L [ k ] PRINT - BREAKS ( L , break , i , k ) PRINT - BREAKS ( L , break , k , j ) $\\text{PRINT-BREAKS}$ uses the information stored in $break$ to print out the actual sequence of breaks.","title":"15-9 Breaking a string"},{"location":"Chap16/16.1/","text":"16.1-1 Give a dynamic-programming algorithm for the activity-selection problem, based on recurrence $\\text{(16.2)}$. Have your algorithm compute the sizes $c[i, j]$ as defined above and also produce the maximum-size subset of mutually compatible activities. Assume that the inputs have been sorted as in equation $\\text{(16.1)}$. Compare the running time of your solution to the running time of $\\text{GREEDY-ACTIVITY-SELECTOR}$. The tricky part is determining which activities are in the set $S_{ij}$. If activity $k$ is in $S_{ij}$, then we must have $i < k < j$, which means that $j - 1 \\ge 2$, but we must also have that $f_i \\le s_k$ and $f_k \\le s_j$. If we start $k$ at $j - 1$ and decrement $k$, we can stop once $k$ reaches $i$, but we can also stop once we find that $f_k \\le f_i$, since then activities $i + 1$ through $k$ cannot be compatible with activity $i$. We create two fictitious activities, $a_0$ with $f_0 = 0$ and $a_{n + 1}$ with $s_{n + 1} = \\infty$. We are interested in a maximum-size set $A_{0, n + 1}$ of mutually compatible activities in $S_{0, n + 1}$. We'll use tables $c[0..n + 1, 0..n + 1]$, as in recurrence $\\text{(16.2)}$ (so that $c[i, j] = |A_{ij}|$), and $act[0..n + 1, 0..n + 1]$, where $act[i, j]$ is the activity $k$ that we choose to put into $A_{ij}$. We fill the tables in according to increasing difference $j - 1$, which we denote by $l$ in the pseudocode. Since $S_{ij} = \\emptyset$; if $j - i < 2$, we initialize $c[i, i] = 0$ for all $i$ and $c[i, i + 1] = 0$ for $0 \\le i \\le n$. As in $\\text{RECURSIVE-ACTIVITY-SELECTOR}$ and $\\text{GREEDY-ACTIVITY-SELECTOR}$, the start and finish times are given as arrays $s$ and $f$, where we assume that the arrays already include the two fictitious activities and that the activities are sorted by monotonically increasing finish time. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 DYNAMIC - ACTIVITY - SELECTOR ( s , f , n ) let c [ 0. . n + 1 , 0. . n + 1 ] and act [ 0. . n + 1 , 0. . n + 1 ] be new tables for i = 0 to n c [ i , i ] = 0 c [ i , i + 1 ] = 0 c [ n + 1 , n + 1 ] = 0 for l = 2 to n + 1 for i = 0 to n - l + 1 j = i + l c [ i , j ] = 0 k = j - 1 while f [ i ] < f [ k ] if f [ i ] \u2264 s [ k ] and f [ k ] \u2264 s [ j ] and c [ i , k ] + c [ k , j ] + 1 > c [ i , j ] c [ i , j ] = c [ i , k ] + c [ k , j ] + 1 act [ i , j ] = k k = k - 1 print \"A maximum size set of mutually compatible activities has size\" c [ 0 , n + 1 ] print \"The set contains\" PRINT - ACTIVITIES ( c , act , 0 , n + 1 ) 1 2 3 4 5 6 PRINT - ACTIVITIES ( c , act , i , j ) if c [ i , j ] > 0 k = act [ i , j ] print k PRINT - ACTIVITIES ( c , act , i , k ) PRINT - ACTIVITIES ( c , act , k , j ) The $\\text{PRINT-ACTIVITIES}$ procedure recursively prints the set of activities placed into the optimal solution $A_{ij}$. It first prints the activity $k$ that achieved the maximum value of $c[i, j]$, and then it recurses to print the activities in $A_{ik}$ and $A_{kj}$. The recursion bottoms out when $c[i, j] = 0$, so that $A_{ij} = \\emptyset$. Whereas $\\text{GREEDY-ACTIVITY-SELECTOR}$ runs in $\\Theta(n)$ time, the $\\text{DYNAMIC-ACTIVITY-SELECTOR}$ procedure runs in $O(n^3)$ time. 16.1-2 Suppose that instead of always selecting the first activity to finish, we instead select the last activity to start that is compatible with all previously selected activities. Describe how this approach is a greedy algorithm, and prove that it yields an optimal solution. The proposed approach\u2014selecting the last activity to start that is compatible with all previously selected activities\u2014is really the greedy algorithm but starting from the end rather than the beginning. Another way to look at it is as follows. We are given a set $S = \\{a_1, a_2, \\ldots, a_n\\}$ of activities, where $a_i = [s_i, f_i)$, and we propose to find an optimal solution by selecting the last activity to start that is compatible with all previously selected activities. Instead, let us create a set $S = \\{a_1', a_2', \\ldots, a_n'\\}$, where $a_i' = [f_i, s_i)$. That is, $a_i'$ is $a_i$ in reverse. Clearly, a subset of $\\{a_{i_1}, a_{i_2}, \\ldots, a_{i_k}\\} \\subseteq S$ is mutually compatible if and only if the corresponding subset $\\{a_{i_1}', a_{i_2}', \\ldots, a_{i_k}'\\} \\subseteq S'$ is also mutually compatible. Thus, an optimal solution for $S$ maps directly to an optimal solution for $S'$ and vice versa. The proposed approach of selecting the last activity to start that is compatible with all previously selected activities, when run on $S$, gives the same answer as the greedy algorithm from the text\u2014selecting the first activity to finish that is compatible with all previously selected activities\u2014when run on $S'$. The solution that the proposed approach finds for $S$ corresponds to the solution that the text's greedy algorithm finds for $S'$, and so it is optimal. 16.1-3 Not just any greedy approach to the activity-selection problem produces a maximum-size set of mutually compatible activities. Give an example to show that the approach of selecting the activity of least duration from among those that are compatible with previously selected activities does not work. Do the same for the approaches of always selecting the compatible activity that overlaps the fewest other remaining activities and always selecting the compatible remaining activity with the earliest start time. For the approach of selecting the activity of least duration from those that are compatible with previously selected activities: $$ \\begin{array}{l|ccc} i & 1 & 2 & 3 \\\\ \\hline s_i & 0 & 2 & 3 \\\\ f_i & 3 & 4 & 6 \\\\ \\text{duration} & 3 & 2 & 3 \\end{array} $$ This approach selects just $\\{a_2\\}$, but the optimal solution selects $\\{a_1, a_3\\}$. For the approach of always selecting the compatible activity that overlaps the fewest other remaining activities: $$ \\begin{array}{l|ccc} i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\\\ \\hline s_i & 0 & 1 & 1 & 1 & 2 & 3 & 4 & 5 & 5 & 5 & 6 \\\\ f_i & 2 & 3 & 3 & 3 & 4 & 5 & 6 & 7 & 7 & 7 & 8 \\\\ \\text{\\# of overlapping activities} & 3 & 4 & 4 & 4 & 4 & 2 & 4 & 4 & 4 & 4 & 3 \\end{array} $$ This approach frst selects $a_6$, and after that choice it can select only two other activities (one of $a_1$, $a_2$, $a_3$, $a_4$ and one of $a_8$, $a_9$, $a_{10}$, $a_{11}$). An optimal solution is $\\{a_1, a_5, a_7, a_{11}\\}$. For the approach of always selecting the compatible remaining activity with the earliest start time, just add one more activity with the interval $[0, 14)$ to the example in Section 16.1. It will be the first activity selected, and no other activities are compatible with it. 16.1-4 Suppose that we have a set of activities to schedule among a large number of lecture halls, where any activity can take place in any lecture hall. We wish to schedule all the activities using as few lecture halls as possible. Give an efficient greedy algorithm to determine which activity should use which lecture hall. (This problem is also known as the interval-graph coloring problem . We can create an interval graph whose vertices are the given activities and whose edges connect incompatible activities. The smallest number of colors required to color every vertex so that no two adjacent vertices have the same color corresponds to finding the fewest lecture halls needed to schedule all of the given activities.) Let $S$ be the set of $n$ activities. The \"obvious\" solution of using $\\text{GREEDY-ACTIVITY-SELECTOR}$ to find a maximum-size set $S_1$ of compatible activities from $S$ for the first lecture hall, then using it again to find a maximum-size set $S_2$ of compatible activities from $S - S_1$ for the second hall, (and so on until all the activities are assigned), requires $\\Theta(n^2)$ time in the worst case. There is a better algorithm, however, whose asymptotic time is just the time needed to sort the activities by time\u2014$O(n\\lg n)$ time for arbitrary times, or possibly as fast as $O(n)$ if the times are small integers. The general idea is to go through the activities in order of start time, assigning each to any hall that is available at that time. To do this, move through the set of events consisting of activities starting and activities finishing, in order of event time. Maintain two lists of lecture halls: Halls that are busy at the current event time $t$ (because they have been assigned an activity $i$ that started at $s_i \\le t$ but won't finish until $f_i > t$) and halls that are free at time $t$. (As in the activity- selection problem in Section 16.1, we are assuming that activity time intervals are half open\u2014i.e., that if $s_i \\ge f_j$, then activities $i$ and $j$ are compatible.) When $t$ is the start time of some activity, assign that activity to a free hall and move the hall from the free list to the busy list. When $t$ is the finish time of some activity, move the activity's hall from the busy list to the free list. (The activity is certainly in some hall, because the event times are processed in order and the activity must have started before its finish time $t$, hence must have been assigned to a hall.) To avoid using more halls than necessary, always pick a hall that has already had an activity assigned to it, if possible, before picking a never-used hall. (This can be done by always working at the front of the free-halls list\u2014putting freed halls onto the front of the list and taking halls from the front of the list\u2014so that a new hall doesn't come to the front and get chosen if there are previously-used halls.) This guarantees that the algorithm uses as few lecture halls as possible: The algorithm will terminate with a schedule requiring $m \\le n$ lecture halls. Let activity $i$ be the first activity scheduled in lecture hall $m$. The reason that $i$ was put in the $m$th lecture hall is that the first $m - 1$ lecture halls were busy at time $s_i$ . So at this time there are $m$ activities occurring simultaneously. Therefore any schedule must use at least $m$ lecture halls, so the schedule returned by the algorithm is optimal. Run time: Sort the $2n$ activity-starts/activity-ends events. (In the sorted order, an activity-ending event should precede an activity-starting event that is at the same time.) $O(n\\lg n)$ time for arbitrary times, possibly $O(n)$ if the times are restricted (e.g., to small integers). Process the events in $O(n)$ time: Scan the $2n$ events, doing $O(1)$ work for each (moving a hall from one list to the other and possibly associating an activity with it). Total: $O(n + \\text{time to sort)}$ For this specific problem, time to sort could range in between $O(n)$ to $O(n\\lg n)$. Therefore, Time complexity could be $O(n)$ or $O(n\\lg n)$ depending on the implementation of the sorting algorithm. Counting Sort can sort the array in $O(n)$ time whereas merge sort requires $O(n\\lg n)$ time. [The idea of this algorithm is related to the rectangle-overlap algorithm in Exercise 14.3-7.] 16.1-5 Consider a modification to the activity-selection problem in which each activity $a_i$ has, in addition to a start and finish time, a value $v_i$. The objective is no longer to maximize the number of activities scheduled, but instead to maximize the total value of the activities scheduled. That is, we wish to choose a set $A$ of compatible activities such that $\\sum_{a_k \\in A} v_k$ is maximized. Give a polynomial-time algorithm for this problem. We can no longer use the greedy algorithm to solve this problem. However, as we show, the problem still has an optimal substructure which allows us to formulate a dynamic programming solution. The analysis here follows closely the analysis of Section 16.1 in the book. We define the value of a set of compatible events as the sum of values of events in that set. Let $S_{ij}$ be defined as in Section 16.1. An optimal solution to $S_{ij}$ is a subset of mutually compatible events of $S_{ij}$ that has maximum value. Let $A_{ij}$ be an optimal solution to $S_{ij}$. Suppose $A_{ij}$ includes an event $a_k$. Let $A_{ik}$ and $A_{kj}$ be defined as in Section 16.1. Thus, we have $A_{ij} = A_{ik} \\cup \\{a_k\\} \\cup A_{kj}$, and so the value of maximum-value set $A_{ij}$ is equal to the value of $A_{ik}$ plus the value of $A_{kj}$ plus $v_k$. The usual cut-and-paste argument shows that the optimal solution $A_{ij}$ must also include optimal solutions to the two subproblems for $S_{ik}$ and $S_{kj}$. If we could find a set $A_{kj}'$ of mutually compatible activities in $S_{kj}$ where the value of $A_{kj}'$ is greater than the value of $A_{kj}$, then we could use $A_{kj}'$, rather than $A_{kj}$, in a solution to the subproblem for $S_{ij}$. We would have constructed a set of mutually compatible activities with greater value than that of $A_{ij}$, which contradicts the assumption that $A_{ij}$ is an optimal solution. A symmetric argument applies to the activities in $S_{ik}$. Let us denote the value of an optimal solution for the set $S_{ij}$ by $val[i, j]$. Then, we would have the recurrence $$val[i, j] = val[i, k] + val[k, j] + v_k.$$ Of course, since we do not know that an optimal solution for the set $S_{ij}$ includes activity $a_k$ , we would have to examine all activities in $S_{ij}$ to find which one to choose, so that $$ val[i, j] = \\begin{cases} 0 & \\text{if $S_{ij} = \\emptyset$}, \\\\ \\max\\limits_{a_k\\in S_{ij}} {val[i, k] + val[k, j] + v_k} & \\text{if $S_{ij} \\ne \\emptyset$}. \\end{cases} $$ While implementing the recurrence, the tricky part is determining which activities are in the set $S_{ij}$ . If activity $k$ is in $S_{ij}$ , then we must have $i < k < j$, which means that $j - i \\ge 2$, but we must also have that $f_i \\le s_k$ and $f_k \\le s_j$. If we start $k$ at $j - 1$ and decrement $k$, we can stop once $k$ reaches $i$, but we can also stop once we find that $f_k \\le f_i$, since then activities $i + 1$ through $k$ cannot be compatible with activity $i$. We create two fictitious activities, $a_0$ with $f_0 = 0$ and $a_{n + 1}$ with $s_{n + 1} = \\infty$. We are interested in a maximum-size set $A_{0, n + 1}$ of mutually compatible activities in $S_{0, n + 1}$. We'll use tables $val[0..n + 1, 0..n + 1]$, as in the recurrence, and $act[0..n + 1, 0..n + 1]$, where $act[i, j]$ is the activity $k$ that we choose to put into $A_{ij}$ . We fill the tables in according to increasing difference $j - i$, which we denote by $l$ in the pseudocode. Since $S_{ij} = \\emptyset$ if $j - i < 2$, we initialize $val[i, i] = 0$ for all $i$ and $val[i, i + 1] = 0$ for $0\\le i\\le n$. As in $\\text{RECURSIVE-ACTIVITY-SELECTOR}$ and $\\text{GREEDY-ACTIVITY-SELECTOR}$, the start and finish times are given as arrays $s$ and $f$, where we assume that the arrays already include the two fictitious activities and that the activities are sorted by monotonically increasing finish time. The array $v$ specifies the value of each activity. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 MAX - VALUE - ACTIVITY - SELECTOR ( s , f , v , n ) let val [ 0. . n + 1 , 0. . n + 1 ] and act [ 0. . n + 1 , 0. . n + 1 ] be new tables for i = 0 to n val [ i , i ] = 0 val [ i , i + 1 ] = 0 val [ n + 1 , n + 1 ] = 0 for l = 2 to n + 1 for i = 0 to n - l + 1 j = i + l val [ i , j ] = 0 k = j - 1 while f [ i ] < f [ k ] if f [ i ] \u2264 s [ k ] and f [ k ] \u2264 s [ j ] and val [ i , k ] + val [ k , j ] + v [ k ] > val [ i , j ] val [ i , j ] = val [ i , k ] + val [ k , j ] + v [ k ] act [ i , j ] = k k = k - 1 print \"A maximum-value set of mutually compatible activities has value\" val [ 0 , n + 1 ] print \"The set contains\" PRINT - ACTIVITIES ( val , act , 0 , n + 1 ) 1 2 3 4 5 6 PRINT - ACTIVITIES ( val , act , i , j ) if val [ i , j ] > 0 k = act [ i , j ] print k PRINT - ACTIVITIES ( val , act , i , k ) PRINT - ACTIVITIES ( val , act , k , j ) The $\\text{PRINT-ACTIVITIES}$ procedure recursively prints the set of activities placed into the optimal solution $A_{ij}$ . It first prints the activity $k$ that achieved the maximum value of $val[i, j]$, and then it recurses to print the activities in $A_{ik}$ and $A_{kj}$. The recursion bottoms out when $val[i, j] = 0$, so that $A_{ij} = \\emptyset$. Whereas $\\text{GREEDY-ACTIVITY-SELECTOR}$ runs in $\\Theta(n)$ time, the $\\text{MAX-VALUE-ACTIVITY-SELECTOR}$ procedure runs in $O(n^3)$ time.","title":"16.1 An activity-selection problem"},{"location":"Chap16/16.1/#161-1","text":"Give a dynamic-programming algorithm for the activity-selection problem, based on recurrence $\\text{(16.2)}$. Have your algorithm compute the sizes $c[i, j]$ as defined above and also produce the maximum-size subset of mutually compatible activities. Assume that the inputs have been sorted as in equation $\\text{(16.1)}$. Compare the running time of your solution to the running time of $\\text{GREEDY-ACTIVITY-SELECTOR}$. The tricky part is determining which activities are in the set $S_{ij}$. If activity $k$ is in $S_{ij}$, then we must have $i < k < j$, which means that $j - 1 \\ge 2$, but we must also have that $f_i \\le s_k$ and $f_k \\le s_j$. If we start $k$ at $j - 1$ and decrement $k$, we can stop once $k$ reaches $i$, but we can also stop once we find that $f_k \\le f_i$, since then activities $i + 1$ through $k$ cannot be compatible with activity $i$. We create two fictitious activities, $a_0$ with $f_0 = 0$ and $a_{n + 1}$ with $s_{n + 1} = \\infty$. We are interested in a maximum-size set $A_{0, n + 1}$ of mutually compatible activities in $S_{0, n + 1}$. We'll use tables $c[0..n + 1, 0..n + 1]$, as in recurrence $\\text{(16.2)}$ (so that $c[i, j] = |A_{ij}|$), and $act[0..n + 1, 0..n + 1]$, where $act[i, j]$ is the activity $k$ that we choose to put into $A_{ij}$. We fill the tables in according to increasing difference $j - 1$, which we denote by $l$ in the pseudocode. Since $S_{ij} = \\emptyset$; if $j - i < 2$, we initialize $c[i, i] = 0$ for all $i$ and $c[i, i + 1] = 0$ for $0 \\le i \\le n$. As in $\\text{RECURSIVE-ACTIVITY-SELECTOR}$ and $\\text{GREEDY-ACTIVITY-SELECTOR}$, the start and finish times are given as arrays $s$ and $f$, where we assume that the arrays already include the two fictitious activities and that the activities are sorted by monotonically increasing finish time. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 DYNAMIC - ACTIVITY - SELECTOR ( s , f , n ) let c [ 0. . n + 1 , 0. . n + 1 ] and act [ 0. . n + 1 , 0. . n + 1 ] be new tables for i = 0 to n c [ i , i ] = 0 c [ i , i + 1 ] = 0 c [ n + 1 , n + 1 ] = 0 for l = 2 to n + 1 for i = 0 to n - l + 1 j = i + l c [ i , j ] = 0 k = j - 1 while f [ i ] < f [ k ] if f [ i ] \u2264 s [ k ] and f [ k ] \u2264 s [ j ] and c [ i , k ] + c [ k , j ] + 1 > c [ i , j ] c [ i , j ] = c [ i , k ] + c [ k , j ] + 1 act [ i , j ] = k k = k - 1 print \"A maximum size set of mutually compatible activities has size\" c [ 0 , n + 1 ] print \"The set contains\" PRINT - ACTIVITIES ( c , act , 0 , n + 1 ) 1 2 3 4 5 6 PRINT - ACTIVITIES ( c , act , i , j ) if c [ i , j ] > 0 k = act [ i , j ] print k PRINT - ACTIVITIES ( c , act , i , k ) PRINT - ACTIVITIES ( c , act , k , j ) The $\\text{PRINT-ACTIVITIES}$ procedure recursively prints the set of activities placed into the optimal solution $A_{ij}$. It first prints the activity $k$ that achieved the maximum value of $c[i, j]$, and then it recurses to print the activities in $A_{ik}$ and $A_{kj}$. The recursion bottoms out when $c[i, j] = 0$, so that $A_{ij} = \\emptyset$. Whereas $\\text{GREEDY-ACTIVITY-SELECTOR}$ runs in $\\Theta(n)$ time, the $\\text{DYNAMIC-ACTIVITY-SELECTOR}$ procedure runs in $O(n^3)$ time.","title":"16.1-1"},{"location":"Chap16/16.1/#161-2","text":"Suppose that instead of always selecting the first activity to finish, we instead select the last activity to start that is compatible with all previously selected activities. Describe how this approach is a greedy algorithm, and prove that it yields an optimal solution. The proposed approach\u2014selecting the last activity to start that is compatible with all previously selected activities\u2014is really the greedy algorithm but starting from the end rather than the beginning. Another way to look at it is as follows. We are given a set $S = \\{a_1, a_2, \\ldots, a_n\\}$ of activities, where $a_i = [s_i, f_i)$, and we propose to find an optimal solution by selecting the last activity to start that is compatible with all previously selected activities. Instead, let us create a set $S = \\{a_1', a_2', \\ldots, a_n'\\}$, where $a_i' = [f_i, s_i)$. That is, $a_i'$ is $a_i$ in reverse. Clearly, a subset of $\\{a_{i_1}, a_{i_2}, \\ldots, a_{i_k}\\} \\subseteq S$ is mutually compatible if and only if the corresponding subset $\\{a_{i_1}', a_{i_2}', \\ldots, a_{i_k}'\\} \\subseteq S'$ is also mutually compatible. Thus, an optimal solution for $S$ maps directly to an optimal solution for $S'$ and vice versa. The proposed approach of selecting the last activity to start that is compatible with all previously selected activities, when run on $S$, gives the same answer as the greedy algorithm from the text\u2014selecting the first activity to finish that is compatible with all previously selected activities\u2014when run on $S'$. The solution that the proposed approach finds for $S$ corresponds to the solution that the text's greedy algorithm finds for $S'$, and so it is optimal.","title":"16.1-2"},{"location":"Chap16/16.1/#161-3","text":"Not just any greedy approach to the activity-selection problem produces a maximum-size set of mutually compatible activities. Give an example to show that the approach of selecting the activity of least duration from among those that are compatible with previously selected activities does not work. Do the same for the approaches of always selecting the compatible activity that overlaps the fewest other remaining activities and always selecting the compatible remaining activity with the earliest start time. For the approach of selecting the activity of least duration from those that are compatible with previously selected activities: $$ \\begin{array}{l|ccc} i & 1 & 2 & 3 \\\\ \\hline s_i & 0 & 2 & 3 \\\\ f_i & 3 & 4 & 6 \\\\ \\text{duration} & 3 & 2 & 3 \\end{array} $$ This approach selects just $\\{a_2\\}$, but the optimal solution selects $\\{a_1, a_3\\}$. For the approach of always selecting the compatible activity that overlaps the fewest other remaining activities: $$ \\begin{array}{l|ccc} i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\\\ \\hline s_i & 0 & 1 & 1 & 1 & 2 & 3 & 4 & 5 & 5 & 5 & 6 \\\\ f_i & 2 & 3 & 3 & 3 & 4 & 5 & 6 & 7 & 7 & 7 & 8 \\\\ \\text{\\# of overlapping activities} & 3 & 4 & 4 & 4 & 4 & 2 & 4 & 4 & 4 & 4 & 3 \\end{array} $$ This approach frst selects $a_6$, and after that choice it can select only two other activities (one of $a_1$, $a_2$, $a_3$, $a_4$ and one of $a_8$, $a_9$, $a_{10}$, $a_{11}$). An optimal solution is $\\{a_1, a_5, a_7, a_{11}\\}$. For the approach of always selecting the compatible remaining activity with the earliest start time, just add one more activity with the interval $[0, 14)$ to the example in Section 16.1. It will be the first activity selected, and no other activities are compatible with it.","title":"16.1-3"},{"location":"Chap16/16.1/#161-4","text":"Suppose that we have a set of activities to schedule among a large number of lecture halls, where any activity can take place in any lecture hall. We wish to schedule all the activities using as few lecture halls as possible. Give an efficient greedy algorithm to determine which activity should use which lecture hall. (This problem is also known as the interval-graph coloring problem . We can create an interval graph whose vertices are the given activities and whose edges connect incompatible activities. The smallest number of colors required to color every vertex so that no two adjacent vertices have the same color corresponds to finding the fewest lecture halls needed to schedule all of the given activities.) Let $S$ be the set of $n$ activities. The \"obvious\" solution of using $\\text{GREEDY-ACTIVITY-SELECTOR}$ to find a maximum-size set $S_1$ of compatible activities from $S$ for the first lecture hall, then using it again to find a maximum-size set $S_2$ of compatible activities from $S - S_1$ for the second hall, (and so on until all the activities are assigned), requires $\\Theta(n^2)$ time in the worst case. There is a better algorithm, however, whose asymptotic time is just the time needed to sort the activities by time\u2014$O(n\\lg n)$ time for arbitrary times, or possibly as fast as $O(n)$ if the times are small integers. The general idea is to go through the activities in order of start time, assigning each to any hall that is available at that time. To do this, move through the set of events consisting of activities starting and activities finishing, in order of event time. Maintain two lists of lecture halls: Halls that are busy at the current event time $t$ (because they have been assigned an activity $i$ that started at $s_i \\le t$ but won't finish until $f_i > t$) and halls that are free at time $t$. (As in the activity- selection problem in Section 16.1, we are assuming that activity time intervals are half open\u2014i.e., that if $s_i \\ge f_j$, then activities $i$ and $j$ are compatible.) When $t$ is the start time of some activity, assign that activity to a free hall and move the hall from the free list to the busy list. When $t$ is the finish time of some activity, move the activity's hall from the busy list to the free list. (The activity is certainly in some hall, because the event times are processed in order and the activity must have started before its finish time $t$, hence must have been assigned to a hall.) To avoid using more halls than necessary, always pick a hall that has already had an activity assigned to it, if possible, before picking a never-used hall. (This can be done by always working at the front of the free-halls list\u2014putting freed halls onto the front of the list and taking halls from the front of the list\u2014so that a new hall doesn't come to the front and get chosen if there are previously-used halls.) This guarantees that the algorithm uses as few lecture halls as possible: The algorithm will terminate with a schedule requiring $m \\le n$ lecture halls. Let activity $i$ be the first activity scheduled in lecture hall $m$. The reason that $i$ was put in the $m$th lecture hall is that the first $m - 1$ lecture halls were busy at time $s_i$ . So at this time there are $m$ activities occurring simultaneously. Therefore any schedule must use at least $m$ lecture halls, so the schedule returned by the algorithm is optimal. Run time: Sort the $2n$ activity-starts/activity-ends events. (In the sorted order, an activity-ending event should precede an activity-starting event that is at the same time.) $O(n\\lg n)$ time for arbitrary times, possibly $O(n)$ if the times are restricted (e.g., to small integers). Process the events in $O(n)$ time: Scan the $2n$ events, doing $O(1)$ work for each (moving a hall from one list to the other and possibly associating an activity with it). Total: $O(n + \\text{time to sort)}$ For this specific problem, time to sort could range in between $O(n)$ to $O(n\\lg n)$. Therefore, Time complexity could be $O(n)$ or $O(n\\lg n)$ depending on the implementation of the sorting algorithm. Counting Sort can sort the array in $O(n)$ time whereas merge sort requires $O(n\\lg n)$ time. [The idea of this algorithm is related to the rectangle-overlap algorithm in Exercise 14.3-7.]","title":"16.1-4"},{"location":"Chap16/16.1/#161-5","text":"Consider a modification to the activity-selection problem in which each activity $a_i$ has, in addition to a start and finish time, a value $v_i$. The objective is no longer to maximize the number of activities scheduled, but instead to maximize the total value of the activities scheduled. That is, we wish to choose a set $A$ of compatible activities such that $\\sum_{a_k \\in A} v_k$ is maximized. Give a polynomial-time algorithm for this problem. We can no longer use the greedy algorithm to solve this problem. However, as we show, the problem still has an optimal substructure which allows us to formulate a dynamic programming solution. The analysis here follows closely the analysis of Section 16.1 in the book. We define the value of a set of compatible events as the sum of values of events in that set. Let $S_{ij}$ be defined as in Section 16.1. An optimal solution to $S_{ij}$ is a subset of mutually compatible events of $S_{ij}$ that has maximum value. Let $A_{ij}$ be an optimal solution to $S_{ij}$. Suppose $A_{ij}$ includes an event $a_k$. Let $A_{ik}$ and $A_{kj}$ be defined as in Section 16.1. Thus, we have $A_{ij} = A_{ik} \\cup \\{a_k\\} \\cup A_{kj}$, and so the value of maximum-value set $A_{ij}$ is equal to the value of $A_{ik}$ plus the value of $A_{kj}$ plus $v_k$. The usual cut-and-paste argument shows that the optimal solution $A_{ij}$ must also include optimal solutions to the two subproblems for $S_{ik}$ and $S_{kj}$. If we could find a set $A_{kj}'$ of mutually compatible activities in $S_{kj}$ where the value of $A_{kj}'$ is greater than the value of $A_{kj}$, then we could use $A_{kj}'$, rather than $A_{kj}$, in a solution to the subproblem for $S_{ij}$. We would have constructed a set of mutually compatible activities with greater value than that of $A_{ij}$, which contradicts the assumption that $A_{ij}$ is an optimal solution. A symmetric argument applies to the activities in $S_{ik}$. Let us denote the value of an optimal solution for the set $S_{ij}$ by $val[i, j]$. Then, we would have the recurrence $$val[i, j] = val[i, k] + val[k, j] + v_k.$$ Of course, since we do not know that an optimal solution for the set $S_{ij}$ includes activity $a_k$ , we would have to examine all activities in $S_{ij}$ to find which one to choose, so that $$ val[i, j] = \\begin{cases} 0 & \\text{if $S_{ij} = \\emptyset$}, \\\\ \\max\\limits_{a_k\\in S_{ij}} {val[i, k] + val[k, j] + v_k} & \\text{if $S_{ij} \\ne \\emptyset$}. \\end{cases} $$ While implementing the recurrence, the tricky part is determining which activities are in the set $S_{ij}$ . If activity $k$ is in $S_{ij}$ , then we must have $i < k < j$, which means that $j - i \\ge 2$, but we must also have that $f_i \\le s_k$ and $f_k \\le s_j$. If we start $k$ at $j - 1$ and decrement $k$, we can stop once $k$ reaches $i$, but we can also stop once we find that $f_k \\le f_i$, since then activities $i + 1$ through $k$ cannot be compatible with activity $i$. We create two fictitious activities, $a_0$ with $f_0 = 0$ and $a_{n + 1}$ with $s_{n + 1} = \\infty$. We are interested in a maximum-size set $A_{0, n + 1}$ of mutually compatible activities in $S_{0, n + 1}$. We'll use tables $val[0..n + 1, 0..n + 1]$, as in the recurrence, and $act[0..n + 1, 0..n + 1]$, where $act[i, j]$ is the activity $k$ that we choose to put into $A_{ij}$ . We fill the tables in according to increasing difference $j - i$, which we denote by $l$ in the pseudocode. Since $S_{ij} = \\emptyset$ if $j - i < 2$, we initialize $val[i, i] = 0$ for all $i$ and $val[i, i + 1] = 0$ for $0\\le i\\le n$. As in $\\text{RECURSIVE-ACTIVITY-SELECTOR}$ and $\\text{GREEDY-ACTIVITY-SELECTOR}$, the start and finish times are given as arrays $s$ and $f$, where we assume that the arrays already include the two fictitious activities and that the activities are sorted by monotonically increasing finish time. The array $v$ specifies the value of each activity. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 MAX - VALUE - ACTIVITY - SELECTOR ( s , f , v , n ) let val [ 0. . n + 1 , 0. . n + 1 ] and act [ 0. . n + 1 , 0. . n + 1 ] be new tables for i = 0 to n val [ i , i ] = 0 val [ i , i + 1 ] = 0 val [ n + 1 , n + 1 ] = 0 for l = 2 to n + 1 for i = 0 to n - l + 1 j = i + l val [ i , j ] = 0 k = j - 1 while f [ i ] < f [ k ] if f [ i ] \u2264 s [ k ] and f [ k ] \u2264 s [ j ] and val [ i , k ] + val [ k , j ] + v [ k ] > val [ i , j ] val [ i , j ] = val [ i , k ] + val [ k , j ] + v [ k ] act [ i , j ] = k k = k - 1 print \"A maximum-value set of mutually compatible activities has value\" val [ 0 , n + 1 ] print \"The set contains\" PRINT - ACTIVITIES ( val , act , 0 , n + 1 ) 1 2 3 4 5 6 PRINT - ACTIVITIES ( val , act , i , j ) if val [ i , j ] > 0 k = act [ i , j ] print k PRINT - ACTIVITIES ( val , act , i , k ) PRINT - ACTIVITIES ( val , act , k , j ) The $\\text{PRINT-ACTIVITIES}$ procedure recursively prints the set of activities placed into the optimal solution $A_{ij}$ . It first prints the activity $k$ that achieved the maximum value of $val[i, j]$, and then it recurses to print the activities in $A_{ik}$ and $A_{kj}$. The recursion bottoms out when $val[i, j] = 0$, so that $A_{ij} = \\emptyset$. Whereas $\\text{GREEDY-ACTIVITY-SELECTOR}$ runs in $\\Theta(n)$ time, the $\\text{MAX-VALUE-ACTIVITY-SELECTOR}$ procedure runs in $O(n^3)$ time.","title":"16.1-5"},{"location":"Chap16/16.2/","text":"16.2-1 Prove that the fractional knapsack problem has the greedy-choice property. Let $I$ be the following instance of the knapsack problem: Let $n$ be the number of items, let $v_i$ be the value of the $i$th item, let $w_i$ be the weight of the $i$th item and let $W$ be the capacity. Assume the items have been ordered in increasing order by $v_i / w_i$ and that $W \\ge w_n$. Let $s = (s_1, s_2, \\ldots, s_n)$ be a solution. The greedy algorithm works by assigning $s_n = \\min(w_n, W)$, and then continuing by solving the subproblem $$I' = (n - 1, \\{v_1, v_2, \\ldots, v_{n - 1}\\}, \\{w_1, w_2, \\ldots, w_{n - 1}\\}, W - w_n)$$ until it either reaches the state $W = 0$ or $n = 0$. We need to show that this strategy always gives an optimal solution. We prove this by contradiction. Suppose the optimal solution to $I$ is $s_1, s_2, \\ldots, s_n$, where $s_n < \\min(w_n, W)$. Let $i$ be the smallest number such that $s_i > 0$. By decreasing $s_i$ to $\\max(0, W - w_n)$ and increasing $s_n$ by the same amount, we get a better solution. Since this a contradiction the assumption must be false. Hence the problem has the greedy-choice property. 16.2-2 Give a dynamic-programming solution to the $0$-$1$ knapsack problem that runs in $O(nW)$ time, where $n$ is the number of items and $W$ is the maximum weight of items that the thief can put in his knapsack. The solution is based on the optimal-substructure observation in the text: Let $i$ be the highest-numbered item in an optimal solution $S$ for $W$ pounds and items $1, \\ldots, n$. Then $S' = S - \\{i\\}$ must be an optimal solution for $W - w_i$ pounds and items $1, \\ldots, i - 1$, and the value of the solution $S$ is $v_i$ plus the value of the subproblem solution $S'$. We can express this relationship in the following formula: Define $c[i, w]$ to be the value of the solution for items $1, \\ldots, i$ and maximum weight $w$. Then $$ c[i,w] = \\begin{cases} 0 & \\text{ if } i = 0 \\text{ or } w = 0, \\\\ c[i - 1, w] & \\text{ if } w_i > w, \\\\ \\max(v_i + c[i - 1, w - w_i], c[i - 1, w]) & \\text{ if } i > 0 \\text{ and } w \\ge w_i. \\end{cases} $$ The last case says that the value of a solution for i items either includes item $i$, in which case it is $v_i$ plus a subproblem solution for $i - 1$ items and the weight excluding $w_i$, or doesn't include item $i$, in which case it is a subproblem solution for $i - 1$ items and the same weight. That is, if the thief picks item $i$, he takes $v_i$ value, and he can choose from items $1, \\ldots, i - 1$ up to the weight limit $w - w_i$ , and get $c[i - 1, w - w_i]$ additional value. On the other hand, if he decides not to take item $i$, he can choose from items $1, \\ldots, i - 1$ up to the weight limit $w$, and get $c[i - 1, w]$ value. The better of these two choices should be made. The algorithm takes as inputs the maximum weight $W$, the number of items $n$, and the two sequences $v = \\langle v_1, v_2, \\ldots, v_n \\rangle$ and $w = \\langle w_1, w_2, \\ldots, w_n \\rangle$. It stores the $c[i, j]$ values in a table $c[0..n, 0..W]$ whose entries are computed in row-major order. (That is, the first row of $c$ is filled in from left to right, then the second row, and so on.) At the end of the computation, $c[n, W]$ contains the maximum value the thief can take. 1 2 3 4 5 6 7 8 9 10 11 12 DYNAMIC - 0 - 1 - KNAPSACK ( v , w , n , W ) let c [ 0. . n , 0. . W ] be a new array for w = 0 to W c [ 0 , w ] = 0 for i = 1 to n c [ i , 0 ] = 0 for w = 1 to W if w [ i ] \u2264 w if v [ i ] + c [ i - 1 , w - w [ i ]] > c [ i - 1 , w ] c [ i , w ] = v [ i ] + c [ i - 1 , w - w [ i ]] else c [ i , w ] = c [ i - 1 , w ] else c [ i , w ] = c [ i - 1 , w ] The set of items to take can be deduced from the $c$ table by starting at $c[n, W]$ and tracing where the optimal values came from. If $c[i, w] = c[i - 1, w]$, then item $i$ is not part of the solution, and we continue tracing with $c[i - 1, w]$. Otherwise item $i$ is part of the solution, and we continue tracing with $c[i - 1, w - w_i]$. The above algorithm takes $\\Theta(nW)$ time total: $\\Theta(nW)$ to fill in the $c$ table: $(n + 1) \\cdot (W + 1)$ entries, each requiring $\\Theta(1)$ time to compute. $O(n)$ time to trace the solution (since it starts in row $n$ of the table and moves up one row at each step). 16.2-3 Suppose that in a $0$-$1$ knapsack problem, the order of the items when sorted by increasing weight is the same as their order when sorted by decreasing value. Give an efficient algorithm to find an optimal solution to this variant of the knapsack problem, and argue that your algorithm is correct. Suppose in an optimal solution we take an item with $v_1$, $w_1$, and drop an item with $v_2$, $w_2$, and $w_1 > w_2$, $v_1 < v_2$, we can substitude $1$ with $2$ and get a better solution. Therefore we should always choose the items with the greatest values. 16.2-4 Professor Gekko has always dreamed of inline skating across North Dakota. He plans to cross the state on highway U.S. 2, which runs from Grand Forks, on the eastern border with Minnesota, to Williston, near the western border with Montana. The professor can carry two liters of water, and he can skate $m$ miles before running out of water. (Because North Dakota is relatively flat, the professor does not have to worry about drinking water at a greater rate on uphill sections than on flat or downhill sections.) The professor will start in Grand Forks with two full liters of water. His official North Dakota state map shows all the places along U.S. 2 at which he can refill his water and the distances between these locations. The professor's goal is to minimize the number of water stops along his route across the state. Give an efficient method by which he can determine which water stops he should make. Prove that your strategy yields an optimal solution, and give its running time. The optimal strategy is the obvious greedy one. Starting will both bottles, Professor Gekko should go to the westernmost place that he can refill his bottles within $m$ miles of Grand Forks. Fill up there. Then go to the westernmost refilling location he can get to within $m$ miles of where he filled up, and fill up there, and so on. Looked at another way, at each refilling location, Professor Gekko should check whether he can make it to the next refilling location without stopping at this one. If he can, skip this one. If he cannot, then fill up. Professor Gekko doesn't need to know how much water he has or how fat the next refilling location is to implement this approach, since at each fillup, he can determine which is the next location at which he'll need to stop. This problem has optimal substructure. Suppose there are $n$ possible refilling locations. Consider an optimal solution with $s$ refilling locations and whose first stop is at the $k$th refilling location. Then the rest of the optimal solution must be an optimal solution to the subproblem of the remaining $n - k$ locations. Otherwise, if there were a better solution to the subproblem, i.e., one with fewer than $s - 1$ stops, we could use it to come up with a solution with fewer than $s$ stops for the full problem, contradicting our supposition of optimality. This problem also has the greedy-choice property. Suppose there are $k$ refilling locations beyond the start that are within $m$ miles of the start. The greedy solution chooses the $k$th location as its first stop. No location beyond the $k$th works as a first stop, since Professor Gekko runs out of gas first. If a solution chooses a location $j < k$ as its first stop, then Professor Gekko could choose the $k$th location instead, having at least as much water when he leaves the $k$th location as if he'd chosen the $j$th location. Therefore, he would get at least as far without filling up again if he had chosen the $k$th location. If there are $n$ refilling locations on the map, Professor Gekko needs to inspect each one just once. The running time is $O(n)$. 16.2-5 Describe an efficient algorithm that, given a set $\\{x_1, x_2, \\ldots, x_n\\}$ of points on the real line, determines the smallest set of unit-length closed intervals that contains all of the given points. Argue that your algorithm is correct. Consider the leftmost interval. It will do no good if it extends any further left than the leftmost point, however, we know that it must contain the leftmost point. So, we know that it's left hand side is exactly the leftmost point. So, we just remove any point that is within a unit distance of the left most point since they are contained in this single interval. Then, we just repeat until all points are covered. Since at each step there is a clearly optimal choice for where to put the leftmost interval, this final solution is optimal. 16.2-6 $\\star$ Show how to solve the fractional knapsack problem in $O(n)$ time. Use a linear-time median algorithm to calculate the median m of the $v_i / w_i$ ratios. Next, partition the items into three sets: $G = \\{i : v_i / w_i > m\\}$, $E = \\{i : v_i / w_i = m\\}$, and $L = \\{i : v_i / w_i < m\\}$; this step takes linear time. Compute $W_G = \\sum_{i \\in G} w_i$ and $W_E = \\sum_{i \\in E} w_i$, the total weight of the items in sets $G$ and $E$, respectively. If $W_G > W$, then do not yet take any items in set $G$, and instead recurse on the set of items $G$ and knapsack capacity $W$. Otherwise $(W_G \\le W)$, take all items in set $G$, and take as much of the items in set $E$ as will fit in the remaining capacity $W - W_G$. If $W_G + W_E \\ge W$ (i.e., there is no capacity left after taking all the items in set $G$ and all the items in set E that fit in the remaining capacity $W - W_G$), then we are done. Otherwise $(W_G + W_E < W)$, then after taking all the items in sets $G$ and $E$, recurse on the set of items $L$ and knapsack capacity $W - W_G - W_E$. To analyze this algorithm, note that each recursive call takes linear time, exclusive of the time for a recursive call that it may make. When there is a recursive call, there is just one, and it's for a problem of at most half the size. Thus, the running time is given by the recurrence $T(n) \\le T (n / 2) + \\Theta(n)$, whose solution is $T(n) = O(n)$. 16.2-7 Suppose you are given two sets $A$ and $B$, each containing $n$ positive integers. You can choose to reorder each set however you like. After reordering, let $a_i$ be the $i$th element of set $A$, and let $b_i$ be the $i$th element of set $B$. You then receive a payoff of $\\prod_{i = 1}^n a_i^{b_i}$. Give an algorithm that will maximize your payoff. Prove that your algorithm maximizes the payoff, and state its running time. Sort $A$ and $B$ into monotonically decreasing order. Here's a proof that this method yields an optimal solution. Consider any indices $i$ and $j$ such that $i < j$, and consider the terms $a_i^{b_i}$ and $a_j^{b_j}$. We want to show that it is no worse to include these terms in the payoff than to include $a_i^{b_j}$ and $a_j^{b_i}$, i.e., that $a_i^{b_i} a_j^{b_j} \\ge a_i^{b_j} a_j^{b_i}$. Since $A$ and $B$ are sorted into monotonically decreasing order and $i < j$, we have $a_i \\ge a_j$ and $b_i \\ge b_j$. Since $a_i$ and $a_j$ are positive and $b_i - b_j$ is nonnegative, we have $a_i^{b_i - b_j} \\ge a_j^{b_i - b_j}$. Multiplying both sides by $a_i^{b_j} a_j^{b_j}$ yields $a_i^{b_i} a_j^{b_j} \\ge a_i^{b_j} a_j^{b_i}$. Since the order of multiplication doesn't matter, sorting $A$ and $B$ into monotonically increasing order works as well.","title":"16.2 Elements of the greedy strategy"},{"location":"Chap16/16.2/#162-1","text":"Prove that the fractional knapsack problem has the greedy-choice property. Let $I$ be the following instance of the knapsack problem: Let $n$ be the number of items, let $v_i$ be the value of the $i$th item, let $w_i$ be the weight of the $i$th item and let $W$ be the capacity. Assume the items have been ordered in increasing order by $v_i / w_i$ and that $W \\ge w_n$. Let $s = (s_1, s_2, \\ldots, s_n)$ be a solution. The greedy algorithm works by assigning $s_n = \\min(w_n, W)$, and then continuing by solving the subproblem $$I' = (n - 1, \\{v_1, v_2, \\ldots, v_{n - 1}\\}, \\{w_1, w_2, \\ldots, w_{n - 1}\\}, W - w_n)$$ until it either reaches the state $W = 0$ or $n = 0$. We need to show that this strategy always gives an optimal solution. We prove this by contradiction. Suppose the optimal solution to $I$ is $s_1, s_2, \\ldots, s_n$, where $s_n < \\min(w_n, W)$. Let $i$ be the smallest number such that $s_i > 0$. By decreasing $s_i$ to $\\max(0, W - w_n)$ and increasing $s_n$ by the same amount, we get a better solution. Since this a contradiction the assumption must be false. Hence the problem has the greedy-choice property.","title":"16.2-1"},{"location":"Chap16/16.2/#162-2","text":"Give a dynamic-programming solution to the $0$-$1$ knapsack problem that runs in $O(nW)$ time, where $n$ is the number of items and $W$ is the maximum weight of items that the thief can put in his knapsack. The solution is based on the optimal-substructure observation in the text: Let $i$ be the highest-numbered item in an optimal solution $S$ for $W$ pounds and items $1, \\ldots, n$. Then $S' = S - \\{i\\}$ must be an optimal solution for $W - w_i$ pounds and items $1, \\ldots, i - 1$, and the value of the solution $S$ is $v_i$ plus the value of the subproblem solution $S'$. We can express this relationship in the following formula: Define $c[i, w]$ to be the value of the solution for items $1, \\ldots, i$ and maximum weight $w$. Then $$ c[i,w] = \\begin{cases} 0 & \\text{ if } i = 0 \\text{ or } w = 0, \\\\ c[i - 1, w] & \\text{ if } w_i > w, \\\\ \\max(v_i + c[i - 1, w - w_i], c[i - 1, w]) & \\text{ if } i > 0 \\text{ and } w \\ge w_i. \\end{cases} $$ The last case says that the value of a solution for i items either includes item $i$, in which case it is $v_i$ plus a subproblem solution for $i - 1$ items and the weight excluding $w_i$, or doesn't include item $i$, in which case it is a subproblem solution for $i - 1$ items and the same weight. That is, if the thief picks item $i$, he takes $v_i$ value, and he can choose from items $1, \\ldots, i - 1$ up to the weight limit $w - w_i$ , and get $c[i - 1, w - w_i]$ additional value. On the other hand, if he decides not to take item $i$, he can choose from items $1, \\ldots, i - 1$ up to the weight limit $w$, and get $c[i - 1, w]$ value. The better of these two choices should be made. The algorithm takes as inputs the maximum weight $W$, the number of items $n$, and the two sequences $v = \\langle v_1, v_2, \\ldots, v_n \\rangle$ and $w = \\langle w_1, w_2, \\ldots, w_n \\rangle$. It stores the $c[i, j]$ values in a table $c[0..n, 0..W]$ whose entries are computed in row-major order. (That is, the first row of $c$ is filled in from left to right, then the second row, and so on.) At the end of the computation, $c[n, W]$ contains the maximum value the thief can take. 1 2 3 4 5 6 7 8 9 10 11 12 DYNAMIC - 0 - 1 - KNAPSACK ( v , w , n , W ) let c [ 0. . n , 0. . W ] be a new array for w = 0 to W c [ 0 , w ] = 0 for i = 1 to n c [ i , 0 ] = 0 for w = 1 to W if w [ i ] \u2264 w if v [ i ] + c [ i - 1 , w - w [ i ]] > c [ i - 1 , w ] c [ i , w ] = v [ i ] + c [ i - 1 , w - w [ i ]] else c [ i , w ] = c [ i - 1 , w ] else c [ i , w ] = c [ i - 1 , w ] The set of items to take can be deduced from the $c$ table by starting at $c[n, W]$ and tracing where the optimal values came from. If $c[i, w] = c[i - 1, w]$, then item $i$ is not part of the solution, and we continue tracing with $c[i - 1, w]$. Otherwise item $i$ is part of the solution, and we continue tracing with $c[i - 1, w - w_i]$. The above algorithm takes $\\Theta(nW)$ time total: $\\Theta(nW)$ to fill in the $c$ table: $(n + 1) \\cdot (W + 1)$ entries, each requiring $\\Theta(1)$ time to compute. $O(n)$ time to trace the solution (since it starts in row $n$ of the table and moves up one row at each step).","title":"16.2-2"},{"location":"Chap16/16.2/#162-3","text":"Suppose that in a $0$-$1$ knapsack problem, the order of the items when sorted by increasing weight is the same as their order when sorted by decreasing value. Give an efficient algorithm to find an optimal solution to this variant of the knapsack problem, and argue that your algorithm is correct. Suppose in an optimal solution we take an item with $v_1$, $w_1$, and drop an item with $v_2$, $w_2$, and $w_1 > w_2$, $v_1 < v_2$, we can substitude $1$ with $2$ and get a better solution. Therefore we should always choose the items with the greatest values.","title":"16.2-3"},{"location":"Chap16/16.2/#162-4","text":"Professor Gekko has always dreamed of inline skating across North Dakota. He plans to cross the state on highway U.S. 2, which runs from Grand Forks, on the eastern border with Minnesota, to Williston, near the western border with Montana. The professor can carry two liters of water, and he can skate $m$ miles before running out of water. (Because North Dakota is relatively flat, the professor does not have to worry about drinking water at a greater rate on uphill sections than on flat or downhill sections.) The professor will start in Grand Forks with two full liters of water. His official North Dakota state map shows all the places along U.S. 2 at which he can refill his water and the distances between these locations. The professor's goal is to minimize the number of water stops along his route across the state. Give an efficient method by which he can determine which water stops he should make. Prove that your strategy yields an optimal solution, and give its running time. The optimal strategy is the obvious greedy one. Starting will both bottles, Professor Gekko should go to the westernmost place that he can refill his bottles within $m$ miles of Grand Forks. Fill up there. Then go to the westernmost refilling location he can get to within $m$ miles of where he filled up, and fill up there, and so on. Looked at another way, at each refilling location, Professor Gekko should check whether he can make it to the next refilling location without stopping at this one. If he can, skip this one. If he cannot, then fill up. Professor Gekko doesn't need to know how much water he has or how fat the next refilling location is to implement this approach, since at each fillup, he can determine which is the next location at which he'll need to stop. This problem has optimal substructure. Suppose there are $n$ possible refilling locations. Consider an optimal solution with $s$ refilling locations and whose first stop is at the $k$th refilling location. Then the rest of the optimal solution must be an optimal solution to the subproblem of the remaining $n - k$ locations. Otherwise, if there were a better solution to the subproblem, i.e., one with fewer than $s - 1$ stops, we could use it to come up with a solution with fewer than $s$ stops for the full problem, contradicting our supposition of optimality. This problem also has the greedy-choice property. Suppose there are $k$ refilling locations beyond the start that are within $m$ miles of the start. The greedy solution chooses the $k$th location as its first stop. No location beyond the $k$th works as a first stop, since Professor Gekko runs out of gas first. If a solution chooses a location $j < k$ as its first stop, then Professor Gekko could choose the $k$th location instead, having at least as much water when he leaves the $k$th location as if he'd chosen the $j$th location. Therefore, he would get at least as far without filling up again if he had chosen the $k$th location. If there are $n$ refilling locations on the map, Professor Gekko needs to inspect each one just once. The running time is $O(n)$.","title":"16.2-4"},{"location":"Chap16/16.2/#162-5","text":"Describe an efficient algorithm that, given a set $\\{x_1, x_2, \\ldots, x_n\\}$ of points on the real line, determines the smallest set of unit-length closed intervals that contains all of the given points. Argue that your algorithm is correct. Consider the leftmost interval. It will do no good if it extends any further left than the leftmost point, however, we know that it must contain the leftmost point. So, we know that it's left hand side is exactly the leftmost point. So, we just remove any point that is within a unit distance of the left most point since they are contained in this single interval. Then, we just repeat until all points are covered. Since at each step there is a clearly optimal choice for where to put the leftmost interval, this final solution is optimal.","title":"16.2-5"},{"location":"Chap16/16.2/#162-6-star","text":"Show how to solve the fractional knapsack problem in $O(n)$ time. Use a linear-time median algorithm to calculate the median m of the $v_i / w_i$ ratios. Next, partition the items into three sets: $G = \\{i : v_i / w_i > m\\}$, $E = \\{i : v_i / w_i = m\\}$, and $L = \\{i : v_i / w_i < m\\}$; this step takes linear time. Compute $W_G = \\sum_{i \\in G} w_i$ and $W_E = \\sum_{i \\in E} w_i$, the total weight of the items in sets $G$ and $E$, respectively. If $W_G > W$, then do not yet take any items in set $G$, and instead recurse on the set of items $G$ and knapsack capacity $W$. Otherwise $(W_G \\le W)$, take all items in set $G$, and take as much of the items in set $E$ as will fit in the remaining capacity $W - W_G$. If $W_G + W_E \\ge W$ (i.e., there is no capacity left after taking all the items in set $G$ and all the items in set E that fit in the remaining capacity $W - W_G$), then we are done. Otherwise $(W_G + W_E < W)$, then after taking all the items in sets $G$ and $E$, recurse on the set of items $L$ and knapsack capacity $W - W_G - W_E$. To analyze this algorithm, note that each recursive call takes linear time, exclusive of the time for a recursive call that it may make. When there is a recursive call, there is just one, and it's for a problem of at most half the size. Thus, the running time is given by the recurrence $T(n) \\le T (n / 2) + \\Theta(n)$, whose solution is $T(n) = O(n)$.","title":"16.2-6 $\\star$"},{"location":"Chap16/16.2/#162-7","text":"Suppose you are given two sets $A$ and $B$, each containing $n$ positive integers. You can choose to reorder each set however you like. After reordering, let $a_i$ be the $i$th element of set $A$, and let $b_i$ be the $i$th element of set $B$. You then receive a payoff of $\\prod_{i = 1}^n a_i^{b_i}$. Give an algorithm that will maximize your payoff. Prove that your algorithm maximizes the payoff, and state its running time. Sort $A$ and $B$ into monotonically decreasing order. Here's a proof that this method yields an optimal solution. Consider any indices $i$ and $j$ such that $i < j$, and consider the terms $a_i^{b_i}$ and $a_j^{b_j}$. We want to show that it is no worse to include these terms in the payoff than to include $a_i^{b_j}$ and $a_j^{b_i}$, i.e., that $a_i^{b_i} a_j^{b_j} \\ge a_i^{b_j} a_j^{b_i}$. Since $A$ and $B$ are sorted into monotonically decreasing order and $i < j$, we have $a_i \\ge a_j$ and $b_i \\ge b_j$. Since $a_i$ and $a_j$ are positive and $b_i - b_j$ is nonnegative, we have $a_i^{b_i - b_j} \\ge a_j^{b_i - b_j}$. Multiplying both sides by $a_i^{b_j} a_j^{b_j}$ yields $a_i^{b_i} a_j^{b_j} \\ge a_i^{b_j} a_j^{b_i}$. Since the order of multiplication doesn't matter, sorting $A$ and $B$ into monotonically increasing order works as well.","title":"16.2-7"},{"location":"Chap16/16.3/","text":"16.3-1 Explain why, in the proof of Lemma 16.2, if $x.freq = b.freq$, then we must have $a.freq = b.freq = x.freq = y.freq$. We are given that $x.freq \\le y.freq$ are the two lowest frequencies in order, and that $a.freq \\le b.freq$. Now, $$ \\begin{array}{rcl} b.freq & = & x.freq \\\\ \\Rightarrow a.freq & \\le & x.freq \\\\ \\Rightarrow a.freq & = & x.freq & \\text{(since $x.freq$ is the lowest frequency)}, \\end{array} $$ and since $y.freq \\le b.freq$, $$ \\begin{array}{rcl} b.freq & = & x.freq \\\\ \\Rightarrow y.freq & \\le & x.freq \\\\ \\Rightarrow y.freq & = & x.freq & \\text{(since $x.freq$ is the lowest frequency)}. \\end{array} $$ Thus, if we assume that $x.freq = b.freq$, then we have that each of $a.freq$, $b.freq$, and $y.freq$ equals $x.freq$, and so $$a.freq = b.freq = x.freq = y.freq.$$ 16.3-2 Prove that a binary tree that is not full cannot correspond to an optimal prefix code. Let $T$ be a binary tree that is not full. $T$ represents a binary prefix code for a file composed of characters from alphabet $C$, where $c \\in C$, $f(c)$ is th number of occurrences of $c$ in the file. The cost of tree $T$, or the number of bits in the encoding, is $\\sum_{c \\in C} d_T(c) \\cdot f(c)$, where $d_T(c)$ is the depth of character $c$ in tree $T$. Let $N$ be a node of greatest depth that has exactly one child. If $N$ is the root of $T$, $N$ can be removed and the deepth of each node reduced by one, yielding a tree representing the same alphabet with a lower cost. This mean the original code was not optimal. Otherwise, let $M$ be the parent of $N$, let $T_1$ be the (possibly non-existent) sibling of $N$, and let $T_2$ be the subtree rooted at the child of $N$. Replace $M$ by $N$, making the children of $N$ the roots of subtrees $T_1$ and $T_2$. If $T_1$ is empty, repeat the process. We have a new prefix code of lower cost, so the original was not optimal. 16.3-3 What is an optimal Huffman code for the following set of frequencies, based on the first $8$ Fibonacci numbers? $$a:1 \\quad b:1 \\quad c:2 \\quad d:3 \\quad e:5 \\quad f:8 \\quad g:13 \\quad h:21$$ Can you generalize your answer to find the optimal code when the frequencies are the first $n$ Fibonacci numbers? $$ \\begin{array}{c|l} a & 1111111 \\\\ b & 1111110 \\\\ c & 111110 \\\\ d & 11110 \\\\ e & 1110 \\\\ f & 110 \\\\ g & 10 \\\\ h & 0 \\end{array} $$ 16.3-4 Prove that we can also express the total cost of a tree for a code as the sum, over all internal nodes, of the combined frequencies of the two children of the node. Let tree be a full binary tree with $n$ leaves. Apply induction hypothesis on the number of leaves in $T$. When $n = 2$ (the case $n = 1$ is trivially true), there are two leaves $x$ and $y$ with the same parent $z$, then the cost of $T$ is $$ \\begin{aligned} B(T) & = f(x) d_T(x) + f(y) d_T(y) \\\\ & = f(x) + f(y) & \\text{since $d_T(x) = d_T(y) = 1$} \\\\ & = f(\\text{child}_1\\text{ of }z) + f(\\text{child}_2\\text{ of }z). \\end{aligned} $$ Thus, the statement of theorem is true. Now suppose $n > 2$ and also suppose that theorem is true for trees on $n - 1$ leaves. Let $c_1$ and $c_2$ are two sibling leaves in $T$ such that they have the same parent $p$. Letting $T'$ be the tree obtained by deleting $c_1$ and $c_2$, by induction we know that $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves } l'\\in T'} f(l')d_T(l') \\\\ & = \\sum_{\\text{internal nodes } i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i'). \\end{aligned} $$ Using this information, calculates the cost of $T$. $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves }l \\in T} f(l)d_T(l) \\\\ & = \\sum_{l \\ne c_1, c_2} f(l)d_T(l) + f(c_1)d_T(c_1) - 1 + f(c_2)d_T(c_2) - 1 + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i') + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i\\in T} f(\\text{child}_1\\text{ of }i) + f(\\text{child}_1\\text{ of }i). \\end{aligned} $$ Thus the statement is true. 16.3-5 Prove that if we order the characters in an alphabet so that their frequencies are monotonically decreasing, then there exists an optimal code whose codeword lengths are monotonically increasing. It were a contradiction to have an optimal tree whose frequencies and codewords were monotonically increasing in the strict sense; since, given $f(x_1) > \\ldots > f(x_n) \\wedge d_T(x_1) > \\cdots > d_T(x_n)$, it follows that (where $n$ is odd): $$ \\begin{aligned} & f(x_1)d_T(x_1) + \\cdots + f(x_n)d_T(x_n) > f(x_1)d_T(x_n) + \\cdots + f(x_n)d_T(x_1) > 0 \\\\ & f(x_1)(d_T(x_1) - d_T(x_n)) + \\cdots + f(x_n)(d_T(x_n) - d_T(x_1)) > 0 \\\\ & f(x_1)(d_T(x_1) - d_T(x_n)) + \\cdots + f(x_{\\lfloor \\frac{n}{2} - 1 \\rfloor})(d_T(x_{\\lfloor \\frac{n}{2} \\rfloor - 1}) - d_T(x_{\\lfloor \\frac{n}{2} \\rfloor + 1})) \\\\ & > f(x_{\\lfloor \\frac{n}{2} + 1 \\rfloor})(d_T(x_{\\lfloor \\frac{n}{2} \\rfloor - 1}) - d_T(x_{\\lfloor \\frac{n}{2} \\rfloor + 1}))+ \\cdots + f(x_n)(d_T(x_1) - d_T(x_n)). \\end{aligned} $$ That is, where $i$ and $j$ are the upper and lower median, respectively; and $c_i = d_T(x_i) - d_T(x_{n - i + 1})$: $$f(x_1)c_1 + \\cdots + f(x_i)c_i > f(x_j)c_i + \\cdots + f(x_n)c_1$$ since $$ \\begin{aligned} f(x_i) & > f(x_{n - i + 1}) & 1 \\le i \\le \\Big\\lfloor \\frac{n}{2} \\Big\\rfloor. \\end{aligned} $$ 16.3-6 Suppose we have an optimal prefix code on a set $C = \\{0, 1, \\ldots, n - 1 \\}$ of characters and we wish to transmit this code using as few bits as possible. Show how to represent any optimal prefix code on $C$ using only $2n - 1 + n \\lceil \\lg n \\rceil$ bits. ($\\textit{Hint:}$ Use $2n - 1$ bits to specify the structure of the tree, as discovered by a walk of the tree.) First observe that any full binary tree has exactly $2n - 1$ nodes. We can encode the structure of our full binary tree by performing a preorder traversal of $T$. For each node that we record in the traversal, write a $0$ if it is an internal node and a $1$ if it is a leaf node. Since we know the tree to be full, this uniquely determines its structure. Next, note that we can encode any character of $C$ in $\\lceil \\lg n \\rceil$ bits. Since there are $n$ characters, we can encode them in order of appearance in our preorder traversal using ndlg ne bits. 16.3-7 Generalize Huffman's algorithm to ternary codewords (i.e., codewords using the symbols $0$, $1$, and $2$), and prove that it yields optimal ternary codes. Instead of grouping together the two with lowest frequency into pairs that have the smallest total frequency, we will group together the three with lowest frequency in order to have a final result that is a ternary tree. The analysis of optimality is almost identical to the binary case. We are placing the symbols of lowest frequency lower down in the final tree and so they will have longer codewords than the more frequently occurring symbols. 16.3-8 Suppose that a data file contains a sequence of $8$-bit characters such that all $256$ characters are about equally common: the maximum character frequency is less than twice the minimum character frequency. Prove that Huffman coding in this case is no more efficient than using an ordinary $8$-bit fixed-length code. For any $2$ characters, the sum of their frequencies exceeds the frequency of any other character, so initially Huffman coding makes $128$ small trees with $2$ leaves each. At the next stage, no internal node has a label which is more than twice that of any other, so we are in the same setup as before. Continuing in this fashion, Huffman coding builds a complete binary tree of height $\\lg 256 = 8$, which is no more efficient than ordinary $8$-bit length codes. 16.3-9 Show that no compression scheme can expect to compress a file of randomly chosen $8$-bit characters by even a single bit. ($\\textit{Hint:}$ Compare the number of possible files with the number of possible encoded files.) If every possible character is equally likely, then, when constructing the Huffman code, we will end up with a complete binary tree of depth $7$. This means that every character, regardless of what it is will be represented using $7$ bits. This is exactly as many bits as was originally used to represent those characters, so the total length of the file will not decrease at all.","title":"16.3 Huffman codes"},{"location":"Chap16/16.3/#163-1","text":"Explain why, in the proof of Lemma 16.2, if $x.freq = b.freq$, then we must have $a.freq = b.freq = x.freq = y.freq$. We are given that $x.freq \\le y.freq$ are the two lowest frequencies in order, and that $a.freq \\le b.freq$. Now, $$ \\begin{array}{rcl} b.freq & = & x.freq \\\\ \\Rightarrow a.freq & \\le & x.freq \\\\ \\Rightarrow a.freq & = & x.freq & \\text{(since $x.freq$ is the lowest frequency)}, \\end{array} $$ and since $y.freq \\le b.freq$, $$ \\begin{array}{rcl} b.freq & = & x.freq \\\\ \\Rightarrow y.freq & \\le & x.freq \\\\ \\Rightarrow y.freq & = & x.freq & \\text{(since $x.freq$ is the lowest frequency)}. \\end{array} $$ Thus, if we assume that $x.freq = b.freq$, then we have that each of $a.freq$, $b.freq$, and $y.freq$ equals $x.freq$, and so $$a.freq = b.freq = x.freq = y.freq.$$","title":"16.3-1"},{"location":"Chap16/16.3/#163-2","text":"Prove that a binary tree that is not full cannot correspond to an optimal prefix code. Let $T$ be a binary tree that is not full. $T$ represents a binary prefix code for a file composed of characters from alphabet $C$, where $c \\in C$, $f(c)$ is th number of occurrences of $c$ in the file. The cost of tree $T$, or the number of bits in the encoding, is $\\sum_{c \\in C} d_T(c) \\cdot f(c)$, where $d_T(c)$ is the depth of character $c$ in tree $T$. Let $N$ be a node of greatest depth that has exactly one child. If $N$ is the root of $T$, $N$ can be removed and the deepth of each node reduced by one, yielding a tree representing the same alphabet with a lower cost. This mean the original code was not optimal. Otherwise, let $M$ be the parent of $N$, let $T_1$ be the (possibly non-existent) sibling of $N$, and let $T_2$ be the subtree rooted at the child of $N$. Replace $M$ by $N$, making the children of $N$ the roots of subtrees $T_1$ and $T_2$. If $T_1$ is empty, repeat the process. We have a new prefix code of lower cost, so the original was not optimal.","title":"16.3-2"},{"location":"Chap16/16.3/#163-3","text":"What is an optimal Huffman code for the following set of frequencies, based on the first $8$ Fibonacci numbers? $$a:1 \\quad b:1 \\quad c:2 \\quad d:3 \\quad e:5 \\quad f:8 \\quad g:13 \\quad h:21$$ Can you generalize your answer to find the optimal code when the frequencies are the first $n$ Fibonacci numbers? $$ \\begin{array}{c|l} a & 1111111 \\\\ b & 1111110 \\\\ c & 111110 \\\\ d & 11110 \\\\ e & 1110 \\\\ f & 110 \\\\ g & 10 \\\\ h & 0 \\end{array} $$","title":"16.3-3"},{"location":"Chap16/16.3/#163-4","text":"Prove that we can also express the total cost of a tree for a code as the sum, over all internal nodes, of the combined frequencies of the two children of the node. Let tree be a full binary tree with $n$ leaves. Apply induction hypothesis on the number of leaves in $T$. When $n = 2$ (the case $n = 1$ is trivially true), there are two leaves $x$ and $y$ with the same parent $z$, then the cost of $T$ is $$ \\begin{aligned} B(T) & = f(x) d_T(x) + f(y) d_T(y) \\\\ & = f(x) + f(y) & \\text{since $d_T(x) = d_T(y) = 1$} \\\\ & = f(\\text{child}_1\\text{ of }z) + f(\\text{child}_2\\text{ of }z). \\end{aligned} $$ Thus, the statement of theorem is true. Now suppose $n > 2$ and also suppose that theorem is true for trees on $n - 1$ leaves. Let $c_1$ and $c_2$ are two sibling leaves in $T$ such that they have the same parent $p$. Letting $T'$ be the tree obtained by deleting $c_1$ and $c_2$, by induction we know that $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves } l'\\in T'} f(l')d_T(l') \\\\ & = \\sum_{\\text{internal nodes } i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i'). \\end{aligned} $$ Using this information, calculates the cost of $T$. $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves }l \\in T} f(l)d_T(l) \\\\ & = \\sum_{l \\ne c_1, c_2} f(l)d_T(l) + f(c_1)d_T(c_1) - 1 + f(c_2)d_T(c_2) - 1 + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i') + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i\\in T} f(\\text{child}_1\\text{ of }i) + f(\\text{child}_1\\text{ of }i). \\end{aligned} $$ Thus the statement is true.","title":"16.3-4"},{"location":"Chap16/16.3/#163-5","text":"Prove that if we order the characters in an alphabet so that their frequencies are monotonically decreasing, then there exists an optimal code whose codeword lengths are monotonically increasing. It were a contradiction to have an optimal tree whose frequencies and codewords were monotonically increasing in the strict sense; since, given $f(x_1) > \\ldots > f(x_n) \\wedge d_T(x_1) > \\cdots > d_T(x_n)$, it follows that (where $n$ is odd): $$ \\begin{aligned} & f(x_1)d_T(x_1) + \\cdots + f(x_n)d_T(x_n) > f(x_1)d_T(x_n) + \\cdots + f(x_n)d_T(x_1) > 0 \\\\ & f(x_1)(d_T(x_1) - d_T(x_n)) + \\cdots + f(x_n)(d_T(x_n) - d_T(x_1)) > 0 \\\\ & f(x_1)(d_T(x_1) - d_T(x_n)) + \\cdots + f(x_{\\lfloor \\frac{n}{2} - 1 \\rfloor})(d_T(x_{\\lfloor \\frac{n}{2} \\rfloor - 1}) - d_T(x_{\\lfloor \\frac{n}{2} \\rfloor + 1})) \\\\ & > f(x_{\\lfloor \\frac{n}{2} + 1 \\rfloor})(d_T(x_{\\lfloor \\frac{n}{2} \\rfloor - 1}) - d_T(x_{\\lfloor \\frac{n}{2} \\rfloor + 1}))+ \\cdots + f(x_n)(d_T(x_1) - d_T(x_n)). \\end{aligned} $$ That is, where $i$ and $j$ are the upper and lower median, respectively; and $c_i = d_T(x_i) - d_T(x_{n - i + 1})$: $$f(x_1)c_1 + \\cdots + f(x_i)c_i > f(x_j)c_i + \\cdots + f(x_n)c_1$$ since $$ \\begin{aligned} f(x_i) & > f(x_{n - i + 1}) & 1 \\le i \\le \\Big\\lfloor \\frac{n}{2} \\Big\\rfloor. \\end{aligned} $$","title":"16.3-5"},{"location":"Chap16/16.3/#163-6","text":"Suppose we have an optimal prefix code on a set $C = \\{0, 1, \\ldots, n - 1 \\}$ of characters and we wish to transmit this code using as few bits as possible. Show how to represent any optimal prefix code on $C$ using only $2n - 1 + n \\lceil \\lg n \\rceil$ bits. ($\\textit{Hint:}$ Use $2n - 1$ bits to specify the structure of the tree, as discovered by a walk of the tree.) First observe that any full binary tree has exactly $2n - 1$ nodes. We can encode the structure of our full binary tree by performing a preorder traversal of $T$. For each node that we record in the traversal, write a $0$ if it is an internal node and a $1$ if it is a leaf node. Since we know the tree to be full, this uniquely determines its structure. Next, note that we can encode any character of $C$ in $\\lceil \\lg n \\rceil$ bits. Since there are $n$ characters, we can encode them in order of appearance in our preorder traversal using ndlg ne bits.","title":"16.3-6"},{"location":"Chap16/16.3/#163-7","text":"Generalize Huffman's algorithm to ternary codewords (i.e., codewords using the symbols $0$, $1$, and $2$), and prove that it yields optimal ternary codes. Instead of grouping together the two with lowest frequency into pairs that have the smallest total frequency, we will group together the three with lowest frequency in order to have a final result that is a ternary tree. The analysis of optimality is almost identical to the binary case. We are placing the symbols of lowest frequency lower down in the final tree and so they will have longer codewords than the more frequently occurring symbols.","title":"16.3-7"},{"location":"Chap16/16.3/#163-8","text":"Suppose that a data file contains a sequence of $8$-bit characters such that all $256$ characters are about equally common: the maximum character frequency is less than twice the minimum character frequency. Prove that Huffman coding in this case is no more efficient than using an ordinary $8$-bit fixed-length code. For any $2$ characters, the sum of their frequencies exceeds the frequency of any other character, so initially Huffman coding makes $128$ small trees with $2$ leaves each. At the next stage, no internal node has a label which is more than twice that of any other, so we are in the same setup as before. Continuing in this fashion, Huffman coding builds a complete binary tree of height $\\lg 256 = 8$, which is no more efficient than ordinary $8$-bit length codes.","title":"16.3-8"},{"location":"Chap16/16.3/#163-9","text":"Show that no compression scheme can expect to compress a file of randomly chosen $8$-bit characters by even a single bit. ($\\textit{Hint:}$ Compare the number of possible files with the number of possible encoded files.) If every possible character is equally likely, then, when constructing the Huffman code, we will end up with a complete binary tree of depth $7$. This means that every character, regardless of what it is will be represented using $7$ bits. This is exactly as many bits as was originally used to represent those characters, so the total length of the file will not decrease at all.","title":"16.3-9"},{"location":"Chap16/16.4/","text":"16.4-1 Show that $(S, \\mathcal I_k)$ is a matroid, where $S$ is any finite set and $\\mathcal I_k$ is the set of all subsets of $S$ of size at most $k$, where $k \\le |S|$. The first condition that $S$ is a finite set is a given. To prove the second condition we assume that $k \\ge 0$, this gets us that $\\mathcal I_k$ is nonempty. Also, to prove the hereditary property, suppose $A \\in \\mathcal I_k$ this means that $|A| \\le k$. Then, if $B \\subseteq A$, this means that $|B| \\le |A| \\le k$, so $B \\in \\mathcal I_k$. Lastly, we prove the exchange property by letting $A, B \\in \\mathcal I_k$ be such that $|A| < |B|$. Then, we can pick any element $x \\in B \\backslash A$, then, $$|A \\cup {x}| = |A| + 1 \\le |B| \\le k,$$ so, we can extend $A$ to $A \\cup \\{x\\} \\in \\mathcal I_k$. 16.4-2 $\\star$ Given an $m \\times n$ matrix $T$ over some field (such as the reals), show that $(S, \\mathcal I)$ is a matroid, where $S$ is the set of columns of $T$ and $A \\in \\mathcal I$ if and only if the columns in $A$ are linearly independent. We need to show three things to prove that $(S, \\mathcal I)$ is a matroid: $S$ is finite. That's because $S$ is the set of $m$ columns of matrix $T$. $\\mathcal I$ is hereditary. That's because if $B \\in \\mathcal I$, then the colums in $B$ are linearly independent. If $A \\subseteq B$, then the columns of $A$ must also be linearly independent, and so $A \\in I$. $(S, \\mathcal I)$ satisfies the exchange property. To see why, let us suppose that $A, B \\in \\mathcal I$ and $|A| < |B|$. We will use the following properties of matrices: The rank of a matrix is the number of columns in a maximal set of linearly independent columns. The rank is also equal to the dimension of the column space of the matrix. If the column space of matrix $B$ is a subspace of the column space of matrix $A$, then $\\text{rank}(B) \\le \\text{rank}(A)$. Because the columns in $A$ are linearly independent, if we take just these columns as a matrix $A$, we have that $\\text{rank}(A) = |A|$. Similarly, if we take the columns of $B$ as a matrix $B$, we have $\\text{rank}(B) = |B|$. Since $|A| < |B|$, we have $\\text{rank}(A) < \\text{rank}(B)$. We shall show that there is some column $b \\in B$ that is not a linear combination of the columns in $A$, and so $A \\cup \\{b\\}$ is linearly independent. The proof proceeds by contradiction. Assume that each column in $B$ is a linear combination of the columns of $A$. That means that any vector that is a linear combination of the columns of $B$ is also a linear combination of the columns of $A$, and so, treating the columns of $A$ and $B$ as matrices, the column space of $B$ is a subspace of the column space of $A$. By the second property above, we have $\\text{rank}(B) \\le \\text{rank}(A)$. But we have already shown that $\\text{rank}(A) < \\text{rank}(B)$ a contradiction. Therefore, some column in $B$ is not a linear combination of the columns of $A$, and $(S, \\mathcal I)$ satisfies the exchange property. 16.4-3 $\\star$ Show that if $(S, \\mathcal I)$ is a matroid, then $(S, \\mathcal I')$ is a matroid, where $\\mathcal I' = \\{A': S - A'$ contains some maximal $A \\in \\mathcal I\\}$. That is, the maximal independent sets of $(S, \\mathcal I')$ are just the complements of the maximal independent sets of $(S, \\mathcal I)$. [This exercise defines what is commonly known as the dual of a matroid, and it asks to prove that the dual of a matroid is itself a matroid. The literature contains simpler proofs of this fact, but they depend on other (equivalent) definitions of a matroid. The proof given here is more complicated, but it relies only on the definition given in the text.] We need to show three things to prove that $(S, \\mathcal I')$ is a matroid: $S$ is finite. We are given that. $\\mathcal I'$ is hereditary. Suppose that $B' \\in \\mathcal I'$ and $A' \\subseteq B'$ . Since $B' \\in \\mathcal I'$, there is some maximal set $B \\in \\mathcal I$ such that $B \\subseteq S - B'$ . But $A' \\subseteq B'$ implies that $S - B' \\subseteq S - A'$, and so $B \\subseteq S - B' \\subseteq S - A'$. Thus, there exists a maximal set $B \\in \\mathcal I$ such that $B \\subseteq S - A'$, proving that $A' \\in \\mathcal I'$. $(S, \\mathcal I')$ satisfies the exchange property. We start with two preliminary facts about sets. The proofs of these facts are omitted. Fact 1: $|X - Y| = |X| - |X \\cap Y|$. Fact 2: Let $S$ be the universe of elements. If $X - Y \\subseteq Z$ and $Z \\subseteq S - Y$, then $|X \\cap Z| = |X| - |X \\cap Y|$. To show that $(S, \\mathcal I')$ satisfies the exchange property, let us assume that $A' \\in \\mathcal I', B' \\in \\mathcal I'$, and that $|A'| < |B'|$. We need to show that there exists some $x \\in B' - A'$ such that $A' \\cup \\{x\\} \\in \\mathcal I'$. Because $A' \\in \\mathcal I'$ and $B' \\in \\mathcal I'$, there are maximal sets $A \\subseteq S - A'$ and $B \\subseteq S - B'$ such that $A \\in \\mathcal I$ and $B \\in \\mathcal I$. Define the set $X = B' - A' - A$, so that $X$ consists of elements in $B'$ but not in $A'$ or $A$. If $X$ is nonempty, then let $x$ be any element of $X$. By how we defined set $X$, we know that $x \\in B'$ and $x \\notin A'$, so that $x \\in B' - A'$. Since $x \\notin A$, we also have that $A \\subseteq S - A' - \\{x\\} = S - (A' \\cup \\{x\\})$, and so $A' \\cup \\{x\\} \\in \\mathcal I'$. If $X$ is empty, the situation is more complicated. Because$|A'| < |B'|$, we have that $B' - A' \\ne \\emptyset$, and so $X$ being empty means that $B' - A' \\subseteq A$. Claim There is an element $y \\in B - A'$ such that $(A - B') \\cup \\{y\\} \\in \\mathcal I$. Proof First, observe that because $A - B' \\subseteq A$ and $A \\in \\mathcal I$, we have that $A - B' \\in \\mathcal I$. Similarly, $B - A' \\subseteq B$ and $B \\in \\mathcal I$, and so $B - A' \\in \\mathcal I$. If we show that $|A - B'| < |B - A'|$, the assumption that $(S, \\mathcal I)$ is a matroid proves the existence of $y$. Because $B' - A' \\subseteq A$ and $A \\subseteq S - A'$, we can apply Fact 2 to conclude that $$|B' \\cap A| = |B'| - |B' \\cap A'|.$$ We claim that $|B \\cap A'| \\le |A' - B'|$. To see why, observe that $A' - B' = A' \\cap (S - B')$ and $B \\subseteq S - B'$, and so $$B \\cap A' \\subseteq (S - B') \\cap A' = A' \\cap (S - B') = A' - B'.$$ Applying Fact 1, we see that $$|A' - B'| = |A'| - |A' \\cap B'| = |A'| - |B' \\cap A'|,$$ and hence $$|B \\cap A'| \\le |A'| - |B' \\cap A'|.$$ Now, we have $$ \\begin{aligned} |A'| & < |B'| & \\text{(by assumption)} \\\\ |A'| - |B'\\cap A'| & < |B'| - |B' \\cap A'| & \\text{(subtracting same quantity)} \\\\ |B \\cap A'| & < |B'| - |B' \\cap A'| & (|B \\cap A'| \\le |A'| - |B' \\cap A'|) \\\\ |B \\cap A'| & < |B' \\cap A| & (|B' \\cap A| = |B'| - |B' \\cap A'|) \\\\ |B| - |B \\cap A'| & > |A| - |B' \\cap A| & (|A| = |B|) \\\\ |B - A'| & > |A - B'| & \\text{(Fact 1)} \\\\ \\end{aligned} $$ Now we know there is an element $y \\in B - A'$ such that $(A - B') \\cup \\{y\\} \\in \\mathcal I$. Moreover, we claim that $y \\notin A$. To see why, we know that by the exchange property, $y \\notin A - B'$. In order for $y$ to be in $A$, it would have to be in $A \\cap B'$. But $y \\in B$, which means that $y \\notin B'$, and hence $y \\notin A \\cap B'$. Therefore $y \\notin A$. Applying the exchange property, we add elements in $B - A'$ to $A - B'$, maintaining that the set we get, say $C$, is in $\\mathcal I$. Then we keep applying the exchange property, adding a new element in $A - C$ to $C$, maintaining that $C$ is in $\\mathcal I$, until $|C| = |A|$. Once $|C| = |A|$, there must exist some element $x \\in A$ that we have not added into $C$. We know that such an element exists because the element $y$ that we first added into $C$ was not in $A$, and so some element $x$ in $A$ must be left over. Also, we must have $x \\in B'$ because all the elements in $A - B'$ are initially in $C$. Therefore, we have $x \\in B' - A'$. The set $C$ so constructed is maximal, because it has the same cardinality as $A$, which is maximal, and $C \\in \\mathcal I$. All the elements but one in $C$ are also in $A$; the one exceptions is in $B - A'$, and so $C$ contains no elements in $A'$. Because we never added $x$ to $C$, we have that $C \\subseteq S - A' - \\{x\\} = S - (A' \\cup \\{x\\})$. Therefore, $A' \\cup \\{x\\} \\in \\mathcal I'$, as we needed to show. 16.4-4 $\\star$ Let $S$ be a finite set and let $S_1, S_2, \\ldots, S_k$ be a partition of $S$ into nonempty disjoint subsets. Define the structure $(S, \\mathcal I)$ by the condition that $\\mathcal I = \\{A: \\mid A \\cap S_i \\mid \\le 1$ for $i = 1, 2, \\ldots, k\\}$. Show that $(S, \\mathcal I)$ is a matroid. That is, the set of all sets $A$ that contain at most one member of each subset in the partition determines the independent sets of a matroid. Suppose $X \\subset Y$ and $Y \\in \\mathcal I$. Then $(X \\cap S_i) \\subset (Y \\cap S_i)$ for all $i$, so $$|X \\cap S_i| \\le |Y \\cap S_i| \\le 1$$ for all $1 \\le i \\le k$. Therefore $\\mathcal M$ is closed under inclusion. Now Let $A, B \\in \\mathcal I$ with $|A| = |B| + 1$. Then there must exist some $j$ such that $|A \\cap S_j| = 1$ but $|B \\cap S_j| = 0$. Let $a = A \\cap S_j$. Then $a \\ne B$ and $|(B \\cup \\{a\\}) \\cap S_j| = 1$. Since $$|(B \\cup \\{a\\}) \\cap S_i| = |B \\cap S_i|$$ for all $i \\ne j$, we must have $B \\cup \\{a\\} \\in \\mathcal I$. Therefore $\\mathcal M$ is a matroid. 16.4-5 Show how to transform the weight function of a weighted matroid problem, where the desired optimal solution is a minimum-weight maximal independent subset, to make it a standard weighted-matroid problem. Argue carefully that your transformation is correct. Suppose that $W$ is the largest weight that any one element takes. Then, define the new weight function $w_2(x) = 1 + W - w(x)$. This then assigns a strictly positive weight, and we will show that any independent set that that has maximum weight with respect to $w_2$ will have minimum weight with respect to $w$. Recall Theorem 16.6 since we will be using it, suppose that for our matriod, all maximal independent sets have size $S$. Then, suppose $M_1$ and $M_2$ are maximal independent sets so that $M_1$ is maximal with respect to $w_2$ and $M_2$ is minimal with respect to $w$. Then, we need to show that $w(M_1) = w(M_2)$. Suppose not to achieve a contradiction, then, by minimality of $M_2$, $w(M_1) > w(M_2)$. Rewriting both sides in terms of $w_2$, we have $$w_2(M_2) - (1 + W)S > w_2(M_1) - (1 + W)S,$$ so, $$w_2(M_2) > w_2(M_1).$$ This however contradicts maximality of $M_1$ with respect to $w_2$. So, we must have that $w(M_1) = w(M_2)$. So, a maximal independent set that has the largest weight with respect to $w_2$ also has the smallest weight with respect to $w$.","title":"16.4 Matroids and greedy methods"},{"location":"Chap16/16.4/#164-1","text":"Show that $(S, \\mathcal I_k)$ is a matroid, where $S$ is any finite set and $\\mathcal I_k$ is the set of all subsets of $S$ of size at most $k$, where $k \\le |S|$. The first condition that $S$ is a finite set is a given. To prove the second condition we assume that $k \\ge 0$, this gets us that $\\mathcal I_k$ is nonempty. Also, to prove the hereditary property, suppose $A \\in \\mathcal I_k$ this means that $|A| \\le k$. Then, if $B \\subseteq A$, this means that $|B| \\le |A| \\le k$, so $B \\in \\mathcal I_k$. Lastly, we prove the exchange property by letting $A, B \\in \\mathcal I_k$ be such that $|A| < |B|$. Then, we can pick any element $x \\in B \\backslash A$, then, $$|A \\cup {x}| = |A| + 1 \\le |B| \\le k,$$ so, we can extend $A$ to $A \\cup \\{x\\} \\in \\mathcal I_k$.","title":"16.4-1"},{"location":"Chap16/16.4/#164-2-star","text":"Given an $m \\times n$ matrix $T$ over some field (such as the reals), show that $(S, \\mathcal I)$ is a matroid, where $S$ is the set of columns of $T$ and $A \\in \\mathcal I$ if and only if the columns in $A$ are linearly independent. We need to show three things to prove that $(S, \\mathcal I)$ is a matroid: $S$ is finite. That's because $S$ is the set of $m$ columns of matrix $T$. $\\mathcal I$ is hereditary. That's because if $B \\in \\mathcal I$, then the colums in $B$ are linearly independent. If $A \\subseteq B$, then the columns of $A$ must also be linearly independent, and so $A \\in I$. $(S, \\mathcal I)$ satisfies the exchange property. To see why, let us suppose that $A, B \\in \\mathcal I$ and $|A| < |B|$. We will use the following properties of matrices: The rank of a matrix is the number of columns in a maximal set of linearly independent columns. The rank is also equal to the dimension of the column space of the matrix. If the column space of matrix $B$ is a subspace of the column space of matrix $A$, then $\\text{rank}(B) \\le \\text{rank}(A)$. Because the columns in $A$ are linearly independent, if we take just these columns as a matrix $A$, we have that $\\text{rank}(A) = |A|$. Similarly, if we take the columns of $B$ as a matrix $B$, we have $\\text{rank}(B) = |B|$. Since $|A| < |B|$, we have $\\text{rank}(A) < \\text{rank}(B)$. We shall show that there is some column $b \\in B$ that is not a linear combination of the columns in $A$, and so $A \\cup \\{b\\}$ is linearly independent. The proof proceeds by contradiction. Assume that each column in $B$ is a linear combination of the columns of $A$. That means that any vector that is a linear combination of the columns of $B$ is also a linear combination of the columns of $A$, and so, treating the columns of $A$ and $B$ as matrices, the column space of $B$ is a subspace of the column space of $A$. By the second property above, we have $\\text{rank}(B) \\le \\text{rank}(A)$. But we have already shown that $\\text{rank}(A) < \\text{rank}(B)$ a contradiction. Therefore, some column in $B$ is not a linear combination of the columns of $A$, and $(S, \\mathcal I)$ satisfies the exchange property.","title":"16.4-2 $\\star$"},{"location":"Chap16/16.4/#164-3-star","text":"Show that if $(S, \\mathcal I)$ is a matroid, then $(S, \\mathcal I')$ is a matroid, where $\\mathcal I' = \\{A': S - A'$ contains some maximal $A \\in \\mathcal I\\}$. That is, the maximal independent sets of $(S, \\mathcal I')$ are just the complements of the maximal independent sets of $(S, \\mathcal I)$. [This exercise defines what is commonly known as the dual of a matroid, and it asks to prove that the dual of a matroid is itself a matroid. The literature contains simpler proofs of this fact, but they depend on other (equivalent) definitions of a matroid. The proof given here is more complicated, but it relies only on the definition given in the text.] We need to show three things to prove that $(S, \\mathcal I')$ is a matroid: $S$ is finite. We are given that. $\\mathcal I'$ is hereditary. Suppose that $B' \\in \\mathcal I'$ and $A' \\subseteq B'$ . Since $B' \\in \\mathcal I'$, there is some maximal set $B \\in \\mathcal I$ such that $B \\subseteq S - B'$ . But $A' \\subseteq B'$ implies that $S - B' \\subseteq S - A'$, and so $B \\subseteq S - B' \\subseteq S - A'$. Thus, there exists a maximal set $B \\in \\mathcal I$ such that $B \\subseteq S - A'$, proving that $A' \\in \\mathcal I'$. $(S, \\mathcal I')$ satisfies the exchange property. We start with two preliminary facts about sets. The proofs of these facts are omitted. Fact 1: $|X - Y| = |X| - |X \\cap Y|$. Fact 2: Let $S$ be the universe of elements. If $X - Y \\subseteq Z$ and $Z \\subseteq S - Y$, then $|X \\cap Z| = |X| - |X \\cap Y|$. To show that $(S, \\mathcal I')$ satisfies the exchange property, let us assume that $A' \\in \\mathcal I', B' \\in \\mathcal I'$, and that $|A'| < |B'|$. We need to show that there exists some $x \\in B' - A'$ such that $A' \\cup \\{x\\} \\in \\mathcal I'$. Because $A' \\in \\mathcal I'$ and $B' \\in \\mathcal I'$, there are maximal sets $A \\subseteq S - A'$ and $B \\subseteq S - B'$ such that $A \\in \\mathcal I$ and $B \\in \\mathcal I$. Define the set $X = B' - A' - A$, so that $X$ consists of elements in $B'$ but not in $A'$ or $A$. If $X$ is nonempty, then let $x$ be any element of $X$. By how we defined set $X$, we know that $x \\in B'$ and $x \\notin A'$, so that $x \\in B' - A'$. Since $x \\notin A$, we also have that $A \\subseteq S - A' - \\{x\\} = S - (A' \\cup \\{x\\})$, and so $A' \\cup \\{x\\} \\in \\mathcal I'$. If $X$ is empty, the situation is more complicated. Because$|A'| < |B'|$, we have that $B' - A' \\ne \\emptyset$, and so $X$ being empty means that $B' - A' \\subseteq A$. Claim There is an element $y \\in B - A'$ such that $(A - B') \\cup \\{y\\} \\in \\mathcal I$. Proof First, observe that because $A - B' \\subseteq A$ and $A \\in \\mathcal I$, we have that $A - B' \\in \\mathcal I$. Similarly, $B - A' \\subseteq B$ and $B \\in \\mathcal I$, and so $B - A' \\in \\mathcal I$. If we show that $|A - B'| < |B - A'|$, the assumption that $(S, \\mathcal I)$ is a matroid proves the existence of $y$. Because $B' - A' \\subseteq A$ and $A \\subseteq S - A'$, we can apply Fact 2 to conclude that $$|B' \\cap A| = |B'| - |B' \\cap A'|.$$ We claim that $|B \\cap A'| \\le |A' - B'|$. To see why, observe that $A' - B' = A' \\cap (S - B')$ and $B \\subseteq S - B'$, and so $$B \\cap A' \\subseteq (S - B') \\cap A' = A' \\cap (S - B') = A' - B'.$$ Applying Fact 1, we see that $$|A' - B'| = |A'| - |A' \\cap B'| = |A'| - |B' \\cap A'|,$$ and hence $$|B \\cap A'| \\le |A'| - |B' \\cap A'|.$$ Now, we have $$ \\begin{aligned} |A'| & < |B'| & \\text{(by assumption)} \\\\ |A'| - |B'\\cap A'| & < |B'| - |B' \\cap A'| & \\text{(subtracting same quantity)} \\\\ |B \\cap A'| & < |B'| - |B' \\cap A'| & (|B \\cap A'| \\le |A'| - |B' \\cap A'|) \\\\ |B \\cap A'| & < |B' \\cap A| & (|B' \\cap A| = |B'| - |B' \\cap A'|) \\\\ |B| - |B \\cap A'| & > |A| - |B' \\cap A| & (|A| = |B|) \\\\ |B - A'| & > |A - B'| & \\text{(Fact 1)} \\\\ \\end{aligned} $$ Now we know there is an element $y \\in B - A'$ such that $(A - B') \\cup \\{y\\} \\in \\mathcal I$. Moreover, we claim that $y \\notin A$. To see why, we know that by the exchange property, $y \\notin A - B'$. In order for $y$ to be in $A$, it would have to be in $A \\cap B'$. But $y \\in B$, which means that $y \\notin B'$, and hence $y \\notin A \\cap B'$. Therefore $y \\notin A$. Applying the exchange property, we add elements in $B - A'$ to $A - B'$, maintaining that the set we get, say $C$, is in $\\mathcal I$. Then we keep applying the exchange property, adding a new element in $A - C$ to $C$, maintaining that $C$ is in $\\mathcal I$, until $|C| = |A|$. Once $|C| = |A|$, there must exist some element $x \\in A$ that we have not added into $C$. We know that such an element exists because the element $y$ that we first added into $C$ was not in $A$, and so some element $x$ in $A$ must be left over. Also, we must have $x \\in B'$ because all the elements in $A - B'$ are initially in $C$. Therefore, we have $x \\in B' - A'$. The set $C$ so constructed is maximal, because it has the same cardinality as $A$, which is maximal, and $C \\in \\mathcal I$. All the elements but one in $C$ are also in $A$; the one exceptions is in $B - A'$, and so $C$ contains no elements in $A'$. Because we never added $x$ to $C$, we have that $C \\subseteq S - A' - \\{x\\} = S - (A' \\cup \\{x\\})$. Therefore, $A' \\cup \\{x\\} \\in \\mathcal I'$, as we needed to show.","title":"16.4-3 $\\star$"},{"location":"Chap16/16.4/#164-4-star","text":"Let $S$ be a finite set and let $S_1, S_2, \\ldots, S_k$ be a partition of $S$ into nonempty disjoint subsets. Define the structure $(S, \\mathcal I)$ by the condition that $\\mathcal I = \\{A: \\mid A \\cap S_i \\mid \\le 1$ for $i = 1, 2, \\ldots, k\\}$. Show that $(S, \\mathcal I)$ is a matroid. That is, the set of all sets $A$ that contain at most one member of each subset in the partition determines the independent sets of a matroid. Suppose $X \\subset Y$ and $Y \\in \\mathcal I$. Then $(X \\cap S_i) \\subset (Y \\cap S_i)$ for all $i$, so $$|X \\cap S_i| \\le |Y \\cap S_i| \\le 1$$ for all $1 \\le i \\le k$. Therefore $\\mathcal M$ is closed under inclusion. Now Let $A, B \\in \\mathcal I$ with $|A| = |B| + 1$. Then there must exist some $j$ such that $|A \\cap S_j| = 1$ but $|B \\cap S_j| = 0$. Let $a = A \\cap S_j$. Then $a \\ne B$ and $|(B \\cup \\{a\\}) \\cap S_j| = 1$. Since $$|(B \\cup \\{a\\}) \\cap S_i| = |B \\cap S_i|$$ for all $i \\ne j$, we must have $B \\cup \\{a\\} \\in \\mathcal I$. Therefore $\\mathcal M$ is a matroid.","title":"16.4-4 $\\star$"},{"location":"Chap16/16.4/#164-5","text":"Show how to transform the weight function of a weighted matroid problem, where the desired optimal solution is a minimum-weight maximal independent subset, to make it a standard weighted-matroid problem. Argue carefully that your transformation is correct. Suppose that $W$ is the largest weight that any one element takes. Then, define the new weight function $w_2(x) = 1 + W - w(x)$. This then assigns a strictly positive weight, and we will show that any independent set that that has maximum weight with respect to $w_2$ will have minimum weight with respect to $w$. Recall Theorem 16.6 since we will be using it, suppose that for our matriod, all maximal independent sets have size $S$. Then, suppose $M_1$ and $M_2$ are maximal independent sets so that $M_1$ is maximal with respect to $w_2$ and $M_2$ is minimal with respect to $w$. Then, we need to show that $w(M_1) = w(M_2)$. Suppose not to achieve a contradiction, then, by minimality of $M_2$, $w(M_1) > w(M_2)$. Rewriting both sides in terms of $w_2$, we have $$w_2(M_2) - (1 + W)S > w_2(M_1) - (1 + W)S,$$ so, $$w_2(M_2) > w_2(M_1).$$ This however contradicts maximality of $M_1$ with respect to $w_2$. So, we must have that $w(M_1) = w(M_2)$. So, a maximal independent set that has the largest weight with respect to $w_2$ also has the smallest weight with respect to $w$.","title":"16.4-5"},{"location":"Chap16/16.5/","text":"16.5-1 Solve the instance of the scheduling problem given in Figure 16.7, but with each penalty $w_i$ replaced by $80 - w_i$. $$ \\begin{array}{c|ccccccc} a_i & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline d_i & 4 & 2 & 4 & 3 & 1 & 4 & 6 \\\\ w_i & 10 & 20 & 30 & 40 & 50 & 60 & 70 \\end{array} $$ We begin by just greedily constructing the matroid, adding the most costly to leave incomplete tasks first. So, we add tasks $7, 6, 5, 4, 3$. Then, in order to schedule tasks $1$ or $2$ we need to leave incomplete more important tasks. So, our final schedule is $\\langle 5, 3, 4, 6, 7, 1, 2 \\rangle$ to have a total penalty of only $w_1 + w_2 = 30$. 16.5-2 Show how to use property 2 of Lemma 16.12 to determine in time $O(|A|)$ whether or not a given set $A$ of tasks is independent. Create an array $B$ of length $n$ containing zeros in each entry. For each element $a \\in A$, add $1$ to $B[a.deadline]$. If $B[a.deadline] > a.deadline$, return that the set is not independent. Otherwise, continue. If successfully examine every element of $A$, return that the set is independent.","title":"16.5 A task-scheduling problem as a matroid"},{"location":"Chap16/16.5/#165-1","text":"Solve the instance of the scheduling problem given in Figure 16.7, but with each penalty $w_i$ replaced by $80 - w_i$. $$ \\begin{array}{c|ccccccc} a_i & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline d_i & 4 & 2 & 4 & 3 & 1 & 4 & 6 \\\\ w_i & 10 & 20 & 30 & 40 & 50 & 60 & 70 \\end{array} $$ We begin by just greedily constructing the matroid, adding the most costly to leave incomplete tasks first. So, we add tasks $7, 6, 5, 4, 3$. Then, in order to schedule tasks $1$ or $2$ we need to leave incomplete more important tasks. So, our final schedule is $\\langle 5, 3, 4, 6, 7, 1, 2 \\rangle$ to have a total penalty of only $w_1 + w_2 = 30$.","title":"16.5-1"},{"location":"Chap16/16.5/#165-2","text":"Show how to use property 2 of Lemma 16.12 to determine in time $O(|A|)$ whether or not a given set $A$ of tasks is independent. Create an array $B$ of length $n$ containing zeros in each entry. For each element $a \\in A$, add $1$ to $B[a.deadline]$. If $B[a.deadline] > a.deadline$, return that the set is not independent. Otherwise, continue. If successfully examine every element of $A$, return that the set is independent.","title":"16.5-2"},{"location":"Chap16/Problems/16-1/","text":"Consider the problem of making change for $n$ cents using the fewest number of coins. Assume that each coin's value is an integer. a. Describe a greedy algorithm to make change consisting of quarters, dimes, nickels, and pennies. Prove that your algorithm yields an optimal solution. b. Suppose that the available coins are in the denominations that are powers of $c$, i.e., the denominations are $c^0, c^1, \\ldots, c^k$ for some integers $c > 1$ and $k \\ge 1$. Show that the greedy algorithm always yields an optimal solution. c. Give a set of coin denominations for which the greedy algorithm does not yield an optimal solution. Your set should include a penny so that there is a solution for every value of $n$. d. Give an $O(nk)$-time algorithm that makes change for any set of $k$ different coin denominations, assuming that one of the coins is a penny. Before we go into the various parts of this problem, let us first prove once and for all that the coin-changing problem has optimal substructure. Suppose we have an optimal solution for a problem of making change for $n$ cents, and we know that this optimal solution uses a coin whose value is $c$ cents; let this optimal solution use $k$ coins. We claim that this optimal solution for the problem of $n$ cents must contain within it an optimal solution for the problem of $n - c$ cents. We use the usual cut-and-paste argument. Clearly, there are $k - 1$ coins in the solution to the $n - c$ cents problem used within our optimal solution to the $n$ cents problem. If we had a solution to the $n - c$ cents problem that used fewer than $k - 1$ coins, then we could use this solution to produce a solution to the $n$ cents problem that uses fewer than $k$ coins, which contradicts the optimality of our solution. a. A greedy algorithm to make change using quarters, dimes, nickels, and pennies works as follows: Give $q = \\lfloor n / 25 \\rfloor$ quarters. That leaves $n_q = n \\mod 25$ cents to make change. Then give $d = \\lfloor n_q / 10 \\rfloor$ dimes. That leaves $n_d = n_q \\mod 10$ cents to make change. Then give $k = \\lfloor n_d / 5 \\rfloor$ nickels. That leaves $n_k = n_d \\mod 5$ cents to make change. Finally, give $p = n_k$ pennies. An equivalent formulation is the following. The problem we wish to solve is making change for $n$ cents. If $n = 0$, the optimal solution is to give no coins. If $n > 0$, determine the largest coin whose value is less than or equal to $n$. Let this coin have value $c$. Give one such coin, and then recursively solve the subproblem of making change for $n - c$ cents. To prove that this algorithm yields an optimal solution, we first need to show that the greedy-choice property holds, that is, that some optimal solution to making change for $n$ cents includes one coin of value $c$, where $c$ is the largest coin value such that $c \\le n$. Consider some optimal solution. If this optimal solution includes a coin of value $c$, then we are done. Otherwise, this optimal solution does not include a coin of value $c$. We have four cases to consider: If $1 \\le n < 5$, then $c = 1$. A solution may consist only of pennies, and so it must contain the greedy choice. If $5 \\le n < 10$, then $c = 5$. By supposition, this optimal solution does not contain a nickel, and so it consists of only pennies. Replace five pennies by one nickel to give a solution with four fewer coins. If $10 \\le n < 25$, then $c = 10$. By supposition, this optimal solution does not contain a dime, and so it contains only nickels and pennies. Some subset of the nickels and pennies in this solution adds up to $10$ cents, and so we can replace these nickels and pennies by a dime to give a solution with (between $1$ and $9$) fewer coins. If $25 \\le n$, then $c = 25$. By supposition, this optimal solution does not contain a quarter, and so it contains only dimes, nickels, and pennies. If it contains three dimes, we can replace these three dimes by a quarter and a nickel, giving a solution with one fewer coin. If it contains at most two dimes, then some subset of the dimes, nickels, and pennies adds up to $25$ cents, and so we can replace these coins by one quarter to give a solution with fewer coins. Thus, we have shown that there is always an optimal solution that includes the greedy choice, and that we can combine the greedy choice with an optimal solution to the remaining subproblem to produce an optimal solution to our original problem. Therefore, the greedy algorithm produces an optimal solution. For the algorithm that chooses one coin at a time and then recurses on subproblems, the running time is $\\Theta(k)$, where $k$ is the number of coins used in an optimal solution. Since $k \\le n$, the running time is $O(n)$. For our first description of the algorithm, we perform a constant number of calculations (since there are only $4$ coin types), and the running time is $O(1)$. b. When the coin denominations are $c^0, c^1, \\ldots, c^k$, the greedy algorithm to make change for $n$ cents works by finding the denomination $c^j$ such that $j = \\max \\{0 \\le i \\le k: c^i \\le n\\}$, giving one coin of denomination $c^j$, and recursing on the subproblem of making change for $n - c^j$ cents. (An equivalent, but more efficient, algorithm is to give $\\lfloor n / c^k \\rfloor$ coins of denomination $c^k$ and $\\lfloor (n \\mod c^{i + 1}) / c^i \\rfloor$ coins of denomination $c^i$ for $i = 0, 1, \\ldots, k - 1$.) To show that the greedy algorithm produces an optimal solution, we start by proving the following lemma: Lemma For $i = 0, 1, \\ldots, k$, let $a_i$ be the number of coins of denomination $c^i$ used in an optimal solution to the problem of making change for $n$ cents. Then for $i = 0, 1, \\ldots, k - 1$, we have $a_i < c$. Proof If $a_i \\ge c$ for some $0 \\le i < k$, then we can improve the solution by using one more coin of denomination $c^{i + 1}$ and $c$ fewer coins of denomination $c^i$. The amount for which we make change remains the same, but we use $c - 1 > 0$ fewer coins. To show that the greedy solution is optimal, we show that any non-greedy solution is not optimal. As above, let $j = \\max\\{0 \\le i \\le k: c^i \\le n\\}$, so that the greedy solution uses at least one coin of denomination $c^j$. Consider a nongreedy solution, which must use no coins of denomination $c^j$ or higher. Let the non-greedy solution use $a_i$ coins of denomination $c^i$, for $i = 0, 1, \\ldots, j - 1$; thus we have $\\sum_{i = 0}^{j - 1} a_i c^i = n$. Since $n \\ge c^j$, we have that $\\sum_{i = 0}^{j - 1} a_i c^i \\ge c^j$.. Now suppose that the non-greedy solution is optimal. By the above lemma, $a_i \\le c - 1$ for $i = 0, 1, \\ldots, j - 1$. Thus, $$ \\begin{aligned} \\sum_{i = 0}^{j - 1} a_i c^i & \\le \\sum_{i = 0}^{j - 1} (c - 1) c^i \\\\ & = (c - 1) \\sum_{i = 0}^{j - 1} c^i \\\\ & = (c - 1) \\frac{c^j - 1}{c - 1} \\\\ & = c^j - 1 \\\\ & < c^j, \\end{aligned} $$ which contradicts our earlier assertion that $\\sum_{i = 0}^{j - 1} a_i c^i \\ge c^j$. We conclude that the non-greedy solution is not optimal. Since any algorithm that does not produce the greedy solution fails to be optimal, only the greedy algorithm produces the optimal solution. The problem did not ask for the running time, but for the more efficient greedy algorithm formulation, it is easy to see that the running time is $O(k)$, since we have to perform at most $k$ each of the division, \ufb02oor, and mod operations. c. With actual U.S. coins, we can use coins of denomination $1$, $10$, and $25$. When $n = 30$ cents, the greedy solution gives one quarter and five pennies, for a total of six coins. The non-greedy solution of three dimes is better. The smallest integer numbers we can use are $1$, $3$, and $4$. When $n = 6$ cents, the greedy solution gives one $4$-cent coin and two $1$-cent coins, for a total of three coins. The non-greedy solution of two $3$-cent coins is better. d. Since we have optimal substructure, dynamic programming might apply. And indeed it does. Let us define $c[j]$ to be the minimum number of coins we need to make change for $j$ cents. Let the coin denominations be $d_1, d_2, \\ldots, d_k$. Since one of the coins is a penny, there is a way to make change for any amount $j \\ge 1$. Because of the optimal substructure, if we knew that an optimal solution for the problem of making change for $j$ cents used a coin of denomination $d_i$, we would have $c[j] = 1 + c[j - d_i]$. As base cases, we have that $c[j] = 0$ for all $j \\le 0$. To develop a recursive formulation, we have to check all denominations, giving $$ c[j] = \\begin{cases} 0 & \\text{if $j \\le 0$}, \\\\ 1 + \\min\\limits_{1 \\le i \\le k} \\{c[j - d_i]\\} & \\text{if $j > 1$}. \\end{cases} $$ We can compute the $c[j]$ values in order of increasing $j$ by using a table. The following procedure does so, producing a table $c[1..n]$. It avoids even examining $c[j]$ for $j \\le 0$ by ensuring that $j \\ge d_i$ before looking up $c[j - d_i]$. The procedure also produces a table $denom[1..n]$, where $denom[j]$ is the denomination of a coin used in an optimal solution to the problem of making change for $j$ cents. 1 2 3 4 5 6 7 8 9 COMPUTE - CHANGE ( n , d , k ) let c [ 1. . n ] and denom [ 1. . n ] be new arrays for j = 1 to n c [ j ] = \u221e for i = 1 to k if j \u2265 d [ i ] and 1 + c [ j - d [ i ]] < c [ j ] c [ j ] = 1 + c [ j - d [ i ]] denom [ j ] = d [ i ] return c and denom This procedure obviously runs in $O(nk)$ time. We use the following procedure to output the coins used in the optimal solution computed by $\\text{COMPUTE-CHANGE}$: 1 2 3 4 GIVE - CHANGE ( j , denom ) if j > 0 give one coin of denomination denom [ j ] GIVE - CHANGE ( j - denom [ j ], denom ) The initial call is $\\text{GIVE-CHANGE}(n, denom)$. Since the value of the first parameter decreases in each recursive call, this procedure runs in $O(n)$ time.","title":"16-1 Coin changing"},{"location":"Chap16/Problems/16-2/","text":"Suppose you are given a set $S = \\{a_1, a_2, \\ldots, a_n\\}$ of tasks, where task $a_i$ requires $p_i$ units of processing time to complete, once it has started. You have one computer on which to run these tasks, and the computer can run only one task at a time. Let $c_i$ be the completion time of task $a_i$ , that is, the time at which task $a_i$ completes processing. Your goal is to minimize the average completion time, that is, to minimize $(1 / n) \\sum_{i = 1}^n c_i$. For example, suppose there are two tasks, $a_1$ and $a_2$, with $p_1 = 3$ and $p_2 = 5$, and consider the schedule in which $a_2$ runs first, followed by $a_1$. Then $c_2 = 5$, $c_1 = 8$, and the average completion time is $(5 + 8) / 2 = 6.5$. If task $a_1$ runs first, however, then $c_1 = 3$, $c_2 = 8$, and the average completion time is $(3 + 8) / 2 = 5.5$. a. Give an algorithm that schedules the tasks so as to minimize the average completion time. Each task must run non-preemptively, that is, once task $a_i$ starts, it must run continuously for $p_i$ units of time. Prove that your algorithm minimizes the average completion time, and state the running time of your algorithm. b. Suppose now that the tasks are not all available at once. That is, each task cannot start until its release time $r_i$. Suppose also that we allow preemption , so that a task can be suspended and restarted at a later time. For example, a task $a_i$ with processing time $p_i = 6$ and release time $r_i = 1$ might start running at time $1$ and be preempted at time $4$. It might then resume at time $10$ but be preempted at time $11$, and it might finally resume at time $13$ and complete at time $15$. Task $a_i$ has run for a total of $6$ time units, but its running time has been divided into three pieces. In this scenario, $a_i$'s completion time is $15$. Give an algorithm that schedules the tasks so as to minimize the average completion time in this new scenario. Prove that your algorithm minimizes the average completion time, and state the running time of your algorithm. a. Order the tasks by processing time from smallest to largest and run them in that order. To see that this greedy solution is optimal, first observe that the problem exhibits optimal substructure: if we run the first task in an optimal solution, then we obtain an optimal solution by running the remaining tasks in a way which minimizes the average completion time. Let $O$ be an optimal solution. Let $a$ be the task which has the smallest processing time and let b be the first task run in $O$. Let $G$ be the solution obtained by switching the order in which we run $a$ and $b$ in $O$. This amounts reducing the completion times of a and the completion times of all tasks in $G$ between $a$ and $b$ by the difference in processing times of $a$ and $b$. Since all other completion times remain the same, the average completion time of $G$ is less than or equal to the average completion time of $O$, proving that the greedy solution gives an optimal solution. This has runtime $O(n\\lg n)$ because we must first sort the elements. b. Without loss of generality we my assume that every task is a unit time task. Apply the same strategy as in part (a), except this time if a task which we would like to add next to the schedule isn't allowed to run yet, we must skip over it. Since there could be many tasks of short processing time which have late release time, the runtime becomes $O(n^2)$ since we might have to spend $O(n)$ time deciding which task to add next at each step.","title":"16-2 Scheduling to minimize average completion time"},{"location":"Chap16/Problems/16-3/","text":"a. The incidence matrix for an undirected graph $G = (V, E)$ is a $|V| \\times |E|$ matrix $M$ such that $M_{ve} = 1$ if edge $e$ is incident on vertex $v$, and $M_{ve} = 0$ otherwise. Argue that a set of columns of $M$ is linearly independent over the field of integers modulo $2$ if and only if the corresponding set of edges is acyclic. Then, use the result of Exercise 16.4-2 to provide an alternate proof that $(E, \\mathcal I)$ of part (a) is a matroid. b. Suppose that we associate a nonnegative weight $w(e)$ with each edge in an undirected graph $G = (V, E)$. Give an efficient algorithm to find an acyclic subset of $E$ of maximum total weight. c. Let $G(V, E)$ be an arbitrary directed graph, and let $(E, \\mathcal I)$ be defined so that $A \\in \\mathcal I$ if and only if $A$ does not contain any directed cycles. Give an example of a directed graph $G$ such that the associated system $(E, \\mathcal I)$ is not a matroid. Specify which defining condition for a matroid fails to hold. d. The incidence matrix for a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $M$ such that $M_{ve} = -1$ if edge $e$ leaves vertex $v$, $M_{ve} = 1$ if edge $e$ enters vertex $v$, and $M_{ve} = 0$ otherwise. Argue that if a set of columns of $M$ is linearly independent, then the corresponding set of edges does not contain a directed cycle. e. Exercise 16.4-2 tells us that the set of linearly independent sets of columns of any matrix $M$ forms a matroid. Explain carefully why the results of parts (d) and (e) are not contradictory. How can there fail to be a perfect correspondence between the notion of a set of edges being acyclic and the notion of the associated set of columns of the incidence matrix being linearly independent? a. First, suppose that a set of columns is not linearly independent over $\\mathbb F_2$ then, there is some subset of those columns, say $S$ so that a linear combination of $S$ is $0$. However, over $\\mathbb F_2$, since the only two elements are $1$ and $0$, a linear combination is a sum over some subset. Suppose that this subset is $S'$, note that it has to be nonempty because of linear dependence. Now, consider the set of edges that these columns correspond to. Since the columns had their total incidence with each vertex $0$ in $\\mathbb F_2$, it is even. So, if we consider the subgraph on these edges, then every vertex has a even degree. Also, since our $S'$ was nonempty, some component has an edge. Restrict our attention to any such component. Since this component is connected and has all even vertex degrees, it contains an Euler Circuit, which is a cycle. Now, suppose that our graph had some subset of edges which was a cycle. Then, the degree of any vertex with respect to this set of edges is even, so, when we add the corresponding columns, we will get a zero column in $\\mathbb F_2$. Since sets of linear independent columns form a matroid, by problem 16.4-2, the acyclic sets of edges form a matroid as well. b. One simple approach is to take the highest weight edge that doesn't complete a cycle. Another way to phrase this is by running Kruskal's algorithm (see Chapter 23) on the graph with negated edge weights. c. Consider the digraph on [3] with the edges $(1, 2), (2, 1), (2, 3), (3, 2), (3, 1)$ where $(u, v)$ indicates there is an edge from $u$ to $v$. Then, consider the two acyclic subsets of edges $B = (3, 1), (3, 2), (2, 1)$ and $A = (1, 2), (2, 3)$. Then, adding any edge in $B - A$ to $A$ will create a cycle. So, the exchange property is violated. d. Suppose that the graph contained a directed cycle consisting of edges corresponding to columns $S$. Then, since each vertex that is involved in this cycle has exactly as many edges going out of it as going into it, the rows corresponding to each vertex will add up to zero, since the outgoing edges count negative and the incoming vertices count positive. This means that the sum of the columns in $S$ is zero, so, the columns were not linearly independent. e. There is not a perfect correspondence because we didn't show that not containing a directed cycle means that the columns are linearly independent, so there is not perfect correspondence between these sets of independent columns (which we know to be a matriod) and the acyclic sets of edges (which we know not to be a matroid).","title":"16-3 Acyclic subgraphs"},{"location":"Chap16/Problems/16-4/","text":"Consider the following algorithm for the problem from Section 16.5 of scheduling unit-time tasks with deadlines and penalties. Let all $n$ time slots be initially empty, where time slot $i$ is the unit-length slot of time that finishes at time $i$. We consider the tasks in order of monotonically decreasing penalty. When considering task $a_j$, if there exists a time slot at or before $a_j$'s deadline $d_j$ that is still empty, assign $a_j$ to the latest such slot, filling it. If there is no such slot, assign task $a_j$ to the latest of the as yet unfilled slots. a. Argue that this algorithm always gives an optimal answer. b. Use the fast disjoint-set forest presented in Section 21.3 to implement the algorithm efficiently. Assume that the set of input tasks has already been sorted into monotonically decreasing order by penalty. Analyze the running time of your implementation. a. Let $O$ be an optimal solution. If $a_j$ is scheduled before its deadline, we can always swap it with whichever activity is scheduled at its deadline without changing the penalty. If it is scheduled after its deadline but $a_j.deadline \\le j$ then there must exist a task from among the first $j$ with penalty less than that of $a_j$ . We can then swap aj with this task to reduce the overall penalty incurred. Since $O$ is optimal, this can't happen. Finally, if $a_j$ is scheduled after its deadline and $a_j.deadline > j$ we can swap $a_j$ with any other late task without increasing the penalty incurred. Since the problem exhibits the greedy choice property as well, this greedy strategy always yields on optimal solution. b. Assume that $\\text{MAKE-SET}(x)$ returns a pointer to the element $x$ which is now it its own set. Our disjoint sets will be collections of elements which have been scheduled at contiguous times. We'll use this structure to quickly find the next available time to schedule a task. Store attributes $x.low$ and $x.high$ at the representative $x$ of each disjoint set. This will give the earliest and latest time of a scheduled task in the block. Assume that $\\text{UNION}(x, y)$ maintains this attribute. This can be done in constant time, so it won't affect the asymptotics. Note that the attribute is well-defined under the union operation because we only union two blocks if they are contiguous. Without loss of generality we may assume that task $a_1$ has the greatest penalty, task $a_2$ has the second greatest penalty, and so on, and they are given to us in the form of an array $A$ where $A[i] = a_i$. We will maintain an array $D$ such that $D[i]$ contains a pointer to the task with deadline $i$. We may assume that the size of $D$ is at most $n$, since a task with deadline later than $n$ can't possibly be scheduled on time. There are at most $3n$ total $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, each of which occur at most $n$ times, so by Theorem 21.14 the runtime is $O(n\\alpha(n))$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 SCHEDULING - VARIATIONS ( A ) let D [ 1. . n ] be a new array for i = 1 to n a [ i ]. time = a [ i ]. deadline if D [ a [ i ]. deadline ] != NIL y = FIND - SET ( D [ a [ i ]. deadline ]) a [ i ]. time = y . low - 1 x = MAKE - SET ( a [ i ]) D [ a [ i ]. time ] = x x . low = x . high = a [ i ]. time if D [ a [ i ]. time - 1 ] != NIL UNION ( D [ a [ i ]. time - 1 ], D [ a [ i ]. time ]) if D [ a [ i ]. time + 1 ] != NIL UNION ( D [ a [ i ]. time ], D [ a [ i ]. time + 1 ])","title":"16-4 Scheduling variations"},{"location":"Chap16/Problems/16-5/","text":"Modern computers use a cache to store a small amount of data in a fast memory. Even though a program may access large amounts of data, by storing a small subset of the main memory in the cache \u2014a small but faster memory\u2014overall access time can greatly decrease. When a computer program executes, it makes a sequence $\\langle r_1, r_2, \\ldots, r_n \\rangle$ of $n$ memory requests, where each request is for a particular data element. For example, a program that accesses 4 distinct elements $\\{a, b, c, d\\}$ might make the sequence of requests $\\langle d, b, d, b, d, a, c, d, b, a, c, b \\rangle$. Let $k$ be the size of the cache. When the cache contains $k$ elements and the program requests the $(k + 1)$st element, the system must decide, for this and each subsequent request, which $k$ elements to keep in the cache. More precisely, for each request $r_i$, the cache-management algorithm checks whether element $r_i$ is already in the cache. If it is, then we have a cache hit ; otherwise, we have a cache miss. Upon a cache miss , the system retrieves $r_i$ from the main memory, and the cache-management algorithm must decide whether to keep $r_i$ in the cache. If it decides to keep $r_i$ and the cache already holds $k$ elements, then it must evict one element to make room for $r_i$ . The cache-management algorithm evicts data with the goal of minimizing the number of cache misses over the entire sequence of requests. Typically, caching is an on-line problem. That is, we have to make decisions about which data to keep in the cache without knowing the future requests. Here, however, we consider the off-line version of this problem, in which we are given in advance the entire sequence of $n$ requests and the cache size $k$, and we wish to minimize the total number of cache misses. We can solve this off-line problem by a greedy strategy called furthest-in-future , which chooses to evict the item in the cache whose next access in the request sequence comes furthest in the future. a. Write pseudocode for a cache manager that uses the furthest-in-future strategy. The input should be a sequence $\\langle r_1, r_2, \\ldots, r_n \\rangle$ of requests and a cache size $k$, and the output should be a sequence of decisions about which data element (if any) to evict upon each request. What is the running time of your algorithm? b. Show that the off-line caching problem exhibits optimal substructure. c. Prove that furthest-in-future produces the minimum possible number of cache misses. a. The procedure $\\text{CACHE-MANAGER}$ is a generic procedure, which initializes a cache by calling $\\text{INITIALIZE-CACHE}$ and then calls $\\text{ACCESS}$ with each data element in turn. The inputs are a sequence $R = \\langle r_1, r_, \\ldots, r_n \\rangle$ of memory requests and a cache size $k$. 1 2 3 4 CACHE - MANAGER ( R , k ) INITIALIZE - CACHE ( R , k ) for i = 1 to n ACCESS ( r [ i ]) The running time of $\\text{CACHE-MANAGER}$ of course depends heavily on how $\\text{ACCESS}$ is implemented. We have several choices for how to implement the greedy strategy outlined in the problem. A straightforward way of implementing the greedy strategy is that when processing request $r_i$, for each of the at most $k$ elements currently in the cache, scan through requests $r_{i + 1}, \\ldots, r_n$ to find which of the elements in the cache and $r_i$ has its next access furthest in the future, and evict this element. Because each scan takes $O(n)$ time, each request entails $O(k)$ scans, and there are $n$ requests, the running time of this straightforward approach is $O(kn^2)$. Instead, we describe an asymptotically faster algorithm, which uses a red-black tree to check whether a given element is currently in the cache, a max-priority queue to retrieve the data element with the furthest access time, and a hash table (resolving collisions by chaining) to map data elements to integer indices. We assume that the data elements can be linearly ordered, so that it makes sense to put them into a red-black tree and a max-priority queue. The following procedure $\\text{INITIALIZE-CACHE}$ creates and initializes some global data structures that are used by $\\text{ACCESS}$. 1 2 3 4 5 6 7 8 9 10 11 12 13 INITIALIZE - CACHE ( R , k ) let T be a new red - black tree let P be a new max - priority queue let H be a new hash table ind = 1 for i = 1 to n j = HASH - SEARCH ( r [ i ]) if j == NIL HASH - INSERT ( r [ i ], ind ) let S [ ind ] be a new linked list j = ind ind = ind + 1 append i to S [ j ] In the above procedure, here is the meaning of various variables: The red-black tree $T$ has at most $k$ nodes and holds the distinct data elements that are currently in the cache. We assume that the red-black tree procedures are modified to keep track of the number of nodes currently in the tree, and that the procedure $\\text{TREE-SIZE}$ returns this value. Because red-black tree $T$ has at most $k$ nodes, we can insert into, delete from, or search in it in $O(\\lg k)$ worst-case time. The max-priority queue $P$ contains elements with two attributes: $key$ is the next access time of a data element, and $value$ is the actual data element for each data element in the cache. $key$ gives the key and $value$ is satellite data in the priority queue. Like the red-black tree $T$, the max-priority queue contains only elements currently in the cache. We need to maintain $T$ and $P$ separately, however, because $T$ is keyed on the data elements and $P$ is keyed on access times. Using a max-heap to implement $P$, we can extract the maximum element or insert a new element in $O(\\lg k)$ time, and we can find the maximum element in $\\Theta(1)$ time. The hash table $H$ is a dictionary or a map, which maps each data element to a unique integer. This integer is used to index linked lists, which are described next. We assume that the $\\text{HASH-INSERT}$ procedure uses the table-expansion technique of Section 17.4.1 to keep the hash table's load factor to be at most some constant $\\alpha$. In this way, the amortized cost per insertion is $\\Theta(1)$ and, under the assumption of simple uniform hashing, then by Theorems 11.1 and 11.2, the average-case search time is also $\\Theta(1)$. For every distinct data element $r_i$, we create a linked list $S_{ind}$ (where $ind$ is obtained through the hash table) holding the indices in the input array where $r_i$ occurs. For example, if the input sequence is $\\langle d, b, d, b, d, a, c, d, b, a, c, b \\rangle$, then we create four linked lists: $S_1$ for $a$, $S_2$ for $b$, $S_3$ for $c$, and $S_4$ for $d$. $S_1$ holds the indices where $a$ is accessed, and so $S_1 = \\langle 6, 10 \\rangle$. Similarly, $S_2 = \\langle 2, 4, 9, 12 \\rangle$, $S_3 = \\langle 7, 11 \\rangle$ and $S_4 = \\langle 1, 3, 5, 8 \\rangle$. For each data element $r_i$, we first check whether there is already a linked list associated with $r_i$ and create a new linked list if not. We retrieve the linked list associated with $r_i$ and append $i$ to it, indicating that an access to $r_i$ occurs at access $i$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ACCESS ( r [ i ]) // Compute the next access time for r[i]. ind = HASH - SEARCH ( r [ i ]) time = \u221e delete the head of S [ ind ] if S [ ind ] is not empty time = head of S [ ind ] // Check to see whether r[i] is currently in the cache. if TREE - SEARCH ( T . root , r [ i ]) != NIL print \"cache hit\" else if TREE - SIZE ( T ) < k // Insert in an empty slot in the cache. let z be a new node for T z . key = r [ i ] RB - INSERT ( T , z ) let event be a new object for P event . key = time event . value = r [ i ] INSERT ( P , event ) print \"cache miss, inserted\" r [ i ] \"in empty slot\" else event = MAXIMUM ( P ) if event . key \u2264 time // r[i] has the furthest access time print \"cache miss, no data element evicted\" else // evict the element with furthest access time print \"cache miss, evict data element\" event . value event = EXTRACT - MAX ( P ) RB - DELETE ( T , TREE - SEARCH ( T . root , event . value )) event . key = time event . value = r [ i ] INSERT ( P , event ) let z be a new node for T z . key = r [ i ] RB - INSERT ( T , z ) The procedure $\\text{ACCESS}$ takes an input $r_i$ and decides which element to evict, if any, from the cache. The first if condition properly sets time to the next access time of $r_i$. The head of the linked list associated with $r_i$ contains $i$; we remove this element from the list, and the new head contains the next access time for $r_i$. Then, we check to see whether $r_i$ is already present in the cache. If $r_i$ is not present in the cache, we check to see whether we can store $r_i$ in an empty slot. If there are no empty slots, we have to evict the element with the furthest access time. We retrieve the element with the furthest access time from the max-priority queue and compare it with that of $r_i$ . If $r_i$'s next access is sooner, we evict the element with the furthest access time from the cache (deleting the element from the tree and from the priority queue) and insert $r_i$ into the tree and priority queue. Under the assumption of simple uniform hashing, the average-case running time of $\\text{ACCESS}$ is $O(\\lg k)$, since it performs a constant number of operations on the red-black tree, priority queue, and hash table. Thus, the average-case running time of $\\text{CACHE-MANAGER}$ is $O(n\\lg k)$. b. To show that the problem exhibits optimal substructure, we define the subproblem $(C, i)$ as the contents of the cache just before the $i$th request, where $C$ is a subset of the set of input data elements containing at most $k$ of them. A solution to $(C, i)$ is a sequence of decisions that specifies which element to evict (if any) for each request $i, i + 1, \\ldots, n$. An optimal solution to $(C, i)$ is a solution that minimizes the number of cache misses. Let $S$ be an optimal solution to $(C, i)$. Let $S'$ be the subsolution of $S$ for requests $i + 1, i + 2, \\ldots, n$. If a cache hit occurs on the $i$th request, then the cache remains unchanged. If a cache miss occurs, then the $i$th request results in the contents of the cache changing to $C'$ (possibly with $C' = C$ if no element was evicted). We claim that $S'$ is an optimal solution to $(C', i + 1)$. Why? If $S'$ were not an optimal solution to $(C', i + 1)$, then there exists another solution $S''$ to $(C', i + 1)$ that makes fewer cache misses than $S'$. By combining $S''$ with the decision of $S$ at the $i$th request, we obtain another solution that makes fewer cache misses than $S$, which contradicts our assumption that $S$ is an optimal solution to $(C, i)$. Suppose the $i$th request results in a cache miss. Let $P_C$ be the set of all cache states that can be reached from $C$ through a single decision of the cache manager. The set $P_C$ contains up to $k + 1$ states: $k$ of them arising from different elements of the cache being evicted and one arising from the decision of evicting no element. For example, if $C = {r_1, r_2, r_3}$ and the requested data element is $r_4$, then $$P_C = \\{\\{r_1, r_2, r_3\\}, \\{r_1, r_2, r_4\\}, \\{r_1, r_3, r_4\\}, \\{r_2, r_3, r_4\\}\\}.$$ Let $miss(C, i)$ denote the minimum number of cache misses for $(C, i)$. We can state a recurrence for $miss(C, i)$ as $$ miss(C, i) = \\begin{cases} 0 & \\text{if $i = n$ and $r_n \\in C$}, \\\\ 1 & \\text{if $i = n$ and $r_n \\notin C$}, \\\\ miss(C, i + 1) & \\text{if $i < n$ and $r_i \\in C$}, \\\\ 1 + \\min\\limits_{C' \\in P_C} \\{miss(C', i + 1)\\} & \\text{if $i < n$ and $r_i \\notin C$}. \\end{cases} $$ Thus, we conclude that the problem exhibits optimal substructure. c. To prove that the furthest-in-future strategy yields an optimal solution, we show that the problem exhibits the greedy-choice property. Combined with the optimal-substructure property from part (b), the greedy-choice property will prove that furthest-in-future produces the minimum possible number of cache misses. We use the definitions of subproblem, solution, and optimal solution from part (b). Since we will be comparing different solutions, let us define $C_{Ai}$ as the state of the cache for solution $A$ just before the ith request. The following theorem is the key. Theorem (Greedy-choice property) Let $A$ be some optimal solution to $(C, i)$. Let b be the element in $C_{Ai} \\cup \\{r_i\\}$ whose next access at the time of the $i$th request is furthest in the future, at time $m$. Then, we can construct another solution $A'$ to $(C, i)$ that has the following properties: On the $i$th request, $A'$ evicts $b$. For $i + 1 \\le j \\le m$, the caches $C_{Aj}$ and $C_{A' j}$ differ by at most one element. If they differ, then $b \\in C_{Aj}$ is always the element in $C_{Aj}$ that is not in $C_{A' j}$. Equivalently, if $C_{Aj}$ and $C_{A' j}$ differ, we can write $C_{Aj} = D_j \\cup \\{b\\}$ and $C_{A' j} = D_j \\cup \\{x\\}$, where $D_j$ is a size-($k - 1$) set and $x \\ne b$ is some data element. For requests $i, \\ldots, m - 1$, if $A$ has a cache hit, then $A'$ has a cache hit. $C_{Aj} = C_{A' j}$ for $j > m$. For requests $i, \\ldots, m$, the number of cache misses produced by $A'$ is at most the number of cache misses produced by $A$. Proof If $A$ evicts $b$ at request $i$, then the proof of the theorem is trivial. Therefore, suppose $A$ evicts data element $a$ on request $i$, where $a \\ne b$. We will prove the theorem by constructing $A'$ inductively for each request. At request $i$, $A'$ evicts $b$ instead of $a$. We proceed with induction on $j$, where $i + 1 \\le j \\le m$. The construction for property 1 establishes the base case because $C_{A, i + 1}$ and $C_{A', i + 1}$ differ by just one element and b is the element in $C_{A, i + 1}$ that is not in $C_{A', i + 1}$. For the induction step, suppose property 2 is true for some request $j$, where $i + 1 \\le j < m$. If $A$ does not evict any element or evicts an element in $D_j$, then construct $A'$ to make the same decision on request $j$ as $A$ makes. If $A$ evicts $b$ on request $j$, then construct $A'$ to evict $x$ and keep the same element as $A$ keeps, namely $r_j$. This construction conserves property 2 for $j + 1$. Note that this construction might sometimes insert duplicate elements in the cache. This situation can easily be dealt with by introducing a dummy element for $x$. Suppose $A$ has a cache hit for request $j$, where $i \\le j \\le m - 1$. Then, $r_j \\in D_j$ since $r_j \\ne b$. Thus, $r_j \\in C_{A' j}$ and $A'$ has a cache hit, too. By property 2, the cache $C_{Am}$ differs from $C_{A' m}$ by at most one element, with $b$ being the element in $C_{Am}$ that might not be in $C_{A' m}$ . If $C_{Am} = C_{A' m}$ , then construct $A'$ to make the same decision on request $m$ as $A$. Otherwise, $C_{Am} \\ne C_{A' m}$ and $b \\in C_{Am}$. Construct $A'$ to evict $x$ and keep $b$ on request $m$. Since the $m$th request is for element $b$ and $b \\in C_{Am}$, $A$ has a cache hit so that it does not evict any element. Thus, we can ensure that $C_{A, m + 1} = C_{A', m + 1}$. From the ($m + 1$)st request on, $A'$ simply makes the same decisions as $A$. By property 3, for requests $i, \\ldots, m - 1$, whenever we have a cache hit for $A$, we also have a cache hit for $A'$. Thus, we have to concern ourselves with only the $m$th request. If $A$ has a cache miss on the $m$th request, we are done. Otherwise, $A$ has a cache hit on the $m$th request, and we will prove that there exists at least one request $j$, where $i + 1 \\le j \\le m - 1$, such that the $j$th request results in a cache miss for $A$ and a cache hit for $A'$. Because $A$ evicts data element $a$ in request $i$, then, by our construction of $A'$, $C_{A', i + 1} = D_{i + 1} \\cup \\{a\\}$. The $m$th request is for data element $b$. If $A$ has a cache hit, then because none of the requests $i + 1, \\ldots, m - 1$ were for $b$, $A$ could not have evicted $b$ and brought it back. Moreover, because $A$ has a cache hit on the $m$th request, $b \\in C_{Am}$. Therefore, $A$ did not evict $b$ in any of requests $i, \\ldots, m - 1$. By our construction, $A'$ did not evict $a$. But a request for $a$ occurs at least once before the $m$th request. Consider the first such instance. At this instance, $A$ has a cache miss and $A'$ has a cache hit. The above theorem and the optimal-substructure property proved in part (b) imply that furthest-in-future produces the minimum number of cache misses.","title":"16-5 Off-line caching"},{"location":"Chap17/17.1/","text":"17.1-1 If the set of stack operations included a $\\text{MULTIPUSH}$ operation, which pushses $k$ items onto the stack, would the $O(1)$ bound on the amortized cost of stack operations continue to hold? No. The time complexity of such a series of operations depends on the number of pushes (pops vise versa) could be made. Since one $\\text{MULTIPUSH}$ needs $\\Theta(k)$ time, performing $n$ $\\text{MULTIPUSH}$ operations, each with $k$ elements, would take $\\Theta(kn)$ time, leading to amortized cost of $\\Theta(k)$. 17.1-2 Show that if a $\\text{DECREMENT}$ operatoin were included in the $k$-bit counter example, $n$ operations could cost as much as $\\Theta(nk)$ time. The logarithmic bit flipping predicate does not hold, and indeed a sequence of events could consist of the incrementation of all $1$s and decrementation of all $0$s; yielding $\\Theta(nk)$. 17.1-3 Suppose we perform a sequence of $n$ operations on a data structure in which the $i$th operation costs $i$ if $i$ is an exact power of $2$, and $1$ otherwise. Use aggregate analysis to determine the amortized cost per operation. Let $c_i =$ cost of $i$th operation. $$ c_i = \\begin{cases} i & \\text{if $i$ is an exact power of $2$}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$ $$ \\begin{array}{cc} \\text{Operation} & \\text{Cost} \\\\ \\hline 1 & 1 \\\\ 2 & 2 \\\\ 3 & 1 \\\\ 4 & 4 \\\\ 5 & 1 \\\\ 6 & 1 \\\\ 7 & 1 \\\\ 8 & 8 \\\\ 9 & 1 \\\\ 10 & 1 \\\\ \\vdots & \\vdots \\end{array} $$ $n$ operations cost: $$\\sum_{i = 1}^n c_i \\le n + \\sum_{j = 0}^{\\lg n} 2^j = n + (2n - 1) < 3n.$$ (Note: Ignoring floor in upper bound of $\\sum 2^j$.) Average cost of operation: $$\\frac{\\text{Total case}}{\\text{\\# operations}} < 3.$$ By aggregate analysis, the amoritzed cost per operation $= O(1)$.","title":"17.1 Aggregate analysis"},{"location":"Chap17/17.1/#171-1","text":"If the set of stack operations included a $\\text{MULTIPUSH}$ operation, which pushses $k$ items onto the stack, would the $O(1)$ bound on the amortized cost of stack operations continue to hold? No. The time complexity of such a series of operations depends on the number of pushes (pops vise versa) could be made. Since one $\\text{MULTIPUSH}$ needs $\\Theta(k)$ time, performing $n$ $\\text{MULTIPUSH}$ operations, each with $k$ elements, would take $\\Theta(kn)$ time, leading to amortized cost of $\\Theta(k)$.","title":"17.1-1"},{"location":"Chap17/17.1/#171-2","text":"Show that if a $\\text{DECREMENT}$ operatoin were included in the $k$-bit counter example, $n$ operations could cost as much as $\\Theta(nk)$ time. The logarithmic bit flipping predicate does not hold, and indeed a sequence of events could consist of the incrementation of all $1$s and decrementation of all $0$s; yielding $\\Theta(nk)$.","title":"17.1-2"},{"location":"Chap17/17.1/#171-3","text":"Suppose we perform a sequence of $n$ operations on a data structure in which the $i$th operation costs $i$ if $i$ is an exact power of $2$, and $1$ otherwise. Use aggregate analysis to determine the amortized cost per operation. Let $c_i =$ cost of $i$th operation. $$ c_i = \\begin{cases} i & \\text{if $i$ is an exact power of $2$}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$ $$ \\begin{array}{cc} \\text{Operation} & \\text{Cost} \\\\ \\hline 1 & 1 \\\\ 2 & 2 \\\\ 3 & 1 \\\\ 4 & 4 \\\\ 5 & 1 \\\\ 6 & 1 \\\\ 7 & 1 \\\\ 8 & 8 \\\\ 9 & 1 \\\\ 10 & 1 \\\\ \\vdots & \\vdots \\end{array} $$ $n$ operations cost: $$\\sum_{i = 1}^n c_i \\le n + \\sum_{j = 0}^{\\lg n} 2^j = n + (2n - 1) < 3n.$$ (Note: Ignoring floor in upper bound of $\\sum 2^j$.) Average cost of operation: $$\\frac{\\text{Total case}}{\\text{\\# operations}} < 3.$$ By aggregate analysis, the amoritzed cost per operation $= O(1)$.","title":"17.1-3"},{"location":"Chap17/17.2/","text":"17.2-1 Suppose we perform a sequence of stack operations on a stack whose size never exceeds $k$. After every $k$ operations, we make a copy of the entire stack for backup purposes. Show that the cost of $n$ stack operations, including copying the stack, is $O(n)$ by assigning suitable amortized costs to the various stack operations. [We assume that the only way in which COPY is invoked is automatically, after every sequence of $k$ PUSH and POP operations.] Charge $\\$2$ for each $\\text{PUSH}$ and $\\text{POP}$ operation and $\\$0$ for each $\\text{COPY}$. When we call $\\text{PUSH}$, we use $\\$1$ to pay for the operation, and we store the other $\\$1$ on the item pushed. When we call $\\text{POP}$, we again use $\\$1$ to pay for the operation, and we store the other $\\$1$ in the stack itself. Because the stack size never exceeds $k$, the actual cost of a $\\text{COPY}$ operation is at most $\\$k$, which is paid by the $\\$k$ found in the items in the stack and the stack itself. Since $k$ $\\text{PUSH}$ and $\\text{POP}$ operations occur between two consecutive $\\text{COPY}$ operations, $k$ of credit stored, either on individual items (from $\\text{PUSH}$ operations) or in the stack itself (from $\\text{POP}$ operations) by the time a $\\text{COPY}$ occurs. Since the amortized cost of each operation is $O(1)$ and the amount of credit never goes negative, the total cost of $n$ operations is $O(n)$. 17.2-2 Redo Exercise 17.1-3 using an accounting method of analysis. Let $c_i =$ csot of $i$th operation. $$ c_i = \\begin{cases} i & \\text{if $i$ is an exact power of $2$}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$ Charge each operation $3$ (amotized cost $\\hat c_i$). If $i$ is not an exact power of $2$, pay $\\$1$, and store $\\$2$ as credit. If $i$ is an exact power of $2$, pay $\\$i$, using stored credit. $$ \\begin{array}{cccc} \\text{Operation} & \\text{Cost} & \\text{Actual cost} & \\text{Credit remaining} \\\\ \\hline 1 & 3 & 1 & 2 \\\\ 2 & 3 & 2 & 3 \\\\ 3 & 3 & 1 & 5 \\\\ 4 & 3 & 4 & 4 \\\\ 5 & 3 & 1 & 6 \\\\ 6 & 3 & 1 & 8 \\\\ 7 & 3 & 1 & 10 \\\\ 8 & 3 & 8 & 5 \\\\ 9 & 3 & 1 & 7 \\\\ 10 & 3 & 1 & 9 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{array} $$ Since the amortized cost is $\\$3$ per operation, $\\sum\\limits_{i = 1}^n \\hat c_i = 3n$. We know from Exercise 17.1-3 that $\\sum\\limits_{i = 1}^n \\hat c_i < 3n$. Then we have $$\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i \\Rightarrow \\text{credit} = \\text{amortized cose} - \\text{actual cost} \\ge 0.$$ Since the amortized cost of each operation is $O(1)$, and the amount of credit never goes negative, the total cost of $n$ operations is $O(n)$. 17.2-3 Suppose we wish not only to increment a counter but also to reset it to zero (i.e., make all bits in it $0$). Counting the time to examine or modify a bit as $\\Theta(1)$, show how to implement a counter as an array of bits so that any sequence of $n$ $\\text{INCREMENT}$ and $\\text{RESET}$ operations takes time $O(n)$ on an initially zero counter. ($\\textit{Hint:}$ Keep a pointer to the high-order $1$.) We introduce a new field $A.max$ to hold the index of the high-order $1$ in $A$. Initially, $A.max$ is set to $-1$, since the low-order bit of $A$ is at index $0$, and there are initially no $1$'s in $A$. The value of $A.max$ is updated as appropriate when the counter is incremented or reset, and we use this value to limit how much of $A$ must be looked at to reset it. By controlling the cost of $\\text{RESET}$ in this way, we can limit it to an amount that can be covered by credit from earlier $\\text{INCREMENT}$s. 1 2 3 4 5 6 7 8 9 10 11 INCREMENT ( A ) i = 0 while i < A . length and A [ i ] == 1 A [ i ] = 0 i = i + 1 if i < A . length A [ i ] = 1 // Additions to book's INCREMENT start here. if i > A . max A . max = i else A . max = - 1 1 2 3 4 RESET ( A ) for i = 0 to A . max A [ i ] = 0 A . max = - 1 As for the counter in the book, we assume that it costs $\\$1$ to flip a bit. In addition, we assume it costs $\\$1$ to update $A.max$. Setting and resetting of bits by $\\text{INCREMENT}$ will work exactly as for the original counter in the book: $\\$1$ will pay to set one bit to $1$; $\\$1$ will be placed on the bit that is set to $1$ as credit; the credit on each $1$ bit will pay to reset the bit during incrementing. In addition, we'll use $\\$1$ to pay to update $max$, and if $max$ increases, we'll place an additional $\\$1$ of credit on the new high-order $1$. (If $max$ doesn't increase, we can just waste that $\\$1$\u2014it won't be needed.) Since $\\text{RESET}$ manipulates bits at positions only up to $A.max$, and since each bit up to there must have become the high-order $1$ at some time before the high-order $1$ got up to $A.max$, every bit seen by $\\text{RESET}$ has $\\$1$ of credit on it. So the zeroing of bits of $A$ by $\\text{RESET}$ can be completely paid for by the credit stored on the bits. We just need $\\$1$ to pay for resetting $max$. Thus charging $\\$4$ for each $\\text{INCREMENT}$ and $\\$1$ for each $\\text{RESET}$ is sufficient, so the sequence of $n$ $\\text{INCREMENT}$ and $\\text{RESET}$ operations takes $O(n)$ time.","title":"17.2 The accounting method"},{"location":"Chap17/17.2/#172-1","text":"Suppose we perform a sequence of stack operations on a stack whose size never exceeds $k$. After every $k$ operations, we make a copy of the entire stack for backup purposes. Show that the cost of $n$ stack operations, including copying the stack, is $O(n)$ by assigning suitable amortized costs to the various stack operations. [We assume that the only way in which COPY is invoked is automatically, after every sequence of $k$ PUSH and POP operations.] Charge $\\$2$ for each $\\text{PUSH}$ and $\\text{POP}$ operation and $\\$0$ for each $\\text{COPY}$. When we call $\\text{PUSH}$, we use $\\$1$ to pay for the operation, and we store the other $\\$1$ on the item pushed. When we call $\\text{POP}$, we again use $\\$1$ to pay for the operation, and we store the other $\\$1$ in the stack itself. Because the stack size never exceeds $k$, the actual cost of a $\\text{COPY}$ operation is at most $\\$k$, which is paid by the $\\$k$ found in the items in the stack and the stack itself. Since $k$ $\\text{PUSH}$ and $\\text{POP}$ operations occur between two consecutive $\\text{COPY}$ operations, $k$ of credit stored, either on individual items (from $\\text{PUSH}$ operations) or in the stack itself (from $\\text{POP}$ operations) by the time a $\\text{COPY}$ occurs. Since the amortized cost of each operation is $O(1)$ and the amount of credit never goes negative, the total cost of $n$ operations is $O(n)$.","title":"17.2-1"},{"location":"Chap17/17.2/#172-2","text":"Redo Exercise 17.1-3 using an accounting method of analysis. Let $c_i =$ csot of $i$th operation. $$ c_i = \\begin{cases} i & \\text{if $i$ is an exact power of $2$}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$ Charge each operation $3$ (amotized cost $\\hat c_i$). If $i$ is not an exact power of $2$, pay $\\$1$, and store $\\$2$ as credit. If $i$ is an exact power of $2$, pay $\\$i$, using stored credit. $$ \\begin{array}{cccc} \\text{Operation} & \\text{Cost} & \\text{Actual cost} & \\text{Credit remaining} \\\\ \\hline 1 & 3 & 1 & 2 \\\\ 2 & 3 & 2 & 3 \\\\ 3 & 3 & 1 & 5 \\\\ 4 & 3 & 4 & 4 \\\\ 5 & 3 & 1 & 6 \\\\ 6 & 3 & 1 & 8 \\\\ 7 & 3 & 1 & 10 \\\\ 8 & 3 & 8 & 5 \\\\ 9 & 3 & 1 & 7 \\\\ 10 & 3 & 1 & 9 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{array} $$ Since the amortized cost is $\\$3$ per operation, $\\sum\\limits_{i = 1}^n \\hat c_i = 3n$. We know from Exercise 17.1-3 that $\\sum\\limits_{i = 1}^n \\hat c_i < 3n$. Then we have $$\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i \\Rightarrow \\text{credit} = \\text{amortized cose} - \\text{actual cost} \\ge 0.$$ Since the amortized cost of each operation is $O(1)$, and the amount of credit never goes negative, the total cost of $n$ operations is $O(n)$.","title":"17.2-2"},{"location":"Chap17/17.2/#172-3","text":"Suppose we wish not only to increment a counter but also to reset it to zero (i.e., make all bits in it $0$). Counting the time to examine or modify a bit as $\\Theta(1)$, show how to implement a counter as an array of bits so that any sequence of $n$ $\\text{INCREMENT}$ and $\\text{RESET}$ operations takes time $O(n)$ on an initially zero counter. ($\\textit{Hint:}$ Keep a pointer to the high-order $1$.) We introduce a new field $A.max$ to hold the index of the high-order $1$ in $A$. Initially, $A.max$ is set to $-1$, since the low-order bit of $A$ is at index $0$, and there are initially no $1$'s in $A$. The value of $A.max$ is updated as appropriate when the counter is incremented or reset, and we use this value to limit how much of $A$ must be looked at to reset it. By controlling the cost of $\\text{RESET}$ in this way, we can limit it to an amount that can be covered by credit from earlier $\\text{INCREMENT}$s. 1 2 3 4 5 6 7 8 9 10 11 INCREMENT ( A ) i = 0 while i < A . length and A [ i ] == 1 A [ i ] = 0 i = i + 1 if i < A . length A [ i ] = 1 // Additions to book's INCREMENT start here. if i > A . max A . max = i else A . max = - 1 1 2 3 4 RESET ( A ) for i = 0 to A . max A [ i ] = 0 A . max = - 1 As for the counter in the book, we assume that it costs $\\$1$ to flip a bit. In addition, we assume it costs $\\$1$ to update $A.max$. Setting and resetting of bits by $\\text{INCREMENT}$ will work exactly as for the original counter in the book: $\\$1$ will pay to set one bit to $1$; $\\$1$ will be placed on the bit that is set to $1$ as credit; the credit on each $1$ bit will pay to reset the bit during incrementing. In addition, we'll use $\\$1$ to pay to update $max$, and if $max$ increases, we'll place an additional $\\$1$ of credit on the new high-order $1$. (If $max$ doesn't increase, we can just waste that $\\$1$\u2014it won't be needed.) Since $\\text{RESET}$ manipulates bits at positions only up to $A.max$, and since each bit up to there must have become the high-order $1$ at some time before the high-order $1$ got up to $A.max$, every bit seen by $\\text{RESET}$ has $\\$1$ of credit on it. So the zeroing of bits of $A$ by $\\text{RESET}$ can be completely paid for by the credit stored on the bits. We just need $\\$1$ to pay for resetting $max$. Thus charging $\\$4$ for each $\\text{INCREMENT}$ and $\\$1$ for each $\\text{RESET}$ is sufficient, so the sequence of $n$ $\\text{INCREMENT}$ and $\\text{RESET}$ operations takes $O(n)$ time.","title":"17.2-3"},{"location":"Chap17/17.3/","text":"17.3-1 Suppose we have a potential function $\\Phi$ such that $\\Phi(D_i) \\ge \\Phi(D_0)$ for all $i$, but $\\Phi(D_0) \\ne 0$. Show that there exists a potential fuction $\\Phi'$ such that $\\Phi'(D_0) = 0$, $\\Phi'(D_i) \\ge 0$ for all $i \\ge 1$, and the amortized costs using $\\Phi'$ are the same as the amortized costs using $\\Phi$. Define the potential function $\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0)$ for all $i \\ge 1$. Then $$\\Phi'(D_0) = \\Phi(D_0) - \\Phi(D_0) = 0,$$ and $$\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0) \\ge 0.$$ The amortized cost is $$ \\begin{aligned} \\hat c_i' & = c_i + \\Phi'(D_i) - \\Phi'(D_{i - 1}) \\\\ & = c_i + (\\Phi(D_i) - \\Phi(D_0)) - (\\Phi(D_{i - 1}) - \\Phi(D_0)) \\\\ & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & = \\hat c_i. \\end{aligned} $$ 17.3-2 Redo Exercise 17.1-3 using a potential method of analysis. Define the potential function $\\Phi(D_0) = 0$, and $\\Phi(D_i) = 2i - 2^{1 + \\lfloor \\lg i \\rfloor}$ for $i > 0$. For operation 1, $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1+ \\lfloor \\lg i \\rfloor} - 0 = 1.$$ For operation $i(i > 1)$, if $i$ is not a power of $2$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1 + \\lfloor \\lg 1 \\rfloor} - (2(i - 1) - 2^{1 + \\lfloor \\lg(i - 1) \\rfloor}) = 3.$$ If $i = 2^j$ for some $j \\in \\mathbb N$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = i + 2i - 2^{1 + j}-(2(i - 1) - 2^{1 + j - 1}) = i + 2i - 2i - 2i + 2 + i = 2.$$ Thus, the amortized cost is $3$ per operation. 17.3-3 Consider an ordinary binary min-heap data structure with $n$ elements supporting the instructions $\\text{INSERT}$ and $\\text{EXTRACT-MIN}$ in $O(\\lg n)$ worst-case time. Give a potential function $\\Phi$ such that the amortized cost of $\\text{INSERT}$ is $O(\\lg n)$ and the amortized cost of $\\text{EXTRACT-MIN}$ is $O(1)$, and show that it works. Let $D_i$ be the heap after the $i$th operation, and let $D_i$ consist of $n_i$ elements. Also, let $k$ be a constant such that each $\\text{INSERT}$ or $\\text{EXTRACT-MIN}$ operation takes at most $k \\ln n$ time, where $n = \\max(n_{i - 1}, n_i)$. (We don't want to worry about taking the log of $0$, and at least one of $n_{n - 1}$ and $n_i$ is at least $1$. We'll see later why we use the natural log.) Define $$ \\Phi(D_i) = \\begin{cases} 0 & \\text{if $n_i = 0$}, \\\\ k n_i \\ln n_i & \\text{if $n_i > 0$}. \\end{cases} $$ This function exhibits the characteristics we like in a potential function: if we start with an empty heap, then $\\Phi(D_0) = 0$, and we always maintain that $\\Phi(D_i) \\ge 0$. Before proving that we achieve the desired amortized times, we show that if $n \\ge 2$, then $n \\ln\\frac{n}{n - 1} \\le 2$. We have $$ \\begin{aligned} n \\ln\\frac{n}{n - 1} & = n\\ln \\Big(1 + \\frac{1}{n - 1} \\Big) \\\\ & = \\ln \\Big(1 + \\frac{1}{n - 1} \\Big)^n \\\\ & \\le \\ln\\big(e^{\\frac{1}{n - 1}} \\big)^n & \\text{(since $1 + x \\le e^x$ for all real $x$)} \\\\ & = \\ln e^{\\frac{n}{n - 1}} \\\\ & = \\frac{n}{n - 1} \\\\ & \\le 2, \\end{aligned} $$ assuming that $n \\ge 2$. (The equation $\\ln e^{\\frac{n}{n - 1}} = \\frac{n}{n - 1}$ is why we use the natural log.) If the $i$th operation is an $\\text{INSERT}$, then $n_i = n_{i - 1} + 1$. If the $i$th operation inserts into an empty heap, then $n_i = 1$, $n_{i - 1} = 0$ and the amortized cost is $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln 1 + k \\cdot 1\\ln 1 - 0 \\\\ & = 0. \\end{aligned} $$ If the $i$th operation inserts into a nonempty heap, then $n_i = n_{i - 1} + 1$, and the amortized cost is $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln n_i + k n_i\\ln n_i - k n_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln n_i + k n_i\\ln n_i - k(n_i - 1) \\ln(n_i - 1) \\\\ & = k\\ln n_i + k n_i\\ln n_i - kn_i\\ln(n_i - 1) + k\\ln(n_i - 1) \\\\ & < 2k\\ln n_i + kn_i \\ln\\frac{n_i}{n_i - 1} \\\\ & \\le 2k\\ln n_i + 2k \\\\ & = O(\\lg n_i). \\end{aligned} $$ If the $i$th operation is an $\\text{EXTRACT-MIN}$, then $n_i = n_{i - 1} - 1$. If the $i$th operation extracts the one and only heap item, then $n_i = 0$, $n_{i - 1} = 1$, and the amortized cost is $$ \\begin{aligned} \\hat{c_i} & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln 1 + 0 - k \\cdot 1\\ln 1 \\\\ & = 0. \\end{aligned} $$ If the $i$th operation extracts from a heap with more than 1 item, then $n_i = n_{i - 1} - 1$, and $n_{i - 1} \\ge 2$, and the amortized cost is $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln n_{i - 1} + kn_i\\ln n_i - kn_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln n_{i - 1} + k(n_{i - 1} - 1)\\ln(n_{i - 1} - 1) - kn_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln n_{i - 1} + kn_{i - 1}\\ln(n_{i - 1} - 1) - k\\ln(n_{i - 1} - 1) - kn_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln\\frac{n_{i - 1}}{n_{i - 1} - 1} + kn_{i - 1}\\ln\\frac{n_{i - 1} - 1}{n_{i - 1}} \\\\ & < k\\ln\\frac{n_{i - 1}}{n_{i - 1} - 1} + kn_{i - 1}\\ln 1 \\\\ & = k\\ln\\frac{n_{i - 1}}{n_{i - 1} - 1} \\\\ & \\le k\\ln 2 \\qquad \\text{(since $n_{i - 1} \\ge 2$)} \\\\ & = O(1). \\end{aligned} $$ A slightly different potential function\u2014which may be easier to work with\u2014is as follows. For each node $x$ in the heap, let $d_i(x)$ be the depth of $x$ in $D_i$ . Define $$ \\begin{aligned} \\Phi(D_i) & = \\sum_{x\\in D_i} k(d_i(x) + 1) \\\\ & = k \\Bigg(n_i + \\sum_{x\\in D_i} d_i(x) \\Bigg), \\end{aligned} $$ where $k$ is defined as before. Initially, the heap has no items, which means that the sum is over an empty set, and so $\\Phi(D_0) = 0$. We always have $\\Phi(D_i) \\ge 0$, as required. Observe that after an $\\text{INSERT}$, the sum changes only by an amount equal to the depth of the new last node of the heap, which is $\\lfloor \\lg n_i \\rfloor$. Thus, the change in potential due to an $\\text{INSERT}$ is $k(1 + \\lfloor \\lg n_i \\rfloor)$, and so the amortized cost is $O(\\lg n_i) + O(\\lg n_i) = O(\\lg n_i) = O(\\lg n)$. After an $\\text{EXTRACT-MIN}$, the sum changes by the negative of the depth of the old last node in the heap, and so the potential decreases by $k(1 + \\lfloor \\lg n_{i - 1} \\rfloor)$. The amortized cost is at most $k\\lg n_{i - 1} - k(1 + \\lfloor \\lg n_{i - 1} \\rfloor) = O(1)$. 17.3-4 What is the total cost of executing $n$ of the stack operations $\\text{PUSH}$, $\\text{POP}$, and $\\text{MULTIPOP}$, assuming that the stack begins with $s_0$ objects and finishes with $s_n$ objects? Let $\\Phi$ be the potential function that returns the number of elements in the stack. We know that for this potential function, we have amortized cost $2$ for $\\text{PUSH}$ operation and amortized cost $0$ for $\\text{POP}$ and $\\text{MULTIPOP}$ operations. The total amortized cost is $$\\sum_{i = 1}^n \\hat c_i = \\sum_{i = 1}^n c_i + \\Phi(D_n) - \\Phi(D_0).$$ Using the potential function and the known amortized costs, we can rewrite the equation as $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i + \\Phi(D_0) - \\Phi(D_n) \\\\ & = \\sum_{i = 1}^n \\hat c_i + s_0 - s_n \\\\ & \\le 2n + s_0 - s_n, \\end{aligned} $$ which gives us the total cost of $O(n + (s_0 - s_n))$. If $s_n \\ge s_0$, then this equals to $O(n)$, that is, if the stack grows, then the work done is limited by the number of operations. (Note that it does not matter here that the potential may go below the starting potential. The condition $\\Phi(D_n) \\ge \\Phi(D_0)$ for all $n$ is only required to have $\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i$, but we do not need for that to hold in this application.) 17.3-5 Suppose that a counter begins at a number with $b$ $1$s in its binary representation, rather than at $0$. Show that the cost of performing $n$ $\\text{INCREMENT}$ operations is $O(n)$ if $n = \\Omega(b)$. (Do not assume that $b$ is constant.) $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i - \\Phi(D_n) + \\Phi(D_0) \\\\ & = n - x + b \\\\ & \\le n - x + n \\\\ & = O(n). \\end{aligned} $$ 17.3-6 Show how to implement a queue with two ordinary stacks (Exercise 10.1-6) so that the amortized cost of each $\\text{ENQUEUE}$ and each $\\text{DEQUEUE}$ operation is $O(1)$. We'll use the accounting method for the analysis. Assign cost $3$ to the $\\text{ENQUEUE}$ operation and $0$ to the $\\text{DEQUEUE}$ operation. Recall the implementation of 10.1-6 where we enqueue by pushing on to the top of stack 1, and dequeue by popping from stack 2. If stack 2 is empty, then we must pop every element from stack 1 and push it onto stack 2 before popping the top element from stack 2. For each item that we enqueue we accumulate 2 credits. Before we can dequeue an element, it must be moved to stack 2. Note: this might happen prior to the time at which we wish to dequeue it, but it will happen only once overall. One of the 2 credits will be used for this move. Once an item is on stack 2 its pop only costs $1$ credit, which is exactly the remaining credit associated to the element. Since each operation's cost is $O(1)$, the amortized cost per operation is $O(1)$. 17.3-7 Design a data structure to support the following two operations for a dynamic multiset $S$ of integers, which allows duplicate values: $\\text{INSERT}(S, x)$ inserts $x$ into $S$. $\\text{DELETE-LARGER-HALF}(S)$ deletes the largest $\\lceil |S| / 2 \\rceil$ elements from $S$. Explain how to implement this data structure so that any sequence of $m$ $\\text{INSERT}$ and $\\text{DELETE-LARGER-HALF}$ operations runs in $O(m)$ time. Your implementation should also include a way to output the elements of $S$ in $O(|S|)$ time. We'll store all our elements in an array, and if ever it is too large, we will copy all the elements out into an array of twice the length. To delete the larger half, we first find the element $m$ with order statistic $\\lceil |S| / 2 \\rceil$ by the algor$i$thm presented in section 9.3. Then, scan through the array and copy out the elements that are smaller or equal to $m$ into an array of half the size. Since the delete half operation takes time $O(|S|)$ and reduces the number of elements by $\\lfloor |S| / 2 \\rfloor \\in \\Omega(|S|)$, we can make these operations take ammortized constant time by selecting our potential function to be linear in $|S|$. Since the insert operation only increases $|S|$ by one, we have that there is only a constant amount of work going towards satisfying the potential, so the total ammortized cost of an insertion is still constant. To output all the elements just iterate through the array and output each.","title":"17.3 The potential method"},{"location":"Chap17/17.3/#173-1","text":"Suppose we have a potential function $\\Phi$ such that $\\Phi(D_i) \\ge \\Phi(D_0)$ for all $i$, but $\\Phi(D_0) \\ne 0$. Show that there exists a potential fuction $\\Phi'$ such that $\\Phi'(D_0) = 0$, $\\Phi'(D_i) \\ge 0$ for all $i \\ge 1$, and the amortized costs using $\\Phi'$ are the same as the amortized costs using $\\Phi$. Define the potential function $\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0)$ for all $i \\ge 1$. Then $$\\Phi'(D_0) = \\Phi(D_0) - \\Phi(D_0) = 0,$$ and $$\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0) \\ge 0.$$ The amortized cost is $$ \\begin{aligned} \\hat c_i' & = c_i + \\Phi'(D_i) - \\Phi'(D_{i - 1}) \\\\ & = c_i + (\\Phi(D_i) - \\Phi(D_0)) - (\\Phi(D_{i - 1}) - \\Phi(D_0)) \\\\ & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & = \\hat c_i. \\end{aligned} $$","title":"17.3-1"},{"location":"Chap17/17.3/#173-2","text":"Redo Exercise 17.1-3 using a potential method of analysis. Define the potential function $\\Phi(D_0) = 0$, and $\\Phi(D_i) = 2i - 2^{1 + \\lfloor \\lg i \\rfloor}$ for $i > 0$. For operation 1, $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1+ \\lfloor \\lg i \\rfloor} - 0 = 1.$$ For operation $i(i > 1)$, if $i$ is not a power of $2$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1 + \\lfloor \\lg 1 \\rfloor} - (2(i - 1) - 2^{1 + \\lfloor \\lg(i - 1) \\rfloor}) = 3.$$ If $i = 2^j$ for some $j \\in \\mathbb N$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = i + 2i - 2^{1 + j}-(2(i - 1) - 2^{1 + j - 1}) = i + 2i - 2i - 2i + 2 + i = 2.$$ Thus, the amortized cost is $3$ per operation.","title":"17.3-2"},{"location":"Chap17/17.3/#173-3","text":"Consider an ordinary binary min-heap data structure with $n$ elements supporting the instructions $\\text{INSERT}$ and $\\text{EXTRACT-MIN}$ in $O(\\lg n)$ worst-case time. Give a potential function $\\Phi$ such that the amortized cost of $\\text{INSERT}$ is $O(\\lg n)$ and the amortized cost of $\\text{EXTRACT-MIN}$ is $O(1)$, and show that it works. Let $D_i$ be the heap after the $i$th operation, and let $D_i$ consist of $n_i$ elements. Also, let $k$ be a constant such that each $\\text{INSERT}$ or $\\text{EXTRACT-MIN}$ operation takes at most $k \\ln n$ time, where $n = \\max(n_{i - 1}, n_i)$. (We don't want to worry about taking the log of $0$, and at least one of $n_{n - 1}$ and $n_i$ is at least $1$. We'll see later why we use the natural log.) Define $$ \\Phi(D_i) = \\begin{cases} 0 & \\text{if $n_i = 0$}, \\\\ k n_i \\ln n_i & \\text{if $n_i > 0$}. \\end{cases} $$ This function exhibits the characteristics we like in a potential function: if we start with an empty heap, then $\\Phi(D_0) = 0$, and we always maintain that $\\Phi(D_i) \\ge 0$. Before proving that we achieve the desired amortized times, we show that if $n \\ge 2$, then $n \\ln\\frac{n}{n - 1} \\le 2$. We have $$ \\begin{aligned} n \\ln\\frac{n}{n - 1} & = n\\ln \\Big(1 + \\frac{1}{n - 1} \\Big) \\\\ & = \\ln \\Big(1 + \\frac{1}{n - 1} \\Big)^n \\\\ & \\le \\ln\\big(e^{\\frac{1}{n - 1}} \\big)^n & \\text{(since $1 + x \\le e^x$ for all real $x$)} \\\\ & = \\ln e^{\\frac{n}{n - 1}} \\\\ & = \\frac{n}{n - 1} \\\\ & \\le 2, \\end{aligned} $$ assuming that $n \\ge 2$. (The equation $\\ln e^{\\frac{n}{n - 1}} = \\frac{n}{n - 1}$ is why we use the natural log.) If the $i$th operation is an $\\text{INSERT}$, then $n_i = n_{i - 1} + 1$. If the $i$th operation inserts into an empty heap, then $n_i = 1$, $n_{i - 1} = 0$ and the amortized cost is $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln 1 + k \\cdot 1\\ln 1 - 0 \\\\ & = 0. \\end{aligned} $$ If the $i$th operation inserts into a nonempty heap, then $n_i = n_{i - 1} + 1$, and the amortized cost is $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln n_i + k n_i\\ln n_i - k n_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln n_i + k n_i\\ln n_i - k(n_i - 1) \\ln(n_i - 1) \\\\ & = k\\ln n_i + k n_i\\ln n_i - kn_i\\ln(n_i - 1) + k\\ln(n_i - 1) \\\\ & < 2k\\ln n_i + kn_i \\ln\\frac{n_i}{n_i - 1} \\\\ & \\le 2k\\ln n_i + 2k \\\\ & = O(\\lg n_i). \\end{aligned} $$ If the $i$th operation is an $\\text{EXTRACT-MIN}$, then $n_i = n_{i - 1} - 1$. If the $i$th operation extracts the one and only heap item, then $n_i = 0$, $n_{i - 1} = 1$, and the amortized cost is $$ \\begin{aligned} \\hat{c_i} & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln 1 + 0 - k \\cdot 1\\ln 1 \\\\ & = 0. \\end{aligned} $$ If the $i$th operation extracts from a heap with more than 1 item, then $n_i = n_{i - 1} - 1$, and $n_{i - 1} \\ge 2$, and the amortized cost is $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & \\le k\\ln n_{i - 1} + kn_i\\ln n_i - kn_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln n_{i - 1} + k(n_{i - 1} - 1)\\ln(n_{i - 1} - 1) - kn_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln n_{i - 1} + kn_{i - 1}\\ln(n_{i - 1} - 1) - k\\ln(n_{i - 1} - 1) - kn_{i - 1}\\ln n_{i - 1} \\\\ & = k\\ln\\frac{n_{i - 1}}{n_{i - 1} - 1} + kn_{i - 1}\\ln\\frac{n_{i - 1} - 1}{n_{i - 1}} \\\\ & < k\\ln\\frac{n_{i - 1}}{n_{i - 1} - 1} + kn_{i - 1}\\ln 1 \\\\ & = k\\ln\\frac{n_{i - 1}}{n_{i - 1} - 1} \\\\ & \\le k\\ln 2 \\qquad \\text{(since $n_{i - 1} \\ge 2$)} \\\\ & = O(1). \\end{aligned} $$ A slightly different potential function\u2014which may be easier to work with\u2014is as follows. For each node $x$ in the heap, let $d_i(x)$ be the depth of $x$ in $D_i$ . Define $$ \\begin{aligned} \\Phi(D_i) & = \\sum_{x\\in D_i} k(d_i(x) + 1) \\\\ & = k \\Bigg(n_i + \\sum_{x\\in D_i} d_i(x) \\Bigg), \\end{aligned} $$ where $k$ is defined as before. Initially, the heap has no items, which means that the sum is over an empty set, and so $\\Phi(D_0) = 0$. We always have $\\Phi(D_i) \\ge 0$, as required. Observe that after an $\\text{INSERT}$, the sum changes only by an amount equal to the depth of the new last node of the heap, which is $\\lfloor \\lg n_i \\rfloor$. Thus, the change in potential due to an $\\text{INSERT}$ is $k(1 + \\lfloor \\lg n_i \\rfloor)$, and so the amortized cost is $O(\\lg n_i) + O(\\lg n_i) = O(\\lg n_i) = O(\\lg n)$. After an $\\text{EXTRACT-MIN}$, the sum changes by the negative of the depth of the old last node in the heap, and so the potential decreases by $k(1 + \\lfloor \\lg n_{i - 1} \\rfloor)$. The amortized cost is at most $k\\lg n_{i - 1} - k(1 + \\lfloor \\lg n_{i - 1} \\rfloor) = O(1)$.","title":"17.3-3"},{"location":"Chap17/17.3/#173-4","text":"What is the total cost of executing $n$ of the stack operations $\\text{PUSH}$, $\\text{POP}$, and $\\text{MULTIPOP}$, assuming that the stack begins with $s_0$ objects and finishes with $s_n$ objects? Let $\\Phi$ be the potential function that returns the number of elements in the stack. We know that for this potential function, we have amortized cost $2$ for $\\text{PUSH}$ operation and amortized cost $0$ for $\\text{POP}$ and $\\text{MULTIPOP}$ operations. The total amortized cost is $$\\sum_{i = 1}^n \\hat c_i = \\sum_{i = 1}^n c_i + \\Phi(D_n) - \\Phi(D_0).$$ Using the potential function and the known amortized costs, we can rewrite the equation as $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i + \\Phi(D_0) - \\Phi(D_n) \\\\ & = \\sum_{i = 1}^n \\hat c_i + s_0 - s_n \\\\ & \\le 2n + s_0 - s_n, \\end{aligned} $$ which gives us the total cost of $O(n + (s_0 - s_n))$. If $s_n \\ge s_0$, then this equals to $O(n)$, that is, if the stack grows, then the work done is limited by the number of operations. (Note that it does not matter here that the potential may go below the starting potential. The condition $\\Phi(D_n) \\ge \\Phi(D_0)$ for all $n$ is only required to have $\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i$, but we do not need for that to hold in this application.)","title":"17.3-4"},{"location":"Chap17/17.3/#173-5","text":"Suppose that a counter begins at a number with $b$ $1$s in its binary representation, rather than at $0$. Show that the cost of performing $n$ $\\text{INCREMENT}$ operations is $O(n)$ if $n = \\Omega(b)$. (Do not assume that $b$ is constant.) $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i - \\Phi(D_n) + \\Phi(D_0) \\\\ & = n - x + b \\\\ & \\le n - x + n \\\\ & = O(n). \\end{aligned} $$","title":"17.3-5"},{"location":"Chap17/17.3/#173-6","text":"Show how to implement a queue with two ordinary stacks (Exercise 10.1-6) so that the amortized cost of each $\\text{ENQUEUE}$ and each $\\text{DEQUEUE}$ operation is $O(1)$. We'll use the accounting method for the analysis. Assign cost $3$ to the $\\text{ENQUEUE}$ operation and $0$ to the $\\text{DEQUEUE}$ operation. Recall the implementation of 10.1-6 where we enqueue by pushing on to the top of stack 1, and dequeue by popping from stack 2. If stack 2 is empty, then we must pop every element from stack 1 and push it onto stack 2 before popping the top element from stack 2. For each item that we enqueue we accumulate 2 credits. Before we can dequeue an element, it must be moved to stack 2. Note: this might happen prior to the time at which we wish to dequeue it, but it will happen only once overall. One of the 2 credits will be used for this move. Once an item is on stack 2 its pop only costs $1$ credit, which is exactly the remaining credit associated to the element. Since each operation's cost is $O(1)$, the amortized cost per operation is $O(1)$.","title":"17.3-6"},{"location":"Chap17/17.3/#173-7","text":"Design a data structure to support the following two operations for a dynamic multiset $S$ of integers, which allows duplicate values: $\\text{INSERT}(S, x)$ inserts $x$ into $S$. $\\text{DELETE-LARGER-HALF}(S)$ deletes the largest $\\lceil |S| / 2 \\rceil$ elements from $S$. Explain how to implement this data structure so that any sequence of $m$ $\\text{INSERT}$ and $\\text{DELETE-LARGER-HALF}$ operations runs in $O(m)$ time. Your implementation should also include a way to output the elements of $S$ in $O(|S|)$ time. We'll store all our elements in an array, and if ever it is too large, we will copy all the elements out into an array of twice the length. To delete the larger half, we first find the element $m$ with order statistic $\\lceil |S| / 2 \\rceil$ by the algor$i$thm presented in section 9.3. Then, scan through the array and copy out the elements that are smaller or equal to $m$ into an array of half the size. Since the delete half operation takes time $O(|S|)$ and reduces the number of elements by $\\lfloor |S| / 2 \\rfloor \\in \\Omega(|S|)$, we can make these operations take ammortized constant time by selecting our potential function to be linear in $|S|$. Since the insert operation only increases $|S|$ by one, we have that there is only a constant amount of work going towards satisfying the potential, so the total ammortized cost of an insertion is still constant. To output all the elements just iterate through the array and output each.","title":"17.3-7"},{"location":"Chap17/17.4/","text":"17.4-1 Suppose that we wish to implement a dynamic, open-address hash table. Why might we consider the table to be full when its load factor reaches some value $\\alpha$ that is strictly less than $1$? Describe briefly how to make insertion into a dynamic, open-address hash table run in such a way that the expected value of the amortized cost per insertion is $O(1)$. Why is the expected value of the actual cost per insertion not necessarily $O(1)$ for all insertions? By theorems 11.6-11.8, the expected cost of performing insertions and searches in an open address hash table approaches infinity as the load factor approaches one, for any load factor fixed away from $1$, the expected time is bounded by a constant though. The expected value of the actual cost my not be $O(1)$ for every insertion because the actual cost may include copying out the current values from the current table into a larger table because it became too full. This would take time that is linear in the number of elements stored. 17.4-2 Show that if $\\alpha_{i - 1} \\ge 1 / 2$ and the $i$th operation on a dynamic table is $\\text{TABLE-DELETE}$, then the amortized cost of the operation with respect to the potential function $\\text{(17.6)}$ is bounded above by a constant. Case 1: When $\\alpha_i \\ge 1 / 2$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (2 \\cdot num_i - size_i) - (2 \\cdot (num_i + 1) - size_i) \\\\ & = -1. \\end{aligned} $$ Case 2: When $\\alpha_i < 1 / 2$ and $\\alpha_i \\ge 1 / 4$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (\\frac{1}{2} size_i - num_i) - (2 \\cdot (num_i + 1) - size_i) \\\\ & = -1 + (\\frac{3}{2} size_i - 3 \\cdot num_i) \\\\ & = -1 + 3 \\quad \\text{(since } size_i - 1 = 2 \\cdot num_i - 1) \\\\ & = 2. \\end{aligned} $$ 17.4-3 Suppose that instead of contracting a table by halving its size when its load factor drops below $1 / 4$, we contract it by multiplying its size by $2 / 3$ when its load factor drops below $1 / 3$. Using the potential function $$\\Phi(T) = | 2 \\cdot T.num - T.size |,$$ show that the amortized cost of a $\\text{TABLE-DELETE}$ that uses this strategy is bounded above by a constant. If $1 / 3 < \\alpha_i \\le 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (size_i - 2 \\cdot num_i) - (size_i - 2 \\cdot (num_i + 1)) \\\\ & = 3. \\end{aligned} $$ If the $i$th operation does trigger a contraction, $$ \\begin{aligned} \\frac{1}{3} size_{i - 1} & = num_i + 1 \\\\ size_{i - 1} & = 3 (num_i + 1) \\\\ size_{i} & = \\frac{2}{3} size_{i - 1} = 2 (num_i + 1). \\end{aligned} $$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = (num_i + 1) + [2 \\cdot (num_i + 1) - 2 \\cdot num_i] - [3 \\cdot (num_i + 1) - 2 \\cdot (num_i + 1)] \\\\ & = 2. \\end{aligned} $$","title":"17.4 Dynamic tables"},{"location":"Chap17/17.4/#174-1","text":"Suppose that we wish to implement a dynamic, open-address hash table. Why might we consider the table to be full when its load factor reaches some value $\\alpha$ that is strictly less than $1$? Describe briefly how to make insertion into a dynamic, open-address hash table run in such a way that the expected value of the amortized cost per insertion is $O(1)$. Why is the expected value of the actual cost per insertion not necessarily $O(1)$ for all insertions? By theorems 11.6-11.8, the expected cost of performing insertions and searches in an open address hash table approaches infinity as the load factor approaches one, for any load factor fixed away from $1$, the expected time is bounded by a constant though. The expected value of the actual cost my not be $O(1)$ for every insertion because the actual cost may include copying out the current values from the current table into a larger table because it became too full. This would take time that is linear in the number of elements stored.","title":"17.4-1"},{"location":"Chap17/17.4/#174-2","text":"Show that if $\\alpha_{i - 1} \\ge 1 / 2$ and the $i$th operation on a dynamic table is $\\text{TABLE-DELETE}$, then the amortized cost of the operation with respect to the potential function $\\text{(17.6)}$ is bounded above by a constant. Case 1: When $\\alpha_i \\ge 1 / 2$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (2 \\cdot num_i - size_i) - (2 \\cdot (num_i + 1) - size_i) \\\\ & = -1. \\end{aligned} $$ Case 2: When $\\alpha_i < 1 / 2$ and $\\alpha_i \\ge 1 / 4$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (\\frac{1}{2} size_i - num_i) - (2 \\cdot (num_i + 1) - size_i) \\\\ & = -1 + (\\frac{3}{2} size_i - 3 \\cdot num_i) \\\\ & = -1 + 3 \\quad \\text{(since } size_i - 1 = 2 \\cdot num_i - 1) \\\\ & = 2. \\end{aligned} $$","title":"17.4-2"},{"location":"Chap17/17.4/#174-3","text":"Suppose that instead of contracting a table by halving its size when its load factor drops below $1 / 4$, we contract it by multiplying its size by $2 / 3$ when its load factor drops below $1 / 3$. Using the potential function $$\\Phi(T) = | 2 \\cdot T.num - T.size |,$$ show that the amortized cost of a $\\text{TABLE-DELETE}$ that uses this strategy is bounded above by a constant. If $1 / 3 < \\alpha_i \\le 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (size_i - 2 \\cdot num_i) - (size_i - 2 \\cdot (num_i + 1)) \\\\ & = 3. \\end{aligned} $$ If the $i$th operation does trigger a contraction, $$ \\begin{aligned} \\frac{1}{3} size_{i - 1} & = num_i + 1 \\\\ size_{i - 1} & = 3 (num_i + 1) \\\\ size_{i} & = \\frac{2}{3} size_{i - 1} = 2 (num_i + 1). \\end{aligned} $$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = (num_i + 1) + [2 \\cdot (num_i + 1) - 2 \\cdot num_i] - [3 \\cdot (num_i + 1) - 2 \\cdot (num_i + 1)] \\\\ & = 2. \\end{aligned} $$","title":"17.4-3"},{"location":"Chap17/Problems/17-1/","text":"Chapter 30 examines an important algorithm called the fast Fourier transform, or $\\text{FFT}$. The first step of the $\\text{FFT}$ algorithm performs a bit-reversal permutation on an input array $A[0..n - 1]$ whose length is $n = 2^k$ for some nonnegative integer $k$. This permutation swaps elements whose indices have binary representations that are the reverse of each other. We can express each index $a$ as a $k$-bit sequence $\\langle a_{k - 1}, a_{k - 2}, \\ldots, a_0 \\rangle$, where $a = \\sum_{i = 0}^{k - 1} a_i 2^i$. We define $$\\text{rev}_k(\\langle a_{k - 1}, a_{k - 2}, \\ldots, a_0 \\rangle) = \\langle a_0, a_1, \\ldots, a_{k - 1} \\rangle;$$ thus, $$\\text{rev}_k(a) = \\sum_{i = 0}^{k - 1} a_{k - i - 1} 2^i.$$ For example, if $n = 16$ (or, equivalently, $k = 4$), then $\\text{rev}_k(3) = 12$, since the $4$-bit representation of $3$ is $0011$, which when reversed gives $1100$, the $4$-bit representation of $12$. a. Given a function $\\text{rev}_k$ that runs in $\\Theta(k)$ time, write an algorithm to perform the bit-reversal permutation on an array of length $n = 2^k$ in $O(nk)$ time. We can use an algorithm based on an amortized analysis to improve the running time of the bit-reversal permutation. We maintain a \"bit-reversed counter\" and a procedure $\\text{BIT-REVERSED-INCREMENT}$ that, when given a bit-reversed-counter value $a$, produces $\\text{rev}_k(\\text{rev}_k(a) + 1)$. If $k = 4$, for example, and the bit-reversed counter starts at $0$, then successive calls to $\\text{BIT-REVERSED-INCREMENT}$ produce the sequence $$0000, 1000, 0100, 1100, 0010, 1010, \\ldots = 0, 8, 4, 12, 2, 10, \\ldots.$$ b. Assume that the words in your computer store $k$-bit values and that in unit time, your computer can manipulate the binary values with operations such as shifting left or right by arbitrary amounts, bitwise-$\\text{AND}$, bitwise-$\\text{OR}$, etc. Describe an implementation of the $\\text{BIT-REVERSED-INCREMENT}$ procedure that allows the bit-reversal permutation on an $n$-element array to be performed in a total of $O(n)$ time. c. Suppose that you can shift a word left or right by only one bit in unit time. Is it still possible to implement an $O(n)$-time bit-reversal permutation? a. Initialize a second array of length $n$ to all trues, then, going through the indices of the original array in any order, if the corresponding entry in the second array is true, then swap the element at the current index with the element at the bit-reversed position, and set the entry in the second array corresponding to the bit-reversed index equal to false. Since we are running $rev_k < n$ times, the total runtime is $O(nk)$. b. Doing a bit reversed increment is the same thing as adding a one to the leftmost position where all carries are going to the left instead of the right. 1 2 3 4 5 6 BIT - REVERSED - INCREMENT ( a ) let m be a 1 followed by k - 1 0 s while m bitwise - AND is not zero a = a bitwise - XOR m shift m right by 1 m bitwise - OR a By a similar analysis to the binary counter (just look at the problem in a mirror), this $\\text{BIT-REVERSED-INCREMENT}$ will take constant ammortized time. So, to perform the bit-reversed permutation, have a normal binary counter and a bit reversed counter, then, swap the values of the two counters and increment. Do not swap however if those pairs of elements have already been swapped, which can be kept track of in a auxiliary array. c. The $\\text{BIT-REVERSED-INCREMENT}$ procedure given in the previous part only uses single shifts to the right, not arbitrary shifts.","title":"17-1 Bit-reversed binary counter"},{"location":"Chap17/Problems/17-2/","text":"Binary search of a sorted array takes logarithmic search time, but the time to insert a new element is linear in the size of the array. We can improve the time for insertion by keeping several sorted arrays. Specifically, suppose that we wish to support $\\text{SEARCH}$ and $\\text{INSERT}$ on a set of $n$ elements. Let $k = \\lceil \\lg(n + 1) \\rceil$, and let the binary representation of $n$ be $\\langle n_{k - 1}, n_{k - 2}, \\ldots, n_0 \\rangle$. We have $k$ sorted arrays $A_0, A_1, \\ldots, A_{k - 1}$, where for $i = 0, 1, \\ldots, k - 1$, the length of array $A_i$ is $2^i$. Each array is either full or empty, depending on whether $n_i = 1$ or $n_i = 0$, respectively. The total number of elements held in all $k$ arrays is therefore $\\sum_{i = 0}^{k - 1} n_i 2^i = n$. Although each individual array is sorted, elements in different arrays bear no particular relationship to each other. a. Describe how to perform the $\\text{SEARCH}$ operation for this data structure. Analyze its worst-case running time. b. Describe how to perform the $\\text{INSERT}$ operation. Analyze its worst-case and amortized running times. c. Discuss how to implement $\\text{DELETE}$. a. The $\\text{SEARCH}$ operation can be performed by searching each of the individually sorted arrays. Since all the individual arrays are sorted, searching one of them using a binary search algorithm takes $O(\\lg m)$ time, where $m$ is the size of the array. In an unsuccessful search, the time is $\\Theta(\\lg m)$. In the worst case, we may assume that all the arrays $A_0, A_1, \\ldots, A_{k - 1}$ are full, $k = \\lceil \\lg(n + 1) \\rceil$, and we perform an unsuccessful search. The total time taken is $$ \\begin{aligned} T(n) & = \\Theta(\\lg 2^{k - 1} + \\lg 2^{k - 2} + \\cdots + \\lg 2^1 + \\lg 2^0) \\\\ & = \\Theta((k - 1) + (k - 2) + \\cdots + 1 + 0) \\\\ & = \\Theta(k(k - 1) / 2) \\\\ & = \\Theta(\\lceil \\lg(n + 1)\\rceil(\\lceil \\lg(n + 1) \\rceil) - 1) / 2) \\\\ & = \\Theta(\\lg^2 n). \\end{aligned} $$ Thus, the worst-case running time is $\\Theta(\\lg^2 n)$. b. We create a new sorted array of size $1$ containing the new element to be inserted. If array $A_0$ (which has size $1$) is empty, then we replace $A_0$ with the new sorted array. Otherwise, we merge sort the two arrays into another sorted array of size $2$. If $A_1$ is empty, then we replace $A_1$ with the new array; otherwise we merge sort the arrays as before and continue. Since array $A_i$ is of size $2^i$, if we merge sort two arrays of size $2^i$ each, we obtain one of size $2^{i + 1}$, which is the size of $A_{i + 1}$. Thus, this method will result in another list of arrays in the same structure that we had before. Let us analyze its worst-case running time. We will assume that merge sort takes $2m$ time to merge two sorted lists of size $m$ each. If all the arrays $A_0, A_1, \\ldots, A_{k - 2}$ are full, then the running time to fill array $A_{k - 1}$ would be $$ \\begin{aligned} T(n) & = 2(2^0 + 2^1 + \\cdots + 2^{k - 2}) \\\\ & = 2(2^{k - 1} - 1) \\\\ & = 2^k - 2 \\\\ & = \\Theta(n). \\end{aligned} $$ Therefore, the worst-case time to insert an element into this data structure is $\\Theta(n)$. However, let us now analyze the amortized running time. Using the aggregate method, we compute the total cost of a sequence of $n$ inserts, starting with the empty data structure. Let $r$ be the position of the rightmost $0$ in the binary representation $\\langle n_{k - 1}, n_{k - 2}, \\ldots, n_0 \\rangle$ of $n$, so that $n_j = 1$ for $j = 0, 1, \\ldots, r - 1$. The cost of an insertion when $n$ items have already been inserted is $$\\sum_{j = 0}^{r - 1} 2 \\cdot 2^j = O(2^r).$$ Furthermore, $r = 0$ half the time, $r = 1$ a quarter of the time, and so on. There are at most $\\lceil n / 2^r \\rceil$ insertions for each value of $r$. The total cost of the $n$ operations is therefore bounded by $$O\\Bigg(\\sum_{r = 0}^{\\lceil \\lg(n + 1) \\rceil} \\bigg(\\bigg\\lceil \\frac{n}{2^r} \\bigg\\rceil\\bigg) 2^r \\Bigg) = O(n\\lg n).$$ The amortized cost per $\\text{INSERT}$ operation, therefore is $O(\\lg n)$. We can also use the accounting method to analyze the running time. We can charge $k$ to insert an element. $\\$1$ pays for the insertion, and we put $\\$(k - 1)$ on the inserted item to pay for it being involved in merges later on. Each time it is merged, it moves to a higher-indexed array, i.e., from $A_i$ to $A_{i + 1}$. It can move to a higher-indexed array at most $k - 1$ times, and so the $\\$(k - 1)$ on the item suffices to pay for all the times it will ever be involved in merges. Since $k = \\Theta(\\lg n)$, we have an amortized cost of $\\Theta(\\lg n)$ per insertion. c. $\\text{DELETE}(x)$ will be implemented as follows: Find the smallest $j$ for which the array $A_j$ with $2^j$ elements is full. Let $y$ be the last element of $A_j$. Let $x$ be in the array $A_i$. If necessary, find which array this is by using the search procedure. Remove $x$ from $A_i$ and put $y$ into $A_i$ . Then move $y$ to its correct place in $A_i$ . Divide $A_j$ (which now has $2^j - 1$ elements left): The first element goes into array $A_0$ , the next 2 elements go into array $A_1$ , the next 4 elements go into array $A_2$, and so forth. Mark array $A_j$ as empty. The new arrays are created already sorted. The cost of $\\text{DELETE}$ is $\\Theta(n)$ in the worst case, where $i = k - 1$ and $j = k - 2: \\Theta(\\lg n)$ to find $A_j$, $\\Theta(\\lg^2 n)$ to find $A_i$, $\\Theta(2^i) = \\Theta(n)$ to put $y$ in its correct place in array $A_i$, and $\\Theta(2^j) = \\Theta(n)$ to divide array $A_j$ . The following sequence of $n$ operations, where $n / 3$ is a power of $2$, yields an amortized cost that is no better: perform $n / 3$ $\\text{INSERT}$ operations, followed by $n / 3$ pairs of $\\text{DELETE}$ and $\\text{INSERT}$. It costs $O(n\\lg n)$ to do the first $n / 3$ $\\text{INSERT}$ operations. This creates a single full array. Each subsequent $\\text{DELETE}$/$\\text{INSERT}$ pair costs $\\Theta(n)$ for the $\\text{DELETE}$ to divide the full array and another $\\Theta(n)$ for the $\\text{INSERT}$ to recombine it. The total is then $\\Theta(n^2)$, or $\\Theta(n)$ per operation.","title":"17-2 Making binary search dynamic"},{"location":"Chap17/Problems/17-3/","text":"Consider an ordinary binary search tree augmented by adding to each node $x$ the attribute $x.size$ giving the number of keys stored in the subtree rooted at $x$. Let $\\alpha$ be a constant in the range $1 / 2 \\le \\alpha < 1$. We say that a given node $x$ is $\\alpha$-balanced if $x.left.size \\le \\alpha \\cdot x.size$ and $x.right.size \\le \\alpha \\cdot x.size$. The tree as a whole is $\\alpha$-balanced if every node in the tree is $\\alpha$-balanced. The following amortized approach to maintaining weight-balanced trees was suggested by G. Varghese. a. A $1 / 2$-balanced tree is, in a sense, as balanced as it can be. Given a node $x$ in an arbitrary binary search tree, show how to rebuild the subtree rooted at $x$ so that it becomes $1 / 2$-balanced. Your algorithm should run in time $\\Theta(x.size)$, and it can use $O(x.size)$ auxiliary storage. b. Show that performing a search in an $n$-node $\\alpha$-balanced binary search tree takes $O(\\lg n)$ worst-case time. For the remainder of this problem, assume that the constant $\\alpha$ is strictly greater than $1 / 2$. Suppose that we implement $\\text{INSERT}$ and $\\text{DELETE}$ as usual for an $n$-node binary search tree, except that after every such operation, if any node in the tree is no longer $\\alpha$-balanced, then we \"rebuild\" the subtree rooted at the highest such node in the tree so that it becomes $1 / 2$-balanced. We shall analyze this rebuilding scheme using the potential method. For a node $x$ in a binary search tree $T$, we define $$\\Delta(x) = |x.left.size - x.right.size|,$$ and we define the potential of $T$ as $$\\Phi(T) = c \\sum_{x \\in T: \\Delta(x) \\ge 2} \\Delta(x),$$ where $c$ is a sufficiently large constant that depends on $\\alpha$. c. Argue that any binary search tree has nonnegative potential and that a $1 / 2$-balanced tree has potential $0$. d. Suppose that $m$ units of potential can pay for rebuilding an $m$-node subtree. How large must $c$ be in terms of $\\alpha$ in order for it to take $O(1)$ amortized time to rebuild a subtree that is not $\\alpha$-balanced? e. Show that inserting a node into or deleting a node from an $n$-node $\\alpha$-balanced tree costs $O(\\lg n)$ amortized time. a. Since we have $O(x.size)$ auxiliary space, we will take the tree rooted at $x$ and write down an inorder traversal of the tree into the extra space. This will only take linear time to do because it will visit each node thrice, once when passing to its left child, once when the nodes value is output and passing to the right child, and once when passing to the parent. Then, once the inorder traversal is written down, we can convert it back to a binary tree by selecting the median of the list to be the root, and recursing on the two halves of the list that remain on both sides. Since we can index into the middle element of a list in constant time, we will have the recurrence $$T(n) = 2T(n / 2) + 1,$$ which has solution that is linear. Since both trees come from the same underlying inorder traversal, the result is a $\\text{BST}$ since the original was. Also, since the root at each point was selected so that half the elements are larger and half the elements are smaller, it is a $1 / 2$-balanced tree. b. We will show by induction that any tree with $\\le \\alpha^{-d} + d$ elements has a depth of at most $d$. This is clearly true for $d = 0$ because any tree with a single node has depth $0$, and since $\\alpha^0 = 1$, we have that our restriction on the number of elements requires there to only be one. Now, suppose that in some inductive step we had a contradiction, that is, some tree of depth $d$ that is $\\alpha$ balanced but has more than $\\alpha - d$ elements. We know that both of the subtrees are alpha balanced, and by being alpha balanced at the root, we have $$root.left.size \\le \\alpha \\cdot root.size,$$ which implies $$root.right.size > root.size - \\alpha \\cdot root.size - 1.$$ So, $$ \\begin{aligned} root.right.size & > (1 - \\alpha)root.size - 1 \\\\ & > (1 - \\alpha)\\alpha - d + d - 1 \\\\ & = (\\alpha - 1 - 1)\\alpha - d + 1 + d - 1 \\\\ & \\ge \\alpha - d + 1 + d - 1, \\end{aligned} $$ which is a contradiction to the fact that it held for all smaller values of $d$ because any child of a tree of depth d has depth $d - 1$. c. The potential function is a sum of $\\Delta(x)$ each of which is the absolute value of a quantity, so, since it is a sum of nonnegative values, it is nonnegative regardless of the input $\\text{BST}$. If we suppose that our tree is $1 / 2$-balanced, then, for every node $x$, we'll have that $\\Delta(x) \\le 1$, so, the sum we compute to find the potential will be over no nonzero terms. d. $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ O(1) & = m + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ \\Phi(D_{i - 1}) & = m + \\Phi(D_i) \\\\ \\Phi(D_{i - 1}) & \\ge m. \\end{aligned} $$ $$ \\begin{aligned} \\Delta(x) & = x.left.size - x.right.size \\\\ & \\ge \\alpha \\cdot m - ((1 - \\alpha) m - 1) \\\\ & = (2\\alpha - 1)m + 1. \\end{aligned} $$ $$ \\begin{aligned} m & \\le c((2\\alpha - 1)m + 1) \\\\ c & \\ge \\frac{m}{(2\\alpha - 1)m + 1} \\\\ & \\ge \\frac{1}{2\\alpha}. \\end{aligned} $$ e. Suppose that our tree is $\\alpha$ balanced. Then, we know that performing a search takes time $O(\\lg(n))$. So, we perform that search and insert the element that we need to insert or delete the element we found. Then, we may have made the tree become unbalanced. However, we know that since we only changed one position, we have only changed the $\\Delta$ value for all of the parents of the node that we either inserted or deleted. Therefore, we can rebuild the balanced properties starting at the lowest such unbalanced node and working up. Since each one only takes ammortized constant time, and there are $O(\\lg(n))$ many trees made unbalanced, tot total time to rebalanced every subtree is $O(\\lg(n))$ ammortized time.","title":"17-3 Amortized weight-balanced trees"},{"location":"Chap17/Problems/17-4/","text":"There are four basic operations on red-black trees that perform structural modifications : node insertions, node deletions, rotations, and color changes. We have seen that $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ use only $O(1)$ rotations, node insertions, and node deletions to maintain the red-black properties, but they may make many more color changes. a. Describe a legal red-black tree with $n$ nodes such that calling $\\text{RB-INSERT}$ to add the $(n + 1)$st node causes $\\Omega(\\lg n)$ color changes. Then describe a legal red-black tree with $n$ nodes for which calling $\\text{RB-DELETE}$ on a particular node causes $\\Omega(\\lg n)$ color changes. Although the worst-case number of color changes per operation can be logarithmic, we shall prove that any sequence of $m$ $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ operations on an initially empty red-black tree causes $O(m)$ structural modifications in the worst case. Note that we count each color change as a structural modification. b. Some of the cases handled by the main loop of the code of both $\\text{RB-INSERT-FIXUP}$ and $\\text{RB-DELETE-FIXUP}$ are terminating : once encountered, they cause the loop to terminate after a constant number of additional operations. For each of the cases of $\\text{RB-INSERT-FIXUP}$ and $\\text{RB-DELETE-FIXUP}$, specify which are terminating and which are not. ($\\textit{Hint:}$ Look at Figures 13.5, 13.6, and 13.7.) We shall first analyze the structural modifications when only insertions are performed. Let $T$ be a red-black tree, and define $\\Phi(T)$ to be the number of red nodes in $T$. Assume that $1$ unit of potential can pay for the structural modifications performed by any of the three cases of $\\text{RB-INSERT-FIXUP}$. c. Let $T'$ be the result of applying Case 1 of $\\text{RB-INSERT-FIXUP}$ to $T$. Argue that $\\Phi(T') = \\Phi(T) - 1$. d. When we insert a node into a red-black tree using $\\text{RB-INSERT}$, we can break the operation into three parts. List the structural modifications and potential changes resulting from lines 1\u201316 of $\\text{RB-INSERT}$, from nonterminating cases of $\\text{RB-INSERT-FIXUP}$, and from terminating cases of $\\text{RB-INSERT-FIXUP}$. e. Using part (d), argue that the amortized number of structural modifications performed by any call of $\\text{RB-INSERT}$ is $O(1)$. We now wish to prove that there are $O(m)$ structural modifications when there are both insertions and deletions. Let us define, for each node $x$, $$ w(x) = \\begin{cases} 0 & \\text{ if } x \\text{ is red}, \\\\ 1 & \\text{ if } x \\text{ is black and has no red children}, \\\\ 0 & \\text{ if } x \\text{ is black and has one red children}, \\\\ 2 & \\text{ if } x \\text{ is black and has two red children}. \\end{cases} $$ Now we redefine the potential of a red-black tree $T$ as $$\\Phi(T) = \\sum_{x \\in T} w(x),$$ and let $T'$ be the tree that results from applying any nonterminating case of $\\text{RB-INSERT-FIXUP}$ or $\\text{RB-DELETE-FIXUP}$ to $T$. f. Show that $\\Phi(T') \\le \\Phi(T) - 1$ for all nonterminating cases of $\\text{RB-INSERT-FIXUP}$. Argue that the amortized number of structural modifications performed by any call of $\\text{RB-INSERT-FIXUP}$ is $O(1)$. g. Show that $\\Phi(T') \\le \\Phi(T) - 1$ for all nonterminating cases of $\\text{RB-DELETE-FIXUP}$. Argue that the amortized number of structural modifications performed by any call of $\\text{RB-DELETE-FIXUP}$ is $O(1)$. h. Complete the proof that in the worst case, any sequence of $m$ $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ operations performs $O(m)$ structural modifications. a. For $\\text{RB-INSERT}$, consider a complete red-black tree in which the colors alternate between levels. That is, the root is black, the children of the root are red, the grandchildren of the root are black, the great-grandchildren of the root are red, and so on. When a node is inserted as a red child of one of the red leaves, then case 1 of $\\text{RB-INSERT-FIXUP}$ occurs $(\\lg(n + 1)) / 2$ times, so that there are \u007f$\\Omega(\\lg n)$ color changes to fix the colors of nodes on the path from the inserted node to the root. For $\\text{RB-DELETE}$, consider a complete red-black tree in which all nodes are black. If a leaf is deleted, then the double blackness will be pushed all the way up to the root, with a color change at each level (case 2 of $\\text{RB-DELETE-FIXUP}$), for a total of \u007f$\\Omega(\\lg n)$ color changes. b. All cases except for case 1 of $\\text{RB-INSERT-FIXUP}$ and case 2 of $\\text{RB-DELETE-FIXUP}$ are terminating. c. Case 1 of $\\text{RB-INSERT-FIXUP}$ reduces the number of red nodes by $1$. As Figure 13.5 shows, node $z$'s parent and uncle change from red to black, and $z$'s grandparent changes from black to red. Hence, $\\Phi(T') = \\Phi(T) - 1$. d. Lines 1\u201316 of $\\text{RB-INSERT}$ cause one node insertion and a unit increase in potential. The nonterminating case of $\\text{RB-INSERT-FIXUP}$ (Case 1) makes three color changes and decreases the potential by $1$. The terminating cases of $\\text{RB-INSERT-FIXUP}$ (cases 2 and 3) cause one rotation each and do not affect the potential. (Although case 3 makes color changes, the potential does not change. As Figure 13.6 shows, node $z$'s parent changes from red to black, and $z$'s grandparent changes from black to red.) e. The number of structural modifications and amount of potential change resulting from lines 1\u201316 of $\\text{RB-INSERT}$ and from the terminating cases of $\\text{RB-INSERT-FIXUP}$ are $O(1)$, and so the amortized number of structural modifications of these parts is $O(1)$. The nonterminating case of $\\text{RB-INSERT-FIXUP}$ may repeat $O(\\lg n)$ times, but its amortized number of structural modifications is $0$, since by our assumption the unit decrease in the potential pays for the structural modifications needed. Therefore, the amortized number of structural modifications performed by $\\text{RB-INSERT}$ is $O(1)$. f. From Figure 13.5, we see that case 1 of $\\text{RB-INSERT-FIXUP}$ makes the following changes to the tree: Changes a black node with two red children (node $C$) to a red node, resulting in a potential change of $-2$. Changes a red node (node $A$ in part (a) and node $B$ in part (b)) to a black node with one red child, resulting in no potential change. Changes a red node (node $D$) to a black node with no red children, resulting in a potential change of $1$. The total change in potential is $-1$, which pays for the structural modifications performed, and thus the amortized number of structural modifications in case 1 (the nonterminating case) is $0$. The terminating cases of $\\text{RB-INSERT-FIXUP}$ cause $O(1)$ structural changes. Because $w(v)$ is based solely on node colors and the number of color changes caused by terminating cases is $O(1)$, the change in potential in terminating cases is $O(1)$. Hence, the amortized number of structural modifications in the terminating cases is $O(1)$. The overall amortized number of structural modifications in $\\text{RB-INSERT}$, therefore, is $O(1)$. g. Figure 13.7 shows that case 2 of $\\text{RB-DELETE-FIXUP}$ makes the following changes to the tree: Changes a black node with no red children (node $D$) to a red node, resulting in a potential change of $-1$. If $B$ is red, then it loses a black child, with no effect on potential. If $B$ is black, then it goes from having no red children to having one red child, resulting in a potential change of $-1$. The total change in potential is either $-1$ or $-2$, depending on the color of $B$. In either case, one unit of potential pays for the structural modifications performed, and thus the amortized number of structural modifications in case 2 (the nonterminating case) is at most $0$. The terminating cases of $\\text{RB-DELETE}$ cause $O(1)$ structural changes. Because $w(v)$ is based solely on node colors and the number of color changes caused by terminating cases is $O(1)$, the change in potential in terminating cases is $O(1)$. Hence, the amortized number of structural changes in the terminating cases is $O(1)$. The overall amortized number of structural modifications in $\\text{RB-DELETE-FIXUP}$, therefore, is $O(1)$. h. Since the amortized number structural modification in each operation is $O(1)$, the actual number of structural modifications for any sequence of $m$ $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ operations on an initially empty red-black tree is $O(m)$ in the worst case.","title":"17-4 The cost of restructuring red-black trees"},{"location":"Chap17/Problems/17-5/","text":"A self-organizing list is a linked list of $n$ elements, in which each element has a unique key. When we search for an element in the list, we are given a key, and we want to find an element with that key. A self-organizing list has two important properties: To find an element in the list, given its key, we must traverse the list from the beginning until we encounter the element with the given key. If that element is the $k$th element from the start of the list, then the cost to find the element is $k$. We may reorder the list elements after any operation, according to a given rule with a given cost. We may choose any heuristic we like to decide how to reorder the list. Assume that we start with a given list of $n$ elements, and we are given an access sequence $\\sigma = \\langle \\sigma_1, \\sigma_2, \\ldots, \\sigma_m \\rangle$ of keys to find, in order. The cost of the sequence is the sum of the costs of the individual accesses in the sequence. Out of the various possible ways to reorder the list after an operation, this problem focuses on transposing adjacent list elements-switching their positions in the list\u2014with a unit cost for each transpose operation. You will show, by means of a potential function, that a particular heuristic for reordering the list, move-to-front, entails a total cost no worse than $4$ times that of any other heuristic for maintaining the list order\u2014even if the other heuristic knows the access sequence in advance! We call this type of analysis a competitive analysis . For a heuristic $H$ and a given initial ordering of the list, denote the access cost of sequence $\\sigma$ by $C_H(\\sigma)$ Let $m$ be the number of accesses in $\\sigma$. a. Argue that if heuristic $\\text H$ does not know the access sequence in advance, then the worst-case cost for $\\text H$ on an access sequence $\\sigma$ is $C_H(\\sigma) = \\Omega(mn)$. With the move-to-front heuristic, immediately after searching for an element $x$, we move $x$ to the first position on the list (i.e., the front of the list). Let $\\text{rank}_L(x)$ denote the rank of element $x$ in list $L$, that is, the position of $x$ in list $L$. For example, if $x$ is the fourth element in $L$, then $\\text{rank}_L(x) = 4$. Let $c_i$ denote the cost of access $\\sigma_i$ using the move-to-front heuristic, which includes the cost of finding the element in the list and the cost of moving it to the front of the list by a series of transpositions of adjacent list elements. b. Show that if $\\sigma_i$ accesses element $x$ in list $L$ using the move-to-front heuristic, then $c_i = 2 \\cdot \\text{rank}_L(x) - 1$. Now we compare move-to-front with any other heuristic $\\text H$ that processes an access sequence according to the two properties above. Heuristic $\\text H$ may transpose elements in the list in any way it wants, and it might even know the entire access sequence in advance. Let $L_i$ be the list after access $\\sigma_i$ using move-to-front, and let $L_i^*$ be the list after access $\\sigma_i$ using heuristic $\\text H$. We denote the cost of access $\\sigma_i$ by $c_i$ for move-to-front and by $c_i^*$ for heuristic $\\text H$. Suppose that heuristic $\\text H$ performs $t_i^*$ transpositions during access $\\sigma_i$. c. In part (b), you showed that $c_i = 2 \\cdot \\text{rank}_{L_{i - 1}}(x) - 1$. Now show that $c_i^* = \\text{rank}_{L_{i - 1}^*}(x) + t_i^*$. We define an inversion in list $L_i$ as a pair of elements $y$ and $z$ such that $y$ precedes $z$ in $L_i$ and $z$ precedes $y$ in list $L_i^*$. Suppose that list $L_i$ has $q_i$ inversions after processing the access sequence $\\langle \\sigma_1, \\sigma_2, \\ldots, \\sigma_i \\rangle$. Then, we define a potential function $\\Phi$ that maps $L_i$ to a real number by $\\Phi(L_i) = 2q_i$. For example, if $L_i$ has the elements $\\langle e, c, a, d, b \\rangle$ and $L_i^*$ has the elements $\\langle c, a, b, d, e \\rangle$, then $L_i$ has 5 inversions $((e, c), (e, a), (e, d), (e, b), (d, b))$, and so $\\Phi(L_i) = 10$. Observe that $\\Phi(L_i) \\ge 0$ for all $i$ and that, if move-to-front and heuristic $\\text H$ start with the same list $L_0$, then $\\Phi(L_0) = 0$. d. Argue that a transposition either increases the potential by $2$ or decreases the potential by $2$. Suppose that access $\\sigma_i$ finds the element $x$. To understand how the potential changes due to $\\sigma_i$, let us partition the elements other than $x$ into four sets, depending on where they are in the lists just before the $i$th access: Set $A$ consists of elements that precede $x$ in both $L_{i - 1}$ and $L_{i - 1}^*$. Set $B$ consists of elements that precede $x$ in $L_{i - 1}$ and follow $x$ in $L_{i - 1}^*$. Set $C$ consists of elements that follow $x$ in $L_{i - 1}$ and precede $x$ in $L_{i - 1}^*$. Set $D$ consists of elements that follow $x$ in both $L_{i - 1}$ and $L_{i - 1}^*$. e. Argue that $\\text{rank}_{L_{i - 1}}(x) = |A| + |B| + 1$ and $\\text{rank}_{L_{i - 1}^*}(x) = |A| + |C| + 1$. f. Show that access $\\sigma_i$ causes a change in potential of $$\\Phi(L_i) - \\Phi(L_{i - 1}) \\le 2(|A| - |B| + t_i^*),$$ where, as before, heuristic $\\text H$ performs $t_i^*$ transpositions during access $\\sigma_i$. Define the amortized cost $\\hat c_i$ of access $\\sigma_i$ by $\\hat c_i = c_i + \\Phi(L_i) - \\Phi(L_{i - 1})$. g. Show that the amortized cost $\\hat c_i$ of access $\\sigma_i$ is bounded from above by $4c_i^*$. h. Conclude that the cost $C_{\\text{MTF}}(\\sigma)$ of access sequence $\\sigma$ with move-to-front is at most $4$ times the cost $C_H(\\sigma)$ of $\\sigma$ withany other heuristic $\\text H$, assuming that both heuristics start with the same list. a. Since the heuristic is picked in advance, given any sequence of requests given so far, we can simulate what ordering the heuristic will call for, then, we will pick our next request to be whatever element will of been in the last position of the list. Continuing until all the requests have been made, we have that the cost of this sequence of accesses is $= mn$. b. The cost of finding an element is $= \\text{rank}_L(x)$ and since it needs to be swapped with all the elements before it, of which there are $\\text{rank}_L(x) - 1$, the total cost is $2 \\cdot \\text{rank}_L(x) - 1$. c. Regardless of the heuristic used, we first need to locate the element, which is left where ever it was after the previous step, so, needs $\\text{rank}_{L_{i - 1}}(x)$. After that, by definition, there are $t_i$ transpositions made, so, $c^*_i = \\text{rank}_{L_{i - 1}}(x) + t_i^*$. d. If we perform a transposition of elements $y$ and $z$, where $y$ is towards the left. Then there are two cases. The first is that the final ordering of the list in $L_i^*$ is with $y$ in front of $z$, in which case we have just increased the number of inversions by $1$, so the potential increases by $2$. The second is that in $L_I^*z$ occurs before $y$, in which case, we have just reduced the number of inversions by one, reducing the potential by $2$. In both cases, whether or not there is an inversion between $y$ or $z$ and any other element has not changed, since the transposition only changed the relative ordering of those two elements. e. By definition, $A$ and $B$ are the only two of the four categories to place elements that precede $x$ in $L_{i - 1}$, since there are $|A| + |B|$ elements preceding it, it's rank in $L_{i - 1}$ is $|A| + |B| + 1$. Similarly, the two categories in which an element can be if it precedes $x$ in $L^*_{i - 1}$ are $A$ and $C$, so, in $L^*_{i - 1}$, $x$ has $\\text{rank} |A| + |C| + 1$. f. We have from part d that the potential increases by $2$ if we transpose two elements that are being swapped so that their relative order in the final ordering is being screwed up, and decreases by two if they are begin placed into their correct order in $L^*_i$. In particular, they increase it by at most $2$, since we are keeping track of the number of inversions that may not be the direct effect of the transpositions that heuristic $H$ made, we see which ones the Move to front heuristic may of added. In particular, since the move to front heuristic only changed the relative order of $x$ with respect to the other elements, moving it in front of the elements that preceded it in $L_{i - 1}$, we only care about sets $A$ and $B$. For an element in $A$, moving it to be behind $A$ created an inversion, since that element preceded $x$ in $L^*_i$. However, if the element were in $B$, we are removing an inversion by placing $x$ in front of it. g. $$ \\begin{aligned} \\hat c_i & \\le 2(|A| + |B| + 1) - 1 + 2(|A| - |B| + t_i^*) \\\\ & = 4|A| + 1 + 2 t_i^* \\\\ & \\le 4(|A| + |C| + 1 + t_i^*) \\\\ & = 4 c_i^*. \\end{aligned} $$ h. We showed that the amortized cost of each operation under the move to front heuristic was at most four times the cost of the operation using any other heuristic. Since the amortized cost added up over all these operation is at most the total (real) cost, so we have that the total cost with movetofront is at most four times the total cost with an arbitrary other heuristic.","title":"17-5 Competitive analysis of self-organizing lists with move-to-front"},{"location":"Chap18/18.1/","text":"18.1-1 Why don't we allow a minimum degree of $t = 1$? According to the definition, minimum degree $t$ means every node other than the root must have at least $t - 1$ keys, and every internal node other than the root thus has at least $t$ children. So, when $t = 1$, it means every node other than the root must have at least $t - 1 = 0$ key, and every internal node other than the root thus has at least $t = 1$ child. Thus, we can see that the minimum case doesn't exist, because no node exists with $0$ key, and no node exists with only $1$ child in a B-tree. 18.1-2 For what values of $t$ is the tree of Figure 18.1 a legal B-tree? According to property 5 of B-tree, every node other than the root must have at least $t - 1$ keys and may contain at most $2t - 1$ keys. In Figure 18.1, the number of keys of each node (except the root) is either $2$ or $3$. So to make it a legal B-tree, we need to guarantee that $t - 1 \\le 2 \\text{ and } 2 t - 1 \\ge 3$, which yields $2 \\le t \\le 3$. So $t$ can be $2$ or $3$. 18.1-3 Show all legal B-trees of minimum degree $2$ that represent $\\{1, 2, 3, 4, 5\\}$. We know that every node except the root must have at least $t - 1 = 2$ keys, and at most $2t - 1 = 5$ keys. Also remember that the leaves stay in the same depth. Thus, there are $2$ possible legal B-trees: $$| 1, 2, 3, 4, 5 |$$ $$| 3 |$$ $$\\swarrow \\quad \\searrow$$ $$| 1, 2 | \\qquad\\qquad | 4, 5 |$$ 18.1-4 As a function of the minimum degree $t$, what is the maximum number of keys that can be stored in a B-tree of height $h$? $$ \\begin{aligned} n & = (1 + 2t + (2t) ^ 2 + \\cdots + (2t) ^ {h}) \\cdot (2t - 1) \\\\ & = (2t)^{h + 1} - 1. \\end{aligned} $$ 18.1-5 Describe the data structure that would result if each black node in a red-black tree were to absorb its red children, incorporating their children with its own. After absorbing each red node into its black parent, each black node may contain $1, 2$ ($1$ red child), or $3$ ($2$ red children) keys, and all leaves of the resulting tree have the same depth, according to property 5 of red-black tree (For each node, all paths from the node to descendant leaves contain the same number of black nodes). Therefore, a red-black tree will become a Btree with minimum degree $t = 2$, i.e., a 2-3-4 tree.","title":"18.1 Definition of B-trees"},{"location":"Chap18/18.1/#181-1","text":"Why don't we allow a minimum degree of $t = 1$? According to the definition, minimum degree $t$ means every node other than the root must have at least $t - 1$ keys, and every internal node other than the root thus has at least $t$ children. So, when $t = 1$, it means every node other than the root must have at least $t - 1 = 0$ key, and every internal node other than the root thus has at least $t = 1$ child. Thus, we can see that the minimum case doesn't exist, because no node exists with $0$ key, and no node exists with only $1$ child in a B-tree.","title":"18.1-1"},{"location":"Chap18/18.1/#181-2","text":"For what values of $t$ is the tree of Figure 18.1 a legal B-tree? According to property 5 of B-tree, every node other than the root must have at least $t - 1$ keys and may contain at most $2t - 1$ keys. In Figure 18.1, the number of keys of each node (except the root) is either $2$ or $3$. So to make it a legal B-tree, we need to guarantee that $t - 1 \\le 2 \\text{ and } 2 t - 1 \\ge 3$, which yields $2 \\le t \\le 3$. So $t$ can be $2$ or $3$.","title":"18.1-2"},{"location":"Chap18/18.1/#181-3","text":"Show all legal B-trees of minimum degree $2$ that represent $\\{1, 2, 3, 4, 5\\}$. We know that every node except the root must have at least $t - 1 = 2$ keys, and at most $2t - 1 = 5$ keys. Also remember that the leaves stay in the same depth. Thus, there are $2$ possible legal B-trees: $$| 1, 2, 3, 4, 5 |$$ $$| 3 |$$ $$\\swarrow \\quad \\searrow$$ $$| 1, 2 | \\qquad\\qquad | 4, 5 |$$","title":"18.1-3"},{"location":"Chap18/18.1/#181-4","text":"As a function of the minimum degree $t$, what is the maximum number of keys that can be stored in a B-tree of height $h$? $$ \\begin{aligned} n & = (1 + 2t + (2t) ^ 2 + \\cdots + (2t) ^ {h}) \\cdot (2t - 1) \\\\ & = (2t)^{h + 1} - 1. \\end{aligned} $$","title":"18.1-4"},{"location":"Chap18/18.1/#181-5","text":"Describe the data structure that would result if each black node in a red-black tree were to absorb its red children, incorporating their children with its own. After absorbing each red node into its black parent, each black node may contain $1, 2$ ($1$ red child), or $3$ ($2$ red children) keys, and all leaves of the resulting tree have the same depth, according to property 5 of red-black tree (For each node, all paths from the node to descendant leaves contain the same number of black nodes). Therefore, a red-black tree will become a Btree with minimum degree $t = 2$, i.e., a 2-3-4 tree.","title":"18.1-5"},{"location":"Chap18/18.2/","text":"18.2-1 Show the results of inserting the keys $$F, S, Q, K, C, L, H, T, V, W, M, R, N, P, A, B, X, Y, D, Z, E$$ in order into an empty B-tree with minimum degree $2$. Draw only the configurations of the tree just before some node must split, and also draw the final configuration. (Omit!) 18.2-2 Explain under what circumstances, if any, redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ operations occur during the course of executing a call to $\\text{B-TREE-INSERT}$. (A redundant $\\text{DISK-READ}$ is a $\\text{DISK-READ}$ for a page that is already in memory. A redundant $\\text{DISK-WRITE}$ writes to disk a page of information that is identical to what is already stored there.) In order to insert the key into a full child node but without its parent being full, we need the following operations: $\\text{DISK-READ}$: Key placement $\\text{DISK-WRITE}$: Split nodes $\\text{DISK-READ}$: Get to the parent $\\text{DISK-WRITE}$: Fill parent If both were full, we'd have to do the same, but instead of the final step, repeat the above to split the parent node and write into the child nodes. With both considerations in mind, there should never be a redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ on a $\\text{B-TREE-INSERT}$. 18.2-3 Explain how to find the minimum key stored in a B-tree and how to find the predecessor of a given key stored in a B-tree. Finding the minimum in a B-tree is quite similar to finding a minimum in a binary search tree. We need to find the left most leaf for the given root, and return the first key. PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MIN}(T.root)$. POST: $\\text{FCTVAL}$ is the minimum key stored in the subtree rooted at $x$. 1 2 3 4 5 6 7 8 B - TREE - FIND - MIN ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x . key [ 1 ] // return the minimum key of x else DISK - READ ( x . c [ 1 ]) return B - TREE - FIND - MIN ( x . c [ 1 ]) Finding the predecessor of a given key $x.key_i$ is according to the following rules: If $x$ is not a leaf, return the maximum key in the $i$-th child of $x$, which is also the maximum key of the subtree rooted at $x.c_i$. If $x$ is a leaf and $i > 1$, return the $(i - 1)$st key of $x$, i.e., $x.key_{i - 1}$. Otherwise, look for the last node y (from the bottom up) and $j > 0$, such that $x.key_i$ is the leftmost key in $y.c_j$; if $j = 1$, return $\\text{NIL}$ since $x.key_i$ is the minimum key in the tree; otherwise we return $y.key_{j - 1}$. PRE: $x$ is a node on the B-tree $T$. $i$ is the index of the key. POST: $\\text{FCTVAL}$ is the predecessor of $x.key_i$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 B - TREE - FIND - PREDECESSOR ( x , i ) if ! x . leaf DISK - READ ( x . c [ i ]) return B - TREE - FIND - MAX ( x . c [ i ]) else if i > 1 // x is a leaf and i > 1 return x . key [ i - 1 ] else z = x while true if z . p == NIL // z is root return NIL // z.key[i] is the minimum key in T; no predecessor y = z . p j = 1 DISK - READ ( y . c [ 1 ]) while y . c [ j ] != x j = j + 1 DISK - READ ( y . c [ j ]) if j == 1 z = y else return y . key [ j - 1 ] PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MAX}(T.root)$. POST: $\\text{FCTVAL}$ is the maximum key stored in the subtree rooted at $x$. 1 2 3 4 5 6 7 8 B - TREE - FIND - MAX ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x .[ x . n ] // return the maximum key of x else DISK - READ ( x . c [ x . n + 1 ]) return B - TREE - FIND - MIN ( x . c [ x . n + 1 ]) 18.2-4 $\\star$ Suppose that we insert the keys $\\{1, 2, \\ldots, n\\}$ into an empty B-tree with minimum degree 2. How many nodes does the final B-tree have? The final tree can have as many as $n - 1$ nodes. Unless $n = 1$ there cannot ever be $n$ nodes since we only ever insert a key into a non-empty node, so there will always be at least one node with $2$ keys. Next observe that we will never have more than one key in a node which is not a right spine of our B-tree. This is because every key we insert is larger than all keys stored in the tree, so it will be inserted into the right spine of the tree. Nodes not in the right spine are a result of splits, and since $t = 2$, every split results in child nodes with one key each. The fewest possible number of nodes occurs when every node in the right spine has $3$ keys. In this case, $n = 2h + 2^{h + 1} - 1$ where $h$ is the height of the B-tree, and the number of nodes is $2^{h + 1} - 1$. Asymptotically these are the same, so the number of nodes is $\\Theta(n)$. 18.2-5 Since leaf nodes require no pointers to children, they could conceivably use a different (larger) $t$ value than internal nodes for the same disk page size. Show how to modify the procedures for creating and inserting into a B-tree to handle this variation. You would modify the insertion procedure by, in $\\text{B-TREE-INSERT}$, check if the node is a leaf, and if it is, only split it if there twice as many keys stored as expected. Also, if an element needs to be inserted into a full leaf, we would split the leaf into two separate leaves, each of which doesn't have too many keys stored in it. 18.2-6 Suppose that we were to implement $\\text{B-TREE-SEARCH}$ to use binary search rather than linear search within each node. Show that this change makes the CPU time required $O(\\lg n)$, independently of how $t$ might be chosen as a function of $n$. As in the $\\text{TREE-SEARCH}$ procedure for binary search trees, the nodes encountered during the recursion form a simple path downward from the root of the tree. Thus, the $\\text{B-TREE-SEARCH}$ procedure needs $O(h) = O(\\log_t n)$ CPU time to search along the path, where $h$ is the height of the B-tree and $n$ is the number of keys in the B-tree, and we know that $h \\le \\log_t \\frac{n + 1}{2}$. Since the number of keys in each nodeis less than $2t - 1$, a binary search within each node is $O(\\lg t)$. So the total time is: $$ \\begin{aligned} O(\\lg t \\cdot \\log_t n) & = O(\\lg t \\cdot \\frac{\\lg n}{\\lg t}) & \\text{by changing the base of the logarithm.} \\\\ & = O(\\lg n). \\end{aligned} $$ Thus, the CPU time required is $O(\\lg n)$. 18.2-7 Suppose that disk hardware allows us to choose the size of a disk page arbitrarily, but that the time it takes to read the disk page is $a + bt$, where $a$ and $b$ are specified constants and $t$ is the minimum degree for a B-tree using pages of the selected size. Describe how to choose $t$ so as to minimize (approximately) the B-tree search time. Suggest an optimal value of $t$ for the case in which $a = 5$ milliseconds and $b = 10$ microseconds. $$\\min \\log_t n \\cdot (a + bt) = \\min \\frac{a + bt}{\\ln t}$$ $$\\frac{\\partial}{\\partial t} (\\frac{a + bt}{\\ln t}) = - \\frac{a + bt - bt \\ln t}{t \\ln^2 t}$$ $$ \\begin{aligned} a + bt & = bt \\ln t \\\\ 5 + 10t & = 10t \\ln t \\\\ t & = e^{W \\left(\\frac{1}{2e} \\right) + 1}, \\\\ \\end{aligned} $$ where $W$ is the LambertW function, and we should choose $t = 3$.","title":"18.2 Basic operations on B-trees"},{"location":"Chap18/18.2/#182-1","text":"Show the results of inserting the keys $$F, S, Q, K, C, L, H, T, V, W, M, R, N, P, A, B, X, Y, D, Z, E$$ in order into an empty B-tree with minimum degree $2$. Draw only the configurations of the tree just before some node must split, and also draw the final configuration. (Omit!)","title":"18.2-1"},{"location":"Chap18/18.2/#182-2","text":"Explain under what circumstances, if any, redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ operations occur during the course of executing a call to $\\text{B-TREE-INSERT}$. (A redundant $\\text{DISK-READ}$ is a $\\text{DISK-READ}$ for a page that is already in memory. A redundant $\\text{DISK-WRITE}$ writes to disk a page of information that is identical to what is already stored there.) In order to insert the key into a full child node but without its parent being full, we need the following operations: $\\text{DISK-READ}$: Key placement $\\text{DISK-WRITE}$: Split nodes $\\text{DISK-READ}$: Get to the parent $\\text{DISK-WRITE}$: Fill parent If both were full, we'd have to do the same, but instead of the final step, repeat the above to split the parent node and write into the child nodes. With both considerations in mind, there should never be a redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ on a $\\text{B-TREE-INSERT}$.","title":"18.2-2"},{"location":"Chap18/18.2/#182-3","text":"Explain how to find the minimum key stored in a B-tree and how to find the predecessor of a given key stored in a B-tree. Finding the minimum in a B-tree is quite similar to finding a minimum in a binary search tree. We need to find the left most leaf for the given root, and return the first key. PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MIN}(T.root)$. POST: $\\text{FCTVAL}$ is the minimum key stored in the subtree rooted at $x$. 1 2 3 4 5 6 7 8 B - TREE - FIND - MIN ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x . key [ 1 ] // return the minimum key of x else DISK - READ ( x . c [ 1 ]) return B - TREE - FIND - MIN ( x . c [ 1 ]) Finding the predecessor of a given key $x.key_i$ is according to the following rules: If $x$ is not a leaf, return the maximum key in the $i$-th child of $x$, which is also the maximum key of the subtree rooted at $x.c_i$. If $x$ is a leaf and $i > 1$, return the $(i - 1)$st key of $x$, i.e., $x.key_{i - 1}$. Otherwise, look for the last node y (from the bottom up) and $j > 0$, such that $x.key_i$ is the leftmost key in $y.c_j$; if $j = 1$, return $\\text{NIL}$ since $x.key_i$ is the minimum key in the tree; otherwise we return $y.key_{j - 1}$. PRE: $x$ is a node on the B-tree $T$. $i$ is the index of the key. POST: $\\text{FCTVAL}$ is the predecessor of $x.key_i$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 B - TREE - FIND - PREDECESSOR ( x , i ) if ! x . leaf DISK - READ ( x . c [ i ]) return B - TREE - FIND - MAX ( x . c [ i ]) else if i > 1 // x is a leaf and i > 1 return x . key [ i - 1 ] else z = x while true if z . p == NIL // z is root return NIL // z.key[i] is the minimum key in T; no predecessor y = z . p j = 1 DISK - READ ( y . c [ 1 ]) while y . c [ j ] != x j = j + 1 DISK - READ ( y . c [ j ]) if j == 1 z = y else return y . key [ j - 1 ] PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MAX}(T.root)$. POST: $\\text{FCTVAL}$ is the maximum key stored in the subtree rooted at $x$. 1 2 3 4 5 6 7 8 B - TREE - FIND - MAX ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x .[ x . n ] // return the maximum key of x else DISK - READ ( x . c [ x . n + 1 ]) return B - TREE - FIND - MIN ( x . c [ x . n + 1 ])","title":"18.2-3"},{"location":"Chap18/18.2/#182-4-star","text":"Suppose that we insert the keys $\\{1, 2, \\ldots, n\\}$ into an empty B-tree with minimum degree 2. How many nodes does the final B-tree have? The final tree can have as many as $n - 1$ nodes. Unless $n = 1$ there cannot ever be $n$ nodes since we only ever insert a key into a non-empty node, so there will always be at least one node with $2$ keys. Next observe that we will never have more than one key in a node which is not a right spine of our B-tree. This is because every key we insert is larger than all keys stored in the tree, so it will be inserted into the right spine of the tree. Nodes not in the right spine are a result of splits, and since $t = 2$, every split results in child nodes with one key each. The fewest possible number of nodes occurs when every node in the right spine has $3$ keys. In this case, $n = 2h + 2^{h + 1} - 1$ where $h$ is the height of the B-tree, and the number of nodes is $2^{h + 1} - 1$. Asymptotically these are the same, so the number of nodes is $\\Theta(n)$.","title":"18.2-4 $\\star$"},{"location":"Chap18/18.2/#182-5","text":"Since leaf nodes require no pointers to children, they could conceivably use a different (larger) $t$ value than internal nodes for the same disk page size. Show how to modify the procedures for creating and inserting into a B-tree to handle this variation. You would modify the insertion procedure by, in $\\text{B-TREE-INSERT}$, check if the node is a leaf, and if it is, only split it if there twice as many keys stored as expected. Also, if an element needs to be inserted into a full leaf, we would split the leaf into two separate leaves, each of which doesn't have too many keys stored in it.","title":"18.2-5"},{"location":"Chap18/18.2/#182-6","text":"Suppose that we were to implement $\\text{B-TREE-SEARCH}$ to use binary search rather than linear search within each node. Show that this change makes the CPU time required $O(\\lg n)$, independently of how $t$ might be chosen as a function of $n$. As in the $\\text{TREE-SEARCH}$ procedure for binary search trees, the nodes encountered during the recursion form a simple path downward from the root of the tree. Thus, the $\\text{B-TREE-SEARCH}$ procedure needs $O(h) = O(\\log_t n)$ CPU time to search along the path, where $h$ is the height of the B-tree and $n$ is the number of keys in the B-tree, and we know that $h \\le \\log_t \\frac{n + 1}{2}$. Since the number of keys in each nodeis less than $2t - 1$, a binary search within each node is $O(\\lg t)$. So the total time is: $$ \\begin{aligned} O(\\lg t \\cdot \\log_t n) & = O(\\lg t \\cdot \\frac{\\lg n}{\\lg t}) & \\text{by changing the base of the logarithm.} \\\\ & = O(\\lg n). \\end{aligned} $$ Thus, the CPU time required is $O(\\lg n)$.","title":"18.2-6"},{"location":"Chap18/18.2/#182-7","text":"Suppose that disk hardware allows us to choose the size of a disk page arbitrarily, but that the time it takes to read the disk page is $a + bt$, where $a$ and $b$ are specified constants and $t$ is the minimum degree for a B-tree using pages of the selected size. Describe how to choose $t$ so as to minimize (approximately) the B-tree search time. Suggest an optimal value of $t$ for the case in which $a = 5$ milliseconds and $b = 10$ microseconds. $$\\min \\log_t n \\cdot (a + bt) = \\min \\frac{a + bt}{\\ln t}$$ $$\\frac{\\partial}{\\partial t} (\\frac{a + bt}{\\ln t}) = - \\frac{a + bt - bt \\ln t}{t \\ln^2 t}$$ $$ \\begin{aligned} a + bt & = bt \\ln t \\\\ 5 + 10t & = 10t \\ln t \\\\ t & = e^{W \\left(\\frac{1}{2e} \\right) + 1}, \\\\ \\end{aligned} $$ where $W$ is the LambertW function, and we should choose $t = 3$.","title":"18.2-7"},{"location":"Chap18/18.3/","text":"18.3-1 Show the results of deleting $C$, $P$, and $V$, in order, from the tree of Figure 18.8(f). Figure 18.8(f) delete $C$ delete $P$ delete $V$ 18.3-2 Write pseudocode for $\\text{B-TREE-DELETE}$. The algorithm $\\text{B-TREE-DELETE}(x, k)$ is a recursive procedure which deletes key $k$ from the B-tree rooted at node $x$. The functions $\\text{PREDECESSOR}(k, x)$ and $\\text{SUCCESSOR}(k, x)$ return the predecessor and successor of $k$ in the B-tree rooted at $x$ respectively. The cases where $k$ is the last key in a node have been omitted because the pseudocode is already unwieldy. For these, we simply use the left sibling as opposed to the right sibling, making the appropriate modifications to the indexing in the for loops.","title":"18.3 Deleting a key from a B-tree"},{"location":"Chap18/18.3/#183-1","text":"Show the results of deleting $C$, $P$, and $V$, in order, from the tree of Figure 18.8(f). Figure 18.8(f) delete $C$ delete $P$ delete $V$","title":"18.3-1"},{"location":"Chap18/18.3/#183-2","text":"Write pseudocode for $\\text{B-TREE-DELETE}$. The algorithm $\\text{B-TREE-DELETE}(x, k)$ is a recursive procedure which deletes key $k$ from the B-tree rooted at node $x$. The functions $\\text{PREDECESSOR}(k, x)$ and $\\text{SUCCESSOR}(k, x)$ return the predecessor and successor of $k$ in the B-tree rooted at $x$ respectively. The cases where $k$ is the last key in a node have been omitted because the pseudocode is already unwieldy. For these, we simply use the left sibling as opposed to the right sibling, making the appropriate modifications to the indexing in the for loops.","title":"18.3-2"},{"location":"Chap18/Problems/18-1/","text":"Consider implementing a stack in a computer that has a relatively small amount of fast primary memory and a relatively large amount of slower disk storage. The operations $\\text{PUSH}$ and $\\text{POP}$ work on single-word values. The stack we wish to support can grow to be much larger than can fit in memory, and thus most of it must be stored on disk. A simple, but inefficient, stack implementation keeps the entire stack on disk. We maintain in - memory a stack pointer, which is the disk address of the top element on the stack. If the pointer has value $p$, the top element is the $(p \\mod m)$th word on page $\\lfloor p / m \\rfloor$ of the disk, where $m$ is the number of words per page. To implement the $\\text{PUSH}$ operation, we increment the stack pointer, read the appropriate page into memory from disk, copy the element to be pushed to the appropriate word on the page, and write the page back to disk. A $\\text{POP}$ operation is similar. We decrement the stack pointer, read in the appropriate page from disk, and return the top of the stack. We need not write back the page, since it was not modified. Because disk operations are relatively expensive, we count two costs for any implementation: the total number of disk accesses and the total CPU time. Any disk access to a page of $m$ words incurs charges of one disk access and $\\Theta(m)$ CPU time. a. Asymptotically, what is the worst-case number of disk accesses for $n$ stack operations using this simple implementation? What is the CPU time for $n$ stack operations? (Express your answer in terms of $m$ and $n$ for this and subsequent parts.) Now consider a stack implementation in which we keep one page of the stack in memory. (We also maintain a small amount of memory to keep track of which page is currently in memory.) We can perform a stack operation only if the relevant disk page resides in memory. If necessary, we can write the page currently in memory to the disk and read in the new page from the disk to memory. If the relevant disk page is already in memory, then no disk accesses are required. b. What is the worst-case number of disk accesses required for $n$ $\\text{PUSH}$ operations? What is the CPU time? c. What is the worst-case number of disk accesses required for $n$ stack operations? What is the CPU time? Suppose that we now implement the stack by keeping two pages in memory (in addition to a small number of words for bookkeeping). d. Describe how to manage the stack pages so that the amortized number of disk accesses for any stack operation is $O(1 / m)$ and the amortized CPU time for any stack operation is $O(1)$. a. We will have to make a disk access for each stack operation. Since each of these disk operations takes time $\\Theta(m)$, the CPU time is $\\Theta(mn)$. b. Since only every mth push starts a new page, the number of disk operations is approximately $n / m$, and the CPU runtime is $\\Theta(n)$, since both the contribution from the cost of the disk access and the actual running of the push operations are both $\\Theta(n)$. c. If we make a sequence of pushes until it just spills over onto the second page, then alternate popping and pulling many times, the asymptotic number of disk accesses and CPU time is of the same order as in part a. This is because when we are doing that alternating of pops and pushes, each one triggers a disk access. d. We define the potential of the stack to be the absolute value of the difference between the current size of the stack and the most recently passed multiple of $m$. This potential function means that the initial stack which has size $0$, is also a multiple of $m$, so the potential is zero. Also, as we do a stack operation we either increase or decrease the potential by one. For us to have to load a new page from disk and write an old one to disk, we would need to be at least $m$ positions away from the most recently visited multiple of $m$, because we would have had to just cross a page boundary. This cost of loading and storing a page takes (real) cpu time of $\\Theta(m)$. However, we just had a drop in the potential function of order $\\Theta(m)$. So, the amortized cost of this operation is $O(1)$.","title":"18-1 Stacks on secondary storage"},{"location":"Chap18/Problems/18-2/","text":"The join operation takes two dynamic sets $S'$ and $S''$ and an element $x$ such that for any $x' \\in S'$ and $x'' \\in S''$, we have $x'.key < x.key < x''.key$. It returns a set $S = S' \\cup \\{x\\} \\cup S''$. The split operation is like an \"inverse\" join: given a dynamic set $S$ and an element $x \\in S$, it creates a set $S'$ that consists of all elements in set $S$ and an element $x \\in S$, it creates a set $S'$ that consists of all elements in $S - \\{x\\}$ whose keys are less than $x.key$ and a set $S''$ that consists of all elements in $S - \\{x\\}$ whose keys are greater than $x.key$. In this problem, we investigate how to implement these operations on 2-3-4 trees. We assume for convenience that elements consist only of keys and that all key values are distinct. a. Show how to maintain, for every node $x$ of a 2-3-4 tree, the height of the subtree rooted at $x$ as an attribute $x.height$. Make sure that your implementation does not affect the asymptotic running times of searching, insertion, and deletion. b. Show how to implement the join operation. Given two 2-3-4 trees $T'$ and $T''$ and a key $k$, the join operation should run in $O(1 + |h' - h''|)$ time, where $h'$ and $h''$ are the heights of $T'$ and $T''$, respectively. c. Consider the simple path $p$ from the root of a 2-3-4 tree $T$ to a given key $k$, the set $S'$ of keys in $T$ that are less than $k$, and the set $S''$ of keys in $T$ that are greater than $k$. Show that $p$ breaks $S'$ into a set of trees $\\{T'_0, T'_1, \\ldots, T'_m\\}$ and a set of keys $\\{k'_1, k'_2, \\ldots, k'_m\\}$, where, for $i = 1, 2, \\ldots, m$, we have $y < k'_i < z$ for any keys $y \\in T'_{i - 1}$ and $z \\in T'_i$. What is the relationship between the heights of $T'_{i - 1}$ and $T'_i$? Describe how $p$ breaks $S''$ into sets of trees and keys. d. Show how to implement the split operation on $T$. Use the join operation to assemble the keys in $S'$ into a single 2-3-4 tree $T'$ and the keys in $S''$ into a single 2-3-4 tree $T''$. The running time of the split operation should be $O(\\lg n)$, where $n$ is the number of keys in $T$. ($\\textit{Hint:}$ The costs for joining should telescope.) a. For insertion it will suffice to explain how to update height when we split a node. Suppose node $x$ is split into nodes $y$ and $z$, and the median of $x$ is merged into node $w$. The height of $w$ remains unchanged unless $x$ was the root (in which case $w.height = x.height + 1$). The height of $y$ or $z$ will often change. We set $$y.height = \\max_i y.c_i .height + 1$$ and $$z.height = \\max_i z.c_i.height + 1.$$ Each update takes $O(t)$. Since a call to $\\text{B-TREE-INSERT}$ makes at most $h$ splits where $h$ is the height of the tree, the total time it takes to update heights is $O(th)$, preserving the asymptotic running time of insert. For deletion the situation is even simple. The only time the height changes is when the root has a single node and it is merged with its subtree nodes, leaving an empty root node to be deleted. In this case, we update the height of the new node to be the (old) height of the root minus $1$. b. Without loss of generality, assume $h' \\ge h''$. We essentially wish to merge $T''$ into $T'$ at a node of height $h''$ using node $x$. To do this, find the node at depth $h' - h''$ on the right spine of $T'$. Add $x$ as a key to this node, and $T''$ as the additional child. If it should happen that the node was already full, perform a split operation. c. Let $x_i$ be the node encountered after $i$ steps on path $p$. Let $l_i$ be the index of the largest key stored in $x_i$ which is less than or equal to $k$. We take $k_i' = x_i.key_{l_i}$ and $T'_{i - 1}$ to be the tree whose root node consists of the keys in $x_i$ which are less than $x_i.key_{l_i}$, and all of their children. In general, $T'_{i - 1}.height \\ge T'_i.height$. For $S''$, we take a similar approach. They keys will be those in nodes passed on $p$ which are immediately greater than $k$, and the trees will be rooted at a node consisting of the larger keys, with the associated subtrees. When we reach the node which contains $k$, we don't assign a key, but we do assign a tree. d. Let $T_1$ and $T_2$ be empty trees. Consider the path $p$ from the root of $T$ to $k$. Suppose we have reached node $x_i$. We join tree $T'_{i - 1}$ to $T_1$, then insert $k' i$ into $T_1$. We join $T''_{i - 1}$ to $T_2$ and insert $k''_i$ into $T_2$. Once we have encountered the node which contains $k$ at $x_m.key_k$, join $x_m.c_k$ with $T_1$ and $x_m.c_{k + 1}$ with $T_2$. We will perform at most $2$ join operations and $1$ insert operation for each level of the tree. Using the runtime determined in part (b), and the fact that when we join a tree $T'$ to $T_1$ (or $T''$ to $T_2$ respectively) the height difference is $$T'.height - T_1.height.$$ Since the heights are nondecreasing of successive tree that are joined, we get a telescoping sum of heights. The first tree has height $h$, where $h$ is the height of $T$, and the last tree has height $0$. Thus, the runtime is $$O(2(h + h)) = O(\\lg n).$$","title":"18-2 Joining and splitting 2-3-4 trees"},{"location":"Chap19/19.1/","text":"There is no exercise in this section.","title":"19.1 Structure of Fibonacci heaps"},{"location":"Chap19/19.2/","text":"19.2-1 Show the Fibonacci heap that results from calling $\\text{FIB-HEAP-EXTRACT-MIN}$ on the Fibonacci heap shown in Figure 19.4(m). (Omit!)","title":"19.2 Mergeable-heap operations"},{"location":"Chap19/19.2/#192-1","text":"Show the Fibonacci heap that results from calling $\\text{FIB-HEAP-EXTRACT-MIN}$ on the Fibonacci heap shown in Figure 19.4(m). (Omit!)","title":"19.2-1"},{"location":"Chap19/19.3/","text":"19.3-1 Suppose that a root $x$ in a Fibonacci heap is marked. Explain how $x$ came to be a marked root. Argue that it doesn't matter to the analysis that $x$ is marked, even though it is not a root that was first linked to another node and then lost one child. A root in the heap became marked because it at some point had a child whose key was decreased. It doesn't add the potential for having to do any more actual work for it to be marked. This is because the only time that markedness is checked is in line 3 of cascading cut. This however is only ever run on nodes whose parent is non $\\text{NIL}$. Since every root has $\\text{NIL}$ as it parent, line 3 of cascading cut will never be run on this marked root. It will still cause the potential function to be larger than needed, but that extra computation that was paid in to get the potential function higher will never be used up later. 19.3-2 Justify the $O(1)$ amortized time of $\\text{FIB-HEAP-DECREASE-KEY}$ as an average cost per operation by using aggregate analysis. Recall that the actual cost of $\\text{FIB-HEAP-DECREASE-KEY}$ is $O(c)$, where $c$ is the number of calls made to $\\text{CASCADING-CUT}$. If $c_i$ is the number of calls made on the $i$th key decrease, then the total time of $n$ calls to $\\text{FIB-HEAPDECREASE-KEY}$ is $\\sum_{i = 1}^n O(c_i)$. Next observe that every call to $\\text{CASCADING-CUT}$ moves a node to the root, and every call to a root node takes $O(1)$. Since no roots ever become children during the course of these calls, we must have that $\\sum_{i = 1}^n c_i = O(n)$. Therefore the aggregate cost is $O(n)$, so the average, or amortized, cost is $O(1)$.","title":"19.3 Decreasing a key and deleting a node"},{"location":"Chap19/19.3/#193-1","text":"Suppose that a root $x$ in a Fibonacci heap is marked. Explain how $x$ came to be a marked root. Argue that it doesn't matter to the analysis that $x$ is marked, even though it is not a root that was first linked to another node and then lost one child. A root in the heap became marked because it at some point had a child whose key was decreased. It doesn't add the potential for having to do any more actual work for it to be marked. This is because the only time that markedness is checked is in line 3 of cascading cut. This however is only ever run on nodes whose parent is non $\\text{NIL}$. Since every root has $\\text{NIL}$ as it parent, line 3 of cascading cut will never be run on this marked root. It will still cause the potential function to be larger than needed, but that extra computation that was paid in to get the potential function higher will never be used up later.","title":"19.3-1"},{"location":"Chap19/19.3/#193-2","text":"Justify the $O(1)$ amortized time of $\\text{FIB-HEAP-DECREASE-KEY}$ as an average cost per operation by using aggregate analysis. Recall that the actual cost of $\\text{FIB-HEAP-DECREASE-KEY}$ is $O(c)$, where $c$ is the number of calls made to $\\text{CASCADING-CUT}$. If $c_i$ is the number of calls made on the $i$th key decrease, then the total time of $n$ calls to $\\text{FIB-HEAPDECREASE-KEY}$ is $\\sum_{i = 1}^n O(c_i)$. Next observe that every call to $\\text{CASCADING-CUT}$ moves a node to the root, and every call to a root node takes $O(1)$. Since no roots ever become children during the course of these calls, we must have that $\\sum_{i = 1}^n c_i = O(n)$. Therefore the aggregate cost is $O(n)$, so the average, or amortized, cost is $O(1)$.","title":"19.3-2"},{"location":"Chap19/19.4/","text":"19.4-1 Professor Pinocchio claims that the height of an $n$-node Fibonacci heap is $O(\\lg n)$. Show that the professor is mistaken by exhibiting, for any positive integer $n$, a sequence of Fibonacci-heap operations that creates a Fibonacci heap consisting of just one tree that is a linear chain of $n$ nodes. Initialize: insert $3$ numbers then extract-min. Iteration: insert $3$ numbers, in which at least two numbers are less than the root of chain, then extract-min. The smallest newly inserted number will be extracted and the remaining two numbers will form a heap whose degree of root is $1$, and since the root of the heap is less than the old chain, the chain will be merged into the newly created heap. Finally we should delete the node which contains the largest number of the 3 inserted numbers. 19.4-2 Suppose we generalize the cascading-cut rule to cut a node $x$ from its parent as soon as it loses its $k$th child, for some integer constant $k$. (The rule in Section 19.3 uses $k = 2$.) For what values of $k$ is $D(n) = O(\\lg n)$? Following the proof of lemma 19.1, if $x$ is any node if a Fibonacci heap, $x.degree = m$, and $x$ has children $y_1, y_2, \\ldots, y_m$, then $y_1.degree \\ge 0$ and $y_i.degree \\ge i - k$. Thus, if $s_m$ denotes the fewest nodes possible in a node of degree $m$, then we have $s_0 = 1, s_1 = 2, \\ldots, s_{k - 1} = k$ and in general, $s_m = k + \\sum_{i = 0}^{m - k} s_i$. Thus, the difference between $s_m$ and $s_{m - 1}$ is $s_{m - k}$. Let $\\{f_m\\}$ be the sequence such that $f_m = m + 1$ for $0 \\le m < k$ and $f_m = f_{m - 1} + f_{m - k}$ for $m \\ge k$. If $F(x)$ is the generating function for $f_m$ then we have $F(x) = \\frac{1 - x^k}{(1 - x)(1 - x - x^k)}$. Let $\\alpha$ be a root of $x^k = x^{k - 1} + 1$. We'll show by induction that $f_{m + k} \\ge \\alpha^m$. For the base cases: $$ \\begin{aligned} f_k & = k + 1 \\ge 1 = \\alpha^0 \\\\ f_{k + 1} & = k + 3 \\ge \\alpha^1 \\\\ & \\vdots \\\\ f_{k + k} & = k + \\frac{(k + 1)(k + 2)}{2} = k + k + 1 + \\frac{k(k + 1)}{2} \\ge 2k + 1+\\alpha^{k - 1} \\ge \\alpha^k. \\end{aligned} $$ In general, we have $$f_{m + k} = f_{m + k - 1} + f_m \\ge \\alpha^{m - 1} + \\alpha^{m - k} = \\alpha^{m - k}(\\alpha^{k - 1} + 1) = \\alpha^m.$$ Next we show that $f_{m + k} = k + \\sum_{i = 0}^m f_i$. The base case is clear, since $f_k = f_0 + k = k + 1$. For the induction step, we have $$f_{m + k} = f_{m - 1 - k} + f_m = k \\sum_{i = 0}^{m - 1} f_i + f_m = k + \\sum_{i = 0}^m f_i.$$ Observe that $s_i \\ge f_{i + k}$ for $0 \\le i < k$. Again, by induction, for $m \\ge k$ we have $$s_m = k + \\sum_{i = 0}^{m - k} s_i \\ge k + \\sum_{i = 0}^{m - k} f_{i + k} \\ge k + \\sum_{i = 0}^m f_i = f_{m + k}.$$ So in general, $s_m \\ge f_{m + k}$. Putting it all together, we have $$ \\begin{aligned} size(x) & \\ge s_m \\\\ & \\ge k + \\sum_{i = k}^m s_{i - k} \\\\ & \\ge k + \\sum_{i = k}^m f_i \\\\ & \\ge f_{m + k} \\\\ & \\ge \\alpha^m. \\end{aligned} $$ Taking logs on both sides, we have $$\\log_\\alpha n \\ge m.$$ In other words, provided that $\\alpha$ is a constant, we have a logarithmic bound on the maximum degree.","title":"19.4 Bounding the maximum degree"},{"location":"Chap19/19.4/#194-1","text":"Professor Pinocchio claims that the height of an $n$-node Fibonacci heap is $O(\\lg n)$. Show that the professor is mistaken by exhibiting, for any positive integer $n$, a sequence of Fibonacci-heap operations that creates a Fibonacci heap consisting of just one tree that is a linear chain of $n$ nodes. Initialize: insert $3$ numbers then extract-min. Iteration: insert $3$ numbers, in which at least two numbers are less than the root of chain, then extract-min. The smallest newly inserted number will be extracted and the remaining two numbers will form a heap whose degree of root is $1$, and since the root of the heap is less than the old chain, the chain will be merged into the newly created heap. Finally we should delete the node which contains the largest number of the 3 inserted numbers.","title":"19.4-1"},{"location":"Chap19/19.4/#194-2","text":"Suppose we generalize the cascading-cut rule to cut a node $x$ from its parent as soon as it loses its $k$th child, for some integer constant $k$. (The rule in Section 19.3 uses $k = 2$.) For what values of $k$ is $D(n) = O(\\lg n)$? Following the proof of lemma 19.1, if $x$ is any node if a Fibonacci heap, $x.degree = m$, and $x$ has children $y_1, y_2, \\ldots, y_m$, then $y_1.degree \\ge 0$ and $y_i.degree \\ge i - k$. Thus, if $s_m$ denotes the fewest nodes possible in a node of degree $m$, then we have $s_0 = 1, s_1 = 2, \\ldots, s_{k - 1} = k$ and in general, $s_m = k + \\sum_{i = 0}^{m - k} s_i$. Thus, the difference between $s_m$ and $s_{m - 1}$ is $s_{m - k}$. Let $\\{f_m\\}$ be the sequence such that $f_m = m + 1$ for $0 \\le m < k$ and $f_m = f_{m - 1} + f_{m - k}$ for $m \\ge k$. If $F(x)$ is the generating function for $f_m$ then we have $F(x) = \\frac{1 - x^k}{(1 - x)(1 - x - x^k)}$. Let $\\alpha$ be a root of $x^k = x^{k - 1} + 1$. We'll show by induction that $f_{m + k} \\ge \\alpha^m$. For the base cases: $$ \\begin{aligned} f_k & = k + 1 \\ge 1 = \\alpha^0 \\\\ f_{k + 1} & = k + 3 \\ge \\alpha^1 \\\\ & \\vdots \\\\ f_{k + k} & = k + \\frac{(k + 1)(k + 2)}{2} = k + k + 1 + \\frac{k(k + 1)}{2} \\ge 2k + 1+\\alpha^{k - 1} \\ge \\alpha^k. \\end{aligned} $$ In general, we have $$f_{m + k} = f_{m + k - 1} + f_m \\ge \\alpha^{m - 1} + \\alpha^{m - k} = \\alpha^{m - k}(\\alpha^{k - 1} + 1) = \\alpha^m.$$ Next we show that $f_{m + k} = k + \\sum_{i = 0}^m f_i$. The base case is clear, since $f_k = f_0 + k = k + 1$. For the induction step, we have $$f_{m + k} = f_{m - 1 - k} + f_m = k \\sum_{i = 0}^{m - 1} f_i + f_m = k + \\sum_{i = 0}^m f_i.$$ Observe that $s_i \\ge f_{i + k}$ for $0 \\le i < k$. Again, by induction, for $m \\ge k$ we have $$s_m = k + \\sum_{i = 0}^{m - k} s_i \\ge k + \\sum_{i = 0}^{m - k} f_{i + k} \\ge k + \\sum_{i = 0}^m f_i = f_{m + k}.$$ So in general, $s_m \\ge f_{m + k}$. Putting it all together, we have $$ \\begin{aligned} size(x) & \\ge s_m \\\\ & \\ge k + \\sum_{i = k}^m s_{i - k} \\\\ & \\ge k + \\sum_{i = k}^m f_i \\\\ & \\ge f_{m + k} \\\\ & \\ge \\alpha^m. \\end{aligned} $$ Taking logs on both sides, we have $$\\log_\\alpha n \\ge m.$$ In other words, provided that $\\alpha$ is a constant, we have a logarithmic bound on the maximum degree.","title":"19.4-2"},{"location":"Chap19/Problems/19-1/","text":"Professor Pisano has proposed the following variant of the $\\text{FIB-HEAP-DELETE}$ procedure, claiming that it runs faster when the node being deleted is not the node pointed to by $H.min$. 1 2 3 4 5 6 7 8 9 PISANO - DELETE ( H , x ) if x == H . min FIB - HEAP - EXTRACT - MIN ( H ) else y = x . p if y != NIL CUT ( H , x , y ) CASCADING - CUT ( H , y ) add x ' s child list to the root list of H remove x from the root list of H a. The professor's claim that this procedure runs faster is based partly on the assumption that line 7 can be performed in $O(1)$ actual time. What is wrong with this assumption? b. Give a good upper bound on the actual time of $\\text{PISANO-DELETE}$ when $x$ is not $H.min$. Your bound should be in terms of $x.degree$ and the number $c$ of calls to the $\\text{CASCADING-CUT}$ procedure. c. Suppose that we call $\\text{PISANO-DELETE}(H, x)$, and let $H'$ be the Fibonacci heap that results. Assuming that node $x$ is not a root, bound the potential of $H'$ in terms of $x.degree$, $c$, $t(H)$, and $m(H)$. d. Conclude that the amortized time for $\\text{PISANO-DELETE}$ is asymptotically no better than for $\\text{FIB-HEAP-DELETE}$, evenwhen $x \\ne H.min$. a. It can take actual time proportional to the number of children that $x$ had because for each child, when placing it in the root list, their parent pointer needs to be updated to be $\\text{NIL}$ instead of $x$. b. Line 7 takes actual time bounded by $x.degree$ since updating each of the children of $x$ only takes constant time. So, if $c$ is the number of cascading cuts that are done, the actual cost is $O(c + x.degree)$. c. From the cascading cut, we marked at most one more node, so, $m(H') \\le 1 + m(H)$ regardless of the number of calls to cascading cut, because only the highest thing in the chain of calls actually goes from unmarked to marked. Also, the number of children increases by the number of children that $x$ had, that is $t(H') = x.degree + t(H)$. Putting these together, we get that $$\\Phi(H') \\le t(H) + x.degree + 2(1 + m(H)).$$ d. The asymptotic time is $$\\Theta(x.degree) = \\Theta(\\lg(n)),$$ which is the same asyptotic time that was required for the original deletion method.","title":"19-1 Alternative implementation of deletion"},{"location":"Chap19/Problems/19-2/","text":"The binomial tree $B_k$ is an ordered tree (see Section B.5.2) defined recursively. As shown in Figure 19.6(a), the binomial tree $B_0$ consists of a single node. The binomial tree $B_k$ consists of two binomial trees $B_{k - 1}$ that are linked together so that the root of one is the leftmost child of the root of the other. Figure 19.6(b) shows the binomial trees $B_0$ through $B_4$. a. Show that for the binomial tree $B_k$, there are $2^k$ nodes, the height of the tree is $k$, there are exactly $\\binom{k}{i}$ nodes at depth $i$ for $i = 0, 1, \\ldots, k$, and the root has degree $k$, which is greater than that of any other node; moreover, as Figure 19.6(c) shows, if we number the children of the root from left to right by $k - 1, k - 2, \\ldots, 0$, then child $i$ is the root of a subtree $B_i$. A binomial heap $H$ is a set of binomial trees that satisfies the following properties: Each node has a $key$ (like a Fibonacci heap). Each binomial tree in $H$ obeys the min-heap property. For any nonnegative integer $k$, there is at most one binomial tree in $H$ whose root has degree $k$. b. Suppose that a binomial heap $H$ has a total of $n$ nodes. Discuss the relationship between the binomial trees that $H$ contains and the binary representation of $n$. Conclude that $H$ consists of at most $\\lfloor \\lg n \\rfloor + 1$ binomial trees. Suppose that we represent a binomial heap as follows. The left-child, right-sibling scheme of Section 10.4 represents each binomial tree within a binomial heap. Each node contains its key; pointers to its parent, to its leftmost child, and to the sibling immediately to its right (these pointers are $\\text{NIL}$ when appropriate); and its degree (as in Fibonacci heaps, how many children it has). The roots form a singly linked root list, ordered by the degrees of the roots (from low to high), and we access the binomial heap by a pointer to the first node on the root list. c. Complete the description of how to represent a binomial heap (i.e., name the attributes, describe when attributes have the value $\\text{NIL}$, and define how the root list is organized), and show how to implement the same seven operations on binomial heaps as this chapter implemented on Fibonacci heaps. Each operation should run in $O(\\lg n)$ worst-case time, where $n$ is the number of nodes in the binomial heap (or in the case of the $\\text{UNION}$ operation, in the two binomial heaps that are being united). The $\\text{MAKE-HEAP}$ operation should take constant time. d. Suppose that we were to implement only the mergeable-heap operations on a Fibonacci heap (i.e., we do not implement the $\\text{DECREASE-KEY}$ or $\\text{DELETE}$ operations). How would the trees in a Fibonacci heap resemble those in a binomial heap? How would they differ? Show that the maximum degree in an $n$-node Fibonacci heap would be at most $\\lfloor \\lg n\\rfloor$. e. Professor McGee has devised a new data structure based on Fibonacci heaps. A McGee heap has the same structure as a Fibonacci heap and supports just the mergeable-heap operations. The implementations of the operations are the same as for Fibonacci heaps, except that insertion and union consolidate the root list as their last step. What are the worst-case running times of operations on McGee heaps? a. $B_k$ consists of two binomial trees $B_{k - 1}$. The height of one $B_{k - 1}$ is increased by $1$. For $i = 0$, $\\binom{k}{0} = 1$ and only root is at depth $0$. Suppose in $B_{k - 1}$, the number of nodes at depth $i$ is $\\binom{k - 1}{i}$, in $B_k$, the number of nodes at depth $i$ is $\\binom{k - 1}{i} + \\binom{k - 1}{i - 1} = \\binom{k}{i}$. The degree of the root increase by $1$. b. Let $n.b$ denote the binary expansion of $n$. The fact that we can have at most one of each binomial tree corresponds to the fact that we can have at most $1$ as any digit of $n.b$. Since each binomial tree has a size which is a power of $2$, the binomial trees required to represent n nodes are uniquely determined. We include $B_k$ if and only if the $k$th position of $n.b$ is $1$. Since the binary representation of $n$ has at most $\\lfloor \\lg n \\rfloor+ 1$ digits, this also bounds the number of trees which can be used to represent $n$ nodes. c. Given a node $x$, let $x.key$, $x.p$, $x.c$, and $x.s$ represent the attributes key, parent, left-most child, and sibling to the right, respectively. The pointer attributes have value $\\text{NIL}$ when no such node exists. The root list will be stored in a singly linked list. MAKE-HEAP initialize an empty list for the root list and return a pointer to the head of the list, which contains $\\text{NIL}$. This takes constant time. To insert: Let $x$ be a node with key $k$, to be inserted. Scan the root list to find the first $m$ such that $B_m$ is not one of the trees in the binomial heap. If there is no $B_0$, simply create a single root node $x$. Otherwise, union $x, B_0, B_1, \\ldots, B_{m - 1}$ into a $B_m$ tree. Remove all root nodes of the unioned trees from the root list, and update it with the new root. Since each join operation is logarithmic in the height of the tree, the total time is $O(\\lg n)$. $\\text{MINIMUM}$ just scans the root list and returns the minimum in $O(\\lg n)$, since the root list has size at most $O(\\lg n)$. EXTRACT-MIN: finds and deletes the minimum, then splits the tree Bm which contained the minimum into its component binomial trees $B_0, B_1, \\ldots, B_{m - 1}$ in $O(\\lg n)$ time. Finally, it unions each of these with any existing trees of the same size in $O(\\lg n)$ time. UNION: suppose we have two binomial heaps consisting of trees $B_{i_1}, B_{i_2}, \\ldots, B_{i_k}$ and $B_{j_1}, B_{j_2}, \\ldots, B_{j_m}$ respectively. Simply union orresponding trees of the same size between the two heaps, then do another check and join any newly created trees which have caused additional duplicates. Note: we will perform at most one union on any fixed size of binomial tree so the total running time is still logarithmic in $n$, where we assume that $n$ is sum of the sizes of the trees which we are unioning. DECREASE-KEY: simply swap the node whose key was decreased up the tree until it satisfies the min-heap property. DELETE: note that every binomial tree consists of two copies of a smaller binomial tree, so we can write the procedure recursively. If the tree is a single node, simply delete it. If we wish to delete from $B_k$, first split the tree into its constituent copies of $B_{k - 1}$, and recursively call delete on the copy of $B_{k - 1}$ which contains $x$. If this results in two binomial trees of the same size, simply union them. d. The Fibonacci heap will look like a binomial heap, except that multiple copies of a given binomial tree will be allowed. Since the only trees which will appear are binomial trees and $B_k$ has $2k$ nodes, we must have $2k \\le n$, which implies $k \\le \\lfloor \\lg n \\rfloor$. Since the largest root of any binomial tree occurs at the root, and on $B_k$ it is degree $k$, this also bounds the largest degree of a node. e. $\\text{INSERT}$ and $\\text{UNION}$ will no longer have amortized $O(1)$ running time because $\\text{CONSOLIDATE}$ has runtime $O(\\lg n)$. Even if no nodes are consolidated, the runtime is dominated by the check that all degrees are distinct. Since calling $\\text{UNION}$ on a heap and a single node is the same as insertion, it must also have runtime $O(\\lg n)$. The other operations remain unchanged.","title":"19-2 Binomial trees and binomial heaps"},{"location":"Chap19/Problems/19-3/","text":"We wish to augment a Fibonacci heap $H$ to support two new operations without changing the amortized running time of any other Fibonacci-heap operations. a. The operation $\\text{FIB-HEAP-CHANGE-KEY}(H, x, k)$ changes the key of node $x$ to the value $k$. Give an efficient implementation of $\\text{FIB-HEAP-CHANGE-KEY}$, and analyze the amortized running time of your implementation for the cases in which $k$ is greater than, less than, or equal to $x.key$. b. Give an efficient implementation of $\\text{FIB-HEAP-PRUNE}(H, r)$, which deletes $q = \\min(r, H.n)$ nodes from $H$. You may choose any $q$ nodes to delete. Analyze the amortized running time of your implementation. ($\\textit{Hint:}$ You may need to modify the data structure and potential function.) a. If $k < x.key$ just run the decrease key procedure. If $k > x.key$, delete the current value $x$ and insert $x$ again with a new key. For the first case, the amortized time is $O(1)$, and for the last case the amortized time is $O(\\lg n)$. b. Suppose that we also had an additional cost to the potential function that was proportional to the size of the structure. This would only increase when we do an insertion, and then only by a constant amount, so there aren't any worries concerning this increased potential function raising the amortized cost of any operations. Once we've made this modification, to the potential function, we also modify the heap itself by having a doubly linked list along all of the leaf nodes in the heap. To prune we then pick any leaf node, remove it from it's parent's child list, and remove it from the list of leaves. We repeat this $\\min(r, H.n)$ times. This causes the potential to drop by an amount proportional to $r$ which is on the order of the actual cost of what just happened since the deletions from the linked list take only constant amounts of time each. So, the amortized time is constant.","title":"19-3 More Fibonacci-heap operations"},{"location":"Chap19/Problems/19-4/","text":"Chapter 18 introduced the 2-3-4 tree, in which every internal node (other than possibly the root) has two, three, or four children and all leaves have the same depth. In this problem, we shall implement 2-3-4 heaps , which support the mergeable-heap operations. The 2-3-4 heaps differ from 2-3-4 trees in the following ways. In 2-3-4 heaps, only leaves store keys, and each leaf $x$ stores exactly one key in the attribute $x.key$. The keys in the leaves may appear in any order. Each internal node $x$ contains a value $x.small$ that is equal to the smallest key stored in any leaf in the subtree rooted at $x$. The root $r$ contains an attribute $r.height$ that gives the height of the tree. Finally, 2-3-4 heaps are designed to be kept in main memory, so that disk reads and writes are not needed. Implement the following 2-3-4 heap operations. In parts (a)\u2013(e), each operation should run in $O(\\lg n)$ time on a 2-3-4 heap with $n$ elements. The $\\text{UNION}$ operation in part (f) should run in $O(\\lg n)$ time, where $n$ is the number of elements in the two input heaps. a. $\\text{MINIMUM}$, which returns a pointer to the leaf with the smallest key. b. $\\text{DECREASE-KEY}$, which decreases the key of a given leaf $x$ to a given value $k \\le x.key$. c. $\\text{INSERT}$, which inserts leaf $x$ with key $k$. d. $\\text{DELETE}$, which deletes a given leaf $x$. e. $\\text{EXTRACT-MIN}$, which extracts the leaf with the smallest key. f. $\\text{UNION}$, which unites two 2-3-4 heaps, returning a single 2-3-4 heap and destroying the input heaps. a. Traverse a path from root to leaf as follows: At a given node, examine the attribute $x.small$ in each child-node of the current node. Proceed to the child node which minimizes this attribute. If the children of the current node are leaves, then simply return a pointer to the child node with smallest key. Since the height of the tree is $O(\\lg n)$ and the number of children of any node is at most $4$, this has runtime $O(\\lg n)$. b. Decrease the key of $x$, then traverse the simple path from $x$ to the root by following the parent pointers. At each node $y$ encountered, check the attribute $y.small$. If $k < y.small$, set $y.small = k$. Otherwise do nothing and continue on the path. c. Insert works the same as in a B-tree, except that at each node it is assumed that the node to be inserted is 'smaller' than every key stored at that node, so the runtime is inherited. If the root is split, we update the height of the tree. When we reach the final node before the leaves, simply insert the new node as the leftmost child of that node. d. As with $\\text{B-TREE-DELETE}$, we'll want to ensure that the tree satisfies the properties of being a 2-3-4 tree after deletion, so we'll need to check that we're never deleting a leaf which only has a single sibling. This is handled in much the same way as in chapter 18. We can imagine that dummy keys are stored in all the internal nodes, and carry out the deletion process in exactly the same way as done in exercise 18.3-2, with the added requirement that we update the height stored in the root if we merge the root with its child nodes. e. $\\text{EXTRACT-MIN}$ simply locates the minimum as done in part (a), then deletes it as in part (d). f. This can be done by implementing the join operation, as in Problem 18-2 (b).","title":"19-4 2-3-4 heaps"},{"location":"Chap20/20.1/","text":"20.1-1 Modify the data structures in this section to support duplicate keys. To modify these structure to allow for multiple elements, instead of just storing a bit in each of the entries, we can store the head of a linked list representing how many elements of that value that are contained in the structure, with a $\\text{NIL}$ value to represent having no elements of that value. 20.1-2 Modify the data structures in this section to support keys that have associated satellite data. All operations will remain the same, except instead of the leaves of the tree being an array of integers, they will be an array of nodes, each of which stores $x.key$ in addition to whatever additional satellite data you wish. 20.1-3 Observe that, using the structures in this section, the way we find the successor and predecessor of a value $x$ does not depend on whether $x$ is in the set at the time. Show how to find the successor of $x$ in a binary search tree when $x$ is not stored in the tree. To find the successor of a given key $k$ from a binary tree, call the procedure $\\text{SUCC}(x, T.root)$. Note that this will return $\\text{NIL}$ if there is no entry in the tree with a larger key. 20.1-4 Suppose that instead of superimposing a tree of degree $\\sqrt u$, we were to superimpose a tree of degree $u^{1 / k}$, where $k > 1$ is a constant. What would be the height of such a tree, and how long would each of the operations take? The new tree would have height $k$. $\\text{INSERT}$ would take $O(k)$, $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, $\\text{PREDECESSOR}$, and $\\text{DELETE}$ would take $O(ku^{1 / k})$.","title":"20.1 Preliminary approaches"},{"location":"Chap20/20.1/#201-1","text":"Modify the data structures in this section to support duplicate keys. To modify these structure to allow for multiple elements, instead of just storing a bit in each of the entries, we can store the head of a linked list representing how many elements of that value that are contained in the structure, with a $\\text{NIL}$ value to represent having no elements of that value.","title":"20.1-1"},{"location":"Chap20/20.1/#201-2","text":"Modify the data structures in this section to support keys that have associated satellite data. All operations will remain the same, except instead of the leaves of the tree being an array of integers, they will be an array of nodes, each of which stores $x.key$ in addition to whatever additional satellite data you wish.","title":"20.1-2"},{"location":"Chap20/20.1/#201-3","text":"Observe that, using the structures in this section, the way we find the successor and predecessor of a value $x$ does not depend on whether $x$ is in the set at the time. Show how to find the successor of $x$ in a binary search tree when $x$ is not stored in the tree. To find the successor of a given key $k$ from a binary tree, call the procedure $\\text{SUCC}(x, T.root)$. Note that this will return $\\text{NIL}$ if there is no entry in the tree with a larger key.","title":"20.1-3"},{"location":"Chap20/20.1/#201-4","text":"Suppose that instead of superimposing a tree of degree $\\sqrt u$, we were to superimpose a tree of degree $u^{1 / k}$, where $k > 1$ is a constant. What would be the height of such a tree, and how long would each of the operations take? The new tree would have height $k$. $\\text{INSERT}$ would take $O(k)$, $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, $\\text{PREDECESSOR}$, and $\\text{DELETE}$ would take $O(ku^{1 / k})$.","title":"20.1-4"},{"location":"Chap20/20.2/","text":"20.2-1 Write pseudocode for the procedures $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$. See the two algorithms, $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$. 20.2-2 Write pseudocode for $\\text{PROTO-vEB-DELETE}$. It should update the appropriate summary bit by scanning the related bits within the cluster. What is the worst-case running time of your procedure? 1 2 3 4 5 6 7 8 9 10 11 12 PROTO - vEB - DELETE ( V , x ) if V . u == 2 V . A [ x ] = 0 else PROTO - vEB - DELETE ( V . cluster [ x . high ], x . low ) inCluster = false for i = x . high * sqrt ( u ) to ( x . high + 1 ) * sqrt ( u ) - 1 if PROTO - vEB - MEMBER ( V . cluster [ x . high ], i ) inCluster = true break if inCluster == false PROTO - vEB - DELETE ( V . summary , x . high ) When we delete a key, we need to check membership of all keys of that cluster to know how to update the summary structure. There are $\\sqrt u$ of these, and each membership takes $O(\\lg\\lg u)$ time to check. With the recursive calls, recurrence for running time is $$T(u) = T(\\sqrt u) + O(\\sqrt u\\lg\\lg u).$$ We make the substitution $m = \\lg u$ and $S(m) = T(2^m)$. Then we apply the Master Theorem, using case 3, to solve the recurrence. Substituting back, we find that the runtime is $T(u) = O(\\sqrt u\\lg\\lg u)$. 20.2-3 Add the attribute $n$ to each $\\text{proto-vEB}$ structure, giving the number of elements currently in the set it represents, and write pseudocode for $\\text{PROTO-vEB-DELETE}$ that uses the attribute $n$ to decide when to reset summary bits to $0$. What is the worst-case running time of your procedure? What other procedures need to change because of the new attribute? Do these changes affect their running times? We would keep the same as before, but insert immediately after the else, a check of whether $n = 1$. If it doesn't continue as usual, but if it does, then we can just immediately set the summary bit to $0$, null out the pointer in the table, and be done immediately. This has the upside that it can sometimes save up to $\\lg\\lg u$. The procedure has the big downside that the number of elements that are in the set could be as high as $\\lg(\\lg u)$, in which case $\\lg u$ many bits are needed to store $n$. 20.2-4 Modify the $\\text{proto-vEB}$ structure to support duplicate keys. The array $A$ found in a proto van Emde Boas structure of size $2$ should now support integers, instead of just bits. All other pats of the structure will remain the same. The integer will store the number of duplicates at that position. The modifications to insert, delete, minimum, successor, etc will be minor. Only the base cases will need to be updated. 1 2 3 4 5 6 7 8 9 10 11 12 PROTO - vEB - DELETE ( V , x ) if V . u == 2 V . A [ x ] = 0 else PROTO - vEB - DELETE ( V . cluster [ x . high ], x . low ) inCluster = false for i = x . high * sqrt ( u ) to ( x . high + 1 ) * sqrt ( u ) - 1 if PROTO - vEB - MEMBER ( V . cluster [ x . high ], i ) inCluster = true break if inCluster = false PROTO - vEB - DELETE ( V . summary , x . high ) 20.2-5 Modify the $\\text{proto-vEB}$ structure to support keys that have associated satellite data. The only modification necessary would be for the $u = 2$ trees. They would need to also include a length two array that had pointers to the corresponding satellite data which would be populated in case the corresponding entry in $A$ were $1$. 20.2-6 Write pseudocode for a procedure that creates a $\\text{proto-vEB}(u)$ structure. This algorithm recursively allocates proper space and appropriately initializes attributes for a proto van Emde Boas structure of size $u$. 1 2 3 4 5 6 7 8 9 10 MAKE - PROTO - vEB ( u ) allocate a new vEB tree V V . u = u if u == 2 let A be an array of size 2 V . A [ 1 ] = V . A [ 0 ] = 0 else V . summary = MAKE - PROTO - vEB ( sqrt ( u )) for i = 0 to sqrt ( u ) - 1 V . cluster [ i ] = MAKE - PROTO - vEB ( sqrt ( u )) 20.2-7 Argue that if line 9 of $\\text{PROTO-vEB-MINIMUM}$ is executed, then the $\\text{proto-vEB}$ structure is empty. For line 9 to be executed, we would need that in the summary data, we also had a $\\text{NIL}$ returned. This could of either happened through line 9, or 6. Eventually though, it would need to happen in line 6, so, there must be some number of summarizations that happened of $V$ that caused us to get an empty $u = 2$ $\\text{vEB}$. However, a summarization has an entry of one if any of the corresponding entries in the data structure are one. This means that there are no entries in $V$, and so, we have that $V$ is empty. 20.2-8 Suppose that we designed a $\\text{proto-vEB}$ structure in which each cluster array had only $u^{1 / 4}$ elements. What would the running times of each operation be? There are $u^{3 / 4}$ clusters in each $\\text{proto-vEB}$. MEMBER/INSERT: $$T(u) = T(u^{1 / 4}) + O(1) = \\Theta(\\lg\\log_4 u) = \\Theta(\\lg\\lg u).$$ MINIMUM/MAXIMUM: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + O(1) = \\Theta(\\lg u).$$ SUCCESSOR/PREDECESSOR/DELETE: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + \\Theta(\\lg u^{1 / 4}) = \\Theta(\\lg u \\lg\\lg u).$$","title":"20.2 A recursive structure"},{"location":"Chap20/20.2/#202-1","text":"Write pseudocode for the procedures $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$. See the two algorithms, $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$.","title":"20.2-1"},{"location":"Chap20/20.2/#202-2","text":"Write pseudocode for $\\text{PROTO-vEB-DELETE}$. It should update the appropriate summary bit by scanning the related bits within the cluster. What is the worst-case running time of your procedure? 1 2 3 4 5 6 7 8 9 10 11 12 PROTO - vEB - DELETE ( V , x ) if V . u == 2 V . A [ x ] = 0 else PROTO - vEB - DELETE ( V . cluster [ x . high ], x . low ) inCluster = false for i = x . high * sqrt ( u ) to ( x . high + 1 ) * sqrt ( u ) - 1 if PROTO - vEB - MEMBER ( V . cluster [ x . high ], i ) inCluster = true break if inCluster == false PROTO - vEB - DELETE ( V . summary , x . high ) When we delete a key, we need to check membership of all keys of that cluster to know how to update the summary structure. There are $\\sqrt u$ of these, and each membership takes $O(\\lg\\lg u)$ time to check. With the recursive calls, recurrence for running time is $$T(u) = T(\\sqrt u) + O(\\sqrt u\\lg\\lg u).$$ We make the substitution $m = \\lg u$ and $S(m) = T(2^m)$. Then we apply the Master Theorem, using case 3, to solve the recurrence. Substituting back, we find that the runtime is $T(u) = O(\\sqrt u\\lg\\lg u)$.","title":"20.2-2"},{"location":"Chap20/20.2/#202-3","text":"Add the attribute $n$ to each $\\text{proto-vEB}$ structure, giving the number of elements currently in the set it represents, and write pseudocode for $\\text{PROTO-vEB-DELETE}$ that uses the attribute $n$ to decide when to reset summary bits to $0$. What is the worst-case running time of your procedure? What other procedures need to change because of the new attribute? Do these changes affect their running times? We would keep the same as before, but insert immediately after the else, a check of whether $n = 1$. If it doesn't continue as usual, but if it does, then we can just immediately set the summary bit to $0$, null out the pointer in the table, and be done immediately. This has the upside that it can sometimes save up to $\\lg\\lg u$. The procedure has the big downside that the number of elements that are in the set could be as high as $\\lg(\\lg u)$, in which case $\\lg u$ many bits are needed to store $n$.","title":"20.2-3"},{"location":"Chap20/20.2/#202-4","text":"Modify the $\\text{proto-vEB}$ structure to support duplicate keys. The array $A$ found in a proto van Emde Boas structure of size $2$ should now support integers, instead of just bits. All other pats of the structure will remain the same. The integer will store the number of duplicates at that position. The modifications to insert, delete, minimum, successor, etc will be minor. Only the base cases will need to be updated. 1 2 3 4 5 6 7 8 9 10 11 12 PROTO - vEB - DELETE ( V , x ) if V . u == 2 V . A [ x ] = 0 else PROTO - vEB - DELETE ( V . cluster [ x . high ], x . low ) inCluster = false for i = x . high * sqrt ( u ) to ( x . high + 1 ) * sqrt ( u ) - 1 if PROTO - vEB - MEMBER ( V . cluster [ x . high ], i ) inCluster = true break if inCluster = false PROTO - vEB - DELETE ( V . summary , x . high )","title":"20.2-4"},{"location":"Chap20/20.2/#202-5","text":"Modify the $\\text{proto-vEB}$ structure to support keys that have associated satellite data. The only modification necessary would be for the $u = 2$ trees. They would need to also include a length two array that had pointers to the corresponding satellite data which would be populated in case the corresponding entry in $A$ were $1$.","title":"20.2-5"},{"location":"Chap20/20.2/#202-6","text":"Write pseudocode for a procedure that creates a $\\text{proto-vEB}(u)$ structure. This algorithm recursively allocates proper space and appropriately initializes attributes for a proto van Emde Boas structure of size $u$. 1 2 3 4 5 6 7 8 9 10 MAKE - PROTO - vEB ( u ) allocate a new vEB tree V V . u = u if u == 2 let A be an array of size 2 V . A [ 1 ] = V . A [ 0 ] = 0 else V . summary = MAKE - PROTO - vEB ( sqrt ( u )) for i = 0 to sqrt ( u ) - 1 V . cluster [ i ] = MAKE - PROTO - vEB ( sqrt ( u ))","title":"20.2-6"},{"location":"Chap20/20.2/#202-7","text":"Argue that if line 9 of $\\text{PROTO-vEB-MINIMUM}$ is executed, then the $\\text{proto-vEB}$ structure is empty. For line 9 to be executed, we would need that in the summary data, we also had a $\\text{NIL}$ returned. This could of either happened through line 9, or 6. Eventually though, it would need to happen in line 6, so, there must be some number of summarizations that happened of $V$ that caused us to get an empty $u = 2$ $\\text{vEB}$. However, a summarization has an entry of one if any of the corresponding entries in the data structure are one. This means that there are no entries in $V$, and so, we have that $V$ is empty.","title":"20.2-7"},{"location":"Chap20/20.2/#202-8","text":"Suppose that we designed a $\\text{proto-vEB}$ structure in which each cluster array had only $u^{1 / 4}$ elements. What would the running times of each operation be? There are $u^{3 / 4}$ clusters in each $\\text{proto-vEB}$. MEMBER/INSERT: $$T(u) = T(u^{1 / 4}) + O(1) = \\Theta(\\lg\\log_4 u) = \\Theta(\\lg\\lg u).$$ MINIMUM/MAXIMUM: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + O(1) = \\Theta(\\lg u).$$ SUCCESSOR/PREDECESSOR/DELETE: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + \\Theta(\\lg u^{1 / 4}) = \\Theta(\\lg u \\lg\\lg u).$$","title":"20.2-8"},{"location":"Chap20/20.3/","text":"20.3-1 Modify vEB trees to support duplicate keys. To support duplicate keys, for each $u = 2$ vEB tree, instead of storing just a bit in each of the entries of its array, it should store an integer representing how many elements of that value the vEB contains. 20.3-2 Modify vEB trees to support keys that have associated satellite data. For any key which is a minimum on some vEB, we'll need to store its satellite data with the min value since the key doesn't appear in the subtree. The rest of the satellite data will be stored alongside the keys of the vEB trees of size $2$. Explicitly, for each non-summary vEB tree, store a pointer in addition to min. If min is $\\text{NIL}$, the pointer should also point to $\\text{NIL}$. Otherwise, the pointer should point to the satellite data associated with that minimum. In a size $2$ vEB tree, we'll have two additional pointers, which will each point to the minimum's and maximum's satellite data, or $\\text{NIL}$ if these don't exist. In the case where $\\min = \\max$, the pointers will point to the same data. 20.3-3 Write pseudocode for a procedure that creates an empty van Emde Boas tree. We define the procedure for any $u$ that is a power of $2$. If $u = 2$, then, just slap that fact together with an array of length $2$ that contains $0$ in both entries. If $u = 2k > 2$, then, we create an empty vEB tree called Summary with $u = 2^{\\lceil k / 2 \\rceil}$. We also make an array called cluster of length $2^{\\lceil k / 2 \\rceil}$ with each entry initialized to an empty vEB tree with $u = 2^{\\lfloor k / 2 \\rfloor}$. Lastly, we create a min and max element, both initialized to $\\text{NIL}$. 20.3-4 What happens if you call $\\text{VEB-TREE-INSERT}$ with an element that is already in the vEB tree? What happens if you call $\\text{VEB-TREE-DELETE}$ with an element that is not in the vEB tree? Explain why the procedures exhibit the behavior that they do. Show how to modify vEB trees and their operations so that we can check in constant time whether an element is present. Suppose that $x$ is already in $V$ and we call $\\text{INSERT}$. Then we can't satisfy lines 1, 3, 6, or 10, so we will enter the else case on line 9 every time, causing an infinite loop. Now suppose we call $\\text{DELETE}$ when $x$ isn't in $V$ . If there is only a single element in $V$, lines 1 through 3 will delete it, regardless of what element it is. To enter the elseif of line 4, $x$ can't be equal to $0$ or $1$ and the vEB tree must be of size $2$. In this case, we delete the max element, regardless of what it is. Since the recursive call always puts us in this case, we always delete an element we shouldn't. To avoid these issue, keep and updated auxiliary array $A$ with $u$ elements. Set $A[i] = 0$ if $i$ is not in the tree, and $1$ if it is. Since we can perform constant time updates to this array, it won't affect the runtime of any of our operations. When inserting $x$, check first to be sure $A[x] = 0$. If it's not, simply return. If it is, set $A[x] = 1$ and proceed with insert as usual. When deleting $x$, check if $A[x] = 1$. If it isn't, simply return. If it is, set $A[x] = 0$ and proceed with delete as usual. 20.3-5 Suppose that instead of $\\sqrt[\\uparrow]u$ clusters, each with universe size $\\sqrt[\\downarrow]u$, we constructed vEB trees to have $u^{1 / k}$ clusters, each with universe size $u^{1 - 1 / k}$, where $k > 1$ is a constant. If we were to modify the operations appropriately, what would be their running times? For the purpose of analysis, assume that $u^{1 / k}$ and $u^{1 - 1 / k}$ are always integers. Similar to the analysis of $\\text{(20.4)}$, we will analyze $$T(u) \\le T(u^{1 - 1 / k}) + T(u^{1 / k}) + O(1).$$ This is a good choice for analysis because for many operations we first check the summary vEB tree, which will have size $u^{1 / k}$ (the second term). And then possible have to check a vEB tree somewhere in cluster, which will have size $u^{1 - 1/k}$ (the first term). We let $T(2^m) = S(m)$, so the equation becomes $$S(m) \\le S(m(1 - 1/k)) + S(m/k) + O(1).$$ If $k > 2$ the first term dominates, so by master theorem, we'll have that $S(m)$ is $O(\\lg m)$, this means that T will be $O(\\lg(\\lg u))$ just as in the original case where we took squareroots. 20.3-6 Creating a vEB tree with universe size $u$ requires $O(u)$ time. Suppose we wish to explicitly account for that time. What is the smallest number of operations $n$ for which the amortized time of each operation in a vEB tree is $O(\\lg\\lg u)$? Set $n = u / \\lg\\lg u$. Then performing $n$ operations takes $c(u + n\\lg\\lg u)$ time for some constant $c$. Using the aggregate amortized analysis, we divide by $n$ to see that the amortized cost of each operations is $c(\\lg\\lg u + \\lg\\lg u) = O(\\lg\\lg u)$ per operation. Thus we need $n \\ge u/ \\lg \\lg u$.","title":"20.3 The van Emde Boas tree"},{"location":"Chap20/20.3/#203-1","text":"Modify vEB trees to support duplicate keys. To support duplicate keys, for each $u = 2$ vEB tree, instead of storing just a bit in each of the entries of its array, it should store an integer representing how many elements of that value the vEB contains.","title":"20.3-1"},{"location":"Chap20/20.3/#203-2","text":"Modify vEB trees to support keys that have associated satellite data. For any key which is a minimum on some vEB, we'll need to store its satellite data with the min value since the key doesn't appear in the subtree. The rest of the satellite data will be stored alongside the keys of the vEB trees of size $2$. Explicitly, for each non-summary vEB tree, store a pointer in addition to min. If min is $\\text{NIL}$, the pointer should also point to $\\text{NIL}$. Otherwise, the pointer should point to the satellite data associated with that minimum. In a size $2$ vEB tree, we'll have two additional pointers, which will each point to the minimum's and maximum's satellite data, or $\\text{NIL}$ if these don't exist. In the case where $\\min = \\max$, the pointers will point to the same data.","title":"20.3-2"},{"location":"Chap20/20.3/#203-3","text":"Write pseudocode for a procedure that creates an empty van Emde Boas tree. We define the procedure for any $u$ that is a power of $2$. If $u = 2$, then, just slap that fact together with an array of length $2$ that contains $0$ in both entries. If $u = 2k > 2$, then, we create an empty vEB tree called Summary with $u = 2^{\\lceil k / 2 \\rceil}$. We also make an array called cluster of length $2^{\\lceil k / 2 \\rceil}$ with each entry initialized to an empty vEB tree with $u = 2^{\\lfloor k / 2 \\rfloor}$. Lastly, we create a min and max element, both initialized to $\\text{NIL}$.","title":"20.3-3"},{"location":"Chap20/20.3/#203-4","text":"What happens if you call $\\text{VEB-TREE-INSERT}$ with an element that is already in the vEB tree? What happens if you call $\\text{VEB-TREE-DELETE}$ with an element that is not in the vEB tree? Explain why the procedures exhibit the behavior that they do. Show how to modify vEB trees and their operations so that we can check in constant time whether an element is present. Suppose that $x$ is already in $V$ and we call $\\text{INSERT}$. Then we can't satisfy lines 1, 3, 6, or 10, so we will enter the else case on line 9 every time, causing an infinite loop. Now suppose we call $\\text{DELETE}$ when $x$ isn't in $V$ . If there is only a single element in $V$, lines 1 through 3 will delete it, regardless of what element it is. To enter the elseif of line 4, $x$ can't be equal to $0$ or $1$ and the vEB tree must be of size $2$. In this case, we delete the max element, regardless of what it is. Since the recursive call always puts us in this case, we always delete an element we shouldn't. To avoid these issue, keep and updated auxiliary array $A$ with $u$ elements. Set $A[i] = 0$ if $i$ is not in the tree, and $1$ if it is. Since we can perform constant time updates to this array, it won't affect the runtime of any of our operations. When inserting $x$, check first to be sure $A[x] = 0$. If it's not, simply return. If it is, set $A[x] = 1$ and proceed with insert as usual. When deleting $x$, check if $A[x] = 1$. If it isn't, simply return. If it is, set $A[x] = 0$ and proceed with delete as usual.","title":"20.3-4"},{"location":"Chap20/20.3/#203-5","text":"Suppose that instead of $\\sqrt[\\uparrow]u$ clusters, each with universe size $\\sqrt[\\downarrow]u$, we constructed vEB trees to have $u^{1 / k}$ clusters, each with universe size $u^{1 - 1 / k}$, where $k > 1$ is a constant. If we were to modify the operations appropriately, what would be their running times? For the purpose of analysis, assume that $u^{1 / k}$ and $u^{1 - 1 / k}$ are always integers. Similar to the analysis of $\\text{(20.4)}$, we will analyze $$T(u) \\le T(u^{1 - 1 / k}) + T(u^{1 / k}) + O(1).$$ This is a good choice for analysis because for many operations we first check the summary vEB tree, which will have size $u^{1 / k}$ (the second term). And then possible have to check a vEB tree somewhere in cluster, which will have size $u^{1 - 1/k}$ (the first term). We let $T(2^m) = S(m)$, so the equation becomes $$S(m) \\le S(m(1 - 1/k)) + S(m/k) + O(1).$$ If $k > 2$ the first term dominates, so by master theorem, we'll have that $S(m)$ is $O(\\lg m)$, this means that T will be $O(\\lg(\\lg u))$ just as in the original case where we took squareroots.","title":"20.3-5"},{"location":"Chap20/20.3/#203-6","text":"Creating a vEB tree with universe size $u$ requires $O(u)$ time. Suppose we wish to explicitly account for that time. What is the smallest number of operations $n$ for which the amortized time of each operation in a vEB tree is $O(\\lg\\lg u)$? Set $n = u / \\lg\\lg u$. Then performing $n$ operations takes $c(u + n\\lg\\lg u)$ time for some constant $c$. Using the aggregate amortized analysis, we divide by $n$ to see that the amortized cost of each operations is $c(\\lg\\lg u + \\lg\\lg u) = O(\\lg\\lg u)$ per operation. Thus we need $n \\ge u/ \\lg \\lg u$.","title":"20.3-6"},{"location":"Chap20/Problems/20-1/","text":"This problem explores the space requirements for van Emde Boas trees and suggests a way to modify the data structure to make its space requirement depend on the number $n$ of elements actually stored in the tree, rather than on the universe size $u$. For simplicity, assume that $\\sqrt u$ is always an integer. a. Explain why the following recurrence characterizes the space requirement $P(u)$ of a van Emde Boas tree with universe size $u$: $$P(u) = (\\sqrt u + 1) P(\\sqrt u) + \\Theta(\\sqrt u). \\tag{20.5}$$ b. Prove that recurrence $\\text{(20.5)}$ has the solution $P(u) = O(u)$. In order to reduce the space requirements, let us define a reduced-space van Emde Boas tree , or RS-vEB tree , as a vEB tree $V$ but with the following changes: The attribute $V.cluster$, rather than being stored as a simple array of pointers to vEB trees with universe size $\\sqrt u$, is a hash table (see Chapter 11) stored as a dynamic table (see Section 17.4). Corresponding to the array version of $V.cluster$, the hash table stores pointers to RS-vEB trees with universe size $\\sqrt u$. To find the $i$th cluster, we look up the key $i$ in the hash table, so that we can find the $i$th cluster by a single search in the hash table. The hash table stores only pointers to nonempty clusters. A search in the hash table for an empty cluster returns $\\text{NIL}$, indicating that the cluster is empty. The attribute $V.summary$ is $\\text{NIL}$ if all clusters are empty. Otherwise, $V.summary$ points to an RS-vEB tree with universe size $\\sqrt u$. Because the hash table is implemented with a dynamic table, the space it requires is proportional to the number of nonempty clusters. When we need to insert an element into an empty RS-vEB tree, we create the RS-vEB tree by calling the following procedure, where the parameter u is the universe size of the RS-vEB tree: 1 2 3 4 5 6 7 8 CREATE - NEW - RS - vEB - TREE ( u ) allocate a new vEB tree V V . u = u V . min = NIL V . max = NIL V . summary = NIL create V . cluster as an empty dynamic hash table return V c. Modify the $\\text{VEB-TREE-INSERT}$ procedure to produce pseudocode for the procedure $\\text{RS-VEB-TREE-INSERT}(V, x)$, which inserts $x$ into the RS-vEB tree $V$, calling $\\text{CREATE-NEW-RS-VEB-TREE}$ as appropriate. d. Modify the $\\text{VEB-TREE-SUCCESSOR}$ procedure to produce pseudocode for the procedure $\\text{RS-VEB-TREE-SUCCESSOR}(V, x)$, which returns the successor of $x$ in RS-vEB tree $V$, or $\\text{NIL}$ if $x$ has no successor in $V$. e. Prove that, under the assumption of simple uniform hashing, your $\\text{RS-VEBTREE-INSERT}$ and $\\text{RS-VEB-TREE-SUCCESSOR}$ procedures run in $O(\\lg\\lg u)$ expected time. f. Assuming that elements are never deleted from a vEB tree, prove that the space requirement for the RS-vEB tree structure is $O(n)$, where $n$ is the number of elements actually stored in the RS-vEB tree. g. RS-vEB trees have another advantage over vEB trees: they require less time to create. How long does it take to create an empty RS-vEB tree? a. Lets look at what has to be stored for a vEB tree. Each vEB tree contains one vEB tree of size $\\sqrt[+]u$ and $\\sqrt[+]u$ vEB trees of size $\\sqrt[1]u$. It also is storing three numbers each of order $O(u)$, so they need $\\Theta(\\lg(u))$ space each. Lastly, it needs to store $\\sqrt u$ many pointers to the cluster vEB trees. We'll combine these last two contributions which are $\\Theta(\\lg(u))$ and $\\Theta(\\sqrt u)$ respectively into a single term that is $\\Theta(\\sqrt u)$. This gets us the recurrence $$P(u) = P(\\sqrt[+]u) + \\sqrt[+]u P(\\sqrt[-]u) + \\Theta(\\sqrt u).$$ Then, we have that $u = 2^{2m}$ (which follows from the assumption that $\\sqrt u$ was an integer), this equation becomes $$ \\begin{aligned} P(u) & = (1 + 2^m)P(2^m) + \\Theta(\\sqrt u) \\\\ & = (1 + \\sqrt u)P(\\sqrt u) + \\Theta(\\sqrt u) \\end{aligned} $$ as desired. b. We recall from our solution to problem 3-6.e (it seems like so long ago now) that given a number $n$, a bound on the number of times that we need to take the squareroot of a number before it falls below $2$ is $\\lg\\lg n$. So, if we just unroll out recurrence, we get that $$P(u) \\le \\Big(\\prod_{i = 1}^{\\lg\\lg u}(u^{1 / 2^i} + 1) \\Big) P(2) + \\sum_{i = 1}^{\\lg\\lg u} \\Theta(u^{1 / 2^i})(u^{1 / 2i} + 1).$$ The first product has a highest power of $u$ corresponding to always multiplying the first terms of each binomial. The power in this term is equal to $\\sum_{i = 1}^{\\lg\\lg u}$ which is a partial sum of a geometric series whose sum is $1$. This means that the first term is $o(u)$. The order of the ith term in the summation appearing in the formula is $u^{2 / 2^i}$. In particular, for $i = 1$ is it $O(u)$, and for any $i > 1$, we have that $2 / 2^i < 1$, so those terms will be $o(u)$. Putting it all together, the largest term appearing is $O(u)$, and so, $P(u)$ is $O(u)$. c. For this problem we just use the version written for normal vEB trees, with minor modifications. That is, since there are entries in cluster that may not exist, and summary may of not yet been initialized, just before we try to access either, we check to see if it's initialized. If it isn't, we do so then. d. As in the previous problem, we just wait until just before either of the two things that may of not been allocated try to get used then allocate them if need be. e. Since the initialization performed only take constant time, those modifications don't ruin the the desired runtime bound for the original algorithms already had. So, our responses to parts (c) and (d) are $O(\\lg\\lg n)$. f. As mentioned in the errata, this part should instead be changed to $O(n\\lg n)$ space. When we are adding an element, we may have to add an entry to a dynamic hash table, which means that a constant amount of extra space would be needed. If we are adding an element to that table, we also have to add an element to the RS-vEB tree in the summary, but the entry that we add in the cluster will be a constant size RS-vEB tree. We can charge the cost of that addition to the summary table to the making the minimum element entry that we added in the cluster table. Since we are always making at least one element be added as a new min entry somewhere, this amortization will mean that it is only a constant amount of time in order to store the new entry. g. It only takes a constant amount of time to create an empty RS-vEB tree. This is immediate since the only dependence on $u$ in $\\text{CREATE-NEW-RSvEB-TREE}(u)$ is on line 2 when $V.u$ is initialized, but this only takes a constant amount of time. Since nothing else in the procedure depends on $u$, it must take a constant amount of time.","title":"20-1 Space requirements for van Emde Boas trees"},{"location":"Chap20/Problems/20-2/","text":"This problem investigates D. Willard's \"$y$-fast tries\" which, like van Emde Boas trees, perform each of the operations $\\text{MEMBER}$, $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{PREDECESSOR}$, and $\\text{SUCCESSOR}$ on elements drawn from a universe with size $u$ in $O(\\lg\\lg u)$ worst-case time. The $\\text{INSERT}$ and $\\text{DELETE}$ operations take $O(\\lg\\lg u)$ amortized time. Like reduced-space van Emde Boas trees (see Problem 20-1), $y$-fast tries use only $O(n)$ space to store $n$ elements. The design of $y$-fast tries relies on perfect hashing (see Section 11.5). As a preliminary structure, suppose that we create a perfect hash table containing not only every element in the dynamic set, but every prefix of the binary representation of every element in the set. For example, if $u = 16$, so that $\\lg u = 4$, and $x = 13$ is in the set, then because the binary representation of $13$ is $1101$, the perfect hash table would contain the strings $1$, $11$, $110$, and $1101$. In addition to the hash table, we create a doubly linked list of the elements currently in the set, in increasing order. a. How much space does this structure require? b. Show how to perform the $\\text{MINIMUM}$ and $\\text{MAXIMUM}$ operations in $O(1)$ time; the $\\text{MEMBER}$, $\\text{PREDECESSOR}$, and $\\text{SUCCESSOR}$ operations in $O(\\lg\\lg u)$ time; and the $\\text{INSERT}$ and $\\text{DELETE}$ operations in $O(\\lg u)$ time. To reduce the space requirement to $O(n)$, we make the following changes to the data structure: We cluster the $n$ elements into $n / \\lg u$ groups of size $\\lg u$. (Assume for now that $\\lg u$ divides $n$.) The first group consists of the $\\lg u$ smallest elements in the set, the second group consists of the next $\\lg u$ smallest elements, and so on. We designate a \"representative\" value for each group. The representative of the $i$th group is at least as large as the largest element in the $i$th group, and it is smaller than every element of the $(i + 1)$st group. (The representative of the last group can be the maximum possible element $u - 1$.) Note that a representative might be a value not currently in the set. We store the $\\lg u$ elements of each group in a balanced binary search tree, such as a red-black tree. Each representative points to the balanced binary search tree for its group, and each balanced binary search tree points to its group's representative. The perfect hash table stores only the representatives, which are also stored in a doubly linked list in increasing order. We call this structure a $y$-fast trie . c. Show that a $y$-fast trie requires only $O(n)$ space to store $n$ elements. d. Show how to perform the $\\text{MINIMUM}$ and $\\text{MAXIMUM}$ operations in $O(\\lg\\lg u)$ time with a $y$-fast trie. e. Show how to perform the $\\text{MEMBER}$ operation in $O(\\lg\\lg u)$ time. f. Show how to perform the $\\text{PREDECESSOR}$ and $\\text{SUCCESSOR}$ operations in $O(\\lg\\lg u)$ time. g. Explain why the $\\text{INSERT}$ and $\\text{DELETE}$ operations take $\\Omega(\\lg\\lg u)$ time. h. Show how to relax the requirement that each group in a $y$-fast trie has exactly $\\lg u$ elements to allow $\\text{INSERT}$ and $\\text{DELETE}$ to run in $O(\\lg\\lg u)$ amortized time without affecting the asymptotic running times of the other operations. a. By 11.5, the perfect hash table uses $O(m)$ space to store m elements. In a universe of size $u$, each element contributes $\\lg u$ entries to the hash table, so the requirement is $O(n\\lg u)$. Since the linked list requires $O(n)$, the total space requirement is $O(n\\lg u)$. b. $\\text{MINIMUM}$ and $\\text{MAXIMUM}$ are easy. We just examine the first and last elements of the associated doubly linked list. $\\text{MEMBER}$ can actually be performed in $O(1)$, since we are simply checking membership in a perfect hash table. $\\text{PREDECESSOR}$ and $\\text{SUCCESSOR}$ are a bit more complicated. Assume that we have a binary tree in which we store all the elements and their prefixes. When we query the hash table for an element, we get a pointer to that element's location in the binary search tree, if the element is in the tree, and $\\text{NIL}$ otherwise. Moreover, assume that every leaf node comes with a pointer to its position in the doubly linked list. Let $x$ be the number whose successor we seek. Begin by performing a binary search of the prefixes in the hash table to find the longest hashed prefix $y$ which matches a prefix of $x$. This takes $O(\\lg\\lg u)$ since we can check if any prefix is in the hash table in $O(1)$. Observe that $y$ can have at most one child in the BST, because if it had both children then one of these would share a longer prefix with $x$. If the left child is missing, have the left child pointer point to the largest labeled leaf node in the BST which is less than $y$. If the right child is missing, use its pointer to point to the successor of $y$. If $y$ is a leaf node then $y = x$, so we simply follow the pointer to $x$ in the doubly linked list, in $O(1)$, and its successor is the next element on the list. If $y$ is not a leaf node, we follow its predecessor or successor node, depending on which we need. This gives us $O(1)$ access to the proper element, so the total runtime is $O(\\lg\\lg u)$. $\\text{INSERT}$ and $\\text{DELETE}$ must take $O(\\lg u)$ since we need to insert one entry into the hash table for each of their bits and update the pointers. c. The doubly linked list has less than $n$ elements, while the binary search trees contains $n$ nodes, thus a $y$-fast trie requires $O(n)$ space. d. $\\text{MINIMUM}$: Find the minimum representative in the doubly linked list in $\\Theta(1)$, then find the minimum element in the binary search tree in $O(\\lg\\lg u)$. e. Find the smallest representative greater than $k$ with binary searching in $\\Theta(\\lg\\lg u)$, find the element in the binary search tree in $O(\\lg\\lg u)$. f. If we can find the largest representative greater than or equal to $x$, we can determine which binary tree contains the predecessor or successor of $x$. To do this, just call $\\text{PREDECESSOR}$ or $\\text{SUCCESSOR}$ on $x$ to locate the appropriate tree in $O(\\lg\\lg u)$. Since the tree has height $\\lg u$, we can find the predecessor or successor in $O(\\lg\\lg u)$. g. Same as e , we need to find the cluster in $\\Theta(\\lg\\lg u)$, then the operations in the binary search tree takes $O(\\lg\\lg u)$. h. We can relax the requirements and only impose the condition that each group has at least $\\frac{1}{2}\\lg u$ elements and at most $2\\lg u$ elements. If a red-black tree is too big, we split it in half at the median. If a red-black tree is too small, we merge it with a neighboring tree. If this causes the merged tree to become too large, we split it at the median. If a tree splits, we create a new representative. If two trees merge, we delete the lost representative. Any split or merge takes $O(\\lg u)$ since we have to insert or delete an element in the data structure storing our representatives, which by part (b) takes $O(\\lg u)$. However, we only split a tree after at least $\\lg u$ insertions, since the size of one of the red-black trees needs to increase from $\\lg u$ to $2\\lg u$ and we only merge two trees after at least $(1 / 2)\\lg u$ deletions, because the size of the merging tree needs to have decreased from $\\lg u$ to $(1 / 2)\\lg u$. Thus, the amortized cost of the merges, splits, and updates to representatives is $O(1)$ per insertion or deletion, so the amortized cost is $O(\\lg\\lg u)$ as desired.","title":"20-2 $y$-fast tries"},{"location":"Chap21/21.1/","text":"21.1-1 Suppose that $\\text{CONNECTED-COMPONENTS}$ is run on the undirected graph $G = (V, E)$, where $V = \\{a, b, c, d, e, f, g, h, i, j, k\\}$ and the edges of $E$ are processed in the order $(d, i)$, $(f, k)$, $(g, i)$, $(b, g)$, $(a, h)$, $(i, j)$, $(d, k)$, $(b, j)$, $(d, f)$, $(g, j)$, $(a, e)$. List the vertices in each connected component after each iteration of lines 3\u20135. $$ \\begin{array}{c|lllllllllll} \\text{Edge processed} & \\\\ \\hline initial & \\{a\\} & \\{b\\} & \\{c\\} & \\{d\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & \\{i\\} & \\{j\\} & \\{k\\} \\\\ (d, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\{k\\} \\\\ (f, k) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f, k\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\\\ (g, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i, g\\} & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (b, g) & \\{a\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (a, h) & \\{a, h\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & \\{j\\} & \\\\ (i, j) & \\{a, h\\} & \\{b, d, i, g, j\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & & \\\\ (d, k) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (b, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (d, f) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (g, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (a, e) & \\{a, h, e\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & & & & & & & \\end{array} $$ So, the connected components that we are left with are $\\{a, h, e\\}$, $\\{b, d, i, g, j, f, k\\}$, and $\\{c\\}$. 21.1-2 Show that after all edges are processed by $\\text{CONNECTED-COMPONENTS}$, two vertices are in the same connected component if and only if they are in the same set. First suppose that two vertices are in the same connected component. Then there exists a path of edges connecting them. If two vertices are connected by a single edge, then they are put into the same set when that edge is processed. At some point during the algorithm every edge of the path will be processed, so all vertices on the path will be in the same set, including the endpoints. Now suppose two vertices $u$ and $v$ wind up in the same set. Since every vertex starts off in its own set, some sequence of edges in $G$ must have resulted in eventually combining the sets containing $u$ and $v$. From among these, there must be a path of edges from $u$ to $v$, implying that $u$ and $v$ are in the same connected component. 21.1-3 During the execution of $\\text{CONNECTED-COMPONENTS}$ on an undirected graph $G = (V, E)$ with $k$ connected components, how many times is $\\text{FIND-SET}$ called? How many times is $\\text{UNION}$ called? Express your answers in terms of $|V|$, $|E|$, and $k$. Find set is called twice on line 4, this is run once per edge in the graph, so, we have that find set is run $2|E|$ times. Since we start with $|V|$ sets, at the end only have $k$, and each call to $\\text{UNION}$ reduces the number of sets by one, we have that we have to made $|V| - k$ calls to $\\text{UNION}$.","title":"21.1 Disjoint-set operations"},{"location":"Chap21/21.1/#211-1","text":"Suppose that $\\text{CONNECTED-COMPONENTS}$ is run on the undirected graph $G = (V, E)$, where $V = \\{a, b, c, d, e, f, g, h, i, j, k\\}$ and the edges of $E$ are processed in the order $(d, i)$, $(f, k)$, $(g, i)$, $(b, g)$, $(a, h)$, $(i, j)$, $(d, k)$, $(b, j)$, $(d, f)$, $(g, j)$, $(a, e)$. List the vertices in each connected component after each iteration of lines 3\u20135. $$ \\begin{array}{c|lllllllllll} \\text{Edge processed} & \\\\ \\hline initial & \\{a\\} & \\{b\\} & \\{c\\} & \\{d\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & \\{i\\} & \\{j\\} & \\{k\\} \\\\ (d, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\{k\\} \\\\ (f, k) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f, k\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\\\ (g, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i, g\\} & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (b, g) & \\{a\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (a, h) & \\{a, h\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & \\{j\\} & \\\\ (i, j) & \\{a, h\\} & \\{b, d, i, g, j\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & & \\\\ (d, k) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (b, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (d, f) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (g, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (a, e) & \\{a, h, e\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & & & & & & & \\end{array} $$ So, the connected components that we are left with are $\\{a, h, e\\}$, $\\{b, d, i, g, j, f, k\\}$, and $\\{c\\}$.","title":"21.1-1"},{"location":"Chap21/21.1/#211-2","text":"Show that after all edges are processed by $\\text{CONNECTED-COMPONENTS}$, two vertices are in the same connected component if and only if they are in the same set. First suppose that two vertices are in the same connected component. Then there exists a path of edges connecting them. If two vertices are connected by a single edge, then they are put into the same set when that edge is processed. At some point during the algorithm every edge of the path will be processed, so all vertices on the path will be in the same set, including the endpoints. Now suppose two vertices $u$ and $v$ wind up in the same set. Since every vertex starts off in its own set, some sequence of edges in $G$ must have resulted in eventually combining the sets containing $u$ and $v$. From among these, there must be a path of edges from $u$ to $v$, implying that $u$ and $v$ are in the same connected component.","title":"21.1-2"},{"location":"Chap21/21.1/#211-3","text":"During the execution of $\\text{CONNECTED-COMPONENTS}$ on an undirected graph $G = (V, E)$ with $k$ connected components, how many times is $\\text{FIND-SET}$ called? How many times is $\\text{UNION}$ called? Express your answers in terms of $|V|$, $|E|$, and $k$. Find set is called twice on line 4, this is run once per edge in the graph, so, we have that find set is run $2|E|$ times. Since we start with $|V|$ sets, at the end only have $k$, and each call to $\\text{UNION}$ reduces the number of sets by one, we have that we have to made $|V| - k$ calls to $\\text{UNION}$.","title":"21.1-3"},{"location":"Chap21/21.2/","text":"21.2-1 Write pseudocode for $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. Make sure to specify the attributes that you assume for set objects and list objects. The three algorithms follow the english description and are provided here. There are alternate versions using the weighted union heuristic, suffixed with $\\text{WU}$. 1 2 3 4 5 6 7 MAKE - SET ( x ) let o be an object with three fields , next , value , and set let L be a linked list object with head = tail = o o . next = NIL o . set = L o . value = x return L 1 2 FIND - SET ( x ) return o . set . head . value 1 2 3 4 5 6 7 8 9 UNION ( x , y ) L1 = x . set L2 = y . set L1 . tail . next = L2 . head z = L2 . head while z . next != NIL z . set = L1 L1 . tail = L2 . tail return L1 21.2-2 Show the data structure that results and the answers returned by the $\\text{FIND-SET}$ operations in the following program. Use the linked-list representation with the weighted-union heuristic. 1 2 3 4 5 6 7 8 9 10 11 for i = 1 to 16 MAKE - SET ( x [ i ]) for i = 1 to 15 by 2 UNION ( x [ i ], x [ i + 1 ]) for i = 1 to 13 by 4 UNION ( x [ i ], x [ i + 2 ]) UNION ( x [ 1 ], x [ 5 ]) UNION ( x [ 11 ], x [ 13 ]) UNION ( x [ 1 ], x [ 10 ]) FIND - SET ( x [ 2 ]) FIND - SET ( x [ 9 ]) Assume that if the sets containing $x_i$ and $x_j$ have the same size, then the operation $\\text{UNION}(x_i, x_j)$ appends $x_j$'s list onto $x_i$'s list. Originally we have $16$ sets, each containing $x_i$. In the following, we'll replace $x_i$ by $i$. After the for loop in line 3 we have: $$\\{1,2\\}, \\{3, 4\\}, \\{5, 6\\}, \\{7, 8\\}, \\{9, 10\\}, \\{11, 12\\}, \\{13, 14\\}, \\{15, 16\\}.$$ After the for loop on line 5 we have $$\\{1, 2, 3, 4\\}, \\{5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 7 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 8 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12, 13, 14, 15, 16\\}.$$ Line 9 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}.$$ $\\text{FIND-SET}(x_2)$ and $\\text{FIND-SET}(x_9)$ each return pointers to $x_1$. 1 2 3 4 MAKE - SET - WU ( x ) L = MAKE - SET ( x ) L . size = 1 return L 1 2 3 4 5 6 7 8 UNION - WU ( x , y ) L1 = x . set L2 = y . set if L1 . size \u2265 L2 . size L = UNION ( x , y ) else L = UNION ( y , x ) L . size = L1 . size + L2 . size return L 21.2-3 Adapt the aggregate proof of Theorem 21.1 to obtain amortized time bounds of $O(1)$ for $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ and $O(\\lg n)$ for $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. We want to show that we can assign $O(1)$ charges to $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ and an $O(\\lg n)$ charge to $\\text{UNION}$ such that the charges for a sequence of these operations are enough to cover the cost of the sequence\u2014$O(m + n\\lg n)$, according to the theorem. When talking about the charge for each kind of operation, it is helpful to also be able to talk about the number of each kind of operation. Consider the usual sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, and let $l < n$ be the number of $\\text{UNION}$ operations. (Recall the discussion in Section 21.1 about there being at most $n - 1$ $\\text{UNION}$ operations.) Then there are $n$ $\\text{MAKE-SET}$ operations, $l$ $\\text{UNION}$ operations, and $m - n - l$ $\\text{FIND-SET}$ operations. The theorem didn't separately name the number $l$ of $\\text{UNION}$s; rather, it bounded the number by $n$. If you go through the proof of the theorem with $l$ $\\text{UNION}$s, you get the time bound $O(m - l + l\\lg l) = O(m + l\\lg l)$ for the sequence of operations. That is, the actual time taken by the sequence of operations is at most $c(m + l\\lg l)$, for some constant $c$. Thus, we want to assign operation charges such that $$ \\begin{array}{ll} \\text{(MAKE-SET charge)} & \\cdot \\quad n \\\\ + \\text{(FIND-SET charge)} & \\cdot \\quad (m - n - l) \\\\ + \\text{(UNION charge)} & \\cdot \\quad l \\\\ \\hline \\ge c(m + l\\lg l), \\end{array} $$ so that the amortized costs give an upper bound on the actual costs. The following assignments work, where $c'$ is some constant $\\ge c$: $\\text{MAKE-SET}$: $c'$ $\\text{FIND-SET}$: $c'$ $\\text{UNION}$: $c'(\\lg n + 1)$ Substituting into the above sum, we get $$ \\begin{aligned} c'n + c'(m - n - l) + c'(\\lg n + 1)l & = c'm + c'l\\lg n \\\\ & = c'(m + l\\lg n) \\\\ & > c(m + l\\lg l). \\end{aligned} $$ 21.2-4 Give a tight asymptotic bound on the running time of the sequence of operations in Figure 21.3 assuming the linked-list representation and the weighted-union heuristic. We call $\\text{MAKE-SET}$ $n$ times, which contributes $\\Theta(n)$. In each union, the smaller set is of size $1$, so each of these takes $\\Theta(1)$ time. Since we union $n - 1$ times, the runtime is $\\Theta(n)$. 21.2-5 Professor Gompers suspects that it might be possible to keep just one pointer in each set object, rather than two ($head$ and $tail$), while keeping the number of pointers in each list element at two. Show that the professor's suspicion is well founded by describing how to represent each set by a linked list such that each operation has the same running time as the operations described in this section. Describe also how the operations work. Your scheme should allow for the weighted-union heuristic, with the same effect as described in this section. ($\\textit{Hint:}$ Use the tail of a linked list as its set's representative.) As the hint suggests, make the representative of each set be the tail of its linked list. Except for the tail element, each element's representative pointer points to the tail. The tail's representative pointer points to the head. An element is the tail if its next pointer is $\\text{NIL}$. Now we can get to the tail in $O(1)$ time: if $x.next == \\text{NIL}$, then $tail = x$, else $tail = x.rep$. We can get to the head in $O(1)$ time as well: if $x.next == \\text{NIL}$, then $head = x.rep$, else $head = x.rep.rep$. The set object needs only to store a pointer to the tail, though a pointer to any list element would suffice. 21.2-6 Suggest a simple change to the $\\text{UNION}$ procedure for the linked-list representation that removes the need to keep the $tail$ pointer to the last object in each list. Whether or not the weighted-union heuristic is used, your change should not change the asymptotic running time of the $\\text{UNION}$ procedure. ($\\textit{Hint:}$ Rather than appending one list to another, splice them together.) Let's call the two lists $A$ and $B$, and suppose that the representative of the new list will be the representative of $A$. Rather than appending $B$ to the end of $A$, instead splice $B$ into $A$ right after the first element of $A$. We have to traverse $B$ to update pointers to the set object anyway, so we can just make the last element of $B$ point to the second element of $A$.","title":"21.2 Linked-list representation of disjoint sets"},{"location":"Chap21/21.2/#212-1","text":"Write pseudocode for $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. Make sure to specify the attributes that you assume for set objects and list objects. The three algorithms follow the english description and are provided here. There are alternate versions using the weighted union heuristic, suffixed with $\\text{WU}$. 1 2 3 4 5 6 7 MAKE - SET ( x ) let o be an object with three fields , next , value , and set let L be a linked list object with head = tail = o o . next = NIL o . set = L o . value = x return L 1 2 FIND - SET ( x ) return o . set . head . value 1 2 3 4 5 6 7 8 9 UNION ( x , y ) L1 = x . set L2 = y . set L1 . tail . next = L2 . head z = L2 . head while z . next != NIL z . set = L1 L1 . tail = L2 . tail return L1","title":"21.2-1"},{"location":"Chap21/21.2/#212-2","text":"Show the data structure that results and the answers returned by the $\\text{FIND-SET}$ operations in the following program. Use the linked-list representation with the weighted-union heuristic. 1 2 3 4 5 6 7 8 9 10 11 for i = 1 to 16 MAKE - SET ( x [ i ]) for i = 1 to 15 by 2 UNION ( x [ i ], x [ i + 1 ]) for i = 1 to 13 by 4 UNION ( x [ i ], x [ i + 2 ]) UNION ( x [ 1 ], x [ 5 ]) UNION ( x [ 11 ], x [ 13 ]) UNION ( x [ 1 ], x [ 10 ]) FIND - SET ( x [ 2 ]) FIND - SET ( x [ 9 ]) Assume that if the sets containing $x_i$ and $x_j$ have the same size, then the operation $\\text{UNION}(x_i, x_j)$ appends $x_j$'s list onto $x_i$'s list. Originally we have $16$ sets, each containing $x_i$. In the following, we'll replace $x_i$ by $i$. After the for loop in line 3 we have: $$\\{1,2\\}, \\{3, 4\\}, \\{5, 6\\}, \\{7, 8\\}, \\{9, 10\\}, \\{11, 12\\}, \\{13, 14\\}, \\{15, 16\\}.$$ After the for loop on line 5 we have $$\\{1, 2, 3, 4\\}, \\{5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 7 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 8 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12, 13, 14, 15, 16\\}.$$ Line 9 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}.$$ $\\text{FIND-SET}(x_2)$ and $\\text{FIND-SET}(x_9)$ each return pointers to $x_1$. 1 2 3 4 MAKE - SET - WU ( x ) L = MAKE - SET ( x ) L . size = 1 return L 1 2 3 4 5 6 7 8 UNION - WU ( x , y ) L1 = x . set L2 = y . set if L1 . size \u2265 L2 . size L = UNION ( x , y ) else L = UNION ( y , x ) L . size = L1 . size + L2 . size return L","title":"21.2-2"},{"location":"Chap21/21.2/#212-3","text":"Adapt the aggregate proof of Theorem 21.1 to obtain amortized time bounds of $O(1)$ for $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ and $O(\\lg n)$ for $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. We want to show that we can assign $O(1)$ charges to $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ and an $O(\\lg n)$ charge to $\\text{UNION}$ such that the charges for a sequence of these operations are enough to cover the cost of the sequence\u2014$O(m + n\\lg n)$, according to the theorem. When talking about the charge for each kind of operation, it is helpful to also be able to talk about the number of each kind of operation. Consider the usual sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, and let $l < n$ be the number of $\\text{UNION}$ operations. (Recall the discussion in Section 21.1 about there being at most $n - 1$ $\\text{UNION}$ operations.) Then there are $n$ $\\text{MAKE-SET}$ operations, $l$ $\\text{UNION}$ operations, and $m - n - l$ $\\text{FIND-SET}$ operations. The theorem didn't separately name the number $l$ of $\\text{UNION}$s; rather, it bounded the number by $n$. If you go through the proof of the theorem with $l$ $\\text{UNION}$s, you get the time bound $O(m - l + l\\lg l) = O(m + l\\lg l)$ for the sequence of operations. That is, the actual time taken by the sequence of operations is at most $c(m + l\\lg l)$, for some constant $c$. Thus, we want to assign operation charges such that $$ \\begin{array}{ll} \\text{(MAKE-SET charge)} & \\cdot \\quad n \\\\ + \\text{(FIND-SET charge)} & \\cdot \\quad (m - n - l) \\\\ + \\text{(UNION charge)} & \\cdot \\quad l \\\\ \\hline \\ge c(m + l\\lg l), \\end{array} $$ so that the amortized costs give an upper bound on the actual costs. The following assignments work, where $c'$ is some constant $\\ge c$: $\\text{MAKE-SET}$: $c'$ $\\text{FIND-SET}$: $c'$ $\\text{UNION}$: $c'(\\lg n + 1)$ Substituting into the above sum, we get $$ \\begin{aligned} c'n + c'(m - n - l) + c'(\\lg n + 1)l & = c'm + c'l\\lg n \\\\ & = c'(m + l\\lg n) \\\\ & > c(m + l\\lg l). \\end{aligned} $$","title":"21.2-3"},{"location":"Chap21/21.2/#212-4","text":"Give a tight asymptotic bound on the running time of the sequence of operations in Figure 21.3 assuming the linked-list representation and the weighted-union heuristic. We call $\\text{MAKE-SET}$ $n$ times, which contributes $\\Theta(n)$. In each union, the smaller set is of size $1$, so each of these takes $\\Theta(1)$ time. Since we union $n - 1$ times, the runtime is $\\Theta(n)$.","title":"21.2-4"},{"location":"Chap21/21.2/#212-5","text":"Professor Gompers suspects that it might be possible to keep just one pointer in each set object, rather than two ($head$ and $tail$), while keeping the number of pointers in each list element at two. Show that the professor's suspicion is well founded by describing how to represent each set by a linked list such that each operation has the same running time as the operations described in this section. Describe also how the operations work. Your scheme should allow for the weighted-union heuristic, with the same effect as described in this section. ($\\textit{Hint:}$ Use the tail of a linked list as its set's representative.) As the hint suggests, make the representative of each set be the tail of its linked list. Except for the tail element, each element's representative pointer points to the tail. The tail's representative pointer points to the head. An element is the tail if its next pointer is $\\text{NIL}$. Now we can get to the tail in $O(1)$ time: if $x.next == \\text{NIL}$, then $tail = x$, else $tail = x.rep$. We can get to the head in $O(1)$ time as well: if $x.next == \\text{NIL}$, then $head = x.rep$, else $head = x.rep.rep$. The set object needs only to store a pointer to the tail, though a pointer to any list element would suffice.","title":"21.2-5"},{"location":"Chap21/21.2/#212-6","text":"Suggest a simple change to the $\\text{UNION}$ procedure for the linked-list representation that removes the need to keep the $tail$ pointer to the last object in each list. Whether or not the weighted-union heuristic is used, your change should not change the asymptotic running time of the $\\text{UNION}$ procedure. ($\\textit{Hint:}$ Rather than appending one list to another, splice them together.) Let's call the two lists $A$ and $B$, and suppose that the representative of the new list will be the representative of $A$. Rather than appending $B$ to the end of $A$, instead splice $B$ into $A$ right after the first element of $A$. We have to traverse $B$ to update pointers to the set object anyway, so we can just make the last element of $B$ point to the second element of $A$.","title":"21.2-6"},{"location":"Chap21/21.3/","text":"21.3-1 Redo Exercise 21.2-2 using a disjoint-set forest with union by rank and path compression. 21.3-2 Write a nonrecursive version of $\\text{FIND-SET}$ with path compression. To implement $\\text{FIND-SET}$ nonrecursively, let $x$ be the element we call the function on. Create a linked list $A$ which contains a pointer to $x$. Each time we most one element up the tree, insert a pointer to that element into $A$. Once the root $r$ has been found, use the linked list to find each node on the path from the root to $x$ and update its parent to $r$. 21.3-3 Give a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, that takes $\\Omega(m\\lg n)$ time when we use union by rank only. You need to find a sequence of $m$ operations on $n$ elements that takes \u007f$\\Omega(m\\lg n)$ time. Start with $n$ $\\text{MAKE-SET}$s to create singleton sets $\\{x_1\\}, \\{x_2\\}, \\ldots, \\{x_n\\}$. Next perform the $n - 1$ $\\text{UNION}$ operations shown below to create a single set whose tree has depth $\\lg n$. $$ \\begin{array}{ll} \\hline \\text{UNION($x_1, x_2$)} & \\text{$n / 2$ of these} \\\\ \\text{UNION($x_3, x_4$)} & \\\\ \\text{UNION($x_5, x_6$)} & \\\\ \\vdots \\\\ \\text{UNION($x_{n - 1}, x_n$)} & \\\\ \\hline \\text{UNION($x_2, x_4$)} & \\text{$n / 4$ of these} \\\\ \\text{UNION($x_6, x_8$)} & \\\\ \\text{UNION($x_{10}, x_{12}$)} & \\\\ \\vdots \\\\ \\text{UNION($x_{n - 2}, x_n$)} & \\\\ \\hline \\text{UNION($x_4, x_8$)} & \\text{$n / 8$ of these} \\\\ \\text{UNION($x_{12}, x_{16}$)} & \\\\ \\text{UNION($x_{20}, x_{24}$)} & \\\\ \\vdots \\\\ \\text{UNION($x_{n - 4}, x_n$)} & \\\\ \\hline \\vdots \\\\ \\hline \\text{UNION($x_{n / 2}, x_n$)} & \\text{$1$ of these} \\\\ \\hline \\end{array} $$ Finally, perform $m - 2n + 1$ $\\text{FIND-SET}$ operations on the deepest element in the tree. Each of these $\\text{FIND-SET}$ operations takes \u007f$\\Omega(\\lg n)$ time. Letting $m \\ge 3n$, we have more than $m / 3$ $\\text{FIND-SET}$ operations, so that the total cost is \u007f$\\Omega(m\\lg n)$. 21.3-4 Suppose that we wish to add the operation $\\text{PRINT-SET}(x)$, which is given a node $x$ and prints all the members of $x$'s set, in any order. Show how we can add just a single attribute to each node in a disjoint-set forest so that $\\text{PRINT-SET}(x)$ takes time linear in the number of members of $x$'s set and the asymptotic running times of the other operations are unchanged. Assume that we can print each member of the set in $O(1)$ time. Maintain a circular, singly linked list of the nodes of each set. To print, just follow the list until you get back to node $x$, printing each member of the list. The only other operations that change are $\\text{FIND-SET}$, which sets $x.next = x$, and $\\text{LINK}$, which exchanges the pointers $x.next$ and $y.next$. 21.3-5 $\\star$ Show that any sequence of $m$ $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{LINK}$ operations, where all the $\\text{LINK}$ operations appear before any of the $\\text{FIND-SET}$ operations, takes only $O(m)$ time if we use both path compression and union by rank. What happens in the same situation if we use only the path-compression heuristic? With the path-compression heuristic, the sequence of $m$ $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{LINK}$ operations, where all the $\\text{LINK}$ operations take place before any of the $\\text{FIND-SET}$ operations, runs in $O(m)$ time. The key observation is that once a node $x$ appears on a find path, $x$ will be either a root or a child of a root at all times thereafter. We use the accounting method to obtain the $O(m)$ time bound. We charge a $\\text{MAKE-SET}$ operation two dollars. One dollar pays for the $\\text{MAKE-SET}$, and one dollar remains on the node $x$ that is created. The latter pays for the first time that $x$ appears on a find path and is turned into a child of a root. We charge one dollar for a $\\text{LINK}$ operation. This dollar pays for the actual linking of one node to another. We charge one dollar for a $\\text{FIND-SET}$. This dollar pays for visiting the root and its child, and for the path compression of these two nodes, during the $\\text{FIND-SET}$. All other nodes on the find path use their stored dollar to pay for their visitation and path compression. As mentioned, after the $\\text{FIND-SET}$, all nodes on the find path become children of a root (except for the root itself), and so whenever they are visited during a subsequent $\\text{FIND-SET}$, the $\\text{FIND-SET}$ operation itself will pay for them. Since we charge each operation either one or two dollars, a sequence of $m$ operations is charged at most $2m$ dollars, and so the total time is $O(m)$. Observe that nothing in the above argument requires union by rank. Therefore, we get an $O(m)$ time bound regardless of whether we use union by rank.","title":"21.3 Disjoint-set forests"},{"location":"Chap21/21.3/#213-1","text":"Redo Exercise 21.2-2 using a disjoint-set forest with union by rank and path compression.","title":"21.3-1"},{"location":"Chap21/21.3/#213-2","text":"Write a nonrecursive version of $\\text{FIND-SET}$ with path compression. To implement $\\text{FIND-SET}$ nonrecursively, let $x$ be the element we call the function on. Create a linked list $A$ which contains a pointer to $x$. Each time we most one element up the tree, insert a pointer to that element into $A$. Once the root $r$ has been found, use the linked list to find each node on the path from the root to $x$ and update its parent to $r$.","title":"21.3-2"},{"location":"Chap21/21.3/#213-3","text":"Give a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, that takes $\\Omega(m\\lg n)$ time when we use union by rank only. You need to find a sequence of $m$ operations on $n$ elements that takes \u007f$\\Omega(m\\lg n)$ time. Start with $n$ $\\text{MAKE-SET}$s to create singleton sets $\\{x_1\\}, \\{x_2\\}, \\ldots, \\{x_n\\}$. Next perform the $n - 1$ $\\text{UNION}$ operations shown below to create a single set whose tree has depth $\\lg n$. $$ \\begin{array}{ll} \\hline \\text{UNION($x_1, x_2$)} & \\text{$n / 2$ of these} \\\\ \\text{UNION($x_3, x_4$)} & \\\\ \\text{UNION($x_5, x_6$)} & \\\\ \\vdots \\\\ \\text{UNION($x_{n - 1}, x_n$)} & \\\\ \\hline \\text{UNION($x_2, x_4$)} & \\text{$n / 4$ of these} \\\\ \\text{UNION($x_6, x_8$)} & \\\\ \\text{UNION($x_{10}, x_{12}$)} & \\\\ \\vdots \\\\ \\text{UNION($x_{n - 2}, x_n$)} & \\\\ \\hline \\text{UNION($x_4, x_8$)} & \\text{$n / 8$ of these} \\\\ \\text{UNION($x_{12}, x_{16}$)} & \\\\ \\text{UNION($x_{20}, x_{24}$)} & \\\\ \\vdots \\\\ \\text{UNION($x_{n - 4}, x_n$)} & \\\\ \\hline \\vdots \\\\ \\hline \\text{UNION($x_{n / 2}, x_n$)} & \\text{$1$ of these} \\\\ \\hline \\end{array} $$ Finally, perform $m - 2n + 1$ $\\text{FIND-SET}$ operations on the deepest element in the tree. Each of these $\\text{FIND-SET}$ operations takes \u007f$\\Omega(\\lg n)$ time. Letting $m \\ge 3n$, we have more than $m / 3$ $\\text{FIND-SET}$ operations, so that the total cost is \u007f$\\Omega(m\\lg n)$.","title":"21.3-3"},{"location":"Chap21/21.3/#213-4","text":"Suppose that we wish to add the operation $\\text{PRINT-SET}(x)$, which is given a node $x$ and prints all the members of $x$'s set, in any order. Show how we can add just a single attribute to each node in a disjoint-set forest so that $\\text{PRINT-SET}(x)$ takes time linear in the number of members of $x$'s set and the asymptotic running times of the other operations are unchanged. Assume that we can print each member of the set in $O(1)$ time. Maintain a circular, singly linked list of the nodes of each set. To print, just follow the list until you get back to node $x$, printing each member of the list. The only other operations that change are $\\text{FIND-SET}$, which sets $x.next = x$, and $\\text{LINK}$, which exchanges the pointers $x.next$ and $y.next$.","title":"21.3-4"},{"location":"Chap21/21.3/#213-5-star","text":"Show that any sequence of $m$ $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{LINK}$ operations, where all the $\\text{LINK}$ operations appear before any of the $\\text{FIND-SET}$ operations, takes only $O(m)$ time if we use both path compression and union by rank. What happens in the same situation if we use only the path-compression heuristic? With the path-compression heuristic, the sequence of $m$ $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{LINK}$ operations, where all the $\\text{LINK}$ operations take place before any of the $\\text{FIND-SET}$ operations, runs in $O(m)$ time. The key observation is that once a node $x$ appears on a find path, $x$ will be either a root or a child of a root at all times thereafter. We use the accounting method to obtain the $O(m)$ time bound. We charge a $\\text{MAKE-SET}$ operation two dollars. One dollar pays for the $\\text{MAKE-SET}$, and one dollar remains on the node $x$ that is created. The latter pays for the first time that $x$ appears on a find path and is turned into a child of a root. We charge one dollar for a $\\text{LINK}$ operation. This dollar pays for the actual linking of one node to another. We charge one dollar for a $\\text{FIND-SET}$. This dollar pays for visiting the root and its child, and for the path compression of these two nodes, during the $\\text{FIND-SET}$. All other nodes on the find path use their stored dollar to pay for their visitation and path compression. As mentioned, after the $\\text{FIND-SET}$, all nodes on the find path become children of a root (except for the root itself), and so whenever they are visited during a subsequent $\\text{FIND-SET}$, the $\\text{FIND-SET}$ operation itself will pay for them. Since we charge each operation either one or two dollars, a sequence of $m$ operations is charged at most $2m$ dollars, and so the total time is $O(m)$. Observe that nothing in the above argument requires union by rank. Therefore, we get an $O(m)$ time bound regardless of whether we use union by rank.","title":"21.3-5 $\\star$"},{"location":"Chap21/21.4/","text":"21.4-1 Prove Lemma 21.4. The lemma states: For all nodes $x$, we have $x.rank \\le x.p.rank$, with strict inequality if $x \\ne x.p$. The value of $x.rank$ is initially $0$ and increases through time until $x \\ne x.p$; from then on, $x.rank$ does not change. The value of $x.p.rank$ monotonically increases over time. The initial value of $x.rank$ is $0$, as it is initialized in line 2 of the $\\text{MAKE-SET}(x)$ procedure. When we run $\\text{LINK}(x, y)$, whichever one has the larger rank is placed as the parent of the other, and if there is a tie, the parent's rank is incremented. This means that after any $\\text{LINK}(y, x)$, the two nodes being linked satisfy this strict inequality of ranks. Also, if we have that $x \\ne x.p$, then, we have that $x$ is not its own set representative, so, any linking together of sets that would occur would not involve $x$, but that's the only way for ranks to increase, so, we have that $x.rank$ must remain constant after that point. 21.4-2 Prove that every node has rank at most $\\lfloor \\lg n \\rfloor$. We'll prove the claim by strong induction on the number of nodes. If $n = 1$, then that node has rank equal to $0 = \\lfloor \\lg 1 \\rfloor$. Now suppose that the claim holds for $1, 2, \\ldots, n$ nodes. Given $n + 1$ nodes, suppose we perform a $\\text{UNION}$ operation on two disjoint sets with $a$ and $b$ nodes respectively, where $a, b \\le n$. Then the root of the first set has rank at most $\\lfloor \\lg a \\rfloor$ and the root of the second set has rank at most $\\lfloor \\lg b\\rfloor$. If the ranks are unequal, then the $\\text{UNION}$ operation preserves rank and we are done, so suppose the ranks are equal. Then the rank of the union increases by $1$, and the resulting set has rank $\\lfloor\\lg a\\rfloor + 1 \\le\\lfloor\\lg(n + 1) / 2\\rfloor + 1 = \\lfloor\\lg(n + 1)\\rfloor$. 21.4-3 In light of Exercise 21.4-2, how many bits are necessary to store $x.rank$ for each node $x$? Since their value is at most $\\lfloor \\lg n \\rfloor$, we can represent them using $\\Theta(\\lg(\\lg(n)))$ bits, and may need to use that many bits to represent a number that can take that many values. 21.4-4 Using Exercise 21.4-2, give a simple proof that operations on a disjoint-set forest with union by rank but without path compression run in $O(m\\lg n)$ time. Clearly, each $\\text{MAKE-SET}$ and $\\text{LINK}$ operation takes $O(1)$ time. Because the rank of a node is an upper bound on its height, each find path has length $O(\\lg n)$, which in turn implies that each $\\text{FIND-SET}$ takes $O(\\lg n)$ time. Thus, any sequence of $m$ $\\text{MAKE-SET}$, $\\text{LINK}$, and $\\text{FIND-SET}$ operations on $n$ elements takes $O(m\\lg n)$ time. It is easy to prove an analogue of Lemma 21.7 to show that if we convert a sequence of $m'$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations into a sequence of $m$ $\\text{MAKE-SET}$, $\\text{LINK}$, and $\\text{FIND-SET}$ operations that take $O(m\\lg n)$ time, then the sequence of $m'$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations takes $O(m'\\lg n)$ time. 21.4-5 Professor Dante reasons that because node ranks increase strictly along a simple path to the root, node levels must monotonically increase along the path. In other words, if $x.rank > 0$ and $x.p$ is not a root, then $\\text{level}(x) \\le \\text{level}(x.p)$. Is the professor correct? Professor Dante is mistaken. Take the following scenario. Let $n = 16$, and make $16$ separate singleton sets using $\\text{MAKE-SET}$. Then do $8$ $\\text{UNION}$ operations to link the sets into $8$ pairs, where each pair has a root with rank $0$ and a child with rank $1$. Now do $4$ $\\text{UNION}$s to link pairs of these trees, so that there are $4$ trees, each with a root of rank $2$, children of the root of ranks $1$ and $0$, and a node of rank $0$ that is the child of the rank-$1$ node. Now link pairs of these trees together, so that there are two resulting trees, each with a root of rank $3$ and each containing a path from a leaf to the root with ranks $0$, $1$, and $3$. Finally, link these two trees together, so that there is a path from a leaf to the root with ranks $0$, $1$, $3$, and $4$. Let $x$ and $y$ be the nodes on this path with ranks $1$ and $3$, respectively. Since $A_1(1) = 3$, $\\text{level}(x) = 1$, and since $A_0(3) = 4$, $\\text{level}(y) = 0$. Yet $y$ follows $x$ on the find path. 21.4-6 $\\star$ Consider the function $\\alpha'(n) = \\min \\{k: A_k(1) \\ge \\lg(n + 1)\\}$. Show that $\\alpha'(n) \\le 3$ for all practical values of $n$ and, using Exercise 21.4-2, show how to modify the potential-function argument to prove that we can perform a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, on a disjoint-set forest with union by rank and path compression in worst-case time $O(m \\alpha'(n))$. First, $\\alpha'(2^{2047} - 1) = \\min\\{k: A_k(1) \\ge 2047\\} = 3$, and $2^{2047} - 1 \\gg 10^{80}$. Second, we need that $0 \\le \\text{level}(x) \\le \\alpha'(n)$ for all nonroots $x$ with $x.rank \\ge 1$. With this definition of $\\alpha'(n)$, we have $$A_{\\alpha'(n)}(x.rank) \\ge A_{\\alpha'(n)}(1) \\ge \\lg(n + 1) > \\lg n \\ge x.p.rank.$$ The rest of the proof goes through with $\\alpha'(n)$ replacing $\\alpha(n)$.","title":"21.4 Analysis of union by rank with path compression"},{"location":"Chap21/21.4/#214-1","text":"Prove Lemma 21.4. The lemma states: For all nodes $x$, we have $x.rank \\le x.p.rank$, with strict inequality if $x \\ne x.p$. The value of $x.rank$ is initially $0$ and increases through time until $x \\ne x.p$; from then on, $x.rank$ does not change. The value of $x.p.rank$ monotonically increases over time. The initial value of $x.rank$ is $0$, as it is initialized in line 2 of the $\\text{MAKE-SET}(x)$ procedure. When we run $\\text{LINK}(x, y)$, whichever one has the larger rank is placed as the parent of the other, and if there is a tie, the parent's rank is incremented. This means that after any $\\text{LINK}(y, x)$, the two nodes being linked satisfy this strict inequality of ranks. Also, if we have that $x \\ne x.p$, then, we have that $x$ is not its own set representative, so, any linking together of sets that would occur would not involve $x$, but that's the only way for ranks to increase, so, we have that $x.rank$ must remain constant after that point.","title":"21.4-1"},{"location":"Chap21/21.4/#214-2","text":"Prove that every node has rank at most $\\lfloor \\lg n \\rfloor$. We'll prove the claim by strong induction on the number of nodes. If $n = 1$, then that node has rank equal to $0 = \\lfloor \\lg 1 \\rfloor$. Now suppose that the claim holds for $1, 2, \\ldots, n$ nodes. Given $n + 1$ nodes, suppose we perform a $\\text{UNION}$ operation on two disjoint sets with $a$ and $b$ nodes respectively, where $a, b \\le n$. Then the root of the first set has rank at most $\\lfloor \\lg a \\rfloor$ and the root of the second set has rank at most $\\lfloor \\lg b\\rfloor$. If the ranks are unequal, then the $\\text{UNION}$ operation preserves rank and we are done, so suppose the ranks are equal. Then the rank of the union increases by $1$, and the resulting set has rank $\\lfloor\\lg a\\rfloor + 1 \\le\\lfloor\\lg(n + 1) / 2\\rfloor + 1 = \\lfloor\\lg(n + 1)\\rfloor$.","title":"21.4-2"},{"location":"Chap21/21.4/#214-3","text":"In light of Exercise 21.4-2, how many bits are necessary to store $x.rank$ for each node $x$? Since their value is at most $\\lfloor \\lg n \\rfloor$, we can represent them using $\\Theta(\\lg(\\lg(n)))$ bits, and may need to use that many bits to represent a number that can take that many values.","title":"21.4-3"},{"location":"Chap21/21.4/#214-4","text":"Using Exercise 21.4-2, give a simple proof that operations on a disjoint-set forest with union by rank but without path compression run in $O(m\\lg n)$ time. Clearly, each $\\text{MAKE-SET}$ and $\\text{LINK}$ operation takes $O(1)$ time. Because the rank of a node is an upper bound on its height, each find path has length $O(\\lg n)$, which in turn implies that each $\\text{FIND-SET}$ takes $O(\\lg n)$ time. Thus, any sequence of $m$ $\\text{MAKE-SET}$, $\\text{LINK}$, and $\\text{FIND-SET}$ operations on $n$ elements takes $O(m\\lg n)$ time. It is easy to prove an analogue of Lemma 21.7 to show that if we convert a sequence of $m'$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations into a sequence of $m$ $\\text{MAKE-SET}$, $\\text{LINK}$, and $\\text{FIND-SET}$ operations that take $O(m\\lg n)$ time, then the sequence of $m'$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations takes $O(m'\\lg n)$ time.","title":"21.4-4"},{"location":"Chap21/21.4/#214-5","text":"Professor Dante reasons that because node ranks increase strictly along a simple path to the root, node levels must monotonically increase along the path. In other words, if $x.rank > 0$ and $x.p$ is not a root, then $\\text{level}(x) \\le \\text{level}(x.p)$. Is the professor correct? Professor Dante is mistaken. Take the following scenario. Let $n = 16$, and make $16$ separate singleton sets using $\\text{MAKE-SET}$. Then do $8$ $\\text{UNION}$ operations to link the sets into $8$ pairs, where each pair has a root with rank $0$ and a child with rank $1$. Now do $4$ $\\text{UNION}$s to link pairs of these trees, so that there are $4$ trees, each with a root of rank $2$, children of the root of ranks $1$ and $0$, and a node of rank $0$ that is the child of the rank-$1$ node. Now link pairs of these trees together, so that there are two resulting trees, each with a root of rank $3$ and each containing a path from a leaf to the root with ranks $0$, $1$, and $3$. Finally, link these two trees together, so that there is a path from a leaf to the root with ranks $0$, $1$, $3$, and $4$. Let $x$ and $y$ be the nodes on this path with ranks $1$ and $3$, respectively. Since $A_1(1) = 3$, $\\text{level}(x) = 1$, and since $A_0(3) = 4$, $\\text{level}(y) = 0$. Yet $y$ follows $x$ on the find path.","title":"21.4-5"},{"location":"Chap21/21.4/#214-6-star","text":"Consider the function $\\alpha'(n) = \\min \\{k: A_k(1) \\ge \\lg(n + 1)\\}$. Show that $\\alpha'(n) \\le 3$ for all practical values of $n$ and, using Exercise 21.4-2, show how to modify the potential-function argument to prove that we can perform a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, on a disjoint-set forest with union by rank and path compression in worst-case time $O(m \\alpha'(n))$. First, $\\alpha'(2^{2047} - 1) = \\min\\{k: A_k(1) \\ge 2047\\} = 3$, and $2^{2047} - 1 \\gg 10^{80}$. Second, we need that $0 \\le \\text{level}(x) \\le \\alpha'(n)$ for all nonroots $x$ with $x.rank \\ge 1$. With this definition of $\\alpha'(n)$, we have $$A_{\\alpha'(n)}(x.rank) \\ge A_{\\alpha'(n)}(1) \\ge \\lg(n + 1) > \\lg n \\ge x.p.rank.$$ The rest of the proof goes through with $\\alpha'(n)$ replacing $\\alpha(n)$.","title":"21.4-6 $\\star$"},{"location":"Chap21/Problems/21-1/","text":"The off-line minimum problem asks us to maintain a dynamic set $T$ of elements from the domain $\\{1, 2, \\ldots, n\\}$ under the operations $\\text{INSERT}$ and $\\text{EXTRACT-MIN}$. We are given a sequence $S$ of $n$ $\\text{INSERT}$ and $m$ $\\text{EXTRACT-MIN}$ calls, where each key in $\\{1, 2, \\ldots, n\\}$ is inserted exactly once. We wish to determine which key is returned by each $\\text{EXTRACT-MIN}$ call. Specifically, we wish to fill in an array $extracted[1..m]$, where for $i = 1, 2, \\ldots, m$, $extracted[i]$ is the key returned by the $i$th $\\text{EXTRACT-MIN}$ call. The problem is \"off-line\" in the sense that we are allowed to process the entire sequence $S$ before determining any of the returned keys. a. In the following instance of the off-line minimum problem, each operation $\\text{INSERT}(i)$ is represented by the value of $i$ and each $\\text{EXTRACT-MIN}$ is represented by the letter $\\text E$: $$4, 8, \\text E, 3, \\text E, 9, 2, 6, \\text E, \\text E, \\text E, 1, 7, \\text E, 5.$$ Fill in the correct values in the extracted array. To develop an algorithm for this problem, we break the sequence $S$ into homogeneous subsequences. That is, we represent $S$ by $$\\text I_1, \\text E, \\text I_2, \\text E, \\text I_3, \\ldots, \\text I_m,\\text E, \\text I_{m + 1},$$ where each $\\text E$ represents a single $\\text{EXTRACT-MIN}$ call and each $\\text{I}_j$ represents a (possibly empty) sequence of $\\text{INSERT}$ calls. For each subsequence $\\text{I}_j$ , we initially place the keys inserted by these operations into a set $K_j$, which is empty if $\\text{I}_j$ is empty. We then do the following: 1 2 3 4 5 6 7 8 OFF - LINE - MINIMUM ( m , n ) for i = 1 to n determine j such that i \u2208 K [ j ] if j != m + 1 extracted [ j ] = i let l be the smallest value greater than j for which set K [ l ] exists K [ l ] = K [ j ] \u222a K [ l ], destroying K [ j ] return extracted b. Argue that the array extracted returned by $\\text{OFF-LINE-MINIMUM}$ is correct. c. Describe how to implement $\\text{OFF-LINE-MINIMUM}$ efficiently with a disjoint-set data structure. Give a tight bound on the worst-case running time of your implementation. a. For the input sequence $$\\text{4, 8, E, 3, E, 9, 2, 6, E, E, E, 1, 7, E, 5},$$ the values in the $extracted$ array would be $[4, 3, 2, 6, 8, 1]$. b. As we run the for loop, we are picking off the smallest of the possible elements to be removed, knowing for sure that it will be removed by the next unused $\\text{EXTRACT-MIN}$ operation. Then, since that $\\text{EXTRACT-MIN}$ operation is used up, we can pretend that it no longer exists, and combine the set of things that were inserted by that segment with those inserted by the next, since we know that the $\\text{EXTRACT-MIN}$ operation that had separated the two is now used up. Since we proceed to figure out what the various extract operations do one at a time, by the time we are done, we have figured them all out. The following table shows the situation after the $i$th iteration of the for loop when we use $\\text{OFF-LINE-MINIMUM}(6, 9)$ on the same input. ($i = 0$ denotes the initial state). Here we use $\\phi$ to represent the destroyed arrays and initialize $extracted$ with $0$s for easy reading. (Note that when $i = 5$ and $i = 7$ the state doesn't change since both $5$ and $7$ $\\in K_7$ and $7 = m + 1$) $$ \\begin{array}{c|cccccccc} i & K_1 & K_2 & K_3 & K_4 & K_5 & K_6 & K_7 & extracted \\\\ \\hline 0 & \\{4, 8\\} & \\{3\\} & \\{9, 2, 6\\} & \\{\\} & \\{\\} & \\{1, 7\\} & \\{5\\} & [0, 0, 0, 0, 0, 0] \\\\ 1 & \\{4, 8\\} & \\{3\\} & \\{9, 2, 6\\} & \\{\\} & \\{\\} & \\phi & \\{1, 7, 5\\} & [0, 0, 0, 0, 0, 1] \\\\ 2 & \\{4, 8\\} & \\{3\\} & \\phi & \\{9, 2, 6\\} & \\{\\} & \\phi & \\{1, 7, 5\\} & [0, 0, 2, 0, 0, 1] \\\\ 3 & \\{4, 8\\} & \\phi & \\phi & \\{3, 9, 2, 6\\} & \\{\\} & \\phi & \\{1, 7, 5\\} & [0, 3, 2, 0, 0, 1] \\\\ 4 & \\phi & \\phi & \\phi & \\{4, 8, 3, 9, 2, 6\\} & \\{\\} & \\phi & \\{1, 7, 5\\} & [4, 3, 2, 0, 0, 1] \\\\ 5 & \\phi & \\phi & \\phi & \\{4, 8, 3, 9, 2, 6\\} & \\{\\} & \\phi & \\{1, 7, 5\\} & [4, 3, 2, 0, 0, 1] \\\\ 6 & \\phi & \\phi & \\phi & \\phi & \\{4, 8, 3, 9, 2, 6\\} & \\phi & \\{1, 7, 5\\} & [4, 3, 2, 6, 0, 1] \\\\ 7 & \\phi & \\phi & \\phi & \\phi & \\{4, 8, 3, 9, 2, 6\\} & \\phi & \\{1, 7, 5\\} & [4, 3, 2, 6, 0, 1] \\\\ 8 & \\phi & \\phi & \\phi & \\phi & \\phi & \\phi & \\{4, 8, 3, 9, 2, 6, 1, 7, 5\\} & [4, 3, 2, 6, 8, 1] \\end{array} $$ c. We let each of the sets be represented by a disjoint set structure. To union them (as on line 6) just call $\\text{UNION}$. Checking that they exist is just a matter of keeping track of a linked list of which ones exist(needed for line 5), initially containing all of them, but then, when deleting the set on line 6, we delete it from the linked list that we were maintaining. The only other interaction with the sets that we have to worry about is on line 2, which just amounts to a call of $\\text{FIND-SET}(j)$. Since line 2 takes amortized time $\\alpha(n)$ and we call it exactly $n$ times, then, since the rest of the for loop only takes constant time, the total runtime is $O(n\\alpha(n))$.","title":"21-1 Off-line minimum"},{"location":"Chap21/Problems/21-2/","text":"In the depth-determination problem , we maintain a forest $\\mathcal F = \\{T_i\\}$ of rooted trees under three operations: $\\text{MAKE-TREE}(v)$ creates a tree whose only node is $v$. $\\text{FIND-DEPTH}(v)$ returns the depth of node $v$ within its tree. $\\text{GRAFT}(r, v)$ makes node $r$, which is assumed to be the root of a tree, become the child of node $v$, which is assumed to be in a different tree than $r$ but may or may not itself be a root. a. Suppose that we use a tree representation similar to a disjoint-set forest: $v.p$ is the parent of node $v$, except that $v.p = v$ if $v$ is a root. Suppose further that we implement $\\text{GRAFT}(r, v)$ by setting $r.p = v$ and $\\text{FIND-DEPTH}(v)$ by following the find path up to the root, returning a count of all nodes other than $v$ encountered. Show that the worst-case running time of a sequence of $m$ $\\text{MAKE-TREE}$, $\\text{FIND-DEPTH}$, and $\\text{GRAFT}$ operations is $\\Theta(m^2)$. By using the union-by-rank and path-compression heuristics, we can reduce the worst-case running time. We use the disjoint-set forest $\\mathcal S = \\{S_i\\}$, where each set $S_i$ (which is itself a tree) corresponds to a tree $T_i$ in the forest $\\mathcal F$. The tree structure within a set $S_i$, however, does not necessarily correspond to that of $T_i$. In fact, the implementation of $S_i$ does not record the exact parent-child relationships but nevertheless allows us to determine any node's depth in $T_i$. The key idea is to maintain in each node $v$ a \"pseudodistance\" $v.d$, which is defined so that the sum of the pseudodistances along the simple path from $v$ to the root of its set $S_i$ equals the depth of $v$ in $T_i$. That is, if the simple path from $v$ to its root in $S_i$ is $v_0, v_1, \\ldots, v_k$, where $v_0 = v$ and $v_k$ is $S_i$'s root, then the depth of $v$ in $T_i$ is $\\sum_{j = 0}^k v_j.d$. b. Give an implementation of $\\text{MAKE-TREE}$. c. Show how to modify $\\text{FIND-SET}$ to implement $\\text{FIND-DEPTH}$. Your implementation should perform path compression, and its running time should be linear in the length of the find path. Make sure that your implementation updates pseudodistances correctly. d. Show how to implement $\\text{GRAFT}(r, v)$, which combines the sets containing $r$ and $v$, by modifying the $\\text{UNION}$ and $\\text{LINK}$ procedures. Make sure that your implementation updates pseudodistances correctly. Note that the root of a set $S_i$ is not necessarily the root of the corresponding tree $T_i$. e. Give a tight bound on the worst-case running time of a sequence of $m$ $\\text{MAKE-TREE}$, $\\text{FIND-DEPTH}$, and $\\text{GRAFT}$ operations, $n$ of which are $\\text{MAKE-TREE}$ operations. a. Denote the number of nodes by $n$, and let $n = (m + 1) / 3$, so that $m = 3n - 1$. First, perform the $n$ operations $\\text{MAKE-TREE}(v_1)$, $\\text{MAKE-TREE}(v_2)$, $\\ldots$, $\\text{MAKE-TREE}(v_n)$. Then perform the sequence of $n - 1$ $\\text{GRAFT}$ operations $\\text{GRAFT}(v_1, v_2)$, $\\text{GRAFT}(v_2, v_3)$, $\\ldots$, $\\text{GRAFT}(v_n - 1, v_n)$; this sequence produces a single disjoint-set tree that is a linear chain of $n$ nodes with $v_n$ at the root and $v_1$ as the only leaf. Then perform $\\text{FIND-DEPTH}(v_1)$ repeatedly, $n$ times. The total number of operations is $n + (n - 1) + n = 3n - 1 = m$. Each $\\text{MAKE-TREE}$ and $\\text{GRAFT}$ operation takes $O(1)$ time. Each $\\text{FIND-DEPTH}$ operation has to follow an $n$-node find path, and so each of the $n$ $\\text{FIND-DEPTH}$ operations takes $\\Theta(n)$ time. The total time is $n \\cdot \\Theta(n) + (2n - 1) \\cdot O(1) = \\Theta(n^2) = \\Theta(m^2)$. b. $\\text{MAKE-TREE}$ is like $\\text{MAKE-SET}$, except that it also sets the $d$ value to $0$: 1 2 3 4 MAKE - TREE ( v ) v . p = v v . rank = 0 v . d = 0 It is correct to set $v.d$ to $0$, because the depth of the node in the single-node disjoint-set tree is $0$, and the sum of the depths on the find path for $v$ consists only of $v.d$. c. $\\text{FIND-DEPTH}$ will call a procedure $\\text{FIND-ROOT}$: 1 2 3 4 5 6 FIND - ROOT ( v ) if v . p != v . p . p y = v . p v . p = FIND - ROOT ( y ) v . d = v . d + y . d return v . p 1 2 3 4 5 FIND - DEPTH ( v ) FIND - ROOT ( v ) // no need to save the return value if v == v . p return v . d else return v . d + v . p . d $\\text{FIND-ROOT}$ performs path compression and updates pseudodistances along the find path from $v$. It is similar to $\\text{FIND-SET}$ on page 571, but with three changes. First, when $v$ is either the root or a child of a root (one of these conditions holds if and only if $v.p = v.p.p$) in the disjoint-set forest, we don't have to recurse; instead, we just return $v.p$. Second, when we do recurse, we save the pointer $v.p$ into a new variable $y$. Third, when we recurse, we update $v.d$ by adding into it the $d$ values of all nodes on the find path that are no longer proper ancestors of $v$ after path compression; these nodes are precisely the proper ancestors of $v$ other than the root. Thus, as long as $v$ does not start out the $\\text{FIND-ROOT}$ call as either the root or a child of the root, we add $y.d$ into $v.d$. Note that $y.d$ has been updated prior to updating $v.d$, if $y$ is also neither the root nor a child of the root. $\\text{FIND-DEPTH}$ first calls $\\text{FIND-ROOT}$ to perform path compression and update pseudodistances. Afterward, the find path from $v$ consists of either just $v$ (if $v$ is a root) or just $v$ and $v.p$ (if $v$ is not a root, in which case it is a child of the root after path compression). In the former case, the depth of $v$ is just $v.d$, and in the latter case, the depth is $v.d + v.p.d$. d. Our procedure for $\\text{GRAFT}$ is a combination of $\\text{UNION}$ and $\\text{LINK}$: 1 2 3 4 5 6 7 8 9 10 11 12 GRAPT ( r , v ) r ' = FIND - ROOT ( r ) v ' = FIND - ROOT ( v ) z = FIND - DEPTH ( v ) if r ' . rank > v ' . rank v ' . p = r ' r ' . d = r ' . d + z + 1 v ' . d = v ' . d - r ' . d else r ' . p = v ' r ' . d = r ' . d + z + 1 - v ' . d if r ' . rank == v ' . rank v ' . rank = v ' . rank + 1 This procedure works as follows. First, we call $\\text{FIND-ROOT}$ on $r$ and $v$ in order to find the roots $r'$ and $v'$, respectively, of their trees in the disjoint-set forest. As we saw in part (c), these $\\text{FIND-ROOT}$ calls also perform path compression and update pseudodistances on the find paths from $r$ and $v$. We then call $\\text{FIND-DEPTH}(v)$, saving the depth of $v$ in the variable $z$. (Since we have just compressed $v$'s find path, this call of $\\text{FIND-DEPTH}$ takes $\\text{O(1)}$ time.) Next, we emulate the action of $\\text{LINK}$, by making the root ($r'$ or $v'$) of smaller rank a child of the root of larger rank; in case of a tie, we make $r'$ a child of $v'$. If $v'$ has the smaller rank, then all nodes in $r$'s tree will have their depths increased by the depth of $v$ plus $1$ (because $r$ is to become a child of $v$). Altering the psuedodistance of the root of a disjoint-set tree changes the computed depth of all nodes in that tree, and so adding $z + 1$ to $r'.d$ accomplishes this update for all nodes in $r$'s disjoint-set tree. Since $v'$ will become a child of $r'$ in the disjoint-set forest, we have just increased the computed depth of all nodes in the disjoint-set tree rooted at $v'$ by $r'.d$. These computed depths should not have changed, however. Thus, we subtract off $r'.d$ from $v'.d$, so that the sum $v'.d + r'.d$ after making $v'$ a child of $r'$ equals $v'.d$ before making $v'$ a child of $r'$. On the other hand, if $r'$ has the smaller rank, or if the ranks are equal, then $r'$ becomes a child of $v'$ in the disjoint-set forest. In this case, $v'$ remains a root in the disjoint-set forest afterward, and we can leave $v'.d$ alone. We have to update $r'.d$, however, so that after making $r'$ a child of $v'$, the depth of each node in $r$'s disjoint-set tree is increased by $z + 1$. We add $z + 1$ to $r'.d$, but we also subtract out $v'.d$, since we have just made $r'$ a child of $v'$. Finally, if the ranks of $r'$ and $v'$ are equal, we increment the rank of $v'$, as is done in the $\\text{LINK}$ procedure. e. The asymptotic running times of $\\text{MAKE-TREE}$, $\\text{FIND-DEPTH}$, and $\\text{GRAFT}$ are equivalent to those of $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{UNION}$, respectively. Thus, a sequence of $m$ operations, $n$ of which are $\\text{MAKE-TREE}$ operations, takes $\\Theta(m\\alpha(n))$ time in the worst case.","title":"21-2 Depth determination"},{"location":"Chap21/Problems/21-3/","text":"The least common ancestor of two nodes $u$ and $v$ in a rooted tree $T$ is the node $w$ that is an ancestor of both $u$ and $v$ and that has the greatest depth in $T$. In the off-line least-common-ancestors problem , we are given a rooted tree $T$ and an arbitrary set $P = \\{\\{u, v\\}\\}$ of unordered pairs of nodes in $T$, and we wish to determine the least common ancestor of each pair in $P$. To solve the off-line least-common-ancestors problem, the following procedure performs a tree walk of $T$ with the initial call $\\text{LCA}(T.root)$. We assume that each node is colored $\\text{WHITE}$ prior to the walk. 1 2 3 4 5 6 7 8 9 10 11 LCA ( u ) MAKE - SET ( u ) FIND - SET ( u ). ancestor = u for each child v of u in T LCA ( v ) UNION ( u , v ) FIND - SET ( u ). ancestor = u u . color = BLACK for each node v such that { u , v } \u2208 P if v . color == BLACK print \"The least common ancestor of\" u \"and\" v \"is\" FIND - SET ( v ). ancestor a. Argue that line 10 executes exactly once for each pair $\\{u, v\\} \\in P$. b. Argue that at the time of the call $\\text{LCA}(u)$, the number of sets in the disjoint-set data structure equals the depth of $u$ in $T$. c. Prove that $\\text{LCA}$ correctly prints the least common ancestor of $u$ and $v$ for each pair $\\{u, v\\} \\in P$. d. Analyze the running time of $\\text{LCA}$, assuming that we use the implementation of the disjoint-set data structure in Section 21.3. a. Suppose that we let $\\le_{LCA}$ to be an ordering on the vertices so that $u \\le_{LCA} v$ if we run line 7 of $\\text{LCA}(u)$ before line 7 of $\\text{LCA}(v)$. Then, when we are running line 7 of $\\text{LCA}(u)$, we immediately go on to the for loop on line 8. So, while we are doing this for loop, we still haven't called line 7 of $\\text{LCA}(v)$. This means that $v.color$ is white, and so, the pair $\\{u, v\\}$ is not considered during the run of $\\text{LCA}(u)$. However, during the for loop of $\\text{LCA}(v)$, since line 7 of $\\text{LCA}(u)$ has already run, $u.color = black$. This means that we will consider the pair $\\{u, v\\}$ during the running of $\\text{LCA}(v)$. It is not obvious what the ordering $\\le_{LCA}$ is, as it will be implementation dependent. It depends on the order in which child vertices are iterated in the for loop on line 3. That is, it doesn't just depend on the graph structure. b. We suppose that it is true prior to a given call of $\\text{LCA}$, and show that this property is preserved throughout a run of the procedure, increasing the number of disjoint sets by one by the end of the procedure. So, supposing that $u$ has depth $d$ and there are $d$ items in the disjoint set data structure before it runs, it increases to $d + 1$ disjoint sets on line 1. So, by the time we get to line 4, and call $\\text{LCA}$ of a child of $u$, there are $d + 1$ disjoint sets, this is exactly the depth of the child. After line 4, there are now $d + 2$ disjoint sets, so, line 5 brings it back down to $d + 1$ disjoint sets for the subsequent times through the loop. After the loop, there are no more changes to the number of disjoint sets, so, the algorithm terminates with $\\text{d + 1}$ disjoint sets, as desired. Since this holds for any arbitrary run of $\\text{LCA}$, it holds for all runs of $\\text{LCA}$. c. Suppose that the pair $u$ and $v$ have the least common ancestor $w$. Then, when running $\\text{LCA}(w)$, $u$ will be in the subtree rooted at one of $w$'s children, and $v$ will be in another. WLOG, suppose that the subtree containing $u$ runs first. So, when we are done with running that subtree, all of their ancestor values will point to $w$ and their colors will be black, and their ancestor values will not change until $\\text{LCA}(w)$ returns. However, we run $\\text{LCA}(v)$ before $\\text{LCA}(w)$ returns, so in the for loop on line 8 of $\\text{LCA}(v)$, we will be considering the pair $\\{u, v\\}$, since $u.color = black$. Since $u.ancestor$ is still $w$, that is what will be output, which is the correct answer for their $\\text{LCA}$. d. The time complexity of lines 1 and 2 are just constant. Then, for each child, we have a call to the same procedure, a $\\text{UNION}$ operation which only takes constant time, and a $\\text{FIND-SET}$ operation which can take at most amortized inverse Ackerman's time. Since we check each and every thing that is adjacent to $u$ for being black, we are only checking each pair in $P$ at most twice in lines 8-10, among all the runs of $\\text{LCA}$. This means that the total runtime is $O(|T|\\alpha(|T|) + |P|)$.","title":"21-3 Tarjan's off-line least-common-ancestors algorithm"},{"location":"Chap22/22.1/","text":"22.1-1 Given an adjacency-list representation of a directed graph, how long does it take to compute the $\\text{out-degree}$ of every vertex? How long does it take to compute the $\\text{in-degree}s$? Since it seems as though the list for the neighbors of each vertex $v$ is just an undecorated list, to find the length of each would take time $O(\\text{out-degree}(v))$. So, the total cost will be $$\\sum_{v \\in V}O(\\text{out-degree}(v)) = O(|E| + |V|).$$ Note that the $|V|$ showing up in the asymptotics is necessary, because it still takes a constant amount of time to know that a list is empty. This time could be reduced to $O(|V|)$ if for each list in the adjacency list representation, we just also stored its length. To compute the in degree of each vertex, we will have to scan through all of the adjacency lists and keep counters for how many times each vertex has appeared. As in the previous case, the time to scan through all of the adjacency lists takes time $O(|E| + |V|)$. 22.1-2 Give an adjacency-list representation for a complete binary tree on $7$ vertices. Give an equivalent adjacency-matrix representation. Assume that vertices are numbered from $1$ to $7$ as in a binary heap. Adjacency-list representation $$ \\begin{aligned} 1 &: 2 \\rightarrow 3 \\\\ 2 &: 1 \\rightarrow 4 \\rightarrow 5 \\\\ 3 &: 1 \\rightarrow 6 \\rightarrow 7 \\\\ 4 &: 2 \\\\ 5 &: 2 \\\\ 6 &: 3 \\\\ 7 &: 3. \\end{aligned} $$ Adjacency-matrix representation $$ \\begin{pmatrix} 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 1 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ \\end{pmatrix}. $$ 22.1-3 The transpose of a directed graph $G = (V, E)$ is the graph $G^\\text T = (V, E^\\text T)$, where $E^\\text T = \\{ (v, u) \\in V \\times V: (u, v) \\in E \\}$. Thus, $G^\\text T$ is $G$ with all its edges reversed. Describe efficient algorithms for computing $G^\\text T$ from $G$, for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation: For the adjacency list representation, we will maintain an initially empty adjacency list representation of the transpose. Then, we scan through every list in the original graph. If we are in the list corresponding to vertex $v$ and see $u$ as an entry in the list, then we add an entry of $v$ to the list in the transpose graph corresponding to vertex $u$. Since this only requires a scan through all of the lists, it only takes time $O(|E| + |V|)$. Adjacency-matrix representation: to compute the graph transpose, we just take the matrix transpose. This means looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. This takes time $O(|V|^2)$. 22.1-4 Given an adjacency-list representation of a multigraph $G = (V, E)$, describe an $O(V + E)$-time algorithm to compute the adjacency-list representation of the \"equivalent\" undirected graph $G' = (V, E')$, where $E'$ consists of the edges in $E$ with all multiple edges between two vertices replaced by a single edge and with all self-loops removed. Create a new adjacency-list $Adj'$ of size $|V|$ and an empty matrix $M$ of size $|V|^2$ first. For each vertex $u$ in the multigraph $G$, we iterably examine each vertex $v$ of $Adj[u]$. If $M(u, v) == \\emptyset$, mark $M(u, v)$ as $\\text{true}$, then add $v$ to $Adj'[u]$. If $M(u, v) == true$, do nothing. Since we lookup in the adjacency-list $Adj$ for $|V + E|$ times, the time complexity is $O(V + E)$. 1 2 3 4 5 6 7 8 EQUIVALENT - UNDIRECTED - GRAPH let Adj ' [ 1. . | V | ] be a new adjacency - list let M be | V | \u00d7 | V | for each vertex u \u2208 G . V for each v \u2208 Adj [ u ] if M [ u , v ] == \u00d8 && u != v M [ u , v ] = true INSERT ( Adj ' [ u ], v ) 22.1-5 The square of a directed graph $G = (V, E)$ is the graph $G^2 = (V, E^2)$ such that $(u, v) \\in E^2$ if and only if $G$ contains a path with at most two edges between $u$ and $v$. Describe efficient algorithms for computing $G^2$ from $G$ for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation: To compute $G^2$ from the adjacency-list representation $Adj$ of $G$, we perform the following for each $Adj[u]$: 1 2 3 4 for each v \u2208 Adj [ u ] for each w \u2208 Adj [ v ] edge ( u , w ) \u2208 E ^ 2 INSERT ( Adj2 [ u ], w ) where $Adj2$ is the adjacency-list representation of $G^2$. After we have computed $Adj2$, we have to remove any duplicate edges from the lists (there may be more than one two-edge path in $G$ between any two vertices). For every edge in $Adj$ we scan at most $|V|$ vertices, we compute $Adj2$ in time $O(VE)$. Removing duplicate edges is done in $O(V + E)$ as shown in exercise 22.1-4. Thus the total running time is $$O(VE) + O(V + E) = O(VE).$$ Adjacency-matrix representation: Let $A$ denote the adjacency-matrix representation of $G$. The adjacency-matrix representation of $G^2$ is the square of $A$. Computing $A^2$ can be done in time $O(V^3)$ (and even faster, theoretically; Strassen's algorithm for example will compute $A^2$ in $O(V^{\\lg 7})$). 22.1-6 Most graph algorithms that take an adjacency-matrix representation as input require time $\\Omega(V^2)$, but there are some exceptions. Show how to determine whether a directed graph $G$ contains a universal sink $-$ a vertex with $\\text{in-degree}$ $|V| - 1$ and $\\text{out-degree}$ $0$ $-$ in time $O(V)$, given an adjacency matrix for $G$. We start by observing that if $a_{ij} = 1$, so that $(i, j) \\in E$, then vertex $i$ cannot be a universal sink, for it has an outgoing edge. Thus, if row $i$ contains a $1$, then vertex $i$ cannot be a universal sink. This observation also means that if there is a $\\text{self-loop}(i, i)$, then vertex $i$ is not a universal sink. Now suppose that $a_{ij} = 0$, so that $(i, j) \\notin E$, and also that $i \\ne j$. Then vertex $j$ cannot be a universal sink, for either its $\\text{in-degree}$ must be strictly less than $|V| - 1$ or it has a self-loop. Thus if column $j$ contains a $0$ in any position other than the diagonal entry $(j, j)$, then vertex $j$ cannot be a universal sink. Using the above observations, the following procedure returns $\\text{TRUE}$ if vertex $k$ is a universal sink, and $\\text{FALSE}$ otherwise. It takes as input a $|V| \\times |V|$ adjacency matrix $A = (a_{ij})$. 1 2 3 4 5 6 7 8 9 IS - SINK ( A , k ) let A be | V | \u00d7 | V | for j = 1 to | V | // Check for a 1 in row k if a [ k , j ] == 1 return false for i = 1 to | V | // Check for an off-diagonal 0 in column k if a [ i , k ] == 0 and i != k return false return true Because this procedure runs in $O(V)$ time, we may call it only $O(1)$ times in order to achieve our $O(V)$-time bound for determining whether directed graph $G$ contains a universal sink. Observe also that a directed graph can have at most one universal sink. This property holds because if vertex $j$ is a universal sink, then we would have $(i, j) \\in E$ for all $i \\ne j$ and so no other vertex $i$ could be a universal sink. The following procedure takes an adjacency matrix $A$ as input and returns either a message that there is no universal sink or a message containing the identity of the universal sink. It works by eliminating all but one vertex as a potential universal sink and then checking the remaining candidate vertex by a single call to $\\text{IS-SINK}$. 1 2 3 4 5 6 7 8 9 10 11 12 UNIVERSAL - SINK ( A ) let A be | V | \u00d7 | V | i = j = 1 while i \u2264 | V | and j \u2264 | V | if a [ i , j ] == 1 i = i + 1 else j = j + 1 if i > | V | return \"there is no universal sink\" else if IS - SINK ( A , i ) = false return \"there is no universal sink\" else return i \"is a universal sink\" $\\text{UNIVERSAL-SINK}$ walks through the adjacency matrix, starting at the upper left corner and always moving either right or down by one position, depending on whether the current entry $a_{ij}$ it is examining is $0$ or $1$. It stops once either $i$ or $j$ exceeds $|V|$. To understand why $\\text{UNIVERSAL-SINK}$ works, we need to show that after the while loop terminates, the only vertex that might be a universal sink is vertex $i$. The call to $\\text{IS-SINK}$ then determines whether vertex $i$ is indeed a universal sink. Let us fix $i$ and $j$ to be values of these variables at the termination of the while loop. We claim that every vertex $k$ such that $1 \\le k < i$ cannot be a universal sink. That is because the way that $i$ achieved its final value at loop termination was by finding a $1$ in each row $k$ for which $1 \\le k < i$. As we observed above, any vertex $k$ whose row contains a $1$ cannot be a universal sink. If $i > |V|$ at loop termination, then we have eliminated all vertices from consid- eration, and so there is no universal sink. If, on the other hand, $i \\le |V|$ at loop termination, we need to show that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink. If $i \\le |V|$ at loop termination, then the while loop terminated because $j > |V|$. That means that we found a $0$ in every column. Recall our earlier observation that if column $k$ contains a $0$ in an off-diagonal position, then vertex $k$ cannot be a universal sink. Since we found a $0$ in every column, we found a $0$ in every column $k$ such that $i < k \\le |V|$. Moreover, we never examined any matrix entries in rows greater than $i$, and so we never examined the diagonal entry in any column $k$ such that $i < k \\le |V|$. Therefore, all the $0$s that we found in columns $k$ such that $i < k \\le |V|$ were off-diagonal. We conclude that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink. Thus, we have shown that every vertex less than $i$ and every vertex greater than $i$ cannot be a universal sink. The only remaining possibility is that vertex $i$ might be a universal sink, and the call to $\\text{IS-SINK}$ checks whether it is. To see that $\\text{UNIVERSAL-SINK}$ runs in $O(V)$ time, observe that either $i$ or $j$ is incremented in each iteration of the while loop. Thus, the while loop makes at most $2|V| - 1$ iterations. Each iteration takes $O(1)$ time, for a total while loop time of $O(V)$ and, combined with the $O(V)$-time call to $\\text{IS-SINK}$, we get a total running time of $O(V)$. 22.1-7 The incidence matrix of a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $B = (b_{ij})$ such that $$ b_{ij} = \\begin{cases} -1 & \\text{if edge $j$ leaves vertex $i$}, \\\\ 1 & \\text{if edge $j$ enters vertex $i$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ Describe what the entries of the matrix product $BB^\\text T$ represent, where $B^\\text T$ is the transpose of $B$. $$BB^\\text T(i, j) = \\sum\\limits_{e\\in E}b_{ie} b_{ej}^\\text T = \\sum\\limits_{e\\in E}b_{ie}b_{je}.$$ If $i = j$, then $b_{ie} b_{je} = 1$ (it is $1 \\cdot 1$ or $(-1) \\cdot (-1)$) whenever $e$ enters or leaves vertex $i$, and $0$ otherwise. If $i \\ne j$, then $b_{ie} b_{je} = -1$ when $e = (i, j)$ or $e = (j, i)$, and $0$ otherwise. Thus, $$ BB^\\text T(i, j) = \\begin{cases} \\text{degree of $i$ = in-degree + out-degree} & \\text{if $i = j$}, \\\\ \\text{$-$(\\# of edges connecting $i$ and $j$)} & \\text{if $i \\ne j$}. \\end{cases} $$ 22.1-8 Suppose that instead of a linked list, each array entry $Adj[u]$ is a hash table containing the vertices $v$ for which $(u, v) \\in E$. If all edge lookups are equally likely, what is the expected time to determine whether an edge is in the graph? What disadvantages does this scheme have? Suggest an alternate data structure for each edge list that solves these problems. Does your alternative have disadvantages compared to the hash table? The expected loopup time is $O(1)$, but in the worst case it could take $O(|V|)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\\lg |V|)$, but this has the disadvantage of having a much worse expected lookup time.","title":"22.1 Representations of graphs"},{"location":"Chap22/22.1/#221-1","text":"Given an adjacency-list representation of a directed graph, how long does it take to compute the $\\text{out-degree}$ of every vertex? How long does it take to compute the $\\text{in-degree}s$? Since it seems as though the list for the neighbors of each vertex $v$ is just an undecorated list, to find the length of each would take time $O(\\text{out-degree}(v))$. So, the total cost will be $$\\sum_{v \\in V}O(\\text{out-degree}(v)) = O(|E| + |V|).$$ Note that the $|V|$ showing up in the asymptotics is necessary, because it still takes a constant amount of time to know that a list is empty. This time could be reduced to $O(|V|)$ if for each list in the adjacency list representation, we just also stored its length. To compute the in degree of each vertex, we will have to scan through all of the adjacency lists and keep counters for how many times each vertex has appeared. As in the previous case, the time to scan through all of the adjacency lists takes time $O(|E| + |V|)$.","title":"22.1-1"},{"location":"Chap22/22.1/#221-2","text":"Give an adjacency-list representation for a complete binary tree on $7$ vertices. Give an equivalent adjacency-matrix representation. Assume that vertices are numbered from $1$ to $7$ as in a binary heap. Adjacency-list representation $$ \\begin{aligned} 1 &: 2 \\rightarrow 3 \\\\ 2 &: 1 \\rightarrow 4 \\rightarrow 5 \\\\ 3 &: 1 \\rightarrow 6 \\rightarrow 7 \\\\ 4 &: 2 \\\\ 5 &: 2 \\\\ 6 &: 3 \\\\ 7 &: 3. \\end{aligned} $$ Adjacency-matrix representation $$ \\begin{pmatrix} 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 1 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ \\end{pmatrix}. $$","title":"22.1-2"},{"location":"Chap22/22.1/#221-3","text":"The transpose of a directed graph $G = (V, E)$ is the graph $G^\\text T = (V, E^\\text T)$, where $E^\\text T = \\{ (v, u) \\in V \\times V: (u, v) \\in E \\}$. Thus, $G^\\text T$ is $G$ with all its edges reversed. Describe efficient algorithms for computing $G^\\text T$ from $G$, for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation: For the adjacency list representation, we will maintain an initially empty adjacency list representation of the transpose. Then, we scan through every list in the original graph. If we are in the list corresponding to vertex $v$ and see $u$ as an entry in the list, then we add an entry of $v$ to the list in the transpose graph corresponding to vertex $u$. Since this only requires a scan through all of the lists, it only takes time $O(|E| + |V|)$. Adjacency-matrix representation: to compute the graph transpose, we just take the matrix transpose. This means looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. This takes time $O(|V|^2)$.","title":"22.1-3"},{"location":"Chap22/22.1/#221-4","text":"Given an adjacency-list representation of a multigraph $G = (V, E)$, describe an $O(V + E)$-time algorithm to compute the adjacency-list representation of the \"equivalent\" undirected graph $G' = (V, E')$, where $E'$ consists of the edges in $E$ with all multiple edges between two vertices replaced by a single edge and with all self-loops removed. Create a new adjacency-list $Adj'$ of size $|V|$ and an empty matrix $M$ of size $|V|^2$ first. For each vertex $u$ in the multigraph $G$, we iterably examine each vertex $v$ of $Adj[u]$. If $M(u, v) == \\emptyset$, mark $M(u, v)$ as $\\text{true}$, then add $v$ to $Adj'[u]$. If $M(u, v) == true$, do nothing. Since we lookup in the adjacency-list $Adj$ for $|V + E|$ times, the time complexity is $O(V + E)$. 1 2 3 4 5 6 7 8 EQUIVALENT - UNDIRECTED - GRAPH let Adj ' [ 1. . | V | ] be a new adjacency - list let M be | V | \u00d7 | V | for each vertex u \u2208 G . V for each v \u2208 Adj [ u ] if M [ u , v ] == \u00d8 && u != v M [ u , v ] = true INSERT ( Adj ' [ u ], v )","title":"22.1-4"},{"location":"Chap22/22.1/#221-5","text":"The square of a directed graph $G = (V, E)$ is the graph $G^2 = (V, E^2)$ such that $(u, v) \\in E^2$ if and only if $G$ contains a path with at most two edges between $u$ and $v$. Describe efficient algorithms for computing $G^2$ from $G$ for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation: To compute $G^2$ from the adjacency-list representation $Adj$ of $G$, we perform the following for each $Adj[u]$: 1 2 3 4 for each v \u2208 Adj [ u ] for each w \u2208 Adj [ v ] edge ( u , w ) \u2208 E ^ 2 INSERT ( Adj2 [ u ], w ) where $Adj2$ is the adjacency-list representation of $G^2$. After we have computed $Adj2$, we have to remove any duplicate edges from the lists (there may be more than one two-edge path in $G$ between any two vertices). For every edge in $Adj$ we scan at most $|V|$ vertices, we compute $Adj2$ in time $O(VE)$. Removing duplicate edges is done in $O(V + E)$ as shown in exercise 22.1-4. Thus the total running time is $$O(VE) + O(V + E) = O(VE).$$ Adjacency-matrix representation: Let $A$ denote the adjacency-matrix representation of $G$. The adjacency-matrix representation of $G^2$ is the square of $A$. Computing $A^2$ can be done in time $O(V^3)$ (and even faster, theoretically; Strassen's algorithm for example will compute $A^2$ in $O(V^{\\lg 7})$).","title":"22.1-5"},{"location":"Chap22/22.1/#221-6","text":"Most graph algorithms that take an adjacency-matrix representation as input require time $\\Omega(V^2)$, but there are some exceptions. Show how to determine whether a directed graph $G$ contains a universal sink $-$ a vertex with $\\text{in-degree}$ $|V| - 1$ and $\\text{out-degree}$ $0$ $-$ in time $O(V)$, given an adjacency matrix for $G$. We start by observing that if $a_{ij} = 1$, so that $(i, j) \\in E$, then vertex $i$ cannot be a universal sink, for it has an outgoing edge. Thus, if row $i$ contains a $1$, then vertex $i$ cannot be a universal sink. This observation also means that if there is a $\\text{self-loop}(i, i)$, then vertex $i$ is not a universal sink. Now suppose that $a_{ij} = 0$, so that $(i, j) \\notin E$, and also that $i \\ne j$. Then vertex $j$ cannot be a universal sink, for either its $\\text{in-degree}$ must be strictly less than $|V| - 1$ or it has a self-loop. Thus if column $j$ contains a $0$ in any position other than the diagonal entry $(j, j)$, then vertex $j$ cannot be a universal sink. Using the above observations, the following procedure returns $\\text{TRUE}$ if vertex $k$ is a universal sink, and $\\text{FALSE}$ otherwise. It takes as input a $|V| \\times |V|$ adjacency matrix $A = (a_{ij})$. 1 2 3 4 5 6 7 8 9 IS - SINK ( A , k ) let A be | V | \u00d7 | V | for j = 1 to | V | // Check for a 1 in row k if a [ k , j ] == 1 return false for i = 1 to | V | // Check for an off-diagonal 0 in column k if a [ i , k ] == 0 and i != k return false return true Because this procedure runs in $O(V)$ time, we may call it only $O(1)$ times in order to achieve our $O(V)$-time bound for determining whether directed graph $G$ contains a universal sink. Observe also that a directed graph can have at most one universal sink. This property holds because if vertex $j$ is a universal sink, then we would have $(i, j) \\in E$ for all $i \\ne j$ and so no other vertex $i$ could be a universal sink. The following procedure takes an adjacency matrix $A$ as input and returns either a message that there is no universal sink or a message containing the identity of the universal sink. It works by eliminating all but one vertex as a potential universal sink and then checking the remaining candidate vertex by a single call to $\\text{IS-SINK}$. 1 2 3 4 5 6 7 8 9 10 11 12 UNIVERSAL - SINK ( A ) let A be | V | \u00d7 | V | i = j = 1 while i \u2264 | V | and j \u2264 | V | if a [ i , j ] == 1 i = i + 1 else j = j + 1 if i > | V | return \"there is no universal sink\" else if IS - SINK ( A , i ) = false return \"there is no universal sink\" else return i \"is a universal sink\" $\\text{UNIVERSAL-SINK}$ walks through the adjacency matrix, starting at the upper left corner and always moving either right or down by one position, depending on whether the current entry $a_{ij}$ it is examining is $0$ or $1$. It stops once either $i$ or $j$ exceeds $|V|$. To understand why $\\text{UNIVERSAL-SINK}$ works, we need to show that after the while loop terminates, the only vertex that might be a universal sink is vertex $i$. The call to $\\text{IS-SINK}$ then determines whether vertex $i$ is indeed a universal sink. Let us fix $i$ and $j$ to be values of these variables at the termination of the while loop. We claim that every vertex $k$ such that $1 \\le k < i$ cannot be a universal sink. That is because the way that $i$ achieved its final value at loop termination was by finding a $1$ in each row $k$ for which $1 \\le k < i$. As we observed above, any vertex $k$ whose row contains a $1$ cannot be a universal sink. If $i > |V|$ at loop termination, then we have eliminated all vertices from consid- eration, and so there is no universal sink. If, on the other hand, $i \\le |V|$ at loop termination, we need to show that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink. If $i \\le |V|$ at loop termination, then the while loop terminated because $j > |V|$. That means that we found a $0$ in every column. Recall our earlier observation that if column $k$ contains a $0$ in an off-diagonal position, then vertex $k$ cannot be a universal sink. Since we found a $0$ in every column, we found a $0$ in every column $k$ such that $i < k \\le |V|$. Moreover, we never examined any matrix entries in rows greater than $i$, and so we never examined the diagonal entry in any column $k$ such that $i < k \\le |V|$. Therefore, all the $0$s that we found in columns $k$ such that $i < k \\le |V|$ were off-diagonal. We conclude that every vertex $k$ such that $i < k \\le |V|$ cannot be a universal sink. Thus, we have shown that every vertex less than $i$ and every vertex greater than $i$ cannot be a universal sink. The only remaining possibility is that vertex $i$ might be a universal sink, and the call to $\\text{IS-SINK}$ checks whether it is. To see that $\\text{UNIVERSAL-SINK}$ runs in $O(V)$ time, observe that either $i$ or $j$ is incremented in each iteration of the while loop. Thus, the while loop makes at most $2|V| - 1$ iterations. Each iteration takes $O(1)$ time, for a total while loop time of $O(V)$ and, combined with the $O(V)$-time call to $\\text{IS-SINK}$, we get a total running time of $O(V)$.","title":"22.1-6"},{"location":"Chap22/22.1/#221-7","text":"The incidence matrix of a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $B = (b_{ij})$ such that $$ b_{ij} = \\begin{cases} -1 & \\text{if edge $j$ leaves vertex $i$}, \\\\ 1 & \\text{if edge $j$ enters vertex $i$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ Describe what the entries of the matrix product $BB^\\text T$ represent, where $B^\\text T$ is the transpose of $B$. $$BB^\\text T(i, j) = \\sum\\limits_{e\\in E}b_{ie} b_{ej}^\\text T = \\sum\\limits_{e\\in E}b_{ie}b_{je}.$$ If $i = j$, then $b_{ie} b_{je} = 1$ (it is $1 \\cdot 1$ or $(-1) \\cdot (-1)$) whenever $e$ enters or leaves vertex $i$, and $0$ otherwise. If $i \\ne j$, then $b_{ie} b_{je} = -1$ when $e = (i, j)$ or $e = (j, i)$, and $0$ otherwise. Thus, $$ BB^\\text T(i, j) = \\begin{cases} \\text{degree of $i$ = in-degree + out-degree} & \\text{if $i = j$}, \\\\ \\text{$-$(\\# of edges connecting $i$ and $j$)} & \\text{if $i \\ne j$}. \\end{cases} $$","title":"22.1-7"},{"location":"Chap22/22.1/#221-8","text":"Suppose that instead of a linked list, each array entry $Adj[u]$ is a hash table containing the vertices $v$ for which $(u, v) \\in E$. If all edge lookups are equally likely, what is the expected time to determine whether an edge is in the graph? What disadvantages does this scheme have? Suggest an alternate data structure for each edge list that solves these problems. Does your alternative have disadvantages compared to the hash table? The expected loopup time is $O(1)$, but in the worst case it could take $O(|V|)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\\lg |V|)$, but this has the disadvantage of having a much worse expected lookup time.","title":"22.1-8"},{"location":"Chap22/22.2/","text":"22.2-1 Show the $d$ and $\\pi$ values that result from running breadth-first search on the directed graph of Figure 22.2(a), using vertex $3$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline d & \\infty & 3 & 0 & 2 & 1 & 1 \\\\ \\pi & \\text{NIL} & 4 & \\text{NIL} & 5 & 3 & 3 \\end{array} $$ 22.2-2 Show the $d$ and $\\pi$ values that result from running breadth-first search on the undirected graph of Figure 22.3, using vertex $u$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & r & s & t & u & v & w & x & y \\\\ \\hline d & 4 & 3 & 1 & 0 & 5 & 2 & 1 & 1 \\\\ \\pi & s & w & u & \\text{NIL} & r & t & u & u \\end{array} $$ 22.2-3 Show that using a single bit to store each vertex color suffices by arguing that the $\\text{BFS}$ procedure would produce the same result if lines 5 and 14 were removed. $\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change. The $\\text{BFS}$ procedure cares only whether a vertex is white or not. $A$ vertex $v$ must become non-white at the same time that $v.d$ is assigned a finite value so that we do not attempt to assign to $v.d$ again, and so we need to change vertex colors in lines 5 and 14. Once we have changed a vertex's color to non-white, we do not need to change it again. 22.2-4 What is the running time of $\\text{BFS}$ if we represent its input graph by an adjacency matrix and modify the algorithm to handle this form of input? The time of iterating all edges becomes $O(V^2)$ from $O(E)$. Therefore, the running time is $O(V + V^2)$. 22.2-5 Argue that in a breadth-first search, the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list. Using Figure 22.3 as an example, show that the breadth-first tree computed by $\\text{BFS}$ can depend on the ordering within adjacency lists. The correctness proof for the $\\text{BFS}$ algorithm shows that $u.d = \\delta(s, u)$, and the algorithm doesn't assume that the adjacency lists are in any particular order. In Figure 22.3, if $t$ precedes $x$ in $Adj[w]$, we can get the breadth-first tree shown in the figure. But if $x$ precedes $t$ in $Adj[w]$ and $u$ precedes $y$ in $Adj[x]$, we can get edge $(x, u)$ in the breadth-first tree. 22.2-6 Give an example of a directed graph $G = (V, E)$, a source vertex $s \\in V$, and a set of tree edges $E_\\pi \\subseteq E$ such that for each vertex $v \\in V$, the unique simple path in the graph $(V, E_\\pi)$ from $s$ to $v$ is a shortest path in $G$, yet the set of edges $E_\\pi$ cannot be produced by running $\\text{BFS}$ on $G$, no matter how the vertices are ordered in each adjacency list. The edges in $E_\\pi$ are shaded in the following graph: To see that $E_\\pi$ cannot be a breadth-first tree, let's suppose that $Adj[s]$ contains $u$ before $v$. $\\text{BFS}$ adds edges $(s, u)$ and $(s, v)$ to the breadth-first tree. Since $u$ is enqueued before $v$, $\\text{BFS}$ then adds edges $(u, w)$ and $(u, x)$. (The order of $w$ and $x$ in $Adj[u]$ doesn't matter.) Symmetrically, if $Adj[s]$ contains $v$ before $u$, then $\\text{BFS}$ adds edges $(s, v)$ and $(s, u)$ to the breadth-first tree, $v$ is enqueued before $u$, and $\\text{BFS}$ adds edges $(v, w)$ and $(v, x)$. (Again, the order of $w$ and $x$ in $Adj[v]$ doesn't matter.) $\\text{BFS}$ will never put both edges $(u, w)$ and $(v, x)$ into the breadth-first tree. In fact, it will also never put both edges $(u, x)$ and $(v, w)$ into the breadth-first tree. 22.2-7 There are two types of professional wrestlers: \"babyfaces\" (\"good guys\") and \"heels\" (\"bad guys\"). Between any pair of professional wrestlers, there may or may not be a rivalry. Suppose we have $n$ professional wrestlers and we have a list of $r$ pairs of wrestlers for which there are rivalries. Give an $O(n + r)$-time algorithm that determines whether it is possible to designate some of the wrestlers as babyfaces and the remainder as heels such that each rivalry is between a babyface and a heel. If it is possible to perform such a designation, your algorithm should produce it. Create a graph $G$ where each vertex represents a wrestler and each edge represents a rivalry. The graph will contain $n$ vertices and $r$ edges. Perform as many $\\text{BFS}$'s as needed to visit all vertices. Assign all wrestlers whose distance is even to be babyfaces and all wrestlers whose distance is odd to be heels. Then check each edge to verify that it goes between a babyface and a heel. This solution would take $O(n + r)$ time for the $\\text{BFS}$, $O(n)$ time to designate each wrestler as a babyface or heel, and $O(r)$ time to check edges, which is $O(n + r)$ time overall. 22.2-8 $\\star$ The diameter of a tree $T = (V, E)$ is defined as $\\max_{u,v \\in V} \\delta(u, v)$, that is, the largest of all shortest-path distances in the tree. Give an efficient algorithm to compute the diameter of a tree, and analyze the running time of your algorithm. Suppose that a and b are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so. Let $s$ be any vertex in $T$. We claim that the result of a single $\\text{BFS}$ will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest. To see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend). Then we have $$d(s, a) < d(s, x)$$ and $$d(s, b) < d(s, x).$$ Let $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s, c)$. Since the graph is in fact a tree, we must have $$d(s, a) = d(s, c) + d(c, a)$$ and $$d(s, b) = d(s, c) + d(c, b).$$ (If there were another path, we could form a cycle). Using the triangle inequality and inequalities and equalities mentioned above we must have $$ \\begin{aligned} d(a, b) + 2d(s, c) & = d(s, c) + d(c, b) + d(s, c) + d(c, a) \\\\ & < d(s, x) + d(s, c) + d(c, b). \\end{aligned} $$ I claim that $d(x, b) = d(s, x) + d(s, b)$. If not, then by the triangle inequality we must have a strict less-than. In other words, there is some path from $x$ to $b$ which does not go through $c$. This gives the contradiction, because it implies there is a cycle formed by concatenating these paths. Then we have $$d(a, b) < d(a, b) + 2d(s, c) < d(x, b).$$ Since it is assumed that $d(a, b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run $\\text{BFS}$ a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running $\\text{BFS}$ again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$. 22.2-9 Let $G = (V, E)$ be a connected, undirected graph. Give an $O(V + E)$-time algorithm to compute a path in $G$ that traverses each edge in $E$ exactly once in each direction. Describe how you can find your way out of a maze if you are given a large supply of pennies. First, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\\pi$ for every $v$. To aide in not double counting edges, fix any ordering $\\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $\\text{MAKE-PATH}(s)$, where $s$ was the root used for the $\\text{BFS}$. 1 2 3 4 5 6 7 MAKE - PATH ( u ) for each v \u2208 Adj [ u ] but not in the tree such that u \u2264 v go to v and back to u for each v \u2208 Adj [ u ] but not equal to u . \u03c0 go to v perform the path proscribed by MAKE - PATH ( v ) go to u . \u03c0","title":"22.2 Breadth-first search"},{"location":"Chap22/22.2/#222-1","text":"Show the $d$ and $\\pi$ values that result from running breadth-first search on the directed graph of Figure 22.2(a), using vertex $3$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline d & \\infty & 3 & 0 & 2 & 1 & 1 \\\\ \\pi & \\text{NIL} & 4 & \\text{NIL} & 5 & 3 & 3 \\end{array} $$","title":"22.2-1"},{"location":"Chap22/22.2/#222-2","text":"Show the $d$ and $\\pi$ values that result from running breadth-first search on the undirected graph of Figure 22.3, using vertex $u$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & r & s & t & u & v & w & x & y \\\\ \\hline d & 4 & 3 & 1 & 0 & 5 & 2 & 1 & 1 \\\\ \\pi & s & w & u & \\text{NIL} & r & t & u & u \\end{array} $$","title":"22.2-2"},{"location":"Chap22/22.2/#222-3","text":"Show that using a single bit to store each vertex color suffices by arguing that the $\\text{BFS}$ procedure would produce the same result if lines 5 and 14 were removed. $\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change. The $\\text{BFS}$ procedure cares only whether a vertex is white or not. $A$ vertex $v$ must become non-white at the same time that $v.d$ is assigned a finite value so that we do not attempt to assign to $v.d$ again, and so we need to change vertex colors in lines 5 and 14. Once we have changed a vertex's color to non-white, we do not need to change it again.","title":"22.2-3"},{"location":"Chap22/22.2/#222-4","text":"What is the running time of $\\text{BFS}$ if we represent its input graph by an adjacency matrix and modify the algorithm to handle this form of input? The time of iterating all edges becomes $O(V^2)$ from $O(E)$. Therefore, the running time is $O(V + V^2)$.","title":"22.2-4"},{"location":"Chap22/22.2/#222-5","text":"Argue that in a breadth-first search, the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list. Using Figure 22.3 as an example, show that the breadth-first tree computed by $\\text{BFS}$ can depend on the ordering within adjacency lists. The correctness proof for the $\\text{BFS}$ algorithm shows that $u.d = \\delta(s, u)$, and the algorithm doesn't assume that the adjacency lists are in any particular order. In Figure 22.3, if $t$ precedes $x$ in $Adj[w]$, we can get the breadth-first tree shown in the figure. But if $x$ precedes $t$ in $Adj[w]$ and $u$ precedes $y$ in $Adj[x]$, we can get edge $(x, u)$ in the breadth-first tree.","title":"22.2-5"},{"location":"Chap22/22.2/#222-6","text":"Give an example of a directed graph $G = (V, E)$, a source vertex $s \\in V$, and a set of tree edges $E_\\pi \\subseteq E$ such that for each vertex $v \\in V$, the unique simple path in the graph $(V, E_\\pi)$ from $s$ to $v$ is a shortest path in $G$, yet the set of edges $E_\\pi$ cannot be produced by running $\\text{BFS}$ on $G$, no matter how the vertices are ordered in each adjacency list. The edges in $E_\\pi$ are shaded in the following graph: To see that $E_\\pi$ cannot be a breadth-first tree, let's suppose that $Adj[s]$ contains $u$ before $v$. $\\text{BFS}$ adds edges $(s, u)$ and $(s, v)$ to the breadth-first tree. Since $u$ is enqueued before $v$, $\\text{BFS}$ then adds edges $(u, w)$ and $(u, x)$. (The order of $w$ and $x$ in $Adj[u]$ doesn't matter.) Symmetrically, if $Adj[s]$ contains $v$ before $u$, then $\\text{BFS}$ adds edges $(s, v)$ and $(s, u)$ to the breadth-first tree, $v$ is enqueued before $u$, and $\\text{BFS}$ adds edges $(v, w)$ and $(v, x)$. (Again, the order of $w$ and $x$ in $Adj[v]$ doesn't matter.) $\\text{BFS}$ will never put both edges $(u, w)$ and $(v, x)$ into the breadth-first tree. In fact, it will also never put both edges $(u, x)$ and $(v, w)$ into the breadth-first tree.","title":"22.2-6"},{"location":"Chap22/22.2/#222-7","text":"There are two types of professional wrestlers: \"babyfaces\" (\"good guys\") and \"heels\" (\"bad guys\"). Between any pair of professional wrestlers, there may or may not be a rivalry. Suppose we have $n$ professional wrestlers and we have a list of $r$ pairs of wrestlers for which there are rivalries. Give an $O(n + r)$-time algorithm that determines whether it is possible to designate some of the wrestlers as babyfaces and the remainder as heels such that each rivalry is between a babyface and a heel. If it is possible to perform such a designation, your algorithm should produce it. Create a graph $G$ where each vertex represents a wrestler and each edge represents a rivalry. The graph will contain $n$ vertices and $r$ edges. Perform as many $\\text{BFS}$'s as needed to visit all vertices. Assign all wrestlers whose distance is even to be babyfaces and all wrestlers whose distance is odd to be heels. Then check each edge to verify that it goes between a babyface and a heel. This solution would take $O(n + r)$ time for the $\\text{BFS}$, $O(n)$ time to designate each wrestler as a babyface or heel, and $O(r)$ time to check edges, which is $O(n + r)$ time overall.","title":"22.2-7"},{"location":"Chap22/22.2/#222-8-star","text":"The diameter of a tree $T = (V, E)$ is defined as $\\max_{u,v \\in V} \\delta(u, v)$, that is, the largest of all shortest-path distances in the tree. Give an efficient algorithm to compute the diameter of a tree, and analyze the running time of your algorithm. Suppose that a and b are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so. Let $s$ be any vertex in $T$. We claim that the result of a single $\\text{BFS}$ will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest. To see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend). Then we have $$d(s, a) < d(s, x)$$ and $$d(s, b) < d(s, x).$$ Let $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s, c)$. Since the graph is in fact a tree, we must have $$d(s, a) = d(s, c) + d(c, a)$$ and $$d(s, b) = d(s, c) + d(c, b).$$ (If there were another path, we could form a cycle). Using the triangle inequality and inequalities and equalities mentioned above we must have $$ \\begin{aligned} d(a, b) + 2d(s, c) & = d(s, c) + d(c, b) + d(s, c) + d(c, a) \\\\ & < d(s, x) + d(s, c) + d(c, b). \\end{aligned} $$ I claim that $d(x, b) = d(s, x) + d(s, b)$. If not, then by the triangle inequality we must have a strict less-than. In other words, there is some path from $x$ to $b$ which does not go through $c$. This gives the contradiction, because it implies there is a cycle formed by concatenating these paths. Then we have $$d(a, b) < d(a, b) + 2d(s, c) < d(x, b).$$ Since it is assumed that $d(a, b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run $\\text{BFS}$ a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running $\\text{BFS}$ again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$.","title":"22.2-8 $\\star$"},{"location":"Chap22/22.2/#222-9","text":"Let $G = (V, E)$ be a connected, undirected graph. Give an $O(V + E)$-time algorithm to compute a path in $G$ that traverses each edge in $E$ exactly once in each direction. Describe how you can find your way out of a maze if you are given a large supply of pennies. First, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\\pi$ for every $v$. To aide in not double counting edges, fix any ordering $\\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $\\text{MAKE-PATH}(s)$, where $s$ was the root used for the $\\text{BFS}$. 1 2 3 4 5 6 7 MAKE - PATH ( u ) for each v \u2208 Adj [ u ] but not in the tree such that u \u2264 v go to v and back to u for each v \u2208 Adj [ u ] but not equal to u . \u03c0 go to v perform the path proscribed by MAKE - PATH ( v ) go to u . \u03c0","title":"22.2-9"},{"location":"Chap22/22.3/","text":"22.3-1 Make a $3$-by-$3$ chart with row and column labels $\\text{WHITE}$, $\\text{GRAY}$, and $\\text{BLACK}$. In each cell $(i, j)$, indicate whether, at any point during a depth-first search of a directed graph, there can be an edge from a vertex of color $i$ to a vertex of color $j$. For each possible edge, indicate what edge types it can be. Make a second such chart for depth-first search of an undirected graph. According to Theorem 22.7, there are 3 cases of relationship between interval of vertex $u$ and $v$, $u.interval \\subset v.interval$, $v.interval \\subset u.interval$, and $u.interval$ is detached from $u.interval$. We judge the possibility according to this Theorem. For Directed Graph: $\\text{BLACK}$ to $\\text{BLACK}$ and $\\text{WHITE}$ to $\\text{WHITE}$ works with everything, we omit this two later. For Cross Edge $(u, v)$, we must have $v.d < v.f < u.d < u.f$, then it could be $\\text{WHITE}$ to $\\text{BLACK}$, $\\text{WHITE}$ to $\\text{GRAY}$ and $\\text{GRAY}$ to $\\text{BLACK}$. For Tree Edge and Forward Edge $(u, v)$, we must have $u.d < v.d < v.f < u.f$, then it could be $\\text{GRAY}$ to $\\text{WHITE}$, $\\text{GRAY}$ to $\\text{GRAY}$ and $\\text{GRAY}$ to $\\text{BLACK}$. For Back Edge $(u, v)$, we must have $v.d < u.d < u.f < v.f$, then it could be $\\text{WHITE}$ to $\\text{GRAY}$, $\\text{GRAY}$ to $\\text{GRAY}$ and $\\text{BLACK}$ to $\\text{GRAY}$. $$ \\begin{array}{c|ccc} from\\backslash to & \\text{BLACK} & \\text{GRAY} & \\text{WHITE} \\\\ \\hline \\text{BLACK} & \\text{Allkinds} & \\text{Back} & - \\\\ \\text{GRAY} & \\text{Tree, Forward, Cross} & \\text{Tree, Forward, Back} & \\text{Tree, Forward} \\\\ \\text{WHITE} & \\text{Cross} & \\text{Cross, Back} & \\text{Allkinds} \\end{array} $$ For Undirected Graph, starting from Directed Chart, we remove the Forward Edge and Cross Edge, and when a Back Edge exist, we add Tree Edge; when a Tree Edge exist, we add Back Edge. This is correct for following reason: Theorem 22.10: In a depth-first search of an undirected graph $G$, every edge of $G$ is either a tree edge or a back edge. So Tree edge and back edge only. If $(u, v)$ is a Tree edge from $u$'s perspective, $(u, v)$ is also a Back Edge from $v$'s perspective. $$ \\begin{array}{c|ccc} from\\backslash to & \\text{BLACK} & \\text{GRAY} & \\text{WHITE} \\\\ \\hline \\text{BLACK} & \\text{Tree, Back} & \\text{Tree, Back} & - \\\\ \\text{GRAY} & \\text{Tree, Back} & \\text{Tree, Back} & \\text{Tree, Back} \\\\ \\text{WHITE} & - & \\text{Tree, Back} & \\text{Tree, Back} \\end{array} $$ 22.3-2 Show how depth-first search works on the graph of Figure 22.6. Assume that the for loop of lines 5\u20137 of the $\\text{DFS}$ procedure considers the vertices in alphabetical order, and assume that each adjacency list is ordered alphabetically. Show the discovery and finishing times for each vertex, and show the classification of each edge. The following table gives the discovery time and finish time for each vetex in the graph. $$ \\begin{array}{ccc} \\text{Vertex} & \\text{Discovered} & \\text{Finished} \\\\ \\hline q & 1 & 16 \\\\ r & 17 & 20 \\\\ s & 2 & 7 \\\\ t & 8 & 15 \\\\ u & 18 & 19 \\\\ v & 3 & 6 \\\\ w & 4 & 5 \\\\ x & 9 & 12 \\\\ y & 13 & 14 \\\\ z & 10 & 11 \\end{array} $$ Tree edges: $(q, s)$, $(s, v)$, $(v, w)$, $(q, t)$, $(t, x)$, $(x, z)$, $(t, y)$, $(r, u)$. Back edges: $(w, s)$, $(z, x)$, $(y, q)$. Forward edges: $(q, w)$. Cross edges: $(r, y)$, $(u, y)$. 22.3-3 Show the parenthesis structure of the depth-first search of Figure 22.4. As pointed out in figure 22.5, the parentheses structure of the $\\text{DFS}$ of figure 22.4 is $(((())()))(()())$. 22.3-4 Show that using a single bit to store each vertex color suffices by arguing that the $\\text{DFS}$ procedure would produce the same result if line 3 of $\\text{DFS-VISIT}$ was removed. $\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change. The $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures care only whether a vertex is white or not. By coloring vertex $u$ gray when it is first visited, in line 3 of $\\text{DFS-VISIT}$, we ensure that $u$ will not be visited again. Once we have changed a vertex's color to non-white, we do not need to change it again. 22.3-5 Show that edge $(u, v)$ is a. a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$, b. a back edge if and only if $v.d \\le u.d < u.f \\le v.f$, and c. a cross edge if and only if $v.d < v.f < u.d < u.f$. a. Edge $(u, v)$ is a tree edge or forward edge if and only if $v$ is a descendant of $u$ in the depth-first forest. (If $(u, v)$ is a back edge, then $u$ is a descendant of $v$, and if $(u, v)$ is a cross edge, then neither of $u$ or $v$ is a descendant of the other.) By Corollary 22.8, therefore, $(u, v)$ is a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$. b. First, suppose that $(u, v)$ is a back edge. A self-loop is by definition a back edge. If $(u, v)$ is a self-loop, then clearly $v.d = u.d < u.f = v.f$. If $(u, v)$ is not a self-loop, then $u$ is a descendant of $v$ in the depth-first forest, and by Corollary 22.8, $v.d < u.d < u.f < v.f$. Now, suppose that $v.d \\le u.d < u.f \\le v.f$. If $u$ and $v$ are the same vertex, then $v.d = u.d < u.f = v.f$, and $(u, v)$ is a self-loop and hence a back edge. If $u$ and $v$ are distinct, then $v.d < u.d < u.f < v.f$. By the parenthesis theorem, interval $[u.d, u.f]$ is contained entirely within the interval $[v.d, v.f]$, and $u$ is a descendant of $v$ in a depth-first tree. Thus, $(u, v)$ is a back edge. c. First, suppose that $(u, v)$ is a cross edge. Since neither $u$ nor $v$ is an ancestor of the other, the parenthesis theorem says that the intervals $[u.d, u.f]$ and $[v.d, v.f]$ are entirely disjoint. Thus, we must have either $u.d < u.f < v.d < v.f$ or $v.d < v.f < u.d < u.f$. We claim that we cannot have $u.d < v.d$ if $(u, v)$ is a cross edge. Why? If $u.d < v.d$, then $v$ is white at time $u.d$. By the white-path theorem, $v$ is a descendant of $u$, which contradicts $(u, v)$ being a cross edge. Thus, we must have $v.d < v.f < u.d < u.f$. Now suppose that $v.d < v.f < u.d < u.f$. By the parenthesis theorem, neither $u$ nor $v$ is a descendant of the other, which means that $(u, v)$ must be a cross edge. 22.3-6 Show that in an undirected graph, classifying an edge $(u, v)$ as a tree edge or a back edge according to whether $(u, v)$ or $(v, u)$ is encountered first during the depth-first search is equivalent to classifying it according to the ordering of the four types in the classification scheme. By Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge. First suppose that $v$ is first discovered by exploring edge $(u, v)$. Then by definition, $(u, v)$ is a tree edge. Moreover, $(u, v)$ must have been discovered before $(v, u)$ because once $(v, u)$ is explored, $v$ is necessarily discovered. Now suppose that $v$ isn't first discovered by $(u, v)$. Then it must be discovered by $(r, v)$ for some $r\\ne u$. If $u$ hasn't yet been discovered then if $(u, v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v, u)$ is a back edge. 22.3-7 Rewrite the procedure $\\text{DFS}$, using a stack to eliminate recursion. See the algorithm $\\text{DFS-STACK}(G)$. Note that by a similar justification to 22.2-3, we may remove line 8 from the original $\\text{DFS-VISIT}$ algorithm without changing the final result of the program, that is just working with the colors white and gray. 22.3-8 Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, and if $u.d < v.d$ in a depth-first search of $G$, then $v$ is a descendant of $u$ in the depth-first forest produced. Let us consider the example graph depth-first search below. $$ \\begin{array}{c|cc} & d & f \\\\ \\hline w & 1 & 6 \\\\ u & 2 & 3 \\\\ v & 4 & 5 \\end{array} $$ Clearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced. We can see that $u.d < v.d$ in the depth-first search but $v$ is not a descendant of $u$ in the forest. 22.3-9 Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, then any depth-first search must result in $v.d \\le u.f$. Let us consider the example graph depth-first search below. $$ \\begin{array}{c|cc} & d & f \\\\ \\hline w & 1 & 6 \\\\ u & 2 & 3 \\\\ v & 4 & 5 \\end{array} $$ Clearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced by search. However, $v.d > u.f$ and the conjecture is false. 22.3-10 Modify the pseudocode for depth-first search so that it prints out every edge in the directed graph $G$, together with its type. Show what modifications, if any, you need to make if $G$ is undirected. We need only update $\\text{DFS-VISIT}$. If $G$ is undirected we don't need to make any modifications. We simply note that lines 11 through 16 will never be executed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DFS - VISIT - PRINT ( G , u ) time = time + 1 u . d = time u . color = GRAY for each v \u2208 G . Adj [ u ] if v . color == white print \"(u, v) is a Tree edge.\" v . \u03c0 = u DFS - VISIT - PRINT ( G , v ) else if v . color == gray print \"(u, v) is a Back edge.\" else if v . d > u . d print \"(u, v) is a Forward edge.\" else print \"(u, v) is a Cross edge.\" 22.3-11 Explain how a vertex $u$ of a directed graph can end up in a depth-first tree containing only $u$, even though $u$ has both incoming and outgoing edges in $G$. Let us consider the example graph and depth-first search below. $$ \\begin{array}{c|cc} & d & f \\\\ \\hline w & 1 & 2 \\\\ u & 3 & 4 \\\\ v & 5 & 6 \\end{array} $$ Cleary $u$ has both incoming and outgoing edges in $G$ but a depth-first search of $G$ produced a depth-first forest where $u$ is in a tree by itself. 22.3-12 Show that we can use a depth-first search of an undirected graph $G$ to identify the connected components of $G$, and that the depth-first forest contains as many trees as $G$ has connected components. More precisely, show how to modify depth-first search so that it assigns to each vertex $v$ an integer label $v.cc$ between $1$ and $k$, where $k$ is the number of connected components of $G$, such that $u.cc = v.cc$ if and only if $u$ and $v$ are in the same connected component. The following pseudocode modifies the $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures to assign values to the $cc$ attributes of vertices. 1 2 3 4 5 6 7 8 9 10 DFS ( G ) for each vertex u \u2208 G . V u . color = WHITE u . \u03c0 = NIL time = 0 counter = 0 for each vertex u \u2208 G . V if u . color == WHITE counter = counter + 1 DFS - VISIT ( G , u , counter ) 1 2 3 4 5 6 7 8 9 10 11 12 DFS - VISIT ( G , u , counter ) u . cc = counter // label the vertex time = time + 1 u . d = time u . color = GRAY for each v \u2208 G . Adj [ u ] if v . color == WHITE v . \u03c0 = u DFS - VISIT ( G , v , counter ) u . color = BLACK time = time + 1 u . f = time This $\\text{DFS}$ increments a counter each time $\\text{DFS-VISIT}$ is called to grow a new tree in the $\\text{DFS}$ forest. Every vertex visited (and added to the tree) by $\\text{DFS-VISIT}$ is labeled with that same counter value. Thus $u.vv = v.cc$ if and only if $u$ and $v$ are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, and the final value of the counter is the number of calls that were made to $\\text{DFS-VISIT}$ by $\\text{DFS}$. Also, since every vertex is visited eventually, every vertex is labeled. Thus all we need to show is that the vertices visited by each call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are exactly the vertices in one connected component of $G$. All vertices in a connected component are visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$: Let $u$ be the first vertex in component $C$ visited by $\\text{DFS-VISIT}$. Since a vertex becomes non-white only when it is visited, all vertices in $C$ are white when $\\text{DFS-VISIT}$ is called for $u$. Thus, by the white-path theorem, all vertices in $C$ become descendants of $u$ in the forest, which means that all vertices in $C$ are visited (by recursive calls to $\\text{DFS-VISIT}$) before $\\text{DFS-VISIT}$ returns to $\\text{DFS}$. All vertices visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are in the same connected component: If two vertices are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, they are in the same connected component, because vertices are visited only by following paths in $G$ (by following edges found in adjacency lists, starting from some vertex). 22.3-13 $\\star$ A directed graph $G = (V, E)$ is singly connected if $u \\leadsto v$ implies that $G$ contains at most one simple path from $u$ to $v$ for all vertices $u, v \\in V$. Give an efficient algorithm to determine whether or not a directed graph is singly connected. This can be done in time $O(|V||E|)$. To do this, first perform a topological sort of the vertices. Then, we will contain for each vertex a list of it's ancestors with $in\\text-degree$ $0$. We compute these lists for each vertex in the order starting from the earlier ones topologically. Then, if we ever have a vertex that has the same degree $0$ vertex appearing in the lists of two of its immediate parents, we know that the graph is not singly connected. however, if at each step we have that at each step all of the parents have disjoint sets of degree $0$ vertices as ancestors, the graph is singly connected. Since, for each vertex, the amount of time required is bounded by the number of vertices times the $in\\text-degree$ of the particular vertex, the total runtime is bounded by $O(|V||E|)$.","title":"22.3 Depth-first search"},{"location":"Chap22/22.3/#223-1","text":"Make a $3$-by-$3$ chart with row and column labels $\\text{WHITE}$, $\\text{GRAY}$, and $\\text{BLACK}$. In each cell $(i, j)$, indicate whether, at any point during a depth-first search of a directed graph, there can be an edge from a vertex of color $i$ to a vertex of color $j$. For each possible edge, indicate what edge types it can be. Make a second such chart for depth-first search of an undirected graph. According to Theorem 22.7, there are 3 cases of relationship between interval of vertex $u$ and $v$, $u.interval \\subset v.interval$, $v.interval \\subset u.interval$, and $u.interval$ is detached from $u.interval$. We judge the possibility according to this Theorem. For Directed Graph: $\\text{BLACK}$ to $\\text{BLACK}$ and $\\text{WHITE}$ to $\\text{WHITE}$ works with everything, we omit this two later. For Cross Edge $(u, v)$, we must have $v.d < v.f < u.d < u.f$, then it could be $\\text{WHITE}$ to $\\text{BLACK}$, $\\text{WHITE}$ to $\\text{GRAY}$ and $\\text{GRAY}$ to $\\text{BLACK}$. For Tree Edge and Forward Edge $(u, v)$, we must have $u.d < v.d < v.f < u.f$, then it could be $\\text{GRAY}$ to $\\text{WHITE}$, $\\text{GRAY}$ to $\\text{GRAY}$ and $\\text{GRAY}$ to $\\text{BLACK}$. For Back Edge $(u, v)$, we must have $v.d < u.d < u.f < v.f$, then it could be $\\text{WHITE}$ to $\\text{GRAY}$, $\\text{GRAY}$ to $\\text{GRAY}$ and $\\text{BLACK}$ to $\\text{GRAY}$. $$ \\begin{array}{c|ccc} from\\backslash to & \\text{BLACK} & \\text{GRAY} & \\text{WHITE} \\\\ \\hline \\text{BLACK} & \\text{Allkinds} & \\text{Back} & - \\\\ \\text{GRAY} & \\text{Tree, Forward, Cross} & \\text{Tree, Forward, Back} & \\text{Tree, Forward} \\\\ \\text{WHITE} & \\text{Cross} & \\text{Cross, Back} & \\text{Allkinds} \\end{array} $$ For Undirected Graph, starting from Directed Chart, we remove the Forward Edge and Cross Edge, and when a Back Edge exist, we add Tree Edge; when a Tree Edge exist, we add Back Edge. This is correct for following reason: Theorem 22.10: In a depth-first search of an undirected graph $G$, every edge of $G$ is either a tree edge or a back edge. So Tree edge and back edge only. If $(u, v)$ is a Tree edge from $u$'s perspective, $(u, v)$ is also a Back Edge from $v$'s perspective. $$ \\begin{array}{c|ccc} from\\backslash to & \\text{BLACK} & \\text{GRAY} & \\text{WHITE} \\\\ \\hline \\text{BLACK} & \\text{Tree, Back} & \\text{Tree, Back} & - \\\\ \\text{GRAY} & \\text{Tree, Back} & \\text{Tree, Back} & \\text{Tree, Back} \\\\ \\text{WHITE} & - & \\text{Tree, Back} & \\text{Tree, Back} \\end{array} $$","title":"22.3-1"},{"location":"Chap22/22.3/#223-2","text":"Show how depth-first search works on the graph of Figure 22.6. Assume that the for loop of lines 5\u20137 of the $\\text{DFS}$ procedure considers the vertices in alphabetical order, and assume that each adjacency list is ordered alphabetically. Show the discovery and finishing times for each vertex, and show the classification of each edge. The following table gives the discovery time and finish time for each vetex in the graph. $$ \\begin{array}{ccc} \\text{Vertex} & \\text{Discovered} & \\text{Finished} \\\\ \\hline q & 1 & 16 \\\\ r & 17 & 20 \\\\ s & 2 & 7 \\\\ t & 8 & 15 \\\\ u & 18 & 19 \\\\ v & 3 & 6 \\\\ w & 4 & 5 \\\\ x & 9 & 12 \\\\ y & 13 & 14 \\\\ z & 10 & 11 \\end{array} $$ Tree edges: $(q, s)$, $(s, v)$, $(v, w)$, $(q, t)$, $(t, x)$, $(x, z)$, $(t, y)$, $(r, u)$. Back edges: $(w, s)$, $(z, x)$, $(y, q)$. Forward edges: $(q, w)$. Cross edges: $(r, y)$, $(u, y)$.","title":"22.3-2"},{"location":"Chap22/22.3/#223-3","text":"Show the parenthesis structure of the depth-first search of Figure 22.4. As pointed out in figure 22.5, the parentheses structure of the $\\text{DFS}$ of figure 22.4 is $(((())()))(()())$.","title":"22.3-3"},{"location":"Chap22/22.3/#223-4","text":"Show that using a single bit to store each vertex color suffices by arguing that the $\\text{DFS}$ procedure would produce the same result if line 3 of $\\text{DFS-VISIT}$ was removed. $\\textit{Note:}$ This exercise changed in the third printing. This solution reflects the change. The $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures care only whether a vertex is white or not. By coloring vertex $u$ gray when it is first visited, in line 3 of $\\text{DFS-VISIT}$, we ensure that $u$ will not be visited again. Once we have changed a vertex's color to non-white, we do not need to change it again.","title":"22.3-4"},{"location":"Chap22/22.3/#223-5","text":"Show that edge $(u, v)$ is a. a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$, b. a back edge if and only if $v.d \\le u.d < u.f \\le v.f$, and c. a cross edge if and only if $v.d < v.f < u.d < u.f$. a. Edge $(u, v)$ is a tree edge or forward edge if and only if $v$ is a descendant of $u$ in the depth-first forest. (If $(u, v)$ is a back edge, then $u$ is a descendant of $v$, and if $(u, v)$ is a cross edge, then neither of $u$ or $v$ is a descendant of the other.) By Corollary 22.8, therefore, $(u, v)$ is a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$. b. First, suppose that $(u, v)$ is a back edge. A self-loop is by definition a back edge. If $(u, v)$ is a self-loop, then clearly $v.d = u.d < u.f = v.f$. If $(u, v)$ is not a self-loop, then $u$ is a descendant of $v$ in the depth-first forest, and by Corollary 22.8, $v.d < u.d < u.f < v.f$. Now, suppose that $v.d \\le u.d < u.f \\le v.f$. If $u$ and $v$ are the same vertex, then $v.d = u.d < u.f = v.f$, and $(u, v)$ is a self-loop and hence a back edge. If $u$ and $v$ are distinct, then $v.d < u.d < u.f < v.f$. By the parenthesis theorem, interval $[u.d, u.f]$ is contained entirely within the interval $[v.d, v.f]$, and $u$ is a descendant of $v$ in a depth-first tree. Thus, $(u, v)$ is a back edge. c. First, suppose that $(u, v)$ is a cross edge. Since neither $u$ nor $v$ is an ancestor of the other, the parenthesis theorem says that the intervals $[u.d, u.f]$ and $[v.d, v.f]$ are entirely disjoint. Thus, we must have either $u.d < u.f < v.d < v.f$ or $v.d < v.f < u.d < u.f$. We claim that we cannot have $u.d < v.d$ if $(u, v)$ is a cross edge. Why? If $u.d < v.d$, then $v$ is white at time $u.d$. By the white-path theorem, $v$ is a descendant of $u$, which contradicts $(u, v)$ being a cross edge. Thus, we must have $v.d < v.f < u.d < u.f$. Now suppose that $v.d < v.f < u.d < u.f$. By the parenthesis theorem, neither $u$ nor $v$ is a descendant of the other, which means that $(u, v)$ must be a cross edge.","title":"22.3-5"},{"location":"Chap22/22.3/#223-6","text":"Show that in an undirected graph, classifying an edge $(u, v)$ as a tree edge or a back edge according to whether $(u, v)$ or $(v, u)$ is encountered first during the depth-first search is equivalent to classifying it according to the ordering of the four types in the classification scheme. By Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge. First suppose that $v$ is first discovered by exploring edge $(u, v)$. Then by definition, $(u, v)$ is a tree edge. Moreover, $(u, v)$ must have been discovered before $(v, u)$ because once $(v, u)$ is explored, $v$ is necessarily discovered. Now suppose that $v$ isn't first discovered by $(u, v)$. Then it must be discovered by $(r, v)$ for some $r\\ne u$. If $u$ hasn't yet been discovered then if $(u, v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v, u)$ is a back edge.","title":"22.3-6"},{"location":"Chap22/22.3/#223-7","text":"Rewrite the procedure $\\text{DFS}$, using a stack to eliminate recursion. See the algorithm $\\text{DFS-STACK}(G)$. Note that by a similar justification to 22.2-3, we may remove line 8 from the original $\\text{DFS-VISIT}$ algorithm without changing the final result of the program, that is just working with the colors white and gray.","title":"22.3-7"},{"location":"Chap22/22.3/#223-8","text":"Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, and if $u.d < v.d$ in a depth-first search of $G$, then $v$ is a descendant of $u$ in the depth-first forest produced. Let us consider the example graph depth-first search below. $$ \\begin{array}{c|cc} & d & f \\\\ \\hline w & 1 & 6 \\\\ u & 2 & 3 \\\\ v & 4 & 5 \\end{array} $$ Clearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced. We can see that $u.d < v.d$ in the depth-first search but $v$ is not a descendant of $u$ in the forest.","title":"22.3-8"},{"location":"Chap22/22.3/#223-9","text":"Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, then any depth-first search must result in $v.d \\le u.f$. Let us consider the example graph depth-first search below. $$ \\begin{array}{c|cc} & d & f \\\\ \\hline w & 1 & 6 \\\\ u & 2 & 3 \\\\ v & 4 & 5 \\end{array} $$ Clearly, there is a path from $u$ to $v$ in $G$. The bold edges are in the depth-first forest produced by search. However, $v.d > u.f$ and the conjecture is false.","title":"22.3-9"},{"location":"Chap22/22.3/#223-10","text":"Modify the pseudocode for depth-first search so that it prints out every edge in the directed graph $G$, together with its type. Show what modifications, if any, you need to make if $G$ is undirected. We need only update $\\text{DFS-VISIT}$. If $G$ is undirected we don't need to make any modifications. We simply note that lines 11 through 16 will never be executed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DFS - VISIT - PRINT ( G , u ) time = time + 1 u . d = time u . color = GRAY for each v \u2208 G . Adj [ u ] if v . color == white print \"(u, v) is a Tree edge.\" v . \u03c0 = u DFS - VISIT - PRINT ( G , v ) else if v . color == gray print \"(u, v) is a Back edge.\" else if v . d > u . d print \"(u, v) is a Forward edge.\" else print \"(u, v) is a Cross edge.\"","title":"22.3-10"},{"location":"Chap22/22.3/#223-11","text":"Explain how a vertex $u$ of a directed graph can end up in a depth-first tree containing only $u$, even though $u$ has both incoming and outgoing edges in $G$. Let us consider the example graph and depth-first search below. $$ \\begin{array}{c|cc} & d & f \\\\ \\hline w & 1 & 2 \\\\ u & 3 & 4 \\\\ v & 5 & 6 \\end{array} $$ Cleary $u$ has both incoming and outgoing edges in $G$ but a depth-first search of $G$ produced a depth-first forest where $u$ is in a tree by itself.","title":"22.3-11"},{"location":"Chap22/22.3/#223-12","text":"Show that we can use a depth-first search of an undirected graph $G$ to identify the connected components of $G$, and that the depth-first forest contains as many trees as $G$ has connected components. More precisely, show how to modify depth-first search so that it assigns to each vertex $v$ an integer label $v.cc$ between $1$ and $k$, where $k$ is the number of connected components of $G$, such that $u.cc = v.cc$ if and only if $u$ and $v$ are in the same connected component. The following pseudocode modifies the $\\text{DFS}$ and $\\text{DFS-VISIT}$ procedures to assign values to the $cc$ attributes of vertices. 1 2 3 4 5 6 7 8 9 10 DFS ( G ) for each vertex u \u2208 G . V u . color = WHITE u . \u03c0 = NIL time = 0 counter = 0 for each vertex u \u2208 G . V if u . color == WHITE counter = counter + 1 DFS - VISIT ( G , u , counter ) 1 2 3 4 5 6 7 8 9 10 11 12 DFS - VISIT ( G , u , counter ) u . cc = counter // label the vertex time = time + 1 u . d = time u . color = GRAY for each v \u2208 G . Adj [ u ] if v . color == WHITE v . \u03c0 = u DFS - VISIT ( G , v , counter ) u . color = BLACK time = time + 1 u . f = time This $\\text{DFS}$ increments a counter each time $\\text{DFS-VISIT}$ is called to grow a new tree in the $\\text{DFS}$ forest. Every vertex visited (and added to the tree) by $\\text{DFS-VISIT}$ is labeled with that same counter value. Thus $u.vv = v.cc$ if and only if $u$ and $v$ are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, and the final value of the counter is the number of calls that were made to $\\text{DFS-VISIT}$ by $\\text{DFS}$. Also, since every vertex is visited eventually, every vertex is labeled. Thus all we need to show is that the vertices visited by each call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are exactly the vertices in one connected component of $G$. All vertices in a connected component are visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$: Let $u$ be the first vertex in component $C$ visited by $\\text{DFS-VISIT}$. Since a vertex becomes non-white only when it is visited, all vertices in $C$ are white when $\\text{DFS-VISIT}$ is called for $u$. Thus, by the white-path theorem, all vertices in $C$ become descendants of $u$ in the forest, which means that all vertices in $C$ are visited (by recursive calls to $\\text{DFS-VISIT}$) before $\\text{DFS-VISIT}$ returns to $\\text{DFS}$. All vertices visited by one call to $\\text{DFS-VISIT}$ from $\\text{DFS}$ are in the same connected component: If two vertices are visited in the same call to $\\text{DFS-VISIT}$ from $\\text{DFS}$, they are in the same connected component, because vertices are visited only by following paths in $G$ (by following edges found in adjacency lists, starting from some vertex).","title":"22.3-12"},{"location":"Chap22/22.3/#223-13-star","text":"A directed graph $G = (V, E)$ is singly connected if $u \\leadsto v$ implies that $G$ contains at most one simple path from $u$ to $v$ for all vertices $u, v \\in V$. Give an efficient algorithm to determine whether or not a directed graph is singly connected. This can be done in time $O(|V||E|)$. To do this, first perform a topological sort of the vertices. Then, we will contain for each vertex a list of it's ancestors with $in\\text-degree$ $0$. We compute these lists for each vertex in the order starting from the earlier ones topologically. Then, if we ever have a vertex that has the same degree $0$ vertex appearing in the lists of two of its immediate parents, we know that the graph is not singly connected. however, if at each step we have that at each step all of the parents have disjoint sets of degree $0$ vertices as ancestors, the graph is singly connected. Since, for each vertex, the amount of time required is bounded by the number of vertices times the $in\\text-degree$ of the particular vertex, the total runtime is bounded by $O(|V||E|)$.","title":"22.3-13 $\\star$"},{"location":"Chap22/22.4/","text":"22.4-1 Show the ordering of vertices produced by $\\text{TOPOLOGICAL-SORT}$ when it is run on the dag of Figure 22.8, under the assumption of Exercise 22.3-2. Our start and finish times from performing the $\\text{DFS}$ are $$ \\begin{array}{ccc} \\text{label} & d & f \\\\ \\hline m & 1 & 20 \\\\ q & 2 & 5 \\\\ t & 3 & 4 \\\\ r & 6 & 19 \\\\ u & 7 & 8 \\\\ y & 9 & 18 \\\\ v & 10 & 17 \\\\ w & 11 & 14 \\\\ z & 12 & 13 \\\\ x & 15 & 16 \\\\ n & 21 & 26 \\\\ o & 22 & 25 \\\\ s & 23 & 24 \\\\ p & 27 & 28 \\end{array} $$ And so, by reading off the entries in decreasing order of finish time, we have the sequence $p, n, o, s, m, r, y, v, x, w, z, u, q, t$. 22.4-2 Give a linear-time algorithm that takes as input a directed acyclic graph $G = (V, E)$ and two vertices $s$ and $t$, and returns the number of simple paths from $s$ to $t$ in $G$. For example, the directed acyclic graph of Figure 22.8 contains exactly four simple paths from vertex $p$ to vertex $v: pov$, $poryv$, $posryv$, and $psryv$. (Your algorithm needs only to count the simple paths, not list them.) The algorithm works as follows. The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process. First of all, a topo sort should be conducted and list the vertex between $u$, $v$ as $\\{v[1], v[2], \\dots, v[k - 1]\\}$. To count the number of paths, we should construct a solution from $v$ to $u$. Let's call $u$ as $v[0]$ and $v$ as $v[k]$, to avoid overlapping subproblem, the number of paths between $v_k$ and $u$ should be remembered and used as $k$ decrease to $0$. Only in this way can we solve the problem in $\\Theta(V + E)$. An bottom-up iterative version is possible only if the graph uses adjacency matrix so whether $v$ is adjacency to $u$ can be determined in $O(1)$ time. But building a adjacency matrix would cost $\\Theta(|V|^2)$, so never mind. 1 2 3 4 5 6 7 8 9 SIMPLE - PATHS ( G , u , v ) TOPO - SORT ( G ) let { v [ 1 ], v [ 2 ].. v [ k - 1 ]} be the vertex between u and v v [ 0 ] = u v [ k ] = v for j = 0 to k - 1 DP [ j ] = \u221e DP [ k ] = 1 return SIMPLE - PATHS - AID ( G , DP , 0 ) 1 2 3 4 5 6 7 8 9 10 SIMPLE - PATHS - AID ( G , DP , i ) if i > k return 0 else if DP [ i ] != \u221e return DP [ i ] else DP [ i ] = 0 for v [ m ] in G . adj [ v [ i ]] and 0 < m \u2264 k DP [ i ] += SIMPLE - PATHS - AID ( G , DP , m ) return DP [ i ] 22.4-3 Give an algorithm that determines whether or not a given undirected graph $G = (V, E)$ contains a cycle. Your algorithm should run in $O(V)$ time, independent of $|E|$. An undirected graph is acyclic (i.e., a forest) if and only if a $\\text{DFS}$ yields no back edges. If there's a back edge, there's a cycle. If there's no back edge, then by Theorem 22.10, there are only tree edges. Hence, the graph is acyclic. Thus, we can run $\\text{DFS}$: if we find a back edge, there's a cycle. Time: $O(V)$. (Not $O(V + E)$!) If we ever see $|V|$ distinct edges, we must have seen a back edge because (by Theorem B.2 on p. 1174) in an acyclic (undirected) forest, $|E| \\le |V| - 1$. 22.4-4 Prove or disprove: If a directed graph $G$ contains cycles, then $\\text{TOPOLOGICAL-SORT}(G)$ produces a vertex ordering that minimizes the number of \"bad\" edges that are inconsistent with the ordering produced. This is not true. Consider the graph $G$ consisting of vertices $a, b, c$, and $d$. Let the edges be $(a, b)$, $(b, c)$, $(a, d)$, $(d, c)$, and $(c, a)$. Suppose that we start the $\\text{DFS}$ of $\\text{TOPOLOGICAL-SORT}$ at vertex $c$. Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$. The \"bad\" edges in this case are $(b, c)$ and $(d, c)$. However, if we had instead ordered them by $a, b, d, c$ then the only bad edges would be $(c, a)$. Thus $\\text{TOPOLOGICAL-SORT}$ doesn't always minimizes the number of \"bad\" edges 22.4-5 Another way to perform topological sorting on a directed acyclic graph $G = (V, E)$ is to repeatedly find a vertex of $\\text{in-degree}$ $0$, output it, and remove it and all of its outgoing edges from the graph. Explain how to implement this idea so that it runs in time $O(V + E)$. What happens to this algorithm if $G$ has cycles? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 TOPOLOGICAL - SORT ( G ) // Initialize in-degree, \u0398(V) time. for each vertex u \u2208 G . V u . in - degree = 0 // Compute in-degree, \u0398(V + E) time. for each vertex u \u2208 G . V for each v \u2208 G . Adj [ u ] v . in - degree = v . in - degree + 1 // Initialize Queue, \u0398(V) time. Q = \u00d8 for each vertex u \u2208 G . V if u . in - degree == 0 ENQUEUE ( Q , u ) // while loop takes O(V + E) time. while Q != \u00d8 u = DEQUEUE ( Q ) output u // for loop executes O(E) times total. for each v \u2208 G . Adj [ u ] v . in - degree = v . in - degree - 1 if v . in - degree == 0 ENQUEUE ( Q , v ) // Check for cycles, O(V) time. for each vertex u \u2208 G . V if u . in - degree != 0 report that there ' s a cycle // Another way to check for cycles would be to count the vertices // that are output and report a cycle if that number is < |V|. To find and output vertices of $\\text{in-degree}$ $0$, we first compute all vertices' $\\text{in-degree}$s by making a pass through all the edges (by scanning the adjacency lists of all the vertices) and incrementing the $\\text{in-degree}$ of each vertex an edge enters. Computing all $\\text{in-degree}$s takes $\\Theta(V + E)$ time ($|V|$ adjacency lists accessed, $|E|$ edges total found in those lists, $\\Theta(1)$ work for each edge). We keep the vertices with $\\text{in-degree}$ $0$ in a FIFO queue, so that they can be enqueued and dequeued in $O(1)$ time. (The order in which vertices in the queue are processed doesn't matter, so any kind of FIFO queue works.) Initializing the queue takes one pass over the vertices doing $\\Theta(1)$ work, for total time $\\Theta(V)$. As we process each vertex from the queue, we effectively remove its outgoing edges from the graph by decrementing the $\\text{in-degree}$ of each vertex one of those edges enters, and we enqueue any vertex whose $\\text{in-degree}$ goes to $0$. We do not need to actually remove the edges from the adjacency list, because that adjacency list will never be processed again by the algorithm: Each vertex is enqueued/dequeued at most once because it is enqueued only if it starts out with $\\text{in-degree}$ $0$ or if its indegree becomes $0$ after being decremented (and never incremented) some number of times. The processing of a vertex from the queue happens $O(V)$ times because no vertex can be enqueued more than once. The per-vertex work (dequeue and output) takes $O(1)$ time, for a total of $O(V)$ time. Because the adjacency list of each vertex is scanned only when the vertex is dequeued, the adjacency list of each vertex is scanned at most once. Since the sum of the lengths of all the adjacency lists is $\\Theta(E)$, at most $O(E)$ time is spent in total scanning adjacency lists. For each edge in an adjacency list, $\\Theta(1)$ work is done, for a total of $O(E)$ time. Thus the total time taken by the algorithm is $O(V + E)$. The algorithm outputs vertices in the right order ($u$ before $v$ for every edge $(u, v)$) because vwill not be output until its $\\text{in-degree}$ becomes $0$, which happens only when every edge $(u, v)$ leading into $v$ has been \"removed\" due to the processing (including output) of $u$. If there are no cycles, all vertices are output. Proof: Assume that some vertex $v_0$ is not output. Vertex $v_0$ cannot start out with $\\text{in-degree}$ $0$ (or it would be output), so there are edges into $v_0$. Since $v_0$'s $\\text{in-degree}$ never becomes $0$, at least one edge $(v_1, v_0)$ is never removed, which means that at least one other vertex $v_1$ was not output. Similarly, $v_1$ not output means that some vertex $v_2$ such that $(v_2, v_1) \\in E$ was not output, and so on. Since the number of vertices is finite, this path ($\\cdots \\to v_2 \\to v_1 \\to v_0$) is finite, so we must have $v_i = v_j$ for some $i$ and $j$ in this sequence, which means there is a cycle. If there are cycles, not all vertices will be output, because some $\\text{in-degree}$s never become $0$. Proof: Assume that a vertex in a cycle is output (its $\\text{in-degree}$ becomes $0$). Let $v$ be the first vertex in its cycle to be output, and let $u$ be $v$'s predecessor in the cycle. In order for $v$'s $\\text{in-degree}$ to become $0$, the edge $(u, v)$ must have been \"removed\", which happens only when $u$ is processed. But this cannot have happened, because $v$ is the first vertex in its cycle to be processed. Thus no vertices in cycles are output.","title":"22.4 Topological sort"},{"location":"Chap22/22.4/#224-1","text":"Show the ordering of vertices produced by $\\text{TOPOLOGICAL-SORT}$ when it is run on the dag of Figure 22.8, under the assumption of Exercise 22.3-2. Our start and finish times from performing the $\\text{DFS}$ are $$ \\begin{array}{ccc} \\text{label} & d & f \\\\ \\hline m & 1 & 20 \\\\ q & 2 & 5 \\\\ t & 3 & 4 \\\\ r & 6 & 19 \\\\ u & 7 & 8 \\\\ y & 9 & 18 \\\\ v & 10 & 17 \\\\ w & 11 & 14 \\\\ z & 12 & 13 \\\\ x & 15 & 16 \\\\ n & 21 & 26 \\\\ o & 22 & 25 \\\\ s & 23 & 24 \\\\ p & 27 & 28 \\end{array} $$ And so, by reading off the entries in decreasing order of finish time, we have the sequence $p, n, o, s, m, r, y, v, x, w, z, u, q, t$.","title":"22.4-1"},{"location":"Chap22/22.4/#224-2","text":"Give a linear-time algorithm that takes as input a directed acyclic graph $G = (V, E)$ and two vertices $s$ and $t$, and returns the number of simple paths from $s$ to $t$ in $G$. For example, the directed acyclic graph of Figure 22.8 contains exactly four simple paths from vertex $p$ to vertex $v: pov$, $poryv$, $posryv$, and $psryv$. (Your algorithm needs only to count the simple paths, not list them.) The algorithm works as follows. The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process. First of all, a topo sort should be conducted and list the vertex between $u$, $v$ as $\\{v[1], v[2], \\dots, v[k - 1]\\}$. To count the number of paths, we should construct a solution from $v$ to $u$. Let's call $u$ as $v[0]$ and $v$ as $v[k]$, to avoid overlapping subproblem, the number of paths between $v_k$ and $u$ should be remembered and used as $k$ decrease to $0$. Only in this way can we solve the problem in $\\Theta(V + E)$. An bottom-up iterative version is possible only if the graph uses adjacency matrix so whether $v$ is adjacency to $u$ can be determined in $O(1)$ time. But building a adjacency matrix would cost $\\Theta(|V|^2)$, so never mind. 1 2 3 4 5 6 7 8 9 SIMPLE - PATHS ( G , u , v ) TOPO - SORT ( G ) let { v [ 1 ], v [ 2 ].. v [ k - 1 ]} be the vertex between u and v v [ 0 ] = u v [ k ] = v for j = 0 to k - 1 DP [ j ] = \u221e DP [ k ] = 1 return SIMPLE - PATHS - AID ( G , DP , 0 ) 1 2 3 4 5 6 7 8 9 10 SIMPLE - PATHS - AID ( G , DP , i ) if i > k return 0 else if DP [ i ] != \u221e return DP [ i ] else DP [ i ] = 0 for v [ m ] in G . adj [ v [ i ]] and 0 < m \u2264 k DP [ i ] += SIMPLE - PATHS - AID ( G , DP , m ) return DP [ i ]","title":"22.4-2"},{"location":"Chap22/22.4/#224-3","text":"Give an algorithm that determines whether or not a given undirected graph $G = (V, E)$ contains a cycle. Your algorithm should run in $O(V)$ time, independent of $|E|$. An undirected graph is acyclic (i.e., a forest) if and only if a $\\text{DFS}$ yields no back edges. If there's a back edge, there's a cycle. If there's no back edge, then by Theorem 22.10, there are only tree edges. Hence, the graph is acyclic. Thus, we can run $\\text{DFS}$: if we find a back edge, there's a cycle. Time: $O(V)$. (Not $O(V + E)$!) If we ever see $|V|$ distinct edges, we must have seen a back edge because (by Theorem B.2 on p. 1174) in an acyclic (undirected) forest, $|E| \\le |V| - 1$.","title":"22.4-3"},{"location":"Chap22/22.4/#224-4","text":"Prove or disprove: If a directed graph $G$ contains cycles, then $\\text{TOPOLOGICAL-SORT}(G)$ produces a vertex ordering that minimizes the number of \"bad\" edges that are inconsistent with the ordering produced. This is not true. Consider the graph $G$ consisting of vertices $a, b, c$, and $d$. Let the edges be $(a, b)$, $(b, c)$, $(a, d)$, $(d, c)$, and $(c, a)$. Suppose that we start the $\\text{DFS}$ of $\\text{TOPOLOGICAL-SORT}$ at vertex $c$. Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$. The \"bad\" edges in this case are $(b, c)$ and $(d, c)$. However, if we had instead ordered them by $a, b, d, c$ then the only bad edges would be $(c, a)$. Thus $\\text{TOPOLOGICAL-SORT}$ doesn't always minimizes the number of \"bad\" edges","title":"22.4-4"},{"location":"Chap22/22.4/#224-5","text":"Another way to perform topological sorting on a directed acyclic graph $G = (V, E)$ is to repeatedly find a vertex of $\\text{in-degree}$ $0$, output it, and remove it and all of its outgoing edges from the graph. Explain how to implement this idea so that it runs in time $O(V + E)$. What happens to this algorithm if $G$ has cycles? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 TOPOLOGICAL - SORT ( G ) // Initialize in-degree, \u0398(V) time. for each vertex u \u2208 G . V u . in - degree = 0 // Compute in-degree, \u0398(V + E) time. for each vertex u \u2208 G . V for each v \u2208 G . Adj [ u ] v . in - degree = v . in - degree + 1 // Initialize Queue, \u0398(V) time. Q = \u00d8 for each vertex u \u2208 G . V if u . in - degree == 0 ENQUEUE ( Q , u ) // while loop takes O(V + E) time. while Q != \u00d8 u = DEQUEUE ( Q ) output u // for loop executes O(E) times total. for each v \u2208 G . Adj [ u ] v . in - degree = v . in - degree - 1 if v . in - degree == 0 ENQUEUE ( Q , v ) // Check for cycles, O(V) time. for each vertex u \u2208 G . V if u . in - degree != 0 report that there ' s a cycle // Another way to check for cycles would be to count the vertices // that are output and report a cycle if that number is < |V|. To find and output vertices of $\\text{in-degree}$ $0$, we first compute all vertices' $\\text{in-degree}$s by making a pass through all the edges (by scanning the adjacency lists of all the vertices) and incrementing the $\\text{in-degree}$ of each vertex an edge enters. Computing all $\\text{in-degree}$s takes $\\Theta(V + E)$ time ($|V|$ adjacency lists accessed, $|E|$ edges total found in those lists, $\\Theta(1)$ work for each edge). We keep the vertices with $\\text{in-degree}$ $0$ in a FIFO queue, so that they can be enqueued and dequeued in $O(1)$ time. (The order in which vertices in the queue are processed doesn't matter, so any kind of FIFO queue works.) Initializing the queue takes one pass over the vertices doing $\\Theta(1)$ work, for total time $\\Theta(V)$. As we process each vertex from the queue, we effectively remove its outgoing edges from the graph by decrementing the $\\text{in-degree}$ of each vertex one of those edges enters, and we enqueue any vertex whose $\\text{in-degree}$ goes to $0$. We do not need to actually remove the edges from the adjacency list, because that adjacency list will never be processed again by the algorithm: Each vertex is enqueued/dequeued at most once because it is enqueued only if it starts out with $\\text{in-degree}$ $0$ or if its indegree becomes $0$ after being decremented (and never incremented) some number of times. The processing of a vertex from the queue happens $O(V)$ times because no vertex can be enqueued more than once. The per-vertex work (dequeue and output) takes $O(1)$ time, for a total of $O(V)$ time. Because the adjacency list of each vertex is scanned only when the vertex is dequeued, the adjacency list of each vertex is scanned at most once. Since the sum of the lengths of all the adjacency lists is $\\Theta(E)$, at most $O(E)$ time is spent in total scanning adjacency lists. For each edge in an adjacency list, $\\Theta(1)$ work is done, for a total of $O(E)$ time. Thus the total time taken by the algorithm is $O(V + E)$. The algorithm outputs vertices in the right order ($u$ before $v$ for every edge $(u, v)$) because vwill not be output until its $\\text{in-degree}$ becomes $0$, which happens only when every edge $(u, v)$ leading into $v$ has been \"removed\" due to the processing (including output) of $u$. If there are no cycles, all vertices are output. Proof: Assume that some vertex $v_0$ is not output. Vertex $v_0$ cannot start out with $\\text{in-degree}$ $0$ (or it would be output), so there are edges into $v_0$. Since $v_0$'s $\\text{in-degree}$ never becomes $0$, at least one edge $(v_1, v_0)$ is never removed, which means that at least one other vertex $v_1$ was not output. Similarly, $v_1$ not output means that some vertex $v_2$ such that $(v_2, v_1) \\in E$ was not output, and so on. Since the number of vertices is finite, this path ($\\cdots \\to v_2 \\to v_1 \\to v_0$) is finite, so we must have $v_i = v_j$ for some $i$ and $j$ in this sequence, which means there is a cycle. If there are cycles, not all vertices will be output, because some $\\text{in-degree}$s never become $0$. Proof: Assume that a vertex in a cycle is output (its $\\text{in-degree}$ becomes $0$). Let $v$ be the first vertex in its cycle to be output, and let $u$ be $v$'s predecessor in the cycle. In order for $v$'s $\\text{in-degree}$ to become $0$, the edge $(u, v)$ must have been \"removed\", which happens only when $u$ is processed. But this cannot have happened, because $v$ is the first vertex in its cycle to be processed. Thus no vertices in cycles are output.","title":"22.4-5"},{"location":"Chap22/22.5/","text":"22.5-1 How can the number of strongly connected components of a graph change if a new edge is added? It can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before. So, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same $\\text{SCC}$ in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices. 22.5-2 Show how the procedure $\\text{STRONGLY-CONNECTED-COMPONENTS}$ works on the graph of Figure 22.6. Specifically, show the finishing times computed in line 1 and the forest produced in line 3. Assume that the loop of lines 5\u20137 of $\\text{DFS}$ considers vertices in alphabetical order and that the adjacency lists are in alphabetical order. The finishing times of each vertex were computed in exercise 22.3-2. The forest consists of 5 trees, each of which is a chain. We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q - y - t$, $x - z$, and $s - w - v$. 22.5-3 Professor Bacon claims that the algorithm for strongly connected components would be simpler if it used the original (instead of the transpose) graph in the second depth-first search and scanned the vertices in order of increasing finishing times. Does this simpler algorithm always produce correct results? Professor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\\{1, 2, 3\\}$ and consists of the edges $(2, 1), (2, 3), (3, 2)$. Then, we should end up with $\\{2, 3\\}$ and $\\{1\\}$ as our $\\text{SCC}$'s. However, a possible $\\text{DFS}$ starting at $2$ could explore $3$ before $1$, this would mean that the finish time of $3$ is lower than of $1$ and $2$. This means that when we first perform the $\\text{DFS}$ starting at $3$. However, a $\\text{DFS}$ starting at $3$ will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single $\\text{SCC}$, even though this is clearly not the case since there is neither a path from $1$ to $2$ of from $1$ to $3$. 22.5-4 Prove that for any directed graph $G$, we have $((G^\\text T)^{\\text{SCC}})^\\text T = G^{\\text{SCC}}$. That is, the transpose of the component graph of $G^\\text T$ is the same as the component graph of $G$. First observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^\\text T$. Thus the vertex sets of $G^{\\text{SCC}}$ and $(G^\\text T)^{\\text{SCC}}$ are the same, which implies the vertex sets of $((G^\\text T)^\\text{SCC})^\\text T$ and $G^{\\text{SCC}}$ are the same. It suffices to show that their edge sets are the same. Suppose $(v_i, v_j)$ is an edge in $((G^\\text T)^{\\text{SCC}})^\\text T$. Then $(v_j, v_i)$ is an edge in $(G^\\text T)^{\\text{SCC}}$. Thus there exist $x \\in C_j$ and $y \\in C_i$ such that $(x, y)$ is an edge of $G^\\text T$, which implies $(y, x)$ is an edge of $G$. Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{\\text{SCC}}$. For the opposite implication we simply note that for any graph $G$ we have $(G^\\text T)^{\\text T} = G$. 22.5-5 Give an $O(V + E)$-time algorithm to compute the component graph of a directed graph $G = (V, E)$. Make sure that there is at most one edge between two vertices in the component graph your algorithm produces. We have at our disposal an $O(V + E)$-time algorithm that computes strongly connected components. Let us assume that the output of this algorithm is a mapping $u.scc$, giving the number of the strongly connected component containing vertex $u$, for each vertex $u$. Without loss of generality, assume that $u.scc$ is an integer in the set $\\{1, 2, \\ldots, |V|\\}$. Construct the multiset (a set that can contain the same object more than once) $T = \\{u.scc: u \\in V\\}$, and sort it by using counting sort. Since the values we are sorting are integers in the range $1$ to $|V|$, the time to sort is $O(V)$. Go through the sorted multiset $T$ and every time we find an element $x$ that is distinct from the one before it, add $x$ to $V^{\\text{SCC}}$. (Consider the first element of the sorted set as \"distinct from the one before it.\") It takes $O(V)$ time to construct $V^{\\text{SCC}}$. Construct the set of ordered pairs $$\\text{$(x, y)$: there is an edge $(u, v) \\in E$, $x = u.scc$, and $y = v.scc$}.$$ We can easily construct this set in $\\Theta(E)$ time by going through all edges in $E$ and looking up $u.scc$ and $v.scc$ for each edge $(u, v) \\in E$. Having constructed $S$, remove all elements of the form $(x, x)$. Alternatively, when we construct $S$, do not put an element in $S$ when we find an edge $(u, v)$ for which $u.scc = v.scc$. $S$ now has at most $|E|$ elements. Now sort the elements of $S$ using radix sort. Sort on one component at a time. The order does not matter. In other words, we are performing two passes of counting sort. The time to do so is $O(V + E)$, since the values we are sorting on are integers in the range $1$ to $|V|$. Finally, go through the sorted set $S$, and every time we find an element $(x, y)$ that is distinct from the element before it (again considering the first element of the sorted set as distinct from the one before it), add $(x, y)$ to $E^{\\text{SCC}}$. Sorting and then adding $(x, y)$ only if it is distinct from the element before it ensures that we add $(x, y)$ at most once. It takes $O(E)$ time to go through $S$ in this way, once $S$ has been sorted. The total time is $O(V + E)$. 22.5-6 Given a directed graph $G = (V, E)$, explain how to create another graph $G' = (V, E')$ such that (a) $G'$ has the same strongly connected components as $G$, (b) $G'$ has the same component graph as $G$, and (c) $E'$ is as small as possible. Describe a fast algorithm to compute $G'$. The basic idea is to replace the edges within each $\\text{SCC}$ by one simple, directed cycle and then remove redundant edges between $\\text{SCC}$'s. Since there must be at least $k$ edges within an $\\text{SCC}$ that has $k$ vertices, a single directed cycle of $k$ edges gives the $k$-vertex $\\text{SCC}$ with the fewest possible edges. The algorithm works as follows: Identify all $\\text{SCC}$'s of $G$. Time: $\\Theta(V + E)$, using the $\\text{SCC}$ algorithm in Section 22.5. Form the component graph $G^{\\text{SCC}}$. Time: $O(V + E)$, by Exercise 22.5-5. Start with $E' = \\emptyset$. Time: $O(1)$. For each $\\text{SCC}$ of $G$, let the vertices in the $\\text{SCC}$ be $v_1, v_2, \\ldots, v_k$, and add to $E'$ the directed edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k), (v_k, v_1)$. These edges form a simple, directed cycle that includes all vertices of the $\\text{SCC}$. Time for all $\\text{SCC}$'s: $O(V)$. For each edge $(u, v)$ in the component graph $G^{\\text{SCC}}$, select any vertex $x$ in $u$'s $\\text{SCC}$ and any vertex $y$ in $v$'s $\\text{SCC}$, and add the directed edge $(x, y)$ to $E'$. Time: $O(E)$. 22.5-7 A directed graph $G = (V, E)$ is semiconnected if, for all pairs of vertices $u, v \\in V$, we have $u \\leadsto v$ or $v \\leadsto u$. Give an efficient algorithm to determine whether or not $G$ is semiconnected. Prove that your algorithm is correct, and analyze its running time. To determine whether $G = (V, E)$ is semiconnected, do the following: Call $\\text{STRONGLY-CONNECTED-COMPONENTS}$. Form the component graph. (By Exercise 22.5-5, you may assume that this takes $O(V + E)$ time.) Topologically sort the component graph. (Recall that it's a dag.) Assuming that $G$ contains $k$ $\\text{SCC}$'s, the topological sort gives a linear ordering $\\langle v_1, v_2, \\ldots, v_k \\rangle$ of the vertices. Verify that the sequence of vertices $\\langle v_1, v_2, \\ldots, v_k \\rangle$ given by topological sort forms a linear chain in the component graph. That is, verify that the edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k)$ exist in the component graph. If the vertices form a linear chain, then the original graph is semiconnected; otherwise it is not. Because we know that all vertices in each $\\text{SCC}$ are mutually reachable from each other, it suffices to show that the component graph is semiconnected if and only if it contains a linear chain. We must also show that if there's a linear chain in the component graph, it's the one returned by topological sort. We'll first show that if there's a linear chain in the component graph, then it's the one returned by topological sort. In fact, this is trivial. A topological sort has to respect every edge in the graph. So if there's a linear chain, a topological sort must give us the vertices in order. Now we'll show that the component graph is semiconnected if and only if it contains a linear chain. First, suppose that the component graph contains a linear chain. Then for every pair of vertices $u$, $v$ in the component graph, there is a path between them. If $u$ precedes $v$ in the linear chain, then there's a path $u \\leadsto v$. Otherwise, $v$ precedes $u$, and there's a path $v \\leadsto u$. Conversely, suppose that the component graph does not contain a linear chain. Then in the list returned by topological sort, there are two consecutive vertices $v_i$ and $v_{i + 1}$, but the edge$(v_i, v_{i + 1})$ is not in the component graph. Any edges out of $v_i$ are to vertices $v_j$, where $j > i + 1$, and so there is no path from $v_i$ to $v_{i + 1}$ in the component graph. And since $v_{i + 1}$ follows $v_i$ in the topological sort, there cannot be any paths at all from $v_{i + 1}$ to $v_i$. Thus, the component graph is not semiconnected. Running time of each step: $\\Theta(V + E)$. $O(V + E)$. Since the component graph has at most $|V|$ vertices and at most $|E|$ edges, $O(V + E)$. Also $O(V + E)$. We just check the adjacency list of each vertex $v_i$ in the component graph to verify that there's an edge $(v_i, v_{i + 1})$. We'll go through each adjacency list once. Thus, the total running time is $\\Theta(V + E)$.","title":"22.5 Strongly connected components"},{"location":"Chap22/22.5/#225-1","text":"How can the number of strongly connected components of a graph change if a new edge is added? It can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before. So, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same $\\text{SCC}$ in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices.","title":"22.5-1"},{"location":"Chap22/22.5/#225-2","text":"Show how the procedure $\\text{STRONGLY-CONNECTED-COMPONENTS}$ works on the graph of Figure 22.6. Specifically, show the finishing times computed in line 1 and the forest produced in line 3. Assume that the loop of lines 5\u20137 of $\\text{DFS}$ considers vertices in alphabetical order and that the adjacency lists are in alphabetical order. The finishing times of each vertex were computed in exercise 22.3-2. The forest consists of 5 trees, each of which is a chain. We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q - y - t$, $x - z$, and $s - w - v$.","title":"22.5-2"},{"location":"Chap22/22.5/#225-3","text":"Professor Bacon claims that the algorithm for strongly connected components would be simpler if it used the original (instead of the transpose) graph in the second depth-first search and scanned the vertices in order of increasing finishing times. Does this simpler algorithm always produce correct results? Professor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\\{1, 2, 3\\}$ and consists of the edges $(2, 1), (2, 3), (3, 2)$. Then, we should end up with $\\{2, 3\\}$ and $\\{1\\}$ as our $\\text{SCC}$'s. However, a possible $\\text{DFS}$ starting at $2$ could explore $3$ before $1$, this would mean that the finish time of $3$ is lower than of $1$ and $2$. This means that when we first perform the $\\text{DFS}$ starting at $3$. However, a $\\text{DFS}$ starting at $3$ will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single $\\text{SCC}$, even though this is clearly not the case since there is neither a path from $1$ to $2$ of from $1$ to $3$.","title":"22.5-3"},{"location":"Chap22/22.5/#225-4","text":"Prove that for any directed graph $G$, we have $((G^\\text T)^{\\text{SCC}})^\\text T = G^{\\text{SCC}}$. That is, the transpose of the component graph of $G^\\text T$ is the same as the component graph of $G$. First observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^\\text T$. Thus the vertex sets of $G^{\\text{SCC}}$ and $(G^\\text T)^{\\text{SCC}}$ are the same, which implies the vertex sets of $((G^\\text T)^\\text{SCC})^\\text T$ and $G^{\\text{SCC}}$ are the same. It suffices to show that their edge sets are the same. Suppose $(v_i, v_j)$ is an edge in $((G^\\text T)^{\\text{SCC}})^\\text T$. Then $(v_j, v_i)$ is an edge in $(G^\\text T)^{\\text{SCC}}$. Thus there exist $x \\in C_j$ and $y \\in C_i$ such that $(x, y)$ is an edge of $G^\\text T$, which implies $(y, x)$ is an edge of $G$. Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{\\text{SCC}}$. For the opposite implication we simply note that for any graph $G$ we have $(G^\\text T)^{\\text T} = G$.","title":"22.5-4"},{"location":"Chap22/22.5/#225-5","text":"Give an $O(V + E)$-time algorithm to compute the component graph of a directed graph $G = (V, E)$. Make sure that there is at most one edge between two vertices in the component graph your algorithm produces. We have at our disposal an $O(V + E)$-time algorithm that computes strongly connected components. Let us assume that the output of this algorithm is a mapping $u.scc$, giving the number of the strongly connected component containing vertex $u$, for each vertex $u$. Without loss of generality, assume that $u.scc$ is an integer in the set $\\{1, 2, \\ldots, |V|\\}$. Construct the multiset (a set that can contain the same object more than once) $T = \\{u.scc: u \\in V\\}$, and sort it by using counting sort. Since the values we are sorting are integers in the range $1$ to $|V|$, the time to sort is $O(V)$. Go through the sorted multiset $T$ and every time we find an element $x$ that is distinct from the one before it, add $x$ to $V^{\\text{SCC}}$. (Consider the first element of the sorted set as \"distinct from the one before it.\") It takes $O(V)$ time to construct $V^{\\text{SCC}}$. Construct the set of ordered pairs $$\\text{$(x, y)$: there is an edge $(u, v) \\in E$, $x = u.scc$, and $y = v.scc$}.$$ We can easily construct this set in $\\Theta(E)$ time by going through all edges in $E$ and looking up $u.scc$ and $v.scc$ for each edge $(u, v) \\in E$. Having constructed $S$, remove all elements of the form $(x, x)$. Alternatively, when we construct $S$, do not put an element in $S$ when we find an edge $(u, v)$ for which $u.scc = v.scc$. $S$ now has at most $|E|$ elements. Now sort the elements of $S$ using radix sort. Sort on one component at a time. The order does not matter. In other words, we are performing two passes of counting sort. The time to do so is $O(V + E)$, since the values we are sorting on are integers in the range $1$ to $|V|$. Finally, go through the sorted set $S$, and every time we find an element $(x, y)$ that is distinct from the element before it (again considering the first element of the sorted set as distinct from the one before it), add $(x, y)$ to $E^{\\text{SCC}}$. Sorting and then adding $(x, y)$ only if it is distinct from the element before it ensures that we add $(x, y)$ at most once. It takes $O(E)$ time to go through $S$ in this way, once $S$ has been sorted. The total time is $O(V + E)$.","title":"22.5-5"},{"location":"Chap22/22.5/#225-6","text":"Given a directed graph $G = (V, E)$, explain how to create another graph $G' = (V, E')$ such that (a) $G'$ has the same strongly connected components as $G$, (b) $G'$ has the same component graph as $G$, and (c) $E'$ is as small as possible. Describe a fast algorithm to compute $G'$. The basic idea is to replace the edges within each $\\text{SCC}$ by one simple, directed cycle and then remove redundant edges between $\\text{SCC}$'s. Since there must be at least $k$ edges within an $\\text{SCC}$ that has $k$ vertices, a single directed cycle of $k$ edges gives the $k$-vertex $\\text{SCC}$ with the fewest possible edges. The algorithm works as follows: Identify all $\\text{SCC}$'s of $G$. Time: $\\Theta(V + E)$, using the $\\text{SCC}$ algorithm in Section 22.5. Form the component graph $G^{\\text{SCC}}$. Time: $O(V + E)$, by Exercise 22.5-5. Start with $E' = \\emptyset$. Time: $O(1)$. For each $\\text{SCC}$ of $G$, let the vertices in the $\\text{SCC}$ be $v_1, v_2, \\ldots, v_k$, and add to $E'$ the directed edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k), (v_k, v_1)$. These edges form a simple, directed cycle that includes all vertices of the $\\text{SCC}$. Time for all $\\text{SCC}$'s: $O(V)$. For each edge $(u, v)$ in the component graph $G^{\\text{SCC}}$, select any vertex $x$ in $u$'s $\\text{SCC}$ and any vertex $y$ in $v$'s $\\text{SCC}$, and add the directed edge $(x, y)$ to $E'$. Time: $O(E)$.","title":"22.5-6"},{"location":"Chap22/22.5/#225-7","text":"A directed graph $G = (V, E)$ is semiconnected if, for all pairs of vertices $u, v \\in V$, we have $u \\leadsto v$ or $v \\leadsto u$. Give an efficient algorithm to determine whether or not $G$ is semiconnected. Prove that your algorithm is correct, and analyze its running time. To determine whether $G = (V, E)$ is semiconnected, do the following: Call $\\text{STRONGLY-CONNECTED-COMPONENTS}$. Form the component graph. (By Exercise 22.5-5, you may assume that this takes $O(V + E)$ time.) Topologically sort the component graph. (Recall that it's a dag.) Assuming that $G$ contains $k$ $\\text{SCC}$'s, the topological sort gives a linear ordering $\\langle v_1, v_2, \\ldots, v_k \\rangle$ of the vertices. Verify that the sequence of vertices $\\langle v_1, v_2, \\ldots, v_k \\rangle$ given by topological sort forms a linear chain in the component graph. That is, verify that the edges $(v_1, v_2), (v_2, v_3), \\ldots, (v_{k - 1}, v_k)$ exist in the component graph. If the vertices form a linear chain, then the original graph is semiconnected; otherwise it is not. Because we know that all vertices in each $\\text{SCC}$ are mutually reachable from each other, it suffices to show that the component graph is semiconnected if and only if it contains a linear chain. We must also show that if there's a linear chain in the component graph, it's the one returned by topological sort. We'll first show that if there's a linear chain in the component graph, then it's the one returned by topological sort. In fact, this is trivial. A topological sort has to respect every edge in the graph. So if there's a linear chain, a topological sort must give us the vertices in order. Now we'll show that the component graph is semiconnected if and only if it contains a linear chain. First, suppose that the component graph contains a linear chain. Then for every pair of vertices $u$, $v$ in the component graph, there is a path between them. If $u$ precedes $v$ in the linear chain, then there's a path $u \\leadsto v$. Otherwise, $v$ precedes $u$, and there's a path $v \\leadsto u$. Conversely, suppose that the component graph does not contain a linear chain. Then in the list returned by topological sort, there are two consecutive vertices $v_i$ and $v_{i + 1}$, but the edge$(v_i, v_{i + 1})$ is not in the component graph. Any edges out of $v_i$ are to vertices $v_j$, where $j > i + 1$, and so there is no path from $v_i$ to $v_{i + 1}$ in the component graph. And since $v_{i + 1}$ follows $v_i$ in the topological sort, there cannot be any paths at all from $v_{i + 1}$ to $v_i$. Thus, the component graph is not semiconnected. Running time of each step: $\\Theta(V + E)$. $O(V + E)$. Since the component graph has at most $|V|$ vertices and at most $|E|$ edges, $O(V + E)$. Also $O(V + E)$. We just check the adjacency list of each vertex $v_i$ in the component graph to verify that there's an edge $(v_i, v_{i + 1})$. We'll go through each adjacency list once. Thus, the total running time is $\\Theta(V + E)$.","title":"22.5-7"},{"location":"Chap22/Problems/22-1/","text":"A depth-first forest classifies the edges of a graph into tree, back, forward, and cross edges. A breadth-first tree can also be used to classify the edges reachable from the source of the search into the same four categories. a. Prove that in a breadth-first search of an undirected graph, the following properties hold: There are no back edges and no forward edges. For each tree edge $(u, v)$, we have $v.d = u.d + 1$. For each cross edge $(u, v)$, we have $v.d = u.d$ or $v.d = u.d + 1$. b. Prove that in a breadth-first search of a directed graph, the following properties hold: There are no forward edges. For each tree edge $(u, v)$, we have $v.d = u.d + 1$. For each cross edge $(u, v)$, we have $v.d \\le u.d + 1$. For each back edge $(u, v)$, we have $0 \\le v.d \\le u.d$. a. Suppose $(u, v)$ is a back edge or a forward edge in a $\\text{BFS}$ of an undirected graph. Then one of $u$ and $v$, say $u$, is a proper ancestor of the other ($v$) in the breadth-first tree. Since we explore all edges of $u$ before exploring any edges of any of $u$'s descendants, we must explore the edge $(u, v)$ at the time we explore $u$. But then $(u, v)$ must be a tree edge. In $\\text{BFS}$, an edge $(u, v)$ is a tree edge when we set $v.\\pi \\leftarrow u$. But we only do so when we set $v.d \\leftarrow u.d + 1$. Since neither $u.d$ nor $v.d$ ever changes thereafter, we have $v.d=u.d+1$ when $\\text{BFS}$ completes. Consider a cross edge $(u, v)$ where, without loss of generality, $u$ is visited before $v$. At the time we visit $u$, vertex $v$ must already be on the queue, for otherwise $(u, v)$ would be a tree edge. Because $v$ is on the queue, we have $v.d \\le u.d + 1$ by Lemma 22.3. By Corollary 22.4, we have $v.d \\ge u.d$. Thus, either $v.d = u.d$ or $v.d = u.d + 1$. b. Suppose $(u, v)$ is a forward edge. Then we would have explored it while visiting $u$, and it would have been a tree edge. Same as for undirected graphs. For any edge $(u, v)$, whether or not it's a cross edge, we cannot have $v.d > u.d + 1$, since we visit $v$ at the latest when we explore edge $(u, v)$. Thus, $v.d \\le u.d + 1$. Clearly, $v.d \\ge 0$ for all vertices $v$. For a back edge $(u, v)$, $v$ is an ancestor of $u$ in the breadth-first tree, which means that $v.d\\le u.d$. (Note that since self-loops are considered to be back edges, we could have $u = v$.)","title":"22-1 Classifying edges by breadth-first search"},{"location":"Chap22/Problems/22-2/","text":"Let $G = (V, E)$ be a connected, undirected graph. An articulation point of $G$ is a vertex whose removal disconnects $G$. A bridge of $G$ is an edge whose removal disconnects $G$. A biconnected component of $G$ is a maximal set of edges such that any two edges in the set lie on a common simple cycle. Figure 22.10 illustrates these definitions. We can determine articulation points, bridges, and biconnected components using depth-first search. Let $G_\\pi = (V, E_\\pi)$ be a depth-first tree of $G$. a. Prove that the root of $G_\\pi$ is an articulation point of $G$ if and only if it has at least two children in $G_\\pi$. b. Let $v$ be a nonroot vertex of $G_\\pi$. Prove that $v$ is an articulation point of $G$ if and only if $v$ has a child $s$ such that there is no back edge from $s$ or any descendant of $s$ to a proper ancestor of $v$. c. Let $$ v.low = \\min \\begin{cases} v.d, \\\\ w.d:(u,w) \\text{ is a back edge for some descendant } u \\text{ of } v. \\end{cases} $$ Show how to computer $v.low$ for all vertices $v \\in V$ in $O(E)$ time. d. Show how to compute all articulation points in $O(E)$ time. e. Prove that an edge of $G$ is a bridge if and only if it does not lie on any simple cycle of $G$. f. Show how to compute all the bridges of $G$ in $O(E)$ time. g. Prove that the biconnected components of $G$ partition the nonbridge edges of $G$. h. Give an $O(E)$-time algorithm to label each edge $e$ of $G$ with a positive integer $e.bcc$ such that $e.bcc = e'.bcc$ if and only if $e$ and $e'$ are in the same biconnected component. a. First suppose the root $r$ of $G_\\pi$ is an articulation point. Then the removal of $r$ from $G$ would cause the graph to disconnect, so $r$ has at least $2$ children in $G$. If $r$ has only one child $v$ in $G_\\pi$ then it must be the case that there is a path from $v$ to each of $r$'s other children. Since removing $r$ disconnects the graph, there must exist vertices $u$ and $w$ such that the only paths from $u$ to $w$ contain $r$. To reach $r$ from $u$, the path must first reach one of $r$'s children. This child is connect to $v$ via a path which doesn't contain $r$. To reach $w$, the path must also leave $r$ through one of its children, which is also reachable by $v$. This implies that there is a path from $u$ to $w$ which doesn't contain $r$, a contradiction. Now suppose $r$ has at least two children $u$ and $v$ in $G_\\pi$. Then there is no path from $u$ to $v$ in $G$ which doesn't go through $r$, since otherwise $u$ would be an ancestor of $v$. Thus, removing $r$ disconnects the component containing $u$ and the component containing $v$, so $r$ is an articulation point. b. Suppose that $v$ is a nonroot vertex of $G_\\pi$ and that $v$ has a child $s$ such that neither $s$ nor any of $s$'s descendants have back edges to a proper ancestor of $v$. Let $r$ be an ancestor of $v$, and remove $v$ from $G$. Since we are in the undirected case, the only edges in the graph are tree edges or back edges, which means that every edge incident with $s$ takes us to a descendant of $s$, and no descendants have back edges, so at no point can we move up the tree by taking edges. Therefore $r$ is unreachable from $s$, so the graph is disconnected and $v$ is an articulation point. Now suppose that for every child of $v$ there exists a descendant of that child which has a back edge to a proper ancestor of $v$. Remove $v$ from $G$. Every subtree of $v$ is a connected component. Within a given subtree, find the vertex which has a back edge to a proper ancestor of $v$. Since the set $T$ of vertices which aren't descendants of $v$ form a connected component, we have that every subtree of $v$ is connected to $T$. Thus, the graph remains connected after the deletion of $v$ so $v$ is not an articulation point. c. Since $v$ is discovered before all of its descendants, the only back edges which could affect $v.low$ are ones which go from a descendant of $v$ to a proper ancestor of $v$. If we know $u.low$ for every child $u$ of $v$, then we can compute $v.low$ easily since all the information is coded in its descendants. Thus, we can write the algorithm recursively: If $v$ is a leaf in $G_\\pi$ then $v.low$ is the minimum of $v.d$ and $w.d$ where $(v, w)$ is a back edge. If $v$ is not a leaf, $v$ is the minimum of $v.d$, $w.d$ where $w$ is a back edge, and $u.low$, where $u$ is a child of $v$. Computing $v.low$ for a vertex is linear in its degree. The sum of the vertices' degrees gives twice the number of edges, so the total runtime is $O(E)$. d. First apply the algorithm of part (c) in $O(E)$ to compute $v.low$ for all $v \\in V$. If $v.low$ = $v.d$ if and only if no descendant of $v$ has a back edge to a proper ancestor of $v$, if and only if $v$ is not an articulation point. Thus, we need only check $v.low$ versus $v.d$ to decide in constant time whether or not $v$ is an articulation point, so the runtime is $O(E)$. e. An edge $(u, v)$ lies on a simple cycle if and only if there exists at least one path from $u$ to $v$ which doesn't contain the edge $(u, v)$, if and only if removing $(u, v)$ doesn't disconnect the graph, if and only if $(u, v)$ is not a bridge. f. A edge $(u, v)$ lies on a simple cycle in an undirected graph if and only if either both of its endpoints are articulation points, or one of its endpoints is an articulation point and the other is a vertex of degree $1$. Since we can compute all articulation points in $O(E)$ and we can decide whether or not a vertex has degree $1$ in constant time, we can run the algorithm in part (d) and then decide whether each edge is a bridge in constant time, so we can find all bridges in $O(E)$ time. g. It is clear that every nonbridge edge is in some biconnected component, so we need to show that if $C_1$ and $C_2$ are distinct biconnected components, then they contain no common edges. Suppose to the contrary that $(u, v)$ is in both $C_1$ and $C_2$. Let $(a, b)$ be any edge in $C_1$ and $(c, d)$ be any edge in $C_2$. Then $(a, b)$ lies on a simple cycle with $(u, v)$, consisting of the path $$a, b, p_1, \\ldots, p_k, u, v, p_{k + 1}, \\ldots, p_n, a.$$ Similarly, $(c, d)$ lies on a simple cycle with $(u, v)$ consisting of the path $$c, d, q_1, \\ldots, q_m, u, v, q_{m + 1}, \\ldots, q_l, c.$$ This means $$a, b, p_1, \\ldots, p_k, u, q_m, \\ldots, q_1, d, c, q_l , \\ldots, q_{m + 1}, v, p_{k + 1}, \\ldots, p_n,$$ is a simple cycle containing $(a, b)$ and $(c, d)$, a contradiction. Thus, the biconnected components form a partition. h. Locate all bridge edges in $O(E)$ time using the algorithm described in part (f). Remove each bridge from $E$. The biconnected components are now simply the edges in the connected components. Assuming this has been done, run the following algorithm, which clearly runs in $O(|E|)$ where $|E|$ is the number of edges originally in $G$.","title":"22-2 Articulation points, bridges, and biconnected components"},{"location":"Chap22/Problems/22-3/","text":"An Euler tour of a strongly connected, directed graph $G = (V, E)$ is a cycle that traverses each edge of $G$ exactly once, although it may visit a vertex more than once. a. Show that $G$ has an Euler tour if and only if $\\text{in-degree}(v) = \\text{out-degree}(v)$ for each vertex $v \\in V$. b. Describe an $O(E)$-time algorithm to find an Euler tour of $G$ if one exists. ($\\textit{Hint:}$ Merge edge-disjoint cycles.) a. An Euler tour is a single cycle that traverses each edge of $G$ exactly once, but it might not be a simple cycle. An Euler tour can be decomposed into a set of edge-disjoint simple cycles, however. If $G$ has an Euler tour, therefore, we can look at the simple cycles that, together, form the tour. In each simple cycle, each vertex in the cycle has one entering edge and one leaving edge. In each simple cycle, therefore, each vertex $v$ has $\\text{in-degree}(v) = \\text{out-degree}(v)$, where the degrees are either $1$ (if $v$ is on the simple cycle) or $0$ (if $v$ is not on the simple cycle). Adding the in- and out- degrees over all edges proves that if $G$ has an Euler tour, then $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$. We prove the converse\u2014that if $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$, then $G$ has an Euler tour\u2014in two different ways. One proof is nonconstructive, and the other proof will help us design the algorithm for part (b). First, we claim that if $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$, then we can pick any vertex $u$ for which $\\text{in-degree}(u) = \\text{out-degree}(u) \\ge 1$ and create a cycle (not necessarily simple) that contains $u$. To prove this claim, let us start by placing vertex $u$ on the cycle, and choose any leaving edge of $u$, say ($u, v$). Now we put $v$ on the cycle. Since $\\text{in-degree}(v) = \\text{out-degree}(v) \\ge 1$, we can pick some leaving edge of $v$ and continue visiting edges and vertices. Each time we pick an edge, we can remove it from further consideration. At each vertex other than $u$, at the time we visit an entering edge, there must be an unvisited leaving edge, since $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$. The only vertex for which there might not be an unvisited leaving edge is $u$, since we started the cycle by visiting one of $u$'s leaving edges. Since there's always a leaving edge we can visit from all vertices other than $u$, eventually the cycle must return to $u$, thus proving the claim. The nonconstructive proof proves the contrapositive\u2014that if $G$ does not have an Euler tour, then $\\text{in-degree}(v) \\ne \\text{out-degree}(v)$ for some vertex $v$\u2014by contradiction. Choose a graph $G = (V, E)$ that does not have an Euler tour but has at least one edge and for which $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$, and let $G$ have the fewest edges of any such graph. By the above claim, $G$ contains a cycle. Let $C$ be a cycle of $G$ with the greatest number of edges, and let $V_C$ be the set of vertices visited by cycle $C$. By our assumption, $C$ is not an Euler tour, and so the set of edges $E' = E - C$ is nonempty. If we use the set $V$ of vertices and the set $E'$ of edges, we get the graph $G' = (V, E')$; this graph has $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$, since we have removed one entering edge and one leaving edge for each vertex on cycle $C$. Consider any component $G'' = (V'' , E'')$ of $G'$, and observe that $G''$ also has $\\text{in-degree}(v) = \\text{out-degree}(v)$ for all vertices $v$. Since $E'' \\subseteq E' \\subsetneq E$, it follows from how we chose $G$ that $G''$ must have an Euler tour, say $C'$. Because the original graph G is connected, there must be some vertex $x \\in V'' \\cup V_C$ and, without loss of generality, consider $x$ to be the first and last vertex on both $C$ and $C'$. But then the cycle $C''$ formed by first traversing $C$ and then traversing $C'$ is a cycle of $G$ with more edges than $C$, contradicting our choice of $C$. We conclude that $C$ must have been an Euler tour. The constructive proof uses the same ideas. Let us start at a vertex $u$ and, via random traversal of edges, create a cycle. We know that once we take any edge entering a vertex $v \\ne u$, we can find an edge leaving $v$ that we have not yet taken. Eventually, we get back to vertex $u$, and if there are still edges leaving $u$ that we have not taken, we can continue the cycle. Eventually, we get back to vertex $u$ and there are no untaken edges leaving $u$. If we have visited every edge in the graph $G$, we are done. Otherwise, since $G$ is connected, there must be some unvisited edge leaving a vertex, say $v$, on the cycle. We can traverse a new cycle starting at $v$, visiting only previously unvisited edges, and we can splice this cycle into the cycle we already know. That is, if the original cycle is $\\langle u, \\ldots, v, w, \\ldots, u \\rangle$, and the new cycle is $\\langle v, x, \\ldots, v\\rangle$, then we can create the cycle $\\langle u, \\ldots, v, x, \\ldots, v, w, \\ldots, u \\rangle$. We continue this process of finding a vertex with an unvisited leaving edge on a visited cycle, visiting a cycle starting and ending at this vertex, and splicing in the newly visited cycle, until we have visited every edge. b. The algorithm is based on the idea in the constructive proof above. We assume that $G$ is represented by adjacency lists, and we work with a copy of the adjacency lists, so that as we visit each edge, we can remove it from its adjacency list. The singly linked form of adjacency list will suffice. The output of this algorithm is a doubly linked list $T$ of vertices which, read in list order, will give an Euler tour. The algorithm constructs $T$ by finding cycles (also represented by doubly linked lists) and splicing them into $T$. By using doubly linked lists for cycles and the Euler tour, splicing a cycle into the Euler tour takes constant time. We also maintain a singly linked list $L$, in which each list element consists of two parts: a vertex $v$, and a pointer to some appearance of $v$ in $T$. Initially, $L$ contains one vertex, which may be any vertex of $G$. Here is the algorithm. 1 2 3 4 5 6 7 8 9 10 EULER - TOUR ( G ) T = empty list L = ( any vertex v \u2208 G . V , NIL ) while L is not empty remore ( v , location - in - T ) from L C = VISIT ( G , L , v ) if location - in - T == NIL T = C else splice C into T just before location - in - T return T 1 2 3 4 5 6 7 8 9 10 11 VISIT ( G , L , v ) C = empty sequence of vertices u = v while out - degree ( u ) > 0 let w be the first vertex in G . Adj [ u ] remove w from G . Adj [ u ], decrementing out - degree ( u ) add u onto the end of C if out - degree ( u ) > 0 add ( u , u ' s location in C ) to L u = w return C The use of $\\text{NIL}$ in the initial assignment to $L$ ensures that the first cycle $C$ returned by $\\text{VISIT}$ becomes the current version of the Euler tour $T$. All cycles returned by $\\text{VISIT}$ thereafter are spliced into $T$. We assume that whenever an empty cycle is returned by $\\text{VISIT}$, splicing it into $T$ leaves $T$ unchanged. Each time that $\\text{EULER-TOUR}$ removes a vertex $v$ from the list $L$, it calls $\\text{VISIT}(G, L, v)$ to find a cycle $C$, possibly empty and possibly not simple, that starts and ends at $v$; the cycle $C$ is represented by a list that starts with $v$ and ends with the last vertex on the cycle before the cycle ends at $v$. $\\text{EULER-TOUR}$ then splices this cycle $C$ into the Euler tour $T$ just before some appearance of $v$ in $T$. When $\\text{VISIT}$ is at a vertex $u$, it looks for some vertex $w$ such that the edge $(u, w)$ has not yet been visited. Removing $w$ from $Adj[u]$ ensures that we will never visit $(u, w)$ again. $\\text{VISIT}$ adds $u$ onto the cycle $C$ that it constructs. If, after removing edge $(u, w)$, vertex $u$ still has any leaving edges, then $u$, along with its location in $C$, is added to $L$. The cycle construction continues from $w$, and it ceases once a vertex with no unvisited leaving edges is found. Using the argument from part (a), at that point, this vertex must close up a cycle. At that point, therefore, the cycle $C$ is returned. It is possible that a vertex $u$ has unvisited leaving edges at the time it is added to list $L$ in $\\text{VISIT}$, but that by the time that $u$ is removed from $L$ in $\\text{EULER-TOUR}$, all of its leaving edges have been visited. In this case, the while loop of $\\text{VISIT}$ executes $0$ iterations, and $\\text{VISIT}$ returns an empty cycle. Once the list $L$ is empty, every edge has been visited. The resulting cycle $T$ is then an Euler tour. To see that $\\text{EULER-TOUR}$ takes $O(E)$ time, observe that because we remove each edge from its adjacency list as it is visited, no edge is visited more than once. Since each edge is visited at some time, the number of times that a vertex is added to $L$, and thus removed from $L$, is at most $|E|$. Thus, the while loop in $\\text{EULER-TOUR}$ executes at most $E$ iterations. The while loop in $\\text{VISIT}$ executes one iteration per edge in the graph, and so it executes at most $E$ iterations as well. Since adding vertex $u$ to the doubly linked list $C$ takes constant time and splicing $C$ into $T$ takes constant time, the entire algorithm takes $O(E)$ time.","title":"22-3 Euler tour"},{"location":"Chap22/Problems/22-4/","text":"Let $G = (V, E)$ be a directed graph in which each vertex $u \\in V$ is labeled with a unique integer $L(U)$ from the set $\\{1, 2, \\ldots, |V|\\}$. For each vertex $u \\in V$, let $R(u) = \\{v \\in V: u \\leadsto v \\}$ be the set of vertices that are reachable from $u$. Define $\\min(u)$ to be the vertex in $R(u)$ whose label is minimum, i.e., $\\min(u)$ is the vertex $v$ such that $L(v) = \\min \\{L(w): w \\in R(u) \\}$. Give an $O(V + E)$-time algorithm that computes $\\min(u)$ for all vertices $u \\in V$. Compute $G^\\text T$ in the usual way, so that $G^\\text T$ is $G$ with its edges reversed. Then do a depth-first search on $G^\\text T$ , but in the main loop of $\\text{DFS}$, consider the vertices in order of increasing values of $L(v)$. If vertex $u$ is in the depth-first tree with root $v$, then $\\min(u) = v$. Clearly, this algorithm takes $O(V + E)$ time. To show correctness, first note that if $u$ is in the depth-first tree rooted at $v$ in $G^\\text T$, then there is a path $v \\leadsto u$ in $G^\\text T$, and so there is a path $u \\leadsto v$ in $G$. Thus, the minimum vertex label of all vertices reachable from $u$ is at most $L(v)$, or in other words, $L(v) \\ge \\min \\{L(w): w \\in R(u)\\}$. Now suppose that $L(v) > \\min \\{L(w): w \\in R(u) \\}$, so that there is a vertex $w \\in R(u)$ such that $L(w) < L(v)$. At the time $v.d$ that we started the depthfirst search from $v$, we would have already discovered $w$, so that $w.d < v.d$. By the parenthesis theorem, either the intervals $[v.d, v.f]$, and $[w.d, w.f]$ are disjoint and neither $v$ nor $w$ is a descendant of the other, or we have the ordering $w.d < v.d < v.f < w.f$ and $v$ is a descendant of $w$. The latter case cannot occur, since $v$ is a root in the depth-first forest (which means that $v$ cannot be a descendant of any other vertex). In the former case, since $w.d < v.d$, we must have $w.d < w.f < v.d < v.f$. In this case, since $u$ is reachable from $w$ in $G^\\text T$ , we would have discovered $u$ by the time $w.f$, so that $u.d < w.f$. Since we discovered $u$ during a search that started at $v$, we have $v.d \\le u.d$. Thus, $v.d \\le u.d < w.f < v.d$, which is a contradiction. We conclude that no such vertex $w$ can exist.","title":"22-4 Reachability"},{"location":"Chap23/23.1/","text":"23.1-1 Let $(u, v)$ be a minimum-weight edge in a connected graph $G$. Show that $(u, v)$ belongs to some minimum spanning tree of $G$. Theorem 23.1 shows this. Let $A$ be the empty set and $S$ be any set containing $u$ but not $v$. 23.1-2 Professor Sabatier conjectures the following converse of Theorem 23.1. Let $G = (V, E)$ be a connected, undirected graph with a real-valued weight function $w$ defined on $E$. Let $A$ be a subset of $E$ that is included in some minimum spanning tree for $G$, let $(S, V - S)$ be any cut of $G$ that respects $A$, and let $(u, v)$ be a safe edge for $A$ crossing $(S, V - S)$. Then, $(u, v)$ is a light edge for the cut. Show that the professor's conjecture is incorrect by giving a counterexample. Let $G$ be the graph with $4$ vertices: $u, v, w, z$. Let the edges of the graph be $(u, v), (u, w), (w, z)$ with weights $3$, $1$, and $2$ respectively. Suppose $A$ is the set $\\{(u, w)\\}$. Let $S = A$. Then $S$ clearly respects $A$. Since $G$ is a tree, its minimum spanning tree is itself, so $A$ is trivially a subset of a minimum spanning tree. Moreover, every edge is safe. In particular, $(u, v)$ is safe but not a light edge for the cut. Therefore Professor Sabatier's conjecture is false. 23.1-3 Show that if an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph. Let $T_0$ and $T_1$ be the two trees that are obtained by removing edge $(u, v)$ from a $\\text{MST}$. Suppose that $V_0$ and $V_1$ are the vertices of $T_0$ and $T_1$ respectively. Consider the cut which separates $V_0$ from $V_1$. Suppose to a contradiction that there is some edge that has weight less than that of $(u, v)$ in this cut. Then, we could construct a minimum spanning tree of the whole graph by adding that edge to $T_1 \\cup T_0$. This would result in a minimum spanning tree that has weight less than the original minimum spanning tree that contained $(u, v)$. 23.1-4 Give a simple example of a connected graph such that the set of edges $\\{(u, v):$ there exists a cut $(S, V - S)$ such that $(u, v)$ is a light edge crossing $(S, V - S)\\}$ does not form a minimum spanning tree. A triangle whose edge weights are all equal is a graph in which every edge is a light edge crossing some cut. But the triangle is cyclic, so it is not a minimum spanning tree. 23.1-5 Let $e$ be a maximum-weight edge on some cycle of connected graph $G = (V, E)$. Prove that there is a minimum spanning tree of $G' = (V, E - \\{e\\})$ that is also a minimum spanning tree of $G$. That is, there is a minimum spanning tree of $G$ that does not include $e$. Let $A$ be any cut that causes some vertices in the cycle on once side of the cut, and some vertices in the cycle on the other. For any of these cuts, we know that the edge $e$ is not a light edge for this cut. Since all the other cuts won't have the edge $e$ crossing it, we won't have that the edge is light for any of those cuts either. This means that we have that e is not safe. 23.1-6 Show that a graph has a unique minimum spanning tree if, for every cut of the graph, there is a unique light edge crossing the cut. Show that the converse is not true by giving a counterexample. Suppose that for every cut of $G$, there is a unique light edge crossing the cut. Let us consider two distinct minimum spanning trees, $T$ and $T'$, of $G$. Because $T$ and $T'$are distinct, $T$ contains some edge $(u, v)$ that is not in $T'$. If we remove $(u, v)$ from $T$, then $T$ becomes disconnected, resulting in a cut $(S, V - S)$. The edge $(u, v)$ is a light edge crossing the cut $(S, V - S)$ (by Exercise 23.1-3) and, by our assumption, it's the only light edge crossing this cut. Because $(u, v)$ is the only light edge crossing $(S, V - S)$ and $(u, v)$ is not in $T'$, each edge in $T'$ that crosses $(S, V - S)$ must have weight strictly greater than w$(u, v)$. As in the proof of Theorem 23.1, we can identify the unique edge $(x, y)$ in $T'$ that crosses $(S, V - S)$ and lies on the cycle that results if we add $(u, v)$ to $T'$. By our assumption, we know that $w(u, v) < w(x, y)$. Then, we can then remove $(x, y)$ from $T'$ and replace it by $(u, v)$, giving a spanning tree with weight strictly less than $w(T')$. Thus, $T'$ was not a minimum spanning tree, contradicting the assumption that the graph had two unique minimum spanning trees. Here's a counterexample for the converse: Here, the graph is its own minimum spanning tree, and so the minimum spanning tree is unique. Consider the cut $(\\{x\\}, \\{y, z\\})$. Both of the edges $(x, y)$ and $(x, z)$ are light edges crossing the cut, and they are both light edges. 23.1-7 Argue that if all edge weights of a graph are positive, then any subset of edges that connects all vertices and has minimum total weight must be a tree. Give an example to show that the same conclusion does not follow if we allow some weights to be nonpositive. First, we show that the subset of edges of minimum total weight that connects all the vertices is a tree. To see this, suppose not, that it had a cycle. This would mean that removing any of the edges in this cycle would mean that the remaining edges would still connect all the vertices, but would have a total weight that's less by the weight of the edge that was removed. This would contradict the minimality of the total weight of the subset of vertices. Since the subset of edges forms a tree, and has minimal total weight, it must also be a minimum spanning tree. To see that this conclusion is not true if we allow negative edge weights, we provide a construction. Consider the graph $K_3$ with all edge weights equal to $-1$. The only minimum weight set of edges that connects the graph has total weight $-3$, and consists of all the edges. This is clearly not a $\\text{MST}$ because it is not a tree, which can be easily seen because it has one more edge than a tree on three vertices should have. Any $\\text{MST}$ of this weighted graph must have weight that is at least $-2$. 23.1-8 Let $T$ be a minimum spanning tree of a graph $G$, and let $L$ be the sorted list of the edge weights of $T$. Show that for any other minimum spanning tree $T'$ of $G$, the list $L$ is also the sorted list of edge weights of $T'$. Suppose that $L'$ is another sorted list of edge weights of a minimum spanning tree. If $L' \\ne L$, there must be a first edge $(u, v)$ in $T$ or $T'$ which is of smaller weight than the corresponding edge $(x, y)$ in the other set. Without loss of generality, assume $(u, v)$ is in $T$. Let $C$ be the graph obtained by adding $(u, v)$ to $L'$. Then we must have introduced a cycle. If there exists an edge on that cycle which is of larger weight than $(u, v)$, we can remove it to obtain a tree $C'$ of weight strictly smaller than the weight of $T'$, contradicting the fact that $T'$ is a minimum spanning tree. Thus, every edge on the cycle must be of lesser or equal weight than $(u, v)$. Suppose that every edge is of strictly smaller weight. Remove $(u, v)$ from $T$ to disconnect it into two components. There must exist some edge besides $(u, v)$ on the cycle which would connect these, and since it has smaller weight we can use that edge instead to create a spanning tree with less weight than $T$, a contradiction. Thus, some edge on the cycle has the same weight as $(u, v)$. Replace that edge by $(u, v)$. The corresponding lists $L$ and $L'$ remain unchanged since we have swapped out an edge of equal weight, but the number of edges which $T$ and $T'$ have in common has increased by $1$. If we continue in this way, eventually they must have every edge in common, contradicting the fact that their edge weights differ somewhere. Therefore all minimum spanning trees have the same sorted list of edge weights. 23.1-9 Let $T$ be a minimum spanning tree of a graph $G = (V, E)$, and let $V'$ be a subset of $V$. Let $T'$ be the subgraph of $T$ induced by $V'$, and let $G'$ be the subgraph of $G$ induced by $V'$. Show that if $T'$ is connected, then $T'$ is a minimum spanning tree of $G'$. Suppose that there was some cheaper spanning tree than $T'$. That is, we have that there is some $T''$ so that $w(T'') < w(T')$. Then, let $S$ be the edges in $T$ but not in $T'$. We can then construct a minimum spanning tree of $G$ by considering $S \\cup T''$. This is a spanning tree since $S \\cup T'$ is, and $T''$ makes all the vertices in $V'$ connected just like $T'$ does. However, we have that $$w(S \\cup T'') = w(S) + w(T'') < w(S) + w(T') = w(S \\cup T') = w(T).$$ This means that we just found a spanning tree that has a lower total weight than a minimum spanning tree. This is a contradiction, and so our assumption that there was a spanning tree of $V'$ cheaper than $T'$ must be false. 23.1-10 Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges in $T$. Show that $T$ is still a minimum spanning tree for $G$. More formally, let $T$ be a minimum spanning tree for $G$ with edge weights given by weight function $w$. Choose one edge $(x, y) \\in T$ and a positive number $k$, and define the weight function $w'$ by $$ w'(u, v) = \\begin{cases} w(u, v) & \\text{ if }(u, v) \\ne (x, y), \\\\ w(x, y) - k & \\text{ if }(u, v) = (x, y). \\end{cases} $$ Show that $T$ is a minimum spanning tree for $G$ with edge weights given by $w'$. Let $x(T) = \\sum_{(x, y) \\in T} w(x, y)$. We have $w'(T) = w(T) - k$. Consider any other spanning tree $T'$, so that $w(T) \\le w(T')$. If $(x, y) \\ne T'$, then $w'(T') = w(T') \\ge w(T) > w'(T)$. If $(x, y) \\in T'$, then $w'(T') = w(T') - k \\ge w(T) - k = w'(T)$. Either way, $w'(T) \\le w'(T')$, and so $T$ is a minimum spanning tree for weight function $w'$. 23.1-11 $\\star$ Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges not in $T$. Give an algorithm for finding the minimum spanning tree in the modified graph. If we were to add in this newly decreased edge to the given tree, we would be creating a cycle. Then, if we were to remove any one of the edges along this cycle, we would still have a spanning tree. This means that we look at all the weights along this cycle formed by adding in the decreased edge, and remove the edge in the cycle of maximum weight. This does exactly what we want since we could only possibly want to add in the single decreased edge, and then, from there we change the graph back to a tree in the way that makes its total weight minimized.","title":"23.1 Growing a minimum spanning tree"},{"location":"Chap23/23.1/#231-1","text":"Let $(u, v)$ be a minimum-weight edge in a connected graph $G$. Show that $(u, v)$ belongs to some minimum spanning tree of $G$. Theorem 23.1 shows this. Let $A$ be the empty set and $S$ be any set containing $u$ but not $v$.","title":"23.1-1"},{"location":"Chap23/23.1/#231-2","text":"Professor Sabatier conjectures the following converse of Theorem 23.1. Let $G = (V, E)$ be a connected, undirected graph with a real-valued weight function $w$ defined on $E$. Let $A$ be a subset of $E$ that is included in some minimum spanning tree for $G$, let $(S, V - S)$ be any cut of $G$ that respects $A$, and let $(u, v)$ be a safe edge for $A$ crossing $(S, V - S)$. Then, $(u, v)$ is a light edge for the cut. Show that the professor's conjecture is incorrect by giving a counterexample. Let $G$ be the graph with $4$ vertices: $u, v, w, z$. Let the edges of the graph be $(u, v), (u, w), (w, z)$ with weights $3$, $1$, and $2$ respectively. Suppose $A$ is the set $\\{(u, w)\\}$. Let $S = A$. Then $S$ clearly respects $A$. Since $G$ is a tree, its minimum spanning tree is itself, so $A$ is trivially a subset of a minimum spanning tree. Moreover, every edge is safe. In particular, $(u, v)$ is safe but not a light edge for the cut. Therefore Professor Sabatier's conjecture is false.","title":"23.1-2"},{"location":"Chap23/23.1/#231-3","text":"Show that if an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph. Let $T_0$ and $T_1$ be the two trees that are obtained by removing edge $(u, v)$ from a $\\text{MST}$. Suppose that $V_0$ and $V_1$ are the vertices of $T_0$ and $T_1$ respectively. Consider the cut which separates $V_0$ from $V_1$. Suppose to a contradiction that there is some edge that has weight less than that of $(u, v)$ in this cut. Then, we could construct a minimum spanning tree of the whole graph by adding that edge to $T_1 \\cup T_0$. This would result in a minimum spanning tree that has weight less than the original minimum spanning tree that contained $(u, v)$.","title":"23.1-3"},{"location":"Chap23/23.1/#231-4","text":"Give a simple example of a connected graph such that the set of edges $\\{(u, v):$ there exists a cut $(S, V - S)$ such that $(u, v)$ is a light edge crossing $(S, V - S)\\}$ does not form a minimum spanning tree. A triangle whose edge weights are all equal is a graph in which every edge is a light edge crossing some cut. But the triangle is cyclic, so it is not a minimum spanning tree.","title":"23.1-4"},{"location":"Chap23/23.1/#231-5","text":"Let $e$ be a maximum-weight edge on some cycle of connected graph $G = (V, E)$. Prove that there is a minimum spanning tree of $G' = (V, E - \\{e\\})$ that is also a minimum spanning tree of $G$. That is, there is a minimum spanning tree of $G$ that does not include $e$. Let $A$ be any cut that causes some vertices in the cycle on once side of the cut, and some vertices in the cycle on the other. For any of these cuts, we know that the edge $e$ is not a light edge for this cut. Since all the other cuts won't have the edge $e$ crossing it, we won't have that the edge is light for any of those cuts either. This means that we have that e is not safe.","title":"23.1-5"},{"location":"Chap23/23.1/#231-6","text":"Show that a graph has a unique minimum spanning tree if, for every cut of the graph, there is a unique light edge crossing the cut. Show that the converse is not true by giving a counterexample. Suppose that for every cut of $G$, there is a unique light edge crossing the cut. Let us consider two distinct minimum spanning trees, $T$ and $T'$, of $G$. Because $T$ and $T'$are distinct, $T$ contains some edge $(u, v)$ that is not in $T'$. If we remove $(u, v)$ from $T$, then $T$ becomes disconnected, resulting in a cut $(S, V - S)$. The edge $(u, v)$ is a light edge crossing the cut $(S, V - S)$ (by Exercise 23.1-3) and, by our assumption, it's the only light edge crossing this cut. Because $(u, v)$ is the only light edge crossing $(S, V - S)$ and $(u, v)$ is not in $T'$, each edge in $T'$ that crosses $(S, V - S)$ must have weight strictly greater than w$(u, v)$. As in the proof of Theorem 23.1, we can identify the unique edge $(x, y)$ in $T'$ that crosses $(S, V - S)$ and lies on the cycle that results if we add $(u, v)$ to $T'$. By our assumption, we know that $w(u, v) < w(x, y)$. Then, we can then remove $(x, y)$ from $T'$ and replace it by $(u, v)$, giving a spanning tree with weight strictly less than $w(T')$. Thus, $T'$ was not a minimum spanning tree, contradicting the assumption that the graph had two unique minimum spanning trees. Here's a counterexample for the converse: Here, the graph is its own minimum spanning tree, and so the minimum spanning tree is unique. Consider the cut $(\\{x\\}, \\{y, z\\})$. Both of the edges $(x, y)$ and $(x, z)$ are light edges crossing the cut, and they are both light edges.","title":"23.1-6"},{"location":"Chap23/23.1/#231-7","text":"Argue that if all edge weights of a graph are positive, then any subset of edges that connects all vertices and has minimum total weight must be a tree. Give an example to show that the same conclusion does not follow if we allow some weights to be nonpositive. First, we show that the subset of edges of minimum total weight that connects all the vertices is a tree. To see this, suppose not, that it had a cycle. This would mean that removing any of the edges in this cycle would mean that the remaining edges would still connect all the vertices, but would have a total weight that's less by the weight of the edge that was removed. This would contradict the minimality of the total weight of the subset of vertices. Since the subset of edges forms a tree, and has minimal total weight, it must also be a minimum spanning tree. To see that this conclusion is not true if we allow negative edge weights, we provide a construction. Consider the graph $K_3$ with all edge weights equal to $-1$. The only minimum weight set of edges that connects the graph has total weight $-3$, and consists of all the edges. This is clearly not a $\\text{MST}$ because it is not a tree, which can be easily seen because it has one more edge than a tree on three vertices should have. Any $\\text{MST}$ of this weighted graph must have weight that is at least $-2$.","title":"23.1-7"},{"location":"Chap23/23.1/#231-8","text":"Let $T$ be a minimum spanning tree of a graph $G$, and let $L$ be the sorted list of the edge weights of $T$. Show that for any other minimum spanning tree $T'$ of $G$, the list $L$ is also the sorted list of edge weights of $T'$. Suppose that $L'$ is another sorted list of edge weights of a minimum spanning tree. If $L' \\ne L$, there must be a first edge $(u, v)$ in $T$ or $T'$ which is of smaller weight than the corresponding edge $(x, y)$ in the other set. Without loss of generality, assume $(u, v)$ is in $T$. Let $C$ be the graph obtained by adding $(u, v)$ to $L'$. Then we must have introduced a cycle. If there exists an edge on that cycle which is of larger weight than $(u, v)$, we can remove it to obtain a tree $C'$ of weight strictly smaller than the weight of $T'$, contradicting the fact that $T'$ is a minimum spanning tree. Thus, every edge on the cycle must be of lesser or equal weight than $(u, v)$. Suppose that every edge is of strictly smaller weight. Remove $(u, v)$ from $T$ to disconnect it into two components. There must exist some edge besides $(u, v)$ on the cycle which would connect these, and since it has smaller weight we can use that edge instead to create a spanning tree with less weight than $T$, a contradiction. Thus, some edge on the cycle has the same weight as $(u, v)$. Replace that edge by $(u, v)$. The corresponding lists $L$ and $L'$ remain unchanged since we have swapped out an edge of equal weight, but the number of edges which $T$ and $T'$ have in common has increased by $1$. If we continue in this way, eventually they must have every edge in common, contradicting the fact that their edge weights differ somewhere. Therefore all minimum spanning trees have the same sorted list of edge weights.","title":"23.1-8"},{"location":"Chap23/23.1/#231-9","text":"Let $T$ be a minimum spanning tree of a graph $G = (V, E)$, and let $V'$ be a subset of $V$. Let $T'$ be the subgraph of $T$ induced by $V'$, and let $G'$ be the subgraph of $G$ induced by $V'$. Show that if $T'$ is connected, then $T'$ is a minimum spanning tree of $G'$. Suppose that there was some cheaper spanning tree than $T'$. That is, we have that there is some $T''$ so that $w(T'') < w(T')$. Then, let $S$ be the edges in $T$ but not in $T'$. We can then construct a minimum spanning tree of $G$ by considering $S \\cup T''$. This is a spanning tree since $S \\cup T'$ is, and $T''$ makes all the vertices in $V'$ connected just like $T'$ does. However, we have that $$w(S \\cup T'') = w(S) + w(T'') < w(S) + w(T') = w(S \\cup T') = w(T).$$ This means that we just found a spanning tree that has a lower total weight than a minimum spanning tree. This is a contradiction, and so our assumption that there was a spanning tree of $V'$ cheaper than $T'$ must be false.","title":"23.1-9"},{"location":"Chap23/23.1/#231-10","text":"Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges in $T$. Show that $T$ is still a minimum spanning tree for $G$. More formally, let $T$ be a minimum spanning tree for $G$ with edge weights given by weight function $w$. Choose one edge $(x, y) \\in T$ and a positive number $k$, and define the weight function $w'$ by $$ w'(u, v) = \\begin{cases} w(u, v) & \\text{ if }(u, v) \\ne (x, y), \\\\ w(x, y) - k & \\text{ if }(u, v) = (x, y). \\end{cases} $$ Show that $T$ is a minimum spanning tree for $G$ with edge weights given by $w'$. Let $x(T) = \\sum_{(x, y) \\in T} w(x, y)$. We have $w'(T) = w(T) - k$. Consider any other spanning tree $T'$, so that $w(T) \\le w(T')$. If $(x, y) \\ne T'$, then $w'(T') = w(T') \\ge w(T) > w'(T)$. If $(x, y) \\in T'$, then $w'(T') = w(T') - k \\ge w(T) - k = w'(T)$. Either way, $w'(T) \\le w'(T')$, and so $T$ is a minimum spanning tree for weight function $w'$.","title":"23.1-10"},{"location":"Chap23/23.1/#231-11-star","text":"Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges not in $T$. Give an algorithm for finding the minimum spanning tree in the modified graph. If we were to add in this newly decreased edge to the given tree, we would be creating a cycle. Then, if we were to remove any one of the edges along this cycle, we would still have a spanning tree. This means that we look at all the weights along this cycle formed by adding in the decreased edge, and remove the edge in the cycle of maximum weight. This does exactly what we want since we could only possibly want to add in the single decreased edge, and then, from there we change the graph back to a tree in the way that makes its total weight minimized.","title":"23.1-11 $\\star$"},{"location":"Chap23/23.2/","text":"23.2-1 Kruskal's algorithm can return different spanning trees for the same input graph $G$, depending on how it breaks ties when the edges are sorted into order. Show that for each minimum spanning tree $T$ of $G$, there is a way to sort the edges of $G$ in Kruskal's algorithm so that the algorithm returns $T$. Suppose that we wanted to pick $T$ as our minimum spanning tree. Then, to obtain this tree with Kruskal's algorithm, we will order the edges first by their weight, but then will resolve ties in edge weights by picking an edge first if it is contained in the minimum spanning tree, and treating all the edges that aren't in $T$ as being slightly larger, even though they have the same actual weight. With this ordering, we will still be finding a tree of the same weight as all the minimum spanning trees $w(T)$. However, since we prioritize the edges in $T$, we have that we will pick them over any other edges that may be in other minimum spanning trees. 23.2-2 Suppose that we represent the graph $G = (V, E)$ as an adjacency matrix. Give a simple implementation of Prim's algorithm for this case that runs in $O(V^2)$ time. At each step of the algorithm we will add an edge from a vertex in the tree created so far to a vertex not in the tree, such that this edge has minimum weight. Thus, it will be useful to know, for each vertex not in the tree, the edge from that vertex to some vertex in the tree of minimal weight. We will store this information in an array $A$, where $A[u] = (v, w)$ if $w$ is the weight of $(u, v)$ and is minimal among the weights of edges from $u$ to some vertex $v$ in the tree built so far. We'll use $A[u].1$ to access $v$ and $A[u].2$ to access $w$. 1 2 3 4 5 6 7 8 9 10 11 12 13 PRIM - ADJ ( G , w , r ) initialize A with every entry = ( NIL , \u221e ) T = { r } for i = 1 to V if Adj [ r , i ] != 0 A [ i ] = ( r , w ( r , i )) for each u in V - T k = min ( A [ i ] .2 ) T = T \u222a { k } k . \u03c0 = A [ k ] .1 for i = 1 to V if Adf [ k , i ] != 0 and Adj [ k , i ] < A [ i ] .2 A [ i ] = ( k , Adj [ k , i ]) 23.2-3 For a sparse graph $G = (V, E)$, where $|E| = \\Theta(V)$, is the implementation of Prim's algorithm with a Fibonacci heap asymptotically faster than the binary-heap implementation? What about for a dense graph, where $|E| = \\Theta(V^2)$? How must the sizes $|E|$ and $|V|$ be related for the Fibonacci-heap implementation to be asymptotically faster than the binary-heap implementation? Prim's algorithm implemented with a Binary heap has runtime $O((V + E)\\lg V)$, which in the sparse case, is just $O(V\\lg V)$. The implementation with Fibonacci heaps is $$O(E + V\\lg V) = O(V + V\\lg V) = O(V \\lg V).$$ In the sparse case, the two algorithms have the same asymptotic runtimes. In the dense case. The binary heap implementation has a runtime of $$O((V + E)\\lg V) = O((V + V^2)\\lg V) = O(V^2\\lg V).$$ The Fibonacci heap implementation has a runtime of $$O(E + V\\lg V) = O(V^2 + V\\lg V) = O(V^2).$$ So, in the dense case, we have that the Fibonacci heap implementation is asymptotically faster. The Fibonacci heap implementation will be asymptotically faster so long as $E = \\omega(V)$. Suppose that we have some function that grows more quickly than linear, say $f$, and $E = f(V)$. The binary heap implementation will have runtime of $$O((V + E)\\lg V) = O((V + f(V))\\lg V) = O(f(V)\\lg V).$$ However, we have that the runtime of the Fibonacci heap implementation will have runtime of $$O(E + V\\lg V) = O(f(V) + V\\lg V).$$ This runtime is either $O(f(V))$ or $O(V\\lg V)$ depending on if $f(V)$ grows more or less quickly than $V\\lg V$ respectively. In either case, we have that the runtime is faster than $O(f(V)\\lg V)$. 23.2-4 Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Kruskal's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? We know that Kruskal's algorithm takes $O(V)$ time for initialization, $O(E\\lg E)$ time to sort the edges, and $O(E\\alpha(V))$ time for the disjoint-set operations, for a total running time of $O(V + E\\lg E + E\\alpha(V)) = O(E\\lg E)$. If we knew that all of the edge weights in the graph were integers in the range from $1$ to $|V|$, then we could sort the edges in $O(V + E)$ time using counting sort. Since the graph is connected, $V = O(E)$, and so the sorting time is reduced to $O(E)$. This would yield a total running time of $O(V + E + E\\alpha(V)) = O(E\\alpha(V))$, again since $V = O(E)$, and since $E = O(E\\alpha(V))$. The time to process the edges, not the time to sort them, is now the dominant term. Knowledge about the weights won't help speed up any other part of the algorithm, since nothing besides the sort uses the weight values. If the edge weights were integers in the range from $1$ to $W$ for some constant $W$, then we could again use counting sort to sort the edges more quickly. This time, sorting would take $O(E + W) = O(E)$ time, since $W$ is a constant. As in the first part, we get a total running time of $O(E\\alpha(V))$. 23.2-5 Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Prim's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? The time taken by Prim's algorithm is determined by the speed of the queue operations. With the queue implemented as a Fibonacci heap, it takes $O(E + V\\lg V)$ time. Since the keys in the priority queue are edge weights, it might be possible to implement the queue even more efficiently when there are restrictions on the possible edge weights. We can improve the running time of Prim's algorithm if $W$ is a constant by implementing the queue as an array $Q[0..W + 1]$ (using the $W + 1$ slot for $\\text{key} = \\infty$), where each slot holds a doubly linked list of vertices with that weight as their key. Then $\\text{EXTRACT-MIN}$ takes only $O(W) = O(1)$ time (just scan for the first nonempty slot), and $\\text{DECREASE-KEY}$ takes only $O(1)$ time (just remove the vertex from the list it's in and insert it at the front of the list indexed by the new key). This gives a total running time of $O(E)$, which is the best possible asymptotic time (since $\\Omega(E)$ edges must be processed). However, if the range of edge weights is $1$ to $|V|$, then $\\text{EXTRACT-MIN}$ takes $\\Theta(V)$ time with this data structure. So the total time spent doing $\\text{EXTRACT-MIN}$ is $\\Theta(V^2)$, slowing the algorithm to $\\Theta(E + V^2) = \\Theta(V^2)$. In this case, it is better to keep the Fibonacci-heap priority queue, which gave the $\\Theta(E + V\\lg V)$ time. Other data structures yield better running times: van Emde Boas trees (see Chapter 20) give an upper bound of $O(E + V\\lg\\lg V)$ time for Prim's algorithm. A redistributive heap (used in the single-source shortest-paths algorithm of Ahuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the chapter notes for Chapter 24) gives an upper bound of $O(E + V \\sqrt{\\lg V})$ for Prim's algorithm. 23.2-6 $\\star$ Suppose that the edge weights in a graph are uniformly distributed over the halfopen interval $[0, 1)$. Which algorithm, Kruskal's or Prim's, can you make run faster? For input drawn from a uniform distribution I would use bucket sort with Kruskal's algorithm, for expected linear time sorting of edges by weight. This would achieve expected runtime $O(E\\alpha(V))$. 23.2-7 $\\star$ Suppose that a graph $G$ has a minimum spanning tree already computed. How quickly can we update the minimum spanning tree if we add a new vertex and incident edges to $G$? We start with the following lemma. Lemma Let $T$ be a minimum spanning tree of $G = (V, E)$, and consider a graph $G' = (V', E')$ for which $G$ is a subgraph, i.e., $V \\subseteq V'$ and $E \\subseteq E'$. Let $\\overline T = E - T$ be the edges of $G$ that are not in $T$. Then there is a minimum spanning tree of $G'$ that includes no edges in $\\overline T$. Proof By Exercise 23.2-1, there is a way to order the edges of $E$ so that Kruskal's algorithm, when run on $G$, produces the minimum spanning tree $T$. We will show that Kruskal's algorithm, run on $G'$, produces a minimum spanning tree $T'$ that includes no edges in $\\overline T$. We assume that the edges in $E$ are considered in the same relative order when Kruskal's algorithm is run on $G$ and on $G'$. We first state and prove the following claim. Claim For any pair of vertices $u, v \\in V$, if these vertices are in the same set after Kruskal's algorithm run on $G$ considers any edge $(x, y) \\in E$, then they are in the same set after Kruskal's algorithm run on $G'$ considers $(x, y)$. Proof of claim Let us order the edges of $E$ by nondecreasing weight as $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_k, y_k) \\rangle$, where $k = |E|$. This sequence gives the order in which the edges of $E$ are considered by Kruskal's algorithm, whether it is run on $G$ or on $G'$. We will use induction, with the inductive hypothesis that if $u$ and $v$ are in the same set after Kruskal's algorithm run on $G$ considers an edge $(x_i, y_i)$, then they are in the same set after Kruskal's algorithm run on $G'$ considers the same edge. We use induction on $i$. Basis: For the basis, $i = 0$. Kruskal's algorithm run on $G$ has not considered any edges, and so all vertices are in different sets. The inductive hypothesis holds trivially. Inductive step: We assume that any vertices that are in the same set after Kruskal's algorithm run on $G$ has considered edges $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_{i - 1}, y_{i - 1}) \\rangle$ are in the same set after Kruskal's algorithm run on $G'$ has considered the same edges. When Kruskal's algorithm runs on $G'$, after it considers $(x_{i - 1}, y_{i - 1})$, it may consider some edges in $E' - E$ before considering $(x_i, y_i)$. The edges in $E' - E$ may cause $\\text{UNION}$ operations to occur, but sets are never divided. Hence, any vertices that are in the same set after Kruskal's algorithm run on $G'$ considers $(x_{i - 1}, y_{i - 1})$ are still in the same set when $(x_i, y_i)$ is considered. When Kruskal's algorithm run on $G$ considers $(x_i, y_i)$, either $x_i$ and $y_i$ are found to be in the same set or they are not. If Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in the same set, then no $\\text{UNION}$ operation occurs. The sets of vertices remain the same, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$. If Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in different sets, then the operation $\\text{UNION}(x_i, y_i)$ will occur. Kruskal's algorithm run on $G'$ will find that either $x_i$ and $y_i$ are in the same set or they are not. By the inductive hypothesis, when edge $(x_i, y_i)$ is considered, all vertices in $x_i$'s set when Kruskal's algorithm runs on $G$ are in $x_i$'s set when Kruskal's algorithm runs on $G'$, and the same holds for $y_i$. Regardless of whether Kruskal's algorithm run on $G'$ finds $x_i$ and $y_i$ to already be in the same set, their sets are united after considering $(x_i, y_i)$, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$. (#claim) With the claim in hand, we suppose that some edge $(u, v) \\in \\overline T$ is placed into $T'$. That means that Kruskal's algorithm run on $G$ found $u$ and $v$ to be in the same set (since $(u, v) \\in \\overline T$ ) but Kruskal's algorithm run on $G'$ found $u$ and $v$ to be in different sets (since $(u, v)$ is placed into $T'$). This fact contradicts the claim, and we conclude that no edge in $\\overline T$ is placed into $T'$. Thus, by running Kruskal's algorithm on $G$ and $G'$, we demonstrate that there exists a minimum spanning tree of $G'$ that includes no edges in $\\overline T$. (#lemma) We use this lemma as follows. Let $G' = (V', E')$ be the graph $G = (V, E)$ with the one new vertex and its incident edges added. Suppose that we have a minimum spanning tree $T$ for $G$. We compute a minimum spanning tree for $G'$ by creating the graph $G'' = (V', E'')$, where $E''$ consists of the edges of $T$ and the edges in $E' - E$ (i.e., the edges added to $G$ that made $G'$), and then finding a minimum spanning tree $T'$ for $G''$. By the lemma, there is a minimum spanning tree for $G'$ that includes no edges of $E - T$. In other words, $G'$ has a minimum spanning tree that includes only edges in $T$ and $E' - E$ ; these edges comprise exactly the set $E''$. Thus, the the minimum spanning tree $T'$ of $G''$ is also a minimum spanning tree of $G'$. Even though the proof of the lemma uses Kruskal's algorithm, we are not required to use this algorithm to find $T'$. We can find a minimum spanning tree by any means we choose. Let us use Prim's algorithm with a Fibonacci-heap priority queue. Since $|V'| = |V| + 1$ and $|E''| \\le 2|V| - 1$ ($E''$ contains the $|V| - 1$ edges of $T$ and at most $|V|$ edges in $E' - E$ ), it takes $O(V)$ time to construct $G''$, and the run of Prim's algorithm with a Fibonacci-heap priority queue takes time $O(E'' + V'\\lg V) = O(V\\lg V)$. Thus, if we are given a minimum spanning tree of $G$, we can compute a minimum spanning tree of $G'$ in $O(V\\lg V)$ time. 23.2-8 Professor Borden proposes a new divide-and-conquer algorithm for computing minimum spanning trees, which goes as follows. Given a graph $G = (V, E)$, partition the set $V$ of vertices into two sets $V_1$ and $V_2$ such that $|V_1|$ and $|V_2|$ differ by at most $1$. Let $E_1$ be the set of edges that are incident only on vertices in $V_1$, and let $E_2$ be the set of edges that are incident only on vertices in $V_2$. Recursively solve a minimum-spanning-tree problem on each of the two subgraphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$. Finally, select the minimum-weight edge in $E$ that crosses the cut $(V_1, V_2)$, and use this edge to unite the resulting two minimum spanning trees into a single spanning tree. Either argue that the algorithm correctly computes a minimum spanning tree of $G$, or provide an example for which the algorithm fails. The algorithm fails. Suppose $E = \\{(u, v), (u, w), (v, w)\\}$, the weight of $(u, v)$ and $(u, w)$ is $1$, and the weight of $(v, w)$ is $1000$, partition the set into two sets $V_1 = \\{u\\}$ and $V_2 = \\{v, w\\}$.","title":"23.2 The algorithms of Kruskal and Prim"},{"location":"Chap23/23.2/#232-1","text":"Kruskal's algorithm can return different spanning trees for the same input graph $G$, depending on how it breaks ties when the edges are sorted into order. Show that for each minimum spanning tree $T$ of $G$, there is a way to sort the edges of $G$ in Kruskal's algorithm so that the algorithm returns $T$. Suppose that we wanted to pick $T$ as our minimum spanning tree. Then, to obtain this tree with Kruskal's algorithm, we will order the edges first by their weight, but then will resolve ties in edge weights by picking an edge first if it is contained in the minimum spanning tree, and treating all the edges that aren't in $T$ as being slightly larger, even though they have the same actual weight. With this ordering, we will still be finding a tree of the same weight as all the minimum spanning trees $w(T)$. However, since we prioritize the edges in $T$, we have that we will pick them over any other edges that may be in other minimum spanning trees.","title":"23.2-1"},{"location":"Chap23/23.2/#232-2","text":"Suppose that we represent the graph $G = (V, E)$ as an adjacency matrix. Give a simple implementation of Prim's algorithm for this case that runs in $O(V^2)$ time. At each step of the algorithm we will add an edge from a vertex in the tree created so far to a vertex not in the tree, such that this edge has minimum weight. Thus, it will be useful to know, for each vertex not in the tree, the edge from that vertex to some vertex in the tree of minimal weight. We will store this information in an array $A$, where $A[u] = (v, w)$ if $w$ is the weight of $(u, v)$ and is minimal among the weights of edges from $u$ to some vertex $v$ in the tree built so far. We'll use $A[u].1$ to access $v$ and $A[u].2$ to access $w$. 1 2 3 4 5 6 7 8 9 10 11 12 13 PRIM - ADJ ( G , w , r ) initialize A with every entry = ( NIL , \u221e ) T = { r } for i = 1 to V if Adj [ r , i ] != 0 A [ i ] = ( r , w ( r , i )) for each u in V - T k = min ( A [ i ] .2 ) T = T \u222a { k } k . \u03c0 = A [ k ] .1 for i = 1 to V if Adf [ k , i ] != 0 and Adj [ k , i ] < A [ i ] .2 A [ i ] = ( k , Adj [ k , i ])","title":"23.2-2"},{"location":"Chap23/23.2/#232-3","text":"For a sparse graph $G = (V, E)$, where $|E| = \\Theta(V)$, is the implementation of Prim's algorithm with a Fibonacci heap asymptotically faster than the binary-heap implementation? What about for a dense graph, where $|E| = \\Theta(V^2)$? How must the sizes $|E|$ and $|V|$ be related for the Fibonacci-heap implementation to be asymptotically faster than the binary-heap implementation? Prim's algorithm implemented with a Binary heap has runtime $O((V + E)\\lg V)$, which in the sparse case, is just $O(V\\lg V)$. The implementation with Fibonacci heaps is $$O(E + V\\lg V) = O(V + V\\lg V) = O(V \\lg V).$$ In the sparse case, the two algorithms have the same asymptotic runtimes. In the dense case. The binary heap implementation has a runtime of $$O((V + E)\\lg V) = O((V + V^2)\\lg V) = O(V^2\\lg V).$$ The Fibonacci heap implementation has a runtime of $$O(E + V\\lg V) = O(V^2 + V\\lg V) = O(V^2).$$ So, in the dense case, we have that the Fibonacci heap implementation is asymptotically faster. The Fibonacci heap implementation will be asymptotically faster so long as $E = \\omega(V)$. Suppose that we have some function that grows more quickly than linear, say $f$, and $E = f(V)$. The binary heap implementation will have runtime of $$O((V + E)\\lg V) = O((V + f(V))\\lg V) = O(f(V)\\lg V).$$ However, we have that the runtime of the Fibonacci heap implementation will have runtime of $$O(E + V\\lg V) = O(f(V) + V\\lg V).$$ This runtime is either $O(f(V))$ or $O(V\\lg V)$ depending on if $f(V)$ grows more or less quickly than $V\\lg V$ respectively. In either case, we have that the runtime is faster than $O(f(V)\\lg V)$.","title":"23.2-3"},{"location":"Chap23/23.2/#232-4","text":"Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Kruskal's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? We know that Kruskal's algorithm takes $O(V)$ time for initialization, $O(E\\lg E)$ time to sort the edges, and $O(E\\alpha(V))$ time for the disjoint-set operations, for a total running time of $O(V + E\\lg E + E\\alpha(V)) = O(E\\lg E)$. If we knew that all of the edge weights in the graph were integers in the range from $1$ to $|V|$, then we could sort the edges in $O(V + E)$ time using counting sort. Since the graph is connected, $V = O(E)$, and so the sorting time is reduced to $O(E)$. This would yield a total running time of $O(V + E + E\\alpha(V)) = O(E\\alpha(V))$, again since $V = O(E)$, and since $E = O(E\\alpha(V))$. The time to process the edges, not the time to sort them, is now the dominant term. Knowledge about the weights won't help speed up any other part of the algorithm, since nothing besides the sort uses the weight values. If the edge weights were integers in the range from $1$ to $W$ for some constant $W$, then we could again use counting sort to sort the edges more quickly. This time, sorting would take $O(E + W) = O(E)$ time, since $W$ is a constant. As in the first part, we get a total running time of $O(E\\alpha(V))$.","title":"23.2-4"},{"location":"Chap23/23.2/#232-5","text":"Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Prim's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? The time taken by Prim's algorithm is determined by the speed of the queue operations. With the queue implemented as a Fibonacci heap, it takes $O(E + V\\lg V)$ time. Since the keys in the priority queue are edge weights, it might be possible to implement the queue even more efficiently when there are restrictions on the possible edge weights. We can improve the running time of Prim's algorithm if $W$ is a constant by implementing the queue as an array $Q[0..W + 1]$ (using the $W + 1$ slot for $\\text{key} = \\infty$), where each slot holds a doubly linked list of vertices with that weight as their key. Then $\\text{EXTRACT-MIN}$ takes only $O(W) = O(1)$ time (just scan for the first nonempty slot), and $\\text{DECREASE-KEY}$ takes only $O(1)$ time (just remove the vertex from the list it's in and insert it at the front of the list indexed by the new key). This gives a total running time of $O(E)$, which is the best possible asymptotic time (since $\\Omega(E)$ edges must be processed). However, if the range of edge weights is $1$ to $|V|$, then $\\text{EXTRACT-MIN}$ takes $\\Theta(V)$ time with this data structure. So the total time spent doing $\\text{EXTRACT-MIN}$ is $\\Theta(V^2)$, slowing the algorithm to $\\Theta(E + V^2) = \\Theta(V^2)$. In this case, it is better to keep the Fibonacci-heap priority queue, which gave the $\\Theta(E + V\\lg V)$ time. Other data structures yield better running times: van Emde Boas trees (see Chapter 20) give an upper bound of $O(E + V\\lg\\lg V)$ time for Prim's algorithm. A redistributive heap (used in the single-source shortest-paths algorithm of Ahuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the chapter notes for Chapter 24) gives an upper bound of $O(E + V \\sqrt{\\lg V})$ for Prim's algorithm.","title":"23.2-5"},{"location":"Chap23/23.2/#232-6-star","text":"Suppose that the edge weights in a graph are uniformly distributed over the halfopen interval $[0, 1)$. Which algorithm, Kruskal's or Prim's, can you make run faster? For input drawn from a uniform distribution I would use bucket sort with Kruskal's algorithm, for expected linear time sorting of edges by weight. This would achieve expected runtime $O(E\\alpha(V))$.","title":"23.2-6 $\\star$"},{"location":"Chap23/23.2/#232-7-star","text":"Suppose that a graph $G$ has a minimum spanning tree already computed. How quickly can we update the minimum spanning tree if we add a new vertex and incident edges to $G$? We start with the following lemma. Lemma Let $T$ be a minimum spanning tree of $G = (V, E)$, and consider a graph $G' = (V', E')$ for which $G$ is a subgraph, i.e., $V \\subseteq V'$ and $E \\subseteq E'$. Let $\\overline T = E - T$ be the edges of $G$ that are not in $T$. Then there is a minimum spanning tree of $G'$ that includes no edges in $\\overline T$. Proof By Exercise 23.2-1, there is a way to order the edges of $E$ so that Kruskal's algorithm, when run on $G$, produces the minimum spanning tree $T$. We will show that Kruskal's algorithm, run on $G'$, produces a minimum spanning tree $T'$ that includes no edges in $\\overline T$. We assume that the edges in $E$ are considered in the same relative order when Kruskal's algorithm is run on $G$ and on $G'$. We first state and prove the following claim. Claim For any pair of vertices $u, v \\in V$, if these vertices are in the same set after Kruskal's algorithm run on $G$ considers any edge $(x, y) \\in E$, then they are in the same set after Kruskal's algorithm run on $G'$ considers $(x, y)$. Proof of claim Let us order the edges of $E$ by nondecreasing weight as $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_k, y_k) \\rangle$, where $k = |E|$. This sequence gives the order in which the edges of $E$ are considered by Kruskal's algorithm, whether it is run on $G$ or on $G'$. We will use induction, with the inductive hypothesis that if $u$ and $v$ are in the same set after Kruskal's algorithm run on $G$ considers an edge $(x_i, y_i)$, then they are in the same set after Kruskal's algorithm run on $G'$ considers the same edge. We use induction on $i$. Basis: For the basis, $i = 0$. Kruskal's algorithm run on $G$ has not considered any edges, and so all vertices are in different sets. The inductive hypothesis holds trivially. Inductive step: We assume that any vertices that are in the same set after Kruskal's algorithm run on $G$ has considered edges $\\langle (x_1, y_1), (x_2, y_2), \\ldots, (x_{i - 1}, y_{i - 1}) \\rangle$ are in the same set after Kruskal's algorithm run on $G'$ has considered the same edges. When Kruskal's algorithm runs on $G'$, after it considers $(x_{i - 1}, y_{i - 1})$, it may consider some edges in $E' - E$ before considering $(x_i, y_i)$. The edges in $E' - E$ may cause $\\text{UNION}$ operations to occur, but sets are never divided. Hence, any vertices that are in the same set after Kruskal's algorithm run on $G'$ considers $(x_{i - 1}, y_{i - 1})$ are still in the same set when $(x_i, y_i)$ is considered. When Kruskal's algorithm run on $G$ considers $(x_i, y_i)$, either $x_i$ and $y_i$ are found to be in the same set or they are not. If Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in the same set, then no $\\text{UNION}$ operation occurs. The sets of vertices remain the same, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$. If Kruskal's algorithm run on $G$ finds $x_i$ and $y_i$ to be in different sets, then the operation $\\text{UNION}(x_i, y_i)$ will occur. Kruskal's algorithm run on $G'$ will find that either $x_i$ and $y_i$ are in the same set or they are not. By the inductive hypothesis, when edge $(x_i, y_i)$ is considered, all vertices in $x_i$'s set when Kruskal's algorithm runs on $G$ are in $x_i$'s set when Kruskal's algorithm runs on $G'$, and the same holds for $y_i$. Regardless of whether Kruskal's algorithm run on $G'$ finds $x_i$ and $y_i$ to already be in the same set, their sets are united after considering $(x_i, y_i)$, and so the inductive hypothesis continues to hold after considering $(x_i, y_i)$. (#claim) With the claim in hand, we suppose that some edge $(u, v) \\in \\overline T$ is placed into $T'$. That means that Kruskal's algorithm run on $G$ found $u$ and $v$ to be in the same set (since $(u, v) \\in \\overline T$ ) but Kruskal's algorithm run on $G'$ found $u$ and $v$ to be in different sets (since $(u, v)$ is placed into $T'$). This fact contradicts the claim, and we conclude that no edge in $\\overline T$ is placed into $T'$. Thus, by running Kruskal's algorithm on $G$ and $G'$, we demonstrate that there exists a minimum spanning tree of $G'$ that includes no edges in $\\overline T$. (#lemma) We use this lemma as follows. Let $G' = (V', E')$ be the graph $G = (V, E)$ with the one new vertex and its incident edges added. Suppose that we have a minimum spanning tree $T$ for $G$. We compute a minimum spanning tree for $G'$ by creating the graph $G'' = (V', E'')$, where $E''$ consists of the edges of $T$ and the edges in $E' - E$ (i.e., the edges added to $G$ that made $G'$), and then finding a minimum spanning tree $T'$ for $G''$. By the lemma, there is a minimum spanning tree for $G'$ that includes no edges of $E - T$. In other words, $G'$ has a minimum spanning tree that includes only edges in $T$ and $E' - E$ ; these edges comprise exactly the set $E''$. Thus, the the minimum spanning tree $T'$ of $G''$ is also a minimum spanning tree of $G'$. Even though the proof of the lemma uses Kruskal's algorithm, we are not required to use this algorithm to find $T'$. We can find a minimum spanning tree by any means we choose. Let us use Prim's algorithm with a Fibonacci-heap priority queue. Since $|V'| = |V| + 1$ and $|E''| \\le 2|V| - 1$ ($E''$ contains the $|V| - 1$ edges of $T$ and at most $|V|$ edges in $E' - E$ ), it takes $O(V)$ time to construct $G''$, and the run of Prim's algorithm with a Fibonacci-heap priority queue takes time $O(E'' + V'\\lg V) = O(V\\lg V)$. Thus, if we are given a minimum spanning tree of $G$, we can compute a minimum spanning tree of $G'$ in $O(V\\lg V)$ time.","title":"23.2-7 $\\star$"},{"location":"Chap23/23.2/#232-8","text":"Professor Borden proposes a new divide-and-conquer algorithm for computing minimum spanning trees, which goes as follows. Given a graph $G = (V, E)$, partition the set $V$ of vertices into two sets $V_1$ and $V_2$ such that $|V_1|$ and $|V_2|$ differ by at most $1$. Let $E_1$ be the set of edges that are incident only on vertices in $V_1$, and let $E_2$ be the set of edges that are incident only on vertices in $V_2$. Recursively solve a minimum-spanning-tree problem on each of the two subgraphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$. Finally, select the minimum-weight edge in $E$ that crosses the cut $(V_1, V_2)$, and use this edge to unite the resulting two minimum spanning trees into a single spanning tree. Either argue that the algorithm correctly computes a minimum spanning tree of $G$, or provide an example for which the algorithm fails. The algorithm fails. Suppose $E = \\{(u, v), (u, w), (v, w)\\}$, the weight of $(u, v)$ and $(u, w)$ is $1$, and the weight of $(v, w)$ is $1000$, partition the set into two sets $V_1 = \\{u\\}$ and $V_2 = \\{v, w\\}$.","title":"23.2-8"},{"location":"Chap23/Problems/23-1/","text":"Let $G = (V, E)$ be an undirected, connected graph whose weight function is $w: E \\rightarrow \\mathbb R$, and suppose that $|E| \\ge |V|$ and all edge weights are distinct. We define a second-best minimum spanning tree as follows. Let $\\mathcal T$ be the set of all spanning trees of $G$, and let $T'$ be a minimum spanning tree of $G$. Then a second-best minimum spanning tree is a spanning tree $T$ such that $W(T) = \\min_{T'' \\in \\mathcal T - \\{T'\\}} \\{w(T'')\\}$. a. Show that the minimum spanning tree is unique, but that the second-best minimum spanning tree need not be unique. b. Let $T$ be the minimum spanning tree of $G$. Prove that $G$ contains edges $(u, v) \\in T$ and $(x, y) \\notin T$ such that $T - \\{(u, v)\\} \\cup \\{(x, y)\\}$ is a second-best minimum spanning tree of $G$. c. Let $T$ be a spanning tree of $G$ and, for any two vertices $u, v \\in V$, let $max[u, v]$ denote an edge of maximum weight on the unique simple path between $u$ and $v$ in $T$. Describe an $O(V^2)$-time algorithm that, given $T$, computes $max[u, v]$ for all $u, v \\in V$. d. Give an efficient algorithm to compute the second-best minimum spanning tree of $G$. a. To see that the minimum spanning tree is unique, observe that since the graph is connected and all edge weights are distinct, then there is a unique light edge crossing every cut. By Exercise 23.1-6, the minimum spanning tree is unique. To see that the second-best minimum spanning tree need not be unique, here is a weighted, undirected graph with a unique minimum spanning tree of weight $7$ and two second-best minimum spanning trees of weight $8$: b. Since any spanning tree has exactly $|V| - 1$ edges, any second-best minimum spanning tree must have at least one edge that is not in the (best) minimum spanning tree. If a second-best minimum spanning tree has exactly one edge, say $(x, y)$, that is not in the minimum spanning tree, then it has the same set of edges as the minimum spanning tree, except that $(x, y)$ replaces some edge, say $(u, v)$, of the minimum spanning tree. In this case, $T' = T - \\{(u, v)\\} \\cup \\{(x, y)\\}$, as we wished to show. Thus, all we need to show is that by replacing two or more edges of the minimum spanning tree, we cannot obtain a second-best minimum spanning tree. Let $T$ be the minimum spanning tree of $G$, and suppose that there exists a second-best minimum spanning tree $T'$ that differs from $T$ by two or more edges. There are at least two edges in $T - T'$, and let $(u, v)$ be the edge in $T - T'$ with minimum weight. If we were to add $(u, v)$ to $T'$, we would get a cycle $c$. This cycle contains some edge $(x, y)$ in $T' - T$ (since otherwise, $T$ would contain a cycle). We claim that $w(x, y) > w(u, v)$. We prove this claim by contradiction, so let us assume that $w(x, y) < w(u, v)$. (Recall the assumption that edge weights are distinct, so that we do not have to concern ourselves with $w(x, y) = w(u, v)$.) If we add $(x, y)$ to $T$, we get a cycle $c'$, which contains some edge$(u', v')$ in $T - T'$ (since otherwise, $T'$ would contain a cycle). Therefore, the set of edges $T'' = T - \\{(u', v')\\} \\cup \\{(x, y)\\}$ forms a spanning tree, and we must also have $w(u', v') < w(x, y)$, since otherwise $T''$ would be a spanning tree with weight less than $w(T)$. Thus, $w(u', v') < w(x, y) < w(u, v)$, which contradicts our choice of $(u, v)$ as the edge in $T - T'$ of minimum weight. Since the edges $(u, v)$ and $(x, y)$ would be on a common cycle $c$ if we were to add $(u, v)$ to $T'$, the set of edges $T' - \\{(x, y)\\} \\cup \\{(u, v)\\}$ is a spanning tree, and its weight is less than $w(T')$. Moreover, it differs from $T$ (because it differs from $T'$ by only one edge). Thus, we have formed a spanning tree whose weight is less than $w(T')$ but is not $T$. Hence, $T'$ was not a second-best minimum spanning tree. c. We can fill in $max[u, v]$ for all $u, v \\in V$ in $O(V^2)$ time by simply doing a search from each vertex $u$, having restricted the edges visited to those of the spanning tree $T$. It doesn't matter what kind of search we do: breadth-first, depth-first, or any other kind. We'll give pseudocode for both breadth-first and depth-first approaches. Each approach differs from the pseudocode given in Chapter 22 in that we don't need to compute $d$ or $f$ values, and we'll use the $max$ table itself to record whether a vertex has been visited in a given search. In particular, $max[u, v] = \\text{NIL}$ if and only if $u = v$ or we have not yet visited vertex $v$ in a search from vertex $u$. Note also that since we're visiting via edges in a spanning tree of an undirected graph, we are guaranteed that the search from each vertex $u$\u2014whether breadth-first or depth-first\u2014will visit all vertices. There will be no need to \"restart\" the search as is done in the $\\text{DFS}$ procedure of Section 22.3. Our pseudocode assumes that the adjacency list of each vertex consists only of edges in the spanning tree $T$. Here's the breadth-first search approach: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 BFS - FILL - MAX ( G , T , w ) let max be a new table with an entry max [ u , v ] for each u , v \u2208 G . V for each vertex u \u2208 G . V for each vertex v \u2208 G . V max [ u , v ] = NIL Q = \u00d8 ENQUEUE ( Q , u ) while Q != \u00d8 x = DEQUEUE ( Q ) for each v \u2208 G . Adj [ x ] if max [ u , v ] == NIL and v != u if x == u or w ( x , v ) > max [ u , x ] max [ u , v ] = ( x , v ) else max [ u , v ] = max [ u , x ] ENQUEUE ( Q , v ) return max Here's the depth-first search approach: 1 2 3 4 5 6 7 DFS - FILL - MAX ( G , T , w ) let max be a new table with an entry max [ u , v ] for each u , v \u2208 G . V for each vertex u \u2208 G . V for each vertex v \u2208 G . V max [ u , v ] = NIL DFS - FILL - MAX - VISIT ( G , u , u , max ) return max 1 2 3 4 5 6 7 DFS - FILL - MAX - VISIT ( G , u , x , max ) for each vertex v \u2208 G . Adj [ x ] if max [ u , v ] == NIL and v != u if x == u or w ( x , v ) > max [ u , x ] max [ u , v ] = ( x , v ) else max [ u , v ] = max [ u , x ] DFS - FILL - MAX - VISIT ( G , u , v , max ) For either approach, we are filling in $|V|$ rows of the $max$ table. Since the number of edges in the spanning tree is $|V| - 1$, each row takes $O(V)$ time to fill in. Thus, the total time to fill in the $max$ table is $O(V^2)$. d. In part (b), we established that we can find a second-best minimum spanning tree by replacing just one edge of the minimum spanning tree $T$ by some edge $(u, v)$ not in $T$. As we know, if we create spanning tree $T'$ by replacing edge $(x, y) \\in T$ by edge $(u, v) \\ne T$, then $w(T') = w(T) - w(x, y) + w(u, v)$. For a given edge $(u, v)$, the edge $(x, y) \\in T$ that minimizes $w(T')$ is the edge of maximum weight on the unique path between $u$ and $v$ in $T$. If we have already computed the $max$ table from part (c) based on $T$, then the identity of this edge is precisely what is stored in $max[u, v]$. All we have to do is determine an edge $(u, v) \\ne T$ for which $w(max[u, v]) - w(u, v)$ is minimum. Thus, our algorithm to find a second-best minimum spanning tree goes as follows: Compute the minimum spanning tree $T$. Time: $O(E + V\\lg V)$, using Prim's algorithm with a Fibonacci-heap implementation of the priority queue. Since $|E| < |V|^2$, this running time is $O(V^2)$. Given the minimum spanning tree $T$, compute the $max$ table, as in part (c). Time: $O(V^2)$. Find an edge $(u, v) \\ne T$ that minimizes $w(max[u, v]) - w(u, v)$. Time: $O(E)$, which is $O(V^2)$. Having found an edge $(u, v)$ in step 3, return $T' = T - \\{max[u, v]\\} \\cup \\{(u, v)\\}$ as a second-best minimum spanning tree. The total time is $O(V^2)$.","title":"23-1 Second-best minimum spanning tree"},{"location":"Chap23/Problems/23-2/","text":"For a very sparse connected graph $G = (V, E)$, we can further improve upon the $O(E + V\\lg V)$ running time of Prim's algorithm with Fibonacci heaps by preprocessing $G$ to decrease the number of vertices before running Prim's algorithm. In particular, we choose, for each vertex $u$, the minimum-weight edge $(u, v)$ incident on $u$, and we put $(u, v)$ into the minimum spanning tree under construction. We then contract all chosen edges (see Section B.4). Rather than contracting these edges one at a time, we first identify sets of vertices that are united into the same new vertex. Then we create the graph that would have resulted from contracting these edges one at a time, but we do so by \"renaming\" edges according to the sets into which their endpoints were placed. Several edges from the original graph may be renamed the same as each other. In such a case, only one edge results, and its weight is the minimum of the weights of the corresponding original edges. Initially, we set the minimum spanning tree $T$ being constructed to be empty, and for each edge $(u, v) \\in E$, we initialize the attributes $(u, v).orig = (u, v)$ and $(u, v).c = w(u, v)$. We use the $orig$ attribute to reference the edge from the initial graph that is associated with an edge in the contracted graph. The $c$ attribute holds the weight of an edge, and as edges are contracted, we update it according to the above scheme for choosing edge weights. The procedure $\\text{MST-REDUCE}$ takes inputs $G$ and $T$, and it returns a contracted graph $G'$ with updated attributes $orig'$ and $c'$. The procedure also accumulates edges of $G$ into the minimum spanning tree $T$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 MST - REDUCE ( G , T ) for each v \u2208 G . V v . mark = false MAKE - SET ( v ) for each u \u2208 G . V if u . mark == false choose v \u2208 G . Adj [ u ] such that ( u , v ). c is minimized UNION ( u , v ) T = T \u222a {( u , v ). orig } u . mark = v . mark = true G ' . V = { FIND - SET ( v ) : v \u2208 G . V } G ' . E = \u00d8 for each ( x , y ) \u2208 G . E u = FIND - SET ( x ) v = FIND - SET ( y ) if ( u , v ) \u2209 G ' . E G ' . E = G ' . E \u222a {( u , v )} ( u , v ). orig ' = ( x , y ). orig ( u , v ). c ' = ( x , y ). c else if ( x , y ). c < ( u , v ). c ' ( u , v ). orig ' = ( x , y ). orig ( u , v ). c ' = ( x , y ). c construct adjacency lists G ' . Adj for G ' return G ' and T a. Let $T$ be the set of edges returned by $\\text{MST-REDUCE}$, and let $A$ be the minimum spanning tree of the graph $G'$ formed by the call $\\text{MST-PRIM}(G', c', r)$, where $c'$ is the weight attribute on the edges of $G'.E$ and $r$ is any vertex in $G'.V$. Prove that $T \\cup \\{(x,y).orig': (x, y) \\in A\\}$ is a minimum spanning tree of $G$. b. Argue that $|G'.V| \\le |V| / 2$. c. Show how to implement $\\text{MST-REDUCE}$ so that it runs in $O(E)$ time. ($\\textit{Hint:}$ Use simple data structures.) d. Suppose that we run $k$ phases of $\\text{MST-REDUCE}$, using the output $G'$ produced by one phase as the input $G$ to the next phase and accumulating edges in $T$. Argue that the overall running time of the $k$ phases is $O(kE)$. e. Suppose that after running $k$ phases of $\\text{MST-REDUCE}$, as in part (d), we run Prim's algorithm by calling $\\text{MST-PRIM}(G', c', r)$, where $G'$, with weight attribute $c'$, is returned by the last phase and $r$ is any vertex in $G'.V$. Show how to pick $k$ so that the overall running time is $O(E\\lg\\lg V)$. Argue that your choice of $k$ minimizes the overall asymptotic running time. f. For what values of $|E|$ (in terms of $|V|$) does Prim's algorithm with preprocessing asymptotically beat Prim's algorithm without preprocessing? a. We'll show that the edges added at each step are safe. Consider an unmarked vertex $u$. Set $S = \\{u\\}$ and let $A$ be the set of edges in the tree so far. Then the cut respects $A$, and the next edge we add is a light edge, so it is safe for $A$. Thus, every edge in $T$ before we run Prim's algorithm is safe for $T$. Any edge that Prim's would normally add at this point would have to connect two of the trees already created, and it would be chosen as minimal. Moreover, we choose exactly one between any two trees. Thus, the fact that we only have the smallest edges available to us is not a problem. The resulting tree must be minimal. b. We argue by induction on the number of vertices in $G$. We'll assume that $|V| > 1$, since otherwise $\\text{MST-REDUCE}$ will encounter an error on line 6 because there is no way to choose $v$. Let $|V| = 2$. Since $G$ is connected, there must be an edge between $u$ and $v$, and it is trivially of minimum weight. They are joined, and $|G'.V| = 1 = |V| / 2$. Suppose the claim holds for $|V| = n$. Let $G$ be a connected graph on $n + 1$ vertices. Then $G'.V \\le n / 2$ prior to the final vertex $v$ being examined in the for-loop of line 4. If $v$ is marked then we're done, and if $v$ isn't marked then we'll connect it to some other vertex, which must be marked since $v$ is the last to be processed. Either way, $v$ can't contribute an additional vertex to $G'.V$. so $$|G'.V| \\le n / 2 \\le (n + 1) / 2.$$ c. Rather than using the disjoint set structures of chapter 21, we can simply use an array to keep track of which component a vertex is in. Let $A$ be an array of length $|V|$ such that $A[u] = v$ if $v = \\text{FIND-SET}(u)$. Then $\\text{FIND-SET}(u)$ can now be replaced with $A[u]$ and $\\text{UNION}(u, v)$ can be replaced by $A[v] = A[u]$. Since these operations run in constant time, the runtime is $O(E)$. d. The number of edges in the output is monotonically decreasing, so each call is $O(E)$. Thus, $k$ calls take $O(kE)$ time. e. The runtime of Prim's algorithm is $O(E + V\\lg V)$. Each time we run $\\text{MST-REDUCE}$, we cut the number of vertices at least in half. Thus, after $k$ calls, the number of vertices is at most $|V| / 2^k$. We need to minimize $$E + V / 2^k\\lg(V / 2^k) + kE = E + \\frac{V\\lg V}{2^k} - \\frac{Vk}{2^k} + kE$$ with respect to $k$. If we choose $k = \\lg\\lg V$ then we achieve the overall running time of $O(E\\lg\\lg V)$ as desired. To see that this value of $k$ minimizes, note that the $\\frac{Vk}{2^k}$ term is always less than the $kE$ term since $E \\ge V$. As $k$ decreases, the contribution of $kE$ decreases, and the contribution of $\\frac{V\\lg V}{2^k}$ increases. Thus, we need to find the value of $k$ which makes them approximately equal in the worst case, when $E = V$. To do this, we set $\\frac{\\lg V}{2^k} = k$. Solving this exactly would involve the Lambert W function, but the nicest elementary function which gets close is $k = \\lg\\lg V$. f. We simply set up the inequality $$E\\lg\\lg V < E + V\\lg V$$ to find that we need $$E < \\frac{V\\lg V}{\\lg\\lg V-1} = O(\\frac{V\\lg V}{\\lg\\lg V}).$$","title":"23-2 Minimum spanning tree in sparse graphs"},{"location":"Chap23/Problems/23-3/","text":"A bottleneck spanning tree $T$ of an undirected graph $G$ is a spanning tree of $G$ whose largest edge weight is minimum over all spanning trees of $G$. We say that the value of the bottleneck spanning tree is the weight of the maximum-weight edge in $T$. a. Argue that a minimum spanning tree is a bottleneck spanning tree. Part (a) shows that finding a bottleneck spanning tree is no harder than finding a minimum spanning tree. In the remaining parts, we will show how to find a bottleneck spanning tree in linear time. b. Give a linear-time algorithm that given a graph $G$ and an integer $b$, determines whether the value of the bottleneck spanning tree is at most $b$. c. Use your algorithm for part (b) as a subroutine in a linear-time algorithm for the bottleneck-spanning-tree problem. ($\\textit{Hint:}$ You may want to use a subroutine that contracts sets of edges, as in the $\\text{MST-REDUCE}$ procedure described in Problem 23-2.) a. To see that every minimum spanning tree is also a bottleneck spanning tree. Suppose that $T$ is a minimum spanning tree. Suppose there is some edge in it $(u, v)$ that has a weight that's greater than the weight of the bottleneck spanning tree. Then, let $V_1$ be the subset of vertices of $V$ that are reachable from $u$ in $T$, without going though $v$. Define $V_2$ symmetrically. Then, consider the cut that separates $V_1$ from $V_2$. The only edge that we could add across this cut is the one of minimum weight, so we know that there are no edge across this cut of weight less than $w(u, v)$. However, we have that there is a bottleneck spanning tree with less than that weight. This is a contradiction because a bottleneck spanning tree, since it is a spanning tree, must have an edge across this cut. b. To do this, we first process the entire graph, and remove any edges that have weight greater than $b$. If the remaining graph is selected, we can just arbitrarily select any tree in it, and it will be a bottleneck spanning tree of weight at most $b$. Testing connectivity of a graph can be done in linear time by running a breadth first search and then making sure that no vertices remain white at the end. c. Write down all of the edge weights of vertices. Use the algorithm from section 9.3 to find the median of this list of numbers in time $O(E)$. Then, run the procedure from part b with this median value as the one that you are testing for there to be a bottleneck spanning tree with weight at most. Then there are two cases: First, we could have that there is a bottleneck spanning tree with weight at most this median. Then just throw the edges with weight more than the median, and repeat the procedure on this new graph with half the edges. Second, we could have that there is no bottleneck spanning tree with at most that weight. Then, we should run the procedure from problem 23-2 to contract all of the edges that have weight at most this median weight. This takes time $O(E\\lg\\lg V)$ and then we are left solving the problem on a graph that now has half the vertices.","title":"23-3 Bottleneck spanning tree"},{"location":"Chap23/Problems/23-4/","text":"In this problem, we give pseudocode for three different algorithms. Each one takes a connected graph and a weight function as input and returns a set of edges $T$. For each algorithm, either prove that $T$ is a minimum spanning tree or prove that $T$ is not a minimum spanning tree. Also describe the most efficient implementation of each algorithm, whether or not it computes a minimum spanning tree. a. 1 2 3 4 5 6 7 MAYBE - MST - A ( G , w ) sort the edges into nonincreasing order of edge weights w T = E for each edge e , taken in nonincreasing order by weight if T - { e } is a connected graph T = T - { e } return T b. 1 2 3 4 5 6 MAYBE - MST - B ( G , w ) T = \u00d8 for each edge e , taken in arbitrary order if T \u222a { e } has no cycles T = T \u222a { e } return T c. 1 2 3 4 5 6 7 8 MAYBE - MST - C ( G , w ) T = \u00d8 for each edge e , taken in arbitrary order T = T \u222a { e } if T has a cycle c let e ' be a maximum - weight edge on c T = T - { e } return T a. This does return an $\\text{MST}$. To see this, we'll show that we never remove an edge which must be part of a minimum spanning tree. If we remove $e$, then $e$ cannot be a bridge, which means that e lies on a simple cycle of the graph. Since we remove edges in nonincreasing order, the weight of every edge on the cycle must be less than or equal to that of $e$. By exercise 23.1-5, there is a minimum spanning tree on $G$ with edge $e$ removed. To implement this, we begin by sorting the edges in $O(E \\lg E)$ time. For each edge we need to check whether or not $T - {e}$ is connected, so we'll need to run a $\\text{DFS}$. Each one takes $O(V + E)$, so doing this for all edges takes $O(E(V + E))$. This dominates the running time, so the total time is $O(E^2)$. b. This doesn't return an $\\text{MST}$. To see this, let $G$ be the graph on 3 vertices $a$, $b$, and $c$. Let the eges be $(a, b)$, $(b, c)$, and $(c, a)$ with weights $3, 2$, and $1$ respectively. If the algorithm examines the edges in their order listed, it will take the two heaviest edges instead of the two lightest. An efficient implementation will use disjoint sets to keep track of connected components, as in $\\text{MST-REDUCE}$ in problem 23-2. Trying to union within the same component will create a cycle. Since we make $|V|$ calls to $\\text{MAKESET}$ and at most $3|E|$ calls to $\\text{FIND-SET}$ and $\\text{UNION}$, the runtime is $O(E\\alpha(V))$. c. This does return an $\\text{MST}$. To see this, we simply quote the result from exercise 23.1-5. The only edges we remove are the edges of maximum weight on some cycle, and there always exists a minimum spanning tree which doesn't include these edges. Moreover, if we remove an edge from every cycle then the resulting graph cannot have any cycles, so it must be a tree. To implement this, we use the approach taken in part (b), except now we also need to find the maximum weight edge on a cycle. For each edge which introduces a cycle we can perform a $\\text{DFS}$ to find the cycle and max weight edge. Since the tree at that time has at most one cycle, it has at most $|V|$ edges, so we can run $\\text{DFS}$ in $O(V)$. The runtime is thus $O(EV)$.","title":"23-4 Alternative minimum-spanning-tree algorithms"},{"location":"Chap24/24.1/","text":"24.1-1 Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using vertex $z$ as the source. In each pass, relax edges in the same order as in the figure, and show the $d$ and $\\pi$ values after each pass. Now, change the weight of edge $(z, x)$ to $4$ and run the algorithm again, using $s$ as the source. $$ \\begin{array}{c|ccccc} & s & t & x & y & z \\\\ \\hline d & 2 & 4 & 6 & 9 & 0 \\\\ \\pi & z & x & y & z & \\text{NIL} \\end{array} $$ $$ \\begin{array}{c|ccccc} & s & t & x & y & z \\\\ \\hline d & 0 & 0 & 2 & 7 & -2 \\\\ \\pi & \\text{NIL} & x & z & s & t \\end{array} $$ 24.1-2 Prove Corollary 24.3. Suppose there is a path from $s$ to $v$. Then there must be a shortest such path of length $\\delta(s, v)$. It must have finite length since it contains at most $|V| - 1$ edges and each edge has finite length. By Lemma 24.2, $v.d = \\delta(s, v) < \\infty$ upon termination. On the other hand, suppose $v.d < \\infty$ when $\\text{BELLMAN-FORD}$ terminates. Recall that $v.d$ is monotonically decreasing throughout the algorithm, and $\\text{RELAX}$ will update $v.d$ only if $u.d + w(u, v) < v.d$ for some $u$ adjacent to $v$. Moreover, we update $v.\\pi = u$ at this point, so $v$ has an ancestor in the predecessor subgraph. Since this is a tree rooted at $s$, there must be a path from $s$ to $v$ in this tree. Every edge in the tree is also an edge in $G$, so there is also a path in $G$ from $s$ to $v$. 24.1-3 Given a weighted, directed graph $G = (V, E)$ with no negative-weight cycles, let $m$ be the maximum over all vertices $v \\in V$ of the minimum number of edges in a shortest path from the source $s$ to $v$. (Here, the shortest path is by weight, not the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that allows it to terminate in $m + 1$ passes, even if $m$ is not known in advance. If the greatest number of edges on any shortest path from the source is $m$, then the path-relaxation property tells us that after $m$ iterations of $\\text{BELLMAN-FORD}$, every vertex $v$ has achieved its shortest-path weight in $v.d$. By the upper-bound property, after $m$ iterations, no $d$ values will ever change. Therefore, no $d$ values will change in the $(m + 1)$st iteration. Because we do not know $m$ in advance, we cannot make the algorithm iterate exactly $m$ times and then terminate. But if we just make the algorithm stop when nothing changes any more, it will stop after $m + 1$ iterations. 1 2 3 4 5 6 7 BELLMAN - FORD - ( M + 1 )( G , w , s ) INITIALIZE - SINGLE - SOURCE ( G , s ) changes = true while changes == true changes = false for each edge ( u , v ) \u2208 G . E RELAX - M ( u , v , w ) 1 2 3 4 5 RELAX - M ( u , v , w ) if v . d > u . d + w ( u , v ) v . d = u . d + w ( u , v ) v . \u03c0 = u changes = true The test for a negative-weight cycle (based on there being a $d$ value that would change if another relaxation step was done) has been removed above, because this version of the algorithm will never get out of the while loop unless all $d$ values stop changing. 24.1-4 Modify the Bellman-Ford algorithm so that it sets $v.d$ to $-\\infty$ for all vertices $v$ for which there is a negative-weight cycle on some path from the source to $v$. 1 2 3 4 5 6 7 8 9 10 11 BELLMAN - FORD ' ( G , w , s ) INITIALIZE - SINGLE - SOURCE ( G , s ) for i = 1 to | G . V | - 1 for each edge ( u , v ) \u2208 G . E RELAX ( u , v , w ) for each edge ( u , v ) \u2208 G . E if v . d > u . d + w ( u , v ) v . d = - \u221e for each vertex v \u2208 G . V if v . d = - \u221e FOLLOW - AND - MARK - PRED ( v ) 1 2 3 4 5 6 FOLLOW - AND - MARK - PRED ( v ) if v . \u03c0 != NIL and v . \u03c0 . d != - \u221e v . \u03c0 . d = - \u221e FOLLOW - AND - MARK - PRED ( v . \u03c0 ) else return 24.1-5 $\\star$ Let $G = (V, E)$ be a weighted, directed graph with weight function $w : E \\rightarrow \\mathbb R$. Give an $O(VE)$-time algorithm to find, for each vertex $v \\in V$, the value $\\delta^*(v) = \\min_{u \\in V} \\{\\delta(u, v)\\}$. 1 2 3 4 RELAX ( u , v , w ) if v . d > min ( w ( u , v ), w ( u , v ) + u . d ) v . d = min ( w ( u , v ), w ( u , v ) + u . d ) v . \u03c0 = u . \u03c0 24.1-6 $\\star$ Suppose that a weighted, directed graph $G = (V, E)$ has a negative-weight cycle. Give an efficient algorithm to list the vertices of one such cycle. Prove that your algorithm is correct. Based on exercise 24.1-4, $\\text{DFS}$ from a vertex $u$ that $u.d = -\\infty$, if the weight sum on the search path is negative and the next vertex is $\\text{BLACK}$, then the search path forms a negative-weight cycle.","title":"24.1 The Bellman-Ford algorithm"},{"location":"Chap24/24.1/#241-1","text":"Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using vertex $z$ as the source. In each pass, relax edges in the same order as in the figure, and show the $d$ and $\\pi$ values after each pass. Now, change the weight of edge $(z, x)$ to $4$ and run the algorithm again, using $s$ as the source. $$ \\begin{array}{c|ccccc} & s & t & x & y & z \\\\ \\hline d & 2 & 4 & 6 & 9 & 0 \\\\ \\pi & z & x & y & z & \\text{NIL} \\end{array} $$ $$ \\begin{array}{c|ccccc} & s & t & x & y & z \\\\ \\hline d & 0 & 0 & 2 & 7 & -2 \\\\ \\pi & \\text{NIL} & x & z & s & t \\end{array} $$","title":"24.1-1"},{"location":"Chap24/24.1/#241-2","text":"Prove Corollary 24.3. Suppose there is a path from $s$ to $v$. Then there must be a shortest such path of length $\\delta(s, v)$. It must have finite length since it contains at most $|V| - 1$ edges and each edge has finite length. By Lemma 24.2, $v.d = \\delta(s, v) < \\infty$ upon termination. On the other hand, suppose $v.d < \\infty$ when $\\text{BELLMAN-FORD}$ terminates. Recall that $v.d$ is monotonically decreasing throughout the algorithm, and $\\text{RELAX}$ will update $v.d$ only if $u.d + w(u, v) < v.d$ for some $u$ adjacent to $v$. Moreover, we update $v.\\pi = u$ at this point, so $v$ has an ancestor in the predecessor subgraph. Since this is a tree rooted at $s$, there must be a path from $s$ to $v$ in this tree. Every edge in the tree is also an edge in $G$, so there is also a path in $G$ from $s$ to $v$.","title":"24.1-2"},{"location":"Chap24/24.1/#241-3","text":"Given a weighted, directed graph $G = (V, E)$ with no negative-weight cycles, let $m$ be the maximum over all vertices $v \\in V$ of the minimum number of edges in a shortest path from the source $s$ to $v$. (Here, the shortest path is by weight, not the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that allows it to terminate in $m + 1$ passes, even if $m$ is not known in advance. If the greatest number of edges on any shortest path from the source is $m$, then the path-relaxation property tells us that after $m$ iterations of $\\text{BELLMAN-FORD}$, every vertex $v$ has achieved its shortest-path weight in $v.d$. By the upper-bound property, after $m$ iterations, no $d$ values will ever change. Therefore, no $d$ values will change in the $(m + 1)$st iteration. Because we do not know $m$ in advance, we cannot make the algorithm iterate exactly $m$ times and then terminate. But if we just make the algorithm stop when nothing changes any more, it will stop after $m + 1$ iterations. 1 2 3 4 5 6 7 BELLMAN - FORD - ( M + 1 )( G , w , s ) INITIALIZE - SINGLE - SOURCE ( G , s ) changes = true while changes == true changes = false for each edge ( u , v ) \u2208 G . E RELAX - M ( u , v , w ) 1 2 3 4 5 RELAX - M ( u , v , w ) if v . d > u . d + w ( u , v ) v . d = u . d + w ( u , v ) v . \u03c0 = u changes = true The test for a negative-weight cycle (based on there being a $d$ value that would change if another relaxation step was done) has been removed above, because this version of the algorithm will never get out of the while loop unless all $d$ values stop changing.","title":"24.1-3"},{"location":"Chap24/24.1/#241-4","text":"Modify the Bellman-Ford algorithm so that it sets $v.d$ to $-\\infty$ for all vertices $v$ for which there is a negative-weight cycle on some path from the source to $v$. 1 2 3 4 5 6 7 8 9 10 11 BELLMAN - FORD ' ( G , w , s ) INITIALIZE - SINGLE - SOURCE ( G , s ) for i = 1 to | G . V | - 1 for each edge ( u , v ) \u2208 G . E RELAX ( u , v , w ) for each edge ( u , v ) \u2208 G . E if v . d > u . d + w ( u , v ) v . d = - \u221e for each vertex v \u2208 G . V if v . d = - \u221e FOLLOW - AND - MARK - PRED ( v ) 1 2 3 4 5 6 FOLLOW - AND - MARK - PRED ( v ) if v . \u03c0 != NIL and v . \u03c0 . d != - \u221e v . \u03c0 . d = - \u221e FOLLOW - AND - MARK - PRED ( v . \u03c0 ) else return","title":"24.1-4"},{"location":"Chap24/24.1/#241-5-star","text":"Let $G = (V, E)$ be a weighted, directed graph with weight function $w : E \\rightarrow \\mathbb R$. Give an $O(VE)$-time algorithm to find, for each vertex $v \\in V$, the value $\\delta^*(v) = \\min_{u \\in V} \\{\\delta(u, v)\\}$. 1 2 3 4 RELAX ( u , v , w ) if v . d > min ( w ( u , v ), w ( u , v ) + u . d ) v . d = min ( w ( u , v ), w ( u , v ) + u . d ) v . \u03c0 = u . \u03c0","title":"24.1-5 $\\star$"},{"location":"Chap24/24.1/#241-6-star","text":"Suppose that a weighted, directed graph $G = (V, E)$ has a negative-weight cycle. Give an efficient algorithm to list the vertices of one such cycle. Prove that your algorithm is correct. Based on exercise 24.1-4, $\\text{DFS}$ from a vertex $u$ that $u.d = -\\infty$, if the weight sum on the search path is negative and the next vertex is $\\text{BLACK}$, then the search path forms a negative-weight cycle.","title":"24.1-6 $\\star$"},{"location":"Chap24/24.2/","text":"24.2-1 Run $\\text{DAG-SHORTEST-PATHS}$ on the directed graph of Figure 24.5, using vertex $r$ as the source. $d$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline 0 & \\infty & \\infty & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & 11 & \\infty & \\infty \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & s & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\end{array} $$ 24.2-2 Suppose we change line 3 of $\\text{DAG-SHORTEST-PATHS}$ to read 1 3 for the first | V | - 1 vertices , taken in topologically sorted order Show that the procedure would remain correct. When we reach vertex $v$, the last vertex in the topological sort, it must have $out\\text-degree$ $0$. Otherwise there would be an edge pointing from a later vertex to an earlier vertex in the ordering, a contradiction. Thus, the body of the for-loop of line 4 is never entered for this final vertex, so we may as well not consider it. 24.2-3 The PERT chart formulation given above is somewhat unnatural. In a more natural structure, vertices would represent jobs and edges would represent sequencing constraints; that is, edge $(u, v)$ would indicate that job $u$ must be performed before job $v$. We would then assign weights to vertices, not edges. Modify the $\\text{DAG-SHORTEST-PATHS}$ procedure so that it finds a longest path in a directed acyclic graph with weighted vertices in linear time. Instead of modifying the $\\text{DAG-SHORTEST-PATHS}$ procedure, we'll modify the structure of the graph so that we can run $\\text{DAG-SHORTEST-PATHS}$ on it. In fact, we'll give two ways to transform a PERT chart $G = (V, E)$ with weights on vertices to a PERT chart $G' = (V', E')$ with weights on edges. In each way, we'll have that $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$. We can then run on $G'$ the same algorithm to find a longest path through a dag as is given in Section 24.2 of the text. In the first way, we transform each vertex $v \\in V$ into two vertices $v'$ and $v''$ in $V'$. All edges in $E$ that enter $v$ will enter $v'$ in $E'$, and all edges in $E$ that leave $v$ will leave $v''$ in $E'$. In other words, if $(u, v) \\in E$, then $(u'', v') \\in E'$. All such edges have weight $0$. We also put edges $(v', v'')$ into $E'$ for all vertices $v \\in V$, and these edges are given the weight of the corresponding vertex $v$ in $G$. Thus, $|V'| = 2|V|$, $|E'| = |V| + |E|$, and the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$. In the second way, we leave vertices in $V$ alone, but we add one new source vertex $s$ to $V'$, so that $V' = V \\cup \\{s\\}$. All edges of $E$ are in $E'$, and $E'$ also includes an edge $(s, v)$ for every vertex $v \\in V$ that has $in\\text-degree$ $0$ in $G$. Thus, the only vertex with $in\\text-degree$ $0$ in $G'$ is the new source $s$. The weight of edge $(u, v) \\in E'$ is the weight of vertex $v$ in $G$. In other words, the weight of each entering edge in $G'$ is the weight of the vertex it enters in $G$. In effect, we have \"pushed back\" the weight of each vertex onto the edges that enter it. Here, $|V'| = |V| + 1$, $|E'| \\le |V| + |E|$ (since no more than $|V|$ vertices have $in\\text-degree$ $0$ in $G$), and again the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$. 24.2-4 Give an efficient algorithm to count the total number of paths in a directed acyclic graph. Analyze your algorithm. We will compute the total number of paths by counting the number of paths whose start point is at each vertex $v$, which will be stored in an attribute $v.paths$. Assume that initial we have $v.paths = 0$ for all $v \\in V$. Since all vertices adjacent to $u$ occur later in the topological sort and the final vertex has no neighbors, line 4 is well-defined. Topological sort takes $O(V + E)$ and the nested for-loops take $O(V + E)$ so the total runtime is $O(V + E)$. 1 2 3 4 5 PATHS ( G ) topologically sort the vertices of G for each vertex u , taken in reverse topologically sorted order for each v \u2208 G . Adj [ u ] u . paths = u . paths + 1 + v . paths","title":"24.2 Single-source shortest paths in directed acyclic graphs"},{"location":"Chap24/24.2/#242-1","text":"Run $\\text{DAG-SHORTEST-PATHS}$ on the directed graph of Figure 24.5, using vertex $r$ as the source. $d$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline 0 & \\infty & \\infty & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & 11 & \\infty & \\infty \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & s & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\end{array} $$","title":"24.2-1"},{"location":"Chap24/24.2/#242-2","text":"Suppose we change line 3 of $\\text{DAG-SHORTEST-PATHS}$ to read 1 3 for the first | V | - 1 vertices , taken in topologically sorted order Show that the procedure would remain correct. When we reach vertex $v$, the last vertex in the topological sort, it must have $out\\text-degree$ $0$. Otherwise there would be an edge pointing from a later vertex to an earlier vertex in the ordering, a contradiction. Thus, the body of the for-loop of line 4 is never entered for this final vertex, so we may as well not consider it.","title":"24.2-2"},{"location":"Chap24/24.2/#242-3","text":"The PERT chart formulation given above is somewhat unnatural. In a more natural structure, vertices would represent jobs and edges would represent sequencing constraints; that is, edge $(u, v)$ would indicate that job $u$ must be performed before job $v$. We would then assign weights to vertices, not edges. Modify the $\\text{DAG-SHORTEST-PATHS}$ procedure so that it finds a longest path in a directed acyclic graph with weighted vertices in linear time. Instead of modifying the $\\text{DAG-SHORTEST-PATHS}$ procedure, we'll modify the structure of the graph so that we can run $\\text{DAG-SHORTEST-PATHS}$ on it. In fact, we'll give two ways to transform a PERT chart $G = (V, E)$ with weights on vertices to a PERT chart $G' = (V', E')$ with weights on edges. In each way, we'll have that $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$. We can then run on $G'$ the same algorithm to find a longest path through a dag as is given in Section 24.2 of the text. In the first way, we transform each vertex $v \\in V$ into two vertices $v'$ and $v''$ in $V'$. All edges in $E$ that enter $v$ will enter $v'$ in $E'$, and all edges in $E$ that leave $v$ will leave $v''$ in $E'$. In other words, if $(u, v) \\in E$, then $(u'', v') \\in E'$. All such edges have weight $0$. We also put edges $(v', v'')$ into $E'$ for all vertices $v \\in V$, and these edges are given the weight of the corresponding vertex $v$ in $G$. Thus, $|V'| = 2|V|$, $|E'| = |V| + |E|$, and the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$. In the second way, we leave vertices in $V$ alone, but we add one new source vertex $s$ to $V'$, so that $V' = V \\cup \\{s\\}$. All edges of $E$ are in $E'$, and $E'$ also includes an edge $(s, v)$ for every vertex $v \\in V$ that has $in\\text-degree$ $0$ in $G$. Thus, the only vertex with $in\\text-degree$ $0$ in $G'$ is the new source $s$. The weight of edge $(u, v) \\in E'$ is the weight of vertex $v$ in $G$. In other words, the weight of each entering edge in $G'$ is the weight of the vertex it enters in $G$. In effect, we have \"pushed back\" the weight of each vertex onto the edges that enter it. Here, $|V'| = |V| + 1$, $|E'| \\le |V| + |E|$ (since no more than $|V|$ vertices have $in\\text-degree$ $0$ in $G$), and again the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$.","title":"24.2-3"},{"location":"Chap24/24.2/#242-4","text":"Give an efficient algorithm to count the total number of paths in a directed acyclic graph. Analyze your algorithm. We will compute the total number of paths by counting the number of paths whose start point is at each vertex $v$, which will be stored in an attribute $v.paths$. Assume that initial we have $v.paths = 0$ for all $v \\in V$. Since all vertices adjacent to $u$ occur later in the topological sort and the final vertex has no neighbors, line 4 is well-defined. Topological sort takes $O(V + E)$ and the nested for-loops take $O(V + E)$ so the total runtime is $O(V + E)$. 1 2 3 4 5 PATHS ( G ) topologically sort the vertices of G for each vertex u , taken in reverse topologically sorted order for each v \u2208 G . Adj [ u ] u . paths = u . paths + 1 + v . paths","title":"24.2-4"},{"location":"Chap24/24.3/","text":"24.3-1 Run Dijkstra's algorithm on the directed graph of Figure 24.2, first using vertex $s$ as the source and then using vertex $z$ as the source. In the style of Figure 24.6, show the $d$ and $\\pi$ values and the vertices in set $S$ after each iteration of the while loop. $s$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 0 & 3 & \\infty & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & s & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & s & t & s & \\text{NIL} \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\end{array} $$ $z$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 3 & \\infty & 7 & \\infty & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline z & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\end{array} $$ 24.3-2 Give a simple example of a directed graph with negative-weight edges for which Dijkstra's algorithm produces incorrect answers. Why doesn't the proof of Theorem 24.6 go through when negative-weight edges are allowed? Consider any graph with a negative cycle. $\\text{RELAX}$ is called a finite number of times but the distance to any vertex on the cycle is $-\\infty$, so Dijkstra's algorithm cannot possibly be correct here. The proof of theorem 24.6 doesn't go through because we can no longer guarantee that $$\\delta(s, y) \\le \\delta(s, u).$$ 24.3-3 Suppose we change line 4 of Dijkstra's algorithm to the following. 1 4 while | Q | > 1 This change causes the while loop to execute $|V| - 1$ times instead of $|V|$ times. Is this proposed algorithm correct? Yes, the algorithm still works. Let $u$ be the leftover vertex that does not get extracted from the priority queue $Q$. If $u$ is not reachable from $s$, then $u.d = \\delta(s, u) = \\infty$. If $u$ is reachable from $s$, then there is a shortest path $p = s \\leadsto x \\to u$. When the vertex $x$ was extracted, $x.d = \\delta(s, x)$ and then the edge $(x, u)$ was relaxed; thus, $u.d = \\delta(s, u)$. 24.3-4 Professor Gaedel has written a program that he claims implements Dijkstra's algorithm. The program produces $v.d$ and $v.\\pi$ for each vertex $v \\in V$. Give an $O(V + E)$-time algorithm to check the output of the professor's program. It should determine whether the $d$ and $\\pi$ attributes match those of some shortest-paths tree. You may assume that all edge weights are nonnegative. Verify that $s.d = 0$ and $s.\\pi = \\text{NIL}$ Verify that $v.d = v.\\pi.d + w(v.\\pi, v)$ for all $v \\notin s$. Verify that $v.d = \\infty$ if and only if $v.\\pi = \\text{NIL}$ for all $v \\notin s$. If any of the above verification tests fail, declare the output to be incorrect. Otherwise, run one pass of Bellman-Ford, i.e., relax each edge $(u, v) \\in E$ one time. If any values of $v.d$ change, then declare the output to be incorrect; otherwise, declare the output to be correct. 24.3-5 Professor Newman thinks that he has worked out a simpler proof of correctness for Dijkstra's algorithm. He claims that Dijkstra's algorithm relaxes the edges of every shortest path in the graph in the order in which they appear on the path, and therefore the path-relaxation property applies to every vertex reachable from the source. Show that the professor is mistaken by constructing a directed graph for which Dijkstra's algorithm could relax the edges of a shortest path out of order. Let the graph have vertices $s$, $x$, $y$, $z$ and edges $(s, x)$, $(x, y)$, $(y, z)$, $(s, y)$, and let every edge have weight $0$. Dijkstra's algorithm could relax edges in the order $(s, y)$, $(s, x)$, $(y, z)$, $(x, y)$. The graph has two shortest paths from $s$ to $z: \\langle s, x, y, z \\rangle$ and $\\langle s, y, z \\rangle$, both with weight $0$. The edges on the shortest path $\\langle s, x, y, z \\rangle$ are relaxed out of order, because $(x, y)$ is relaxed after $(y, z)$. 24.3-6 We are given a directed graph $G = (V, E)$ on which each edge $(u, v) \\in E$ has an associated value $r(u, v)$, which is a real number in the range $0 \\le r(u, v) \\le 1$ that represents the reliability of a communication channel from vertex $u$ to vertex $v$. We interpret $r(u, v)$ as the probability that the channel from $u$ to $v$ will not fail, and we assume that these probabilities are independent. Give an efficient algorithm to find the most reliable path between two given vertices. To find the most reliable path between $s$ and $t$, run Dijkstra's algorithm with edge weights $w(u, v) = -\\lg r(u, v)$ to find shortest paths from $s$ in $O(E + V\\lg V)$ time. The most reliable path is the shortest path from $s$ to $t$, and that path's reliability is the product of the reliabilities of its edges. Here's why this method works. Because the probabilities are independent, the probability that a path will not fail is the product of the probabilities that its edges will not fail. We want to find a path $s \\overset{p}{\\leadsto} t$ such that $\\prod_{(u, v) \\in p} r(u, v)$ is maximized. This is equivalent to maximizing $\\lg(\\prod_{(u, v) \\in p} r(u, v)) = \\sum_{(u, v) \\in p} \\lg r(u, v)$, which is in turn equivalent to minimizing $\\sum_{(u, v) \\in p} -\\lg r(u, v)$. (Note: $r(u, v)$ can be $0$, and $\\lg 0$ is undefined. So in this algorithm, define $\\lg 0 = -\\infty$.) Thus if we assign weights $w(u, v) = -\\lg r(u, v)$, we have a shortest-path problem. Since $\\lg 1 = 0$, $\\lg x < 0$ for $0 < x < 1$, and we have defined $\\lg 0 = -\\infty$, all the weights $w$ are nonnegative, and we can use Dijkstra's algorithm to find the shortest paths from $s$ in $O(E + V\\lg V)$ time. Alternative solution You can also work with the original probabilities by running a modified version of Dijkstra's algorithm that maximizes the product of reliabilities along a path instead of minimizing the sum of weights along a path. In Dijkstra's algorithm, use the reliabilities as edge weights and substitute max (and $\\text{EXTRACT-MAX}$) for min (and $\\text{EXTRACT-MIN}$) in relaxation and the queue, $\\cdot$ for $+$ in relaxation, $1$ (identity for $\\cdot$) for $0$ (identity for $+$) and $-\\infty$ (identity for min) for $\\infty$ (identity for max). For example, we would use the following instead of the usual $\\text{RELAX}$ procedure: 1 2 3 4 RELAX - RELIABILITY ( u , v , r ) if v . d < u . d * r ( u , v ) v . d = u . d * r ( u , v ) v . \u03c0 = u This algorithm is isomorphic to the one above: it performs the same operations except that it is working with the original probabilities instead of the transformed ones. 24.3-7 Let $G = (V, E)$ be a weighted, directed graph with positive weight function $w: E \\rightarrow \\{1, 2, \\ldots, W\\}$ for some positive integer $W$, and assume that no two vertices have the same shortest-path weights from source vertex $s$. Now suppose that we define an unweighted, directed graph $G' = (V \\cup V', E')$ by replacing each edge $(u, v) \\in E$ with $w(u, v)$ unit-weight edges in series. How many vertices does $G'$ have? Now suppose that we run a breadth-first search on $G'$. Show that the order in which the breadth-first search of $G'$ colors vertices in $V$ black is the same as the order in which Dijkstra's algorithm extracts the vertices of $V$ from the priority queue when it runs on $G$. $V + \\sum_{(u, v) \\in E} w(u, v) - E$. 24.3-8 Let $G = (V, E)$ be a weighted, directed graph with nonnegative weight function $w: E \\rightarrow \\{0, 1, \\ldots, W\\}$ for some nonnegative integer $W$. Modify Dijkstra's algorithm to compute the shortest paths from a given source vertex s in $O(WV + E)$ time. Observe that if a shortest-path estimate is not $\\infty$, then it's at most $(|V| - 1)W$. Why? In order to have $v.d < 1$, we must have relaxed an edge $(u, v)$ with $u.d < \\infty$. By induction, we can show that if we relax $(u, v)$, then $v.d$ is at most the number of edges on a path from $s$ to $v$ times the maximum edge weight. Since any acyclic path has at most $|V| - 1$ edges and the maximum edge weight is $W$, we see that $v.d \\le (|V| - 1)W$. Note also that $v.d$ must also be an integer, unless it is $\\infty$. We also observe that in Dijkstra's algorithm, the values returned by the $\\text{EXTRACT-MIN}$ calls are monotonically increasing over time. Why? After we do our initial $|V|$ $\\text{INSERT}$ operations, we never do another. The only other way that a key value can change is by a $\\text{DECREASE-KEY}$ operation. Since edge weights are nonnegative, when we relax an edge $(u, v)$, we have that $u.d \\le v.d$. Since $u$ is the minimum vertex that we just extracted, we know that any other vertex we extract later has a key value that is at least $u.d$. When keys are known to be integers in the range $0$ to $k$ and the key values extracted are monotonically increasing over time, we can implement a min-priority queue so that any sequence of $m$ $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$ operations takes $O(m + k)$ time. Here's how. We use an array, say $A[0..k]$, where $A[j]$ is a linked list of each element whose key is $j$. Think of $A[j]$ as a bucket for all elements with key $j$. We implement each bucket by a circular, doubly linked list with a sentinel, so that we can insert into or delete from each bucket in $O(1)$ time. We perform the min-priority queue operations as follows: $\\text{INSERT}$: To insert an element with key $j$, just insert it into the linked list in $A[j]$. Time: $O(1)$ per $\\text{INSERT}$. $\\text{EXTRACT-MIN}$: We maintain an index $min$ of the value of the smallest key extracted. Initially, $min$ is $0$. To find the smallest key, look in $A[min]$ and, if this list is nonempty, use any element in it, removing the element from the list and returning it to the caller. Otherwise, we rely on the monotonicity property and increment $min$ until we either find a list $A[min]$ that is nonempty (using any element in $A[min]$ as before) or we run off the end of the array $A$ (in which case the min-priority queue is empty). Since there are at most $m$ $\\text{INSERT}$ operations, there are at most $m$ elements in the min-priority queue. We increment $min$ at most $k$ times, and we remove and return some element at most $m$ times. Thus, the total time over all $\\text{EXTRACT-MIN}$ operations is $O(m + k)$. $\\text{DECREASE-KEY}$: To decrease the key of an element from $j$ to $i$, first check whether $i \\le j$, \ufb02agging an error if not. Otherwise, we remove the element from its list $A[j]$ in $O(1)$ time and insert it into the list $A[i]$ in $O(1)$ time. Time: $O(1)$ per $\\text{DECREASE-KEY}$. To apply this kind of min-priority queue to Dijkstra's algorithm, we need to let $k = (|V| - 1)W$, and we also need a separate list for keys with value $\\infty$. The number of operations $m$ is $O(V + E)$ (since there are $|V|$ $\\text{INSERT}$ and $|V|$ $\\text{EXTRACT-MIN}$ operations and at most $|E|$ $\\text{DECREASE-KEY}$ operations), and so the total time is $O(V + E + VW) = O(VW + E)$. 24.3-9 Modify your algorithm from Exercise 24.3-8 to run in $O((V + E) \\lg W)$ time. ($\\textit{Hint:}$ How many distinct shortest-path estimates can there be in $V - S$ at any point in time?) First, observe that at any time, there are at most $W + 2$ distinct key values in the priority queue. Why? A key value is either $1$ or it is not. Consider what happens whenever a key value $v.d$ becomes finite. It must have occurred due to the relaxation of an edge $(u, v)$. At that time, $u$ was being placed into $S$, and $u.d \\le y.d$ for all vertices $y \\in V - S$. After relaxing edge $(u, v)$, we have $v.d \\le u.d + W$. Since any other vertex $y \\in V - S$ with $y.d < \\infty$ also had its estimate changed by a relaxation of some edge $x$ with $x.d \\le u.d$, we must have $y.d \\le x.d + W \\le u.d + W$. Thus, at the time that we are relaxing edges from a vertex $u$, we must have, for all vertices $v \\in V - S$, that $u.d \\le v.d \\le u.d + W$ or $v.d = \\infty$. Since shortest-path estimates are integer values (except for $\\infty$), at any given moment we have at most $W + 2$ different ones: $u.d$, $u.d + 1$, $u.d + 2$, $\\ldots$, $u.d + W$ and $\\infty$. Therefore, we can maintain the min-priorty queue as a binary min-heap in which each node points to a doubly linked list of all vertices with a given key value. There are at most $W + 2$ nodes in the heap, and so $\\text{EXTRACT-MIN}$ runs in $O(\\lg W)$ time. To perform $\\text{DECREASE-KEY}$, we need to be able to find the heap node corresponding to a given key in $O(\\lg W)$ time. We can do so in $O(1)$ time as follows. First, keep a pointer $inf$ to the node containing all the $\\infty$ keys. Second, maintain an array $loc[0..W]$, where $loc[i]$ points to the unique heap entry whose key value is congruent to $i(\\mod(W + 1))$. As keys move around in the heap, we can update this array in $O(1)$ time per movement. Alternatively, instead of using a binary min-heap, we could use a red-black tree. Now $\\text{INSERT}$, $\\text{DELETE}$, $\\text{MINIMUM}$, and $\\text{SEARCH}$\u2014from which we can construct the priority-queue operations\u2014each run in $O(\\lg W)$ time. 24.3-10 Suppose that we are given a weighted, directed graph $G = (V, E)$ in which edges that leave the source vertex $s$ may have negative weights, all other edge weights are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra's algorithm correctly finds shortest paths from $s$ in this graph. The proof of correctness, Theorem 24.6, goes through exactly as stated in the text. The key fact was that $\\delta(s, y) \\le \\delta(s, u)$. It is claimed that this holds because there are no negative edge weights, but in fact that is stronger than is needed. This always holds if $y$ occurs on a shortest path from $s$ to $u$ and $y \\ne s$ because all edges on the path from $y$ to $u$ have nonnegative weight. If any had negative weight, this would imply that we had \"gone back\" to an edge incident with $s$, which implies that a cycle is involved in the path, which would only be the case if it were a negative-weight cycle. However, these are still forbidden.","title":"24.3 Dijkstra's algorithm"},{"location":"Chap24/24.3/#243-1","text":"Run Dijkstra's algorithm on the directed graph of Figure 24.2, first using vertex $s$ as the source and then using vertex $z$ as the source. In the style of Figure 24.6, show the $d$ and $\\pi$ values and the vertices in set $S$ after each iteration of the while loop. $s$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 0 & 3 & \\infty & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & s & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & s & t & s & \\text{NIL} \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\end{array} $$ $z$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 3 & \\infty & 7 & \\infty & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline z & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\end{array} $$","title":"24.3-1"},{"location":"Chap24/24.3/#243-2","text":"Give a simple example of a directed graph with negative-weight edges for which Dijkstra's algorithm produces incorrect answers. Why doesn't the proof of Theorem 24.6 go through when negative-weight edges are allowed? Consider any graph with a negative cycle. $\\text{RELAX}$ is called a finite number of times but the distance to any vertex on the cycle is $-\\infty$, so Dijkstra's algorithm cannot possibly be correct here. The proof of theorem 24.6 doesn't go through because we can no longer guarantee that $$\\delta(s, y) \\le \\delta(s, u).$$","title":"24.3-2"},{"location":"Chap24/24.3/#243-3","text":"Suppose we change line 4 of Dijkstra's algorithm to the following. 1 4 while | Q | > 1 This change causes the while loop to execute $|V| - 1$ times instead of $|V|$ times. Is this proposed algorithm correct? Yes, the algorithm still works. Let $u$ be the leftover vertex that does not get extracted from the priority queue $Q$. If $u$ is not reachable from $s$, then $u.d = \\delta(s, u) = \\infty$. If $u$ is reachable from $s$, then there is a shortest path $p = s \\leadsto x \\to u$. When the vertex $x$ was extracted, $x.d = \\delta(s, x)$ and then the edge $(x, u)$ was relaxed; thus, $u.d = \\delta(s, u)$.","title":"24.3-3"},{"location":"Chap24/24.3/#243-4","text":"Professor Gaedel has written a program that he claims implements Dijkstra's algorithm. The program produces $v.d$ and $v.\\pi$ for each vertex $v \\in V$. Give an $O(V + E)$-time algorithm to check the output of the professor's program. It should determine whether the $d$ and $\\pi$ attributes match those of some shortest-paths tree. You may assume that all edge weights are nonnegative. Verify that $s.d = 0$ and $s.\\pi = \\text{NIL}$ Verify that $v.d = v.\\pi.d + w(v.\\pi, v)$ for all $v \\notin s$. Verify that $v.d = \\infty$ if and only if $v.\\pi = \\text{NIL}$ for all $v \\notin s$. If any of the above verification tests fail, declare the output to be incorrect. Otherwise, run one pass of Bellman-Ford, i.e., relax each edge $(u, v) \\in E$ one time. If any values of $v.d$ change, then declare the output to be incorrect; otherwise, declare the output to be correct.","title":"24.3-4"},{"location":"Chap24/24.3/#243-5","text":"Professor Newman thinks that he has worked out a simpler proof of correctness for Dijkstra's algorithm. He claims that Dijkstra's algorithm relaxes the edges of every shortest path in the graph in the order in which they appear on the path, and therefore the path-relaxation property applies to every vertex reachable from the source. Show that the professor is mistaken by constructing a directed graph for which Dijkstra's algorithm could relax the edges of a shortest path out of order. Let the graph have vertices $s$, $x$, $y$, $z$ and edges $(s, x)$, $(x, y)$, $(y, z)$, $(s, y)$, and let every edge have weight $0$. Dijkstra's algorithm could relax edges in the order $(s, y)$, $(s, x)$, $(y, z)$, $(x, y)$. The graph has two shortest paths from $s$ to $z: \\langle s, x, y, z \\rangle$ and $\\langle s, y, z \\rangle$, both with weight $0$. The edges on the shortest path $\\langle s, x, y, z \\rangle$ are relaxed out of order, because $(x, y)$ is relaxed after $(y, z)$.","title":"24.3-5"},{"location":"Chap24/24.3/#243-6","text":"We are given a directed graph $G = (V, E)$ on which each edge $(u, v) \\in E$ has an associated value $r(u, v)$, which is a real number in the range $0 \\le r(u, v) \\le 1$ that represents the reliability of a communication channel from vertex $u$ to vertex $v$. We interpret $r(u, v)$ as the probability that the channel from $u$ to $v$ will not fail, and we assume that these probabilities are independent. Give an efficient algorithm to find the most reliable path between two given vertices. To find the most reliable path between $s$ and $t$, run Dijkstra's algorithm with edge weights $w(u, v) = -\\lg r(u, v)$ to find shortest paths from $s$ in $O(E + V\\lg V)$ time. The most reliable path is the shortest path from $s$ to $t$, and that path's reliability is the product of the reliabilities of its edges. Here's why this method works. Because the probabilities are independent, the probability that a path will not fail is the product of the probabilities that its edges will not fail. We want to find a path $s \\overset{p}{\\leadsto} t$ such that $\\prod_{(u, v) \\in p} r(u, v)$ is maximized. This is equivalent to maximizing $\\lg(\\prod_{(u, v) \\in p} r(u, v)) = \\sum_{(u, v) \\in p} \\lg r(u, v)$, which is in turn equivalent to minimizing $\\sum_{(u, v) \\in p} -\\lg r(u, v)$. (Note: $r(u, v)$ can be $0$, and $\\lg 0$ is undefined. So in this algorithm, define $\\lg 0 = -\\infty$.) Thus if we assign weights $w(u, v) = -\\lg r(u, v)$, we have a shortest-path problem. Since $\\lg 1 = 0$, $\\lg x < 0$ for $0 < x < 1$, and we have defined $\\lg 0 = -\\infty$, all the weights $w$ are nonnegative, and we can use Dijkstra's algorithm to find the shortest paths from $s$ in $O(E + V\\lg V)$ time. Alternative solution You can also work with the original probabilities by running a modified version of Dijkstra's algorithm that maximizes the product of reliabilities along a path instead of minimizing the sum of weights along a path. In Dijkstra's algorithm, use the reliabilities as edge weights and substitute max (and $\\text{EXTRACT-MAX}$) for min (and $\\text{EXTRACT-MIN}$) in relaxation and the queue, $\\cdot$ for $+$ in relaxation, $1$ (identity for $\\cdot$) for $0$ (identity for $+$) and $-\\infty$ (identity for min) for $\\infty$ (identity for max). For example, we would use the following instead of the usual $\\text{RELAX}$ procedure: 1 2 3 4 RELAX - RELIABILITY ( u , v , r ) if v . d < u . d * r ( u , v ) v . d = u . d * r ( u , v ) v . \u03c0 = u This algorithm is isomorphic to the one above: it performs the same operations except that it is working with the original probabilities instead of the transformed ones.","title":"24.3-6"},{"location":"Chap24/24.3/#243-7","text":"Let $G = (V, E)$ be a weighted, directed graph with positive weight function $w: E \\rightarrow \\{1, 2, \\ldots, W\\}$ for some positive integer $W$, and assume that no two vertices have the same shortest-path weights from source vertex $s$. Now suppose that we define an unweighted, directed graph $G' = (V \\cup V', E')$ by replacing each edge $(u, v) \\in E$ with $w(u, v)$ unit-weight edges in series. How many vertices does $G'$ have? Now suppose that we run a breadth-first search on $G'$. Show that the order in which the breadth-first search of $G'$ colors vertices in $V$ black is the same as the order in which Dijkstra's algorithm extracts the vertices of $V$ from the priority queue when it runs on $G$. $V + \\sum_{(u, v) \\in E} w(u, v) - E$.","title":"24.3-7"},{"location":"Chap24/24.3/#243-8","text":"Let $G = (V, E)$ be a weighted, directed graph with nonnegative weight function $w: E \\rightarrow \\{0, 1, \\ldots, W\\}$ for some nonnegative integer $W$. Modify Dijkstra's algorithm to compute the shortest paths from a given source vertex s in $O(WV + E)$ time. Observe that if a shortest-path estimate is not $\\infty$, then it's at most $(|V| - 1)W$. Why? In order to have $v.d < 1$, we must have relaxed an edge $(u, v)$ with $u.d < \\infty$. By induction, we can show that if we relax $(u, v)$, then $v.d$ is at most the number of edges on a path from $s$ to $v$ times the maximum edge weight. Since any acyclic path has at most $|V| - 1$ edges and the maximum edge weight is $W$, we see that $v.d \\le (|V| - 1)W$. Note also that $v.d$ must also be an integer, unless it is $\\infty$. We also observe that in Dijkstra's algorithm, the values returned by the $\\text{EXTRACT-MIN}$ calls are monotonically increasing over time. Why? After we do our initial $|V|$ $\\text{INSERT}$ operations, we never do another. The only other way that a key value can change is by a $\\text{DECREASE-KEY}$ operation. Since edge weights are nonnegative, when we relax an edge $(u, v)$, we have that $u.d \\le v.d$. Since $u$ is the minimum vertex that we just extracted, we know that any other vertex we extract later has a key value that is at least $u.d$. When keys are known to be integers in the range $0$ to $k$ and the key values extracted are monotonically increasing over time, we can implement a min-priority queue so that any sequence of $m$ $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$ operations takes $O(m + k)$ time. Here's how. We use an array, say $A[0..k]$, where $A[j]$ is a linked list of each element whose key is $j$. Think of $A[j]$ as a bucket for all elements with key $j$. We implement each bucket by a circular, doubly linked list with a sentinel, so that we can insert into or delete from each bucket in $O(1)$ time. We perform the min-priority queue operations as follows: $\\text{INSERT}$: To insert an element with key $j$, just insert it into the linked list in $A[j]$. Time: $O(1)$ per $\\text{INSERT}$. $\\text{EXTRACT-MIN}$: We maintain an index $min$ of the value of the smallest key extracted. Initially, $min$ is $0$. To find the smallest key, look in $A[min]$ and, if this list is nonempty, use any element in it, removing the element from the list and returning it to the caller. Otherwise, we rely on the monotonicity property and increment $min$ until we either find a list $A[min]$ that is nonempty (using any element in $A[min]$ as before) or we run off the end of the array $A$ (in which case the min-priority queue is empty). Since there are at most $m$ $\\text{INSERT}$ operations, there are at most $m$ elements in the min-priority queue. We increment $min$ at most $k$ times, and we remove and return some element at most $m$ times. Thus, the total time over all $\\text{EXTRACT-MIN}$ operations is $O(m + k)$. $\\text{DECREASE-KEY}$: To decrease the key of an element from $j$ to $i$, first check whether $i \\le j$, \ufb02agging an error if not. Otherwise, we remove the element from its list $A[j]$ in $O(1)$ time and insert it into the list $A[i]$ in $O(1)$ time. Time: $O(1)$ per $\\text{DECREASE-KEY}$. To apply this kind of min-priority queue to Dijkstra's algorithm, we need to let $k = (|V| - 1)W$, and we also need a separate list for keys with value $\\infty$. The number of operations $m$ is $O(V + E)$ (since there are $|V|$ $\\text{INSERT}$ and $|V|$ $\\text{EXTRACT-MIN}$ operations and at most $|E|$ $\\text{DECREASE-KEY}$ operations), and so the total time is $O(V + E + VW) = O(VW + E)$.","title":"24.3-8"},{"location":"Chap24/24.3/#243-9","text":"Modify your algorithm from Exercise 24.3-8 to run in $O((V + E) \\lg W)$ time. ($\\textit{Hint:}$ How many distinct shortest-path estimates can there be in $V - S$ at any point in time?) First, observe that at any time, there are at most $W + 2$ distinct key values in the priority queue. Why? A key value is either $1$ or it is not. Consider what happens whenever a key value $v.d$ becomes finite. It must have occurred due to the relaxation of an edge $(u, v)$. At that time, $u$ was being placed into $S$, and $u.d \\le y.d$ for all vertices $y \\in V - S$. After relaxing edge $(u, v)$, we have $v.d \\le u.d + W$. Since any other vertex $y \\in V - S$ with $y.d < \\infty$ also had its estimate changed by a relaxation of some edge $x$ with $x.d \\le u.d$, we must have $y.d \\le x.d + W \\le u.d + W$. Thus, at the time that we are relaxing edges from a vertex $u$, we must have, for all vertices $v \\in V - S$, that $u.d \\le v.d \\le u.d + W$ or $v.d = \\infty$. Since shortest-path estimates are integer values (except for $\\infty$), at any given moment we have at most $W + 2$ different ones: $u.d$, $u.d + 1$, $u.d + 2$, $\\ldots$, $u.d + W$ and $\\infty$. Therefore, we can maintain the min-priorty queue as a binary min-heap in which each node points to a doubly linked list of all vertices with a given key value. There are at most $W + 2$ nodes in the heap, and so $\\text{EXTRACT-MIN}$ runs in $O(\\lg W)$ time. To perform $\\text{DECREASE-KEY}$, we need to be able to find the heap node corresponding to a given key in $O(\\lg W)$ time. We can do so in $O(1)$ time as follows. First, keep a pointer $inf$ to the node containing all the $\\infty$ keys. Second, maintain an array $loc[0..W]$, where $loc[i]$ points to the unique heap entry whose key value is congruent to $i(\\mod(W + 1))$. As keys move around in the heap, we can update this array in $O(1)$ time per movement. Alternatively, instead of using a binary min-heap, we could use a red-black tree. Now $\\text{INSERT}$, $\\text{DELETE}$, $\\text{MINIMUM}$, and $\\text{SEARCH}$\u2014from which we can construct the priority-queue operations\u2014each run in $O(\\lg W)$ time.","title":"24.3-9"},{"location":"Chap24/24.3/#243-10","text":"Suppose that we are given a weighted, directed graph $G = (V, E)$ in which edges that leave the source vertex $s$ may have negative weights, all other edge weights are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra's algorithm correctly finds shortest paths from $s$ in this graph. The proof of correctness, Theorem 24.6, goes through exactly as stated in the text. The key fact was that $\\delta(s, y) \\le \\delta(s, u)$. It is claimed that this holds because there are no negative edge weights, but in fact that is stronger than is needed. This always holds if $y$ occurs on a shortest path from $s$ to $u$ and $y \\ne s$ because all edges on the path from $y$ to $u$ have nonnegative weight. If any had negative weight, this would imply that we had \"gone back\" to an edge incident with $s$, which implies that a cycle is involved in the path, which would only be the case if it were a negative-weight cycle. However, these are still forbidden.","title":"24.3-10"},{"location":"Chap24/24.4/","text":"24.4-1 Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le & 1, \\\\ x_1 - x_4 & \\le & -4, \\\\ x_2 - x_3 & \\le & 2, \\\\ x_2 - x_5 & \\le & 7, \\\\ x_2 - x_6 & \\le & 5, \\\\ x_3 - x_6 & \\le & 10, \\\\ x_4 - x_2 & \\le & 2, \\\\ x_5 - x_1 & \\le & -1, \\\\ x_5 - x_4 & \\le & 3, \\\\ x_6 - x_3 & \\le & 8 \\end{aligned} $$ Our vertices of the constraint graph will be $$\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}.$$ The edges will be $$(v_0, v_1), (v_0, v_2), (v_0, v_3), (v_0, v_4), (v_0, v_5), (v_0, v_6), (v_2, v_1), (v_4, v_1), (v_3, v_2), (v_5, v_2), (v_6, v_2), (v_6, v_3),$$ with edge weights $$0, 0, 0, 0, 0, 0, 1, -4, 2, 7, 5, 10, 2, -1, 3, -8$$ respectively. Then, computing $$(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\delta(v_0, v_4), \\delta(v_0, v_5), \\delta(v_0, v_6)),$$ we get $$(-5, -3, 0, -1, -6, -8),$$ which is a feasible solution by Theorem 24.9. 24.4-2 Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le &4, \\\\ x_1 - x_5 & \\le &5, \\\\ x_2 - x_4 & \\le &-6, \\\\ x_3 - x_2 & \\le &1, \\\\ x_4 - x_1 & \\le &3, \\\\ x_4 - x_3 & \\le &5, \\\\ x_4 - x_5 & \\le &10, \\\\ x_5 - x_3 & \\le &-4, \\\\ x_5 - x_4 & \\le &-8. \\end{aligned} $$ There is no feasible solution because the constraint graph contains a negative-weight cycle: $(v_1, v_4, v_2, v_3, v_5, v_1)$ has weight $-1$. 24.4-3 Can any shortest-path weight from the new vertex $v_0$ in a constraint graph be positive? Explain. No, it cannot be positive. This is because for every vertex $v \\ne v_0$, there is an edge $(v_0, v)$ with weight zero. So, there is some path from the new vertex to every other of weight zero. Since $\\delta(v_0, v)$ is a minimum weight of all paths, it cannot be greater than the weight of this weight zero path that consists of a single edge. 24.4-4 Express the single-pair shortest-path problem as a linear program. Let $\\delta(u)$ be the shortest-path weight from $s$ to $u$. Then we want to find $\\delta(t)$. $\\delta$ must satisfy $$ \\begin{aligned} \\delta(s) & = 0 \\\\ \\delta(v) - \\delta(u) & \\le w(u, v) \\text{ for all $(u, v) \\in E$} & \\text{(Lemma 24.10)}, \\end{aligned} $$ where $w(u, v)$ is the weight of edge $(u, v)$. Thus $x_v = \\delta(v)$ is a solution to $$ \\begin{aligned} x_s & = 0 \\\\ x_v - x_u & \\le w(u, v). \\end{aligned} $$ To turn this into a set of inequalities of the required form, replace $x_s = 0$ by $x_s \\le 0$ and $-x_s \\le 0$ (i.e., $x_s \\ge$). The constraints are now $$ \\begin{aligned} x_s & \\le 0, \\\\ -x_s & \\le 0. \\\\ x_v - x_u & \\le w(u, v), \\end{aligned} $$ which still has $x_v = \\delta(v)$ as a solution. However, $\\delta$ isn't the only solution to this set of inequalities. (For example, if all edge weights are nonnegative, all $x_i = 0$ is a solution.) To force $x_t = \\delta(t)$ as required by the shortest-path problem, add the requirement to maximize (the objective function) $x_t$. This is correct because $\\max(x_t) \\ge \\delta(t)$ because $x_t = \\delta(t)$ is part of one solution to the set of inequalities, $\\max(x_t) \\le \\delta(t)$ can be demonstrated by a technique similar to the proof of Theorem 24.9: Let $p$ be a shortest path from $s$ to $t$. Then by definition, $$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v).$$ But for each edge $(u, v)$ we have the inequality $x_v - x_u \\le w(u, v)$, so $$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v) \\ge \\sum_{(u, v) \\in p} (x_v - x_u) = x_t - x_s.$$ But $x_s = 0$, so $x_t \\le \\delta(t)$. Note: Maximizing $x_t$ subject to the above inequalities solves the single-pair shortest-path problem when $t$ is reachable from $s$ and there are no negative-weight cycles. But if there's a negative-weight cycle, the inequalities have no feasible solution (as demonstrated in the proof of Theorem 24.9); and if $t$ is not reachable from $s$, then $x_t$ is unbounded. 24.4-5 Show how to modify the Bellman-Ford algorithm slightly so that when we use it to solve a system of difference constraints with $m$ inequalities on $n$ unknowns, the running time is $O(nm)$. We can follow the advice of problem 14.4-7 and solve the system of constraints on a modified constraint graph in which there is no new vertex $v_0$. This is simply done by initializing all of the vertices to have a $d$ value of $0$ before running the iterated relaxations of Bellman Ford. Since we don't add a new vertex and the $n$ edges going from it to to vertex corresponding to each variable, we are just running Bellman Ford on a graph with $n$ vertices and $m$ edges, and so it will have a runtime of $O(mn)$. 24.4-6 Suppose that in addition to a system of difference constraints, we want to handle equality constraints of the form $x_i = x_j + b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. To obtain the equality constraint $x_i = x_j + b_k$ we simply use the inequalities $x_i - x_j \\le b_k$ and $x_j - x_i \\le -bk$, then solve the problem as usual. 24.4-7 Show how to solve a system of difference constraints by a Bellman-Ford-like algorithm that runs on a constraint graph without the extra vertex $v_0$. Observe that after the first pass, all $d$ values are at most $0$, and that relaxing edges $(v_0, v_i)$ will never again change a $d$ value. Therefore, we can eliminate $v_0$ by running the Bellman-Ford algorithm on the constraint graph without the $v_0$ vertex but initializing all shortest path estimates to $0$ instead of $\\infty$. 24.4-8 $\\star$ Let $Ax \\le b$ be a system of $m$ difference constraints in $n$ unknowns. Show that the Bellman-Ford algorithm, when run on the corresponding constraint graph, maximizes $\\sum_{i = 1}^n x_i$ subject to $Ax \\le b$ and $x_i \\le 0$ for all $x_i$. Bellman-Ford correctly solves the system of difference constraints so $Ax \\le b$ is always satisfied. We also have that $x_i = \\delta(v_0, v_i) \\le w(v_0, v_i) = 0$ so $x_i \\le 0$ for all $i$. To show that $\\sum x_i$ is maximized, we'll show that for any feasible solution $(y_1, y_2, \\ldots, y_n)$ which satisfies the constraints we have $yi \\le \\delta(v_0, v_i) = x_i$. Let $v_0, v_{i_1}, \\ldots, v_{i_k}$ be a shortest path from $v_0$ to $v_i$ in the constraint graph. Then we must have the constraints $y_{i_2} - y_{i_1} \\le w(v_{i_1}, v_{i_2}), \\ldots, y_{i_k} - y_{i_{k - 1}} \\le w(v_{i_{k - 1}},v_{i_k})$. Summing these up we have $$y_i \\le y_i - y_1 \\le \\sum_{m = 2}^k w(v_{i_m}, v_{i_{m - 1}}) = \\delta(v_0, v_i) = x_i.$$ 24.4-9 $\\star$ Show that the Bellman-Ford algorithm, when run on the constraint graph for a system $Ax \\le b$ of difference constraints, minimizes the quantity $(\\max\\{x_i\\} - \\min\\{x_i\\})$ subject to $Ax \\le b$. Explain how this fact might come in handy if the algorithm is used to schedule construction jobs. We can see that the Bellman-Ford algorithm run on the graph whose construction is described in this section causes the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ to be minimized. We know that the largest value assigned to any of the vertices in the constraint graph is a $0$. It is clear that it won't be greater than zero, since just the single edge path to each of the vertices has cost zero. We also know that we cannot have every vertex having a shortest path with negative weight. To see this, notice that this would mean that the pointer for each vertex has it's $p$ value going to some other vertex that is not the source. This means that if we follow the procedure for reconstructing the shortest path for any of the vertices, we have that it can never get back to the source, a contradiction to the fact that it is a shortest path from the source to that vertex. Next, we note that when we run Bellman-Ford, we are maximizing $\\min\\{x_i\\}$. The shortest distance in the constraint graphs is the bare minimum of what is required in order to have all the constraints satisfied, if we were to increase any of the values we would be violating a constraint. This could be in handy when scheduling construction jobs because the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ is equal to the difference in time between the last task and the first task. Therefore, it means that minimizing it would mean that the total time that all the jobs takes is also minimized. And, most people want the entire process of construction to take as short of a time as possible. 24.4-10 Suppose that every row in the matrix $A$ of a linear program $Ax \\le b$ corresponds to a difference constraint, a single-variable constraint of the form $x_i \\le b_k$, or a singlevariable constraint of the form $-x_i \\le b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. To allow for single-variable constraints, we add the variable $x_0$ and let it correspond to the source vertex $v_0$ of the constraint graph. The idea is that, if there are no negative-weight cycles containing $v_0$, we will find that $\\delta(v_0, v_0) = 0$. In this case, we set $x_0 = 0$, and so we can treat any single-variable constraint using $x_i$ as if it were a $2$-variable constraint with $x_0$ as the other variable. Specifically, we treat the constraint $x_i \\le b_k$ as if it were $x_i - x_0 \\le b_k$, and we add the edge $(v_0, v_i)$ with weight $b_k$ to the constraint graph. We treat the constraint $-x_i \\le b_k$ as if it were $x_0 - x_i \\le b_k$, and we add the edge $(v_i, v_0)$ with weight $b_k$ to the constraint graph. Once we find shortest-path weights from $v_0$, we set $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$; that is, we do as before but also include $x_0$ as one of the variables that we set to a shortest-path weight. Since $v_0$ is the source vertex, either $x_0 = 0$ or $x_0 < 0$. If $\\delta(v_0, v_0) = 0$, so that $x_0 = 0$, then setting $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$ gives a feasible solution for the system. The only new constraints beyond those in the text are those involving $x_0$. For constraints $x_i \\le b_k$, we use $x_i - x_0 \\le b_k$. By the triangle inequality, $\\delta(v_0, v_i) \\le \\delta(v_0, v_0) + w(v_0, v_i) = b_k$, and so $x_i \\le b_k$. For constraints $x_i \\le b_k$, we use $x_0 - x_i \\le b_k$. By the triangle inequality, $0 = \\delta(v_0, v_0) \\le \\delta(v_0, v_i) + w(v_i, v_0)$; thus, $0 \\le x_i + b_k$ or, equivalently, $-x_i \\le b_k$. If $\\delta(v_0, v_0) < 0$, so that $x_0 < 0$, then there is a negative-weight cycle containing $v_0$. The portion of the proof of Theorem 24.9 that deals with negative-weight cycles carries through but with $v_0$ on the negative-weight cycle, and we see that there is no feasible solution. 24.4-11 Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and all of the unknowns $x_i$ must be integers. To do this, just take the floor of (largest integer that is less than or equal to) each of the $b$ values and solve the resulting integer difference problem. These modified constraints will be admitting exactly the same set of assignments since we required that the solution have integer values assigned to the variables. This is because since the variables are integers, all of their differences will also be integers. For an integer to be less than or equal to a real number, it is necessary and sufficient for it to be less than or equal to the floor of that real number. 24.4-12 $\\star$ Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and a specified subset of some, but not necessarily all, of the unknowns $x_i$ must be integers. To solve the problem of $Ax \\le b$ where the elements of $b$ are real-valued we carry out the same procedure as before, running Bellman-Ford, but allowing our edge weights to be real-valued. To impose the integer condition on the $x_i$'s, we modify the $\\text{RELAX}$ procedure. Suppose we call $\\text{RELAX}(v_i, v_j, w)$ where $v_j$ is required to be integral valued. If $v_j.d > \\lfloor v_i.d + w(v_i, v_j) \\rfloor$, set $v_j.d = \\lfloor v_i.d + w(v_i, v_j) \\rfloor$. This guarantees that the condition that $v_j.d - v_i.d \\le w(v_i, v_j)$ as desired. It also ensures that $v_j$ is integer valued. Since the triangle inequality still holds, $x = (v_1.d, v_2.d, \\ldots, v_n.d)$ is a feasible solution for the system, provided that $G$ contains no negative weight cycles.","title":"24.4 Difference constraints and shortest paths"},{"location":"Chap24/24.4/#244-1","text":"Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le & 1, \\\\ x_1 - x_4 & \\le & -4, \\\\ x_2 - x_3 & \\le & 2, \\\\ x_2 - x_5 & \\le & 7, \\\\ x_2 - x_6 & \\le & 5, \\\\ x_3 - x_6 & \\le & 10, \\\\ x_4 - x_2 & \\le & 2, \\\\ x_5 - x_1 & \\le & -1, \\\\ x_5 - x_4 & \\le & 3, \\\\ x_6 - x_3 & \\le & 8 \\end{aligned} $$ Our vertices of the constraint graph will be $$\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}.$$ The edges will be $$(v_0, v_1), (v_0, v_2), (v_0, v_3), (v_0, v_4), (v_0, v_5), (v_0, v_6), (v_2, v_1), (v_4, v_1), (v_3, v_2), (v_5, v_2), (v_6, v_2), (v_6, v_3),$$ with edge weights $$0, 0, 0, 0, 0, 0, 1, -4, 2, 7, 5, 10, 2, -1, 3, -8$$ respectively. Then, computing $$(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\delta(v_0, v_4), \\delta(v_0, v_5), \\delta(v_0, v_6)),$$ we get $$(-5, -3, 0, -1, -6, -8),$$ which is a feasible solution by Theorem 24.9.","title":"24.4-1"},{"location":"Chap24/24.4/#244-2","text":"Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le &4, \\\\ x_1 - x_5 & \\le &5, \\\\ x_2 - x_4 & \\le &-6, \\\\ x_3 - x_2 & \\le &1, \\\\ x_4 - x_1 & \\le &3, \\\\ x_4 - x_3 & \\le &5, \\\\ x_4 - x_5 & \\le &10, \\\\ x_5 - x_3 & \\le &-4, \\\\ x_5 - x_4 & \\le &-8. \\end{aligned} $$ There is no feasible solution because the constraint graph contains a negative-weight cycle: $(v_1, v_4, v_2, v_3, v_5, v_1)$ has weight $-1$.","title":"24.4-2"},{"location":"Chap24/24.4/#244-3","text":"Can any shortest-path weight from the new vertex $v_0$ in a constraint graph be positive? Explain. No, it cannot be positive. This is because for every vertex $v \\ne v_0$, there is an edge $(v_0, v)$ with weight zero. So, there is some path from the new vertex to every other of weight zero. Since $\\delta(v_0, v)$ is a minimum weight of all paths, it cannot be greater than the weight of this weight zero path that consists of a single edge.","title":"24.4-3"},{"location":"Chap24/24.4/#244-4","text":"Express the single-pair shortest-path problem as a linear program. Let $\\delta(u)$ be the shortest-path weight from $s$ to $u$. Then we want to find $\\delta(t)$. $\\delta$ must satisfy $$ \\begin{aligned} \\delta(s) & = 0 \\\\ \\delta(v) - \\delta(u) & \\le w(u, v) \\text{ for all $(u, v) \\in E$} & \\text{(Lemma 24.10)}, \\end{aligned} $$ where $w(u, v)$ is the weight of edge $(u, v)$. Thus $x_v = \\delta(v)$ is a solution to $$ \\begin{aligned} x_s & = 0 \\\\ x_v - x_u & \\le w(u, v). \\end{aligned} $$ To turn this into a set of inequalities of the required form, replace $x_s = 0$ by $x_s \\le 0$ and $-x_s \\le 0$ (i.e., $x_s \\ge$). The constraints are now $$ \\begin{aligned} x_s & \\le 0, \\\\ -x_s & \\le 0. \\\\ x_v - x_u & \\le w(u, v), \\end{aligned} $$ which still has $x_v = \\delta(v)$ as a solution. However, $\\delta$ isn't the only solution to this set of inequalities. (For example, if all edge weights are nonnegative, all $x_i = 0$ is a solution.) To force $x_t = \\delta(t)$ as required by the shortest-path problem, add the requirement to maximize (the objective function) $x_t$. This is correct because $\\max(x_t) \\ge \\delta(t)$ because $x_t = \\delta(t)$ is part of one solution to the set of inequalities, $\\max(x_t) \\le \\delta(t)$ can be demonstrated by a technique similar to the proof of Theorem 24.9: Let $p$ be a shortest path from $s$ to $t$. Then by definition, $$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v).$$ But for each edge $(u, v)$ we have the inequality $x_v - x_u \\le w(u, v)$, so $$\\delta(t) = \\sum_{(u, v) \\in p} w(u, v) \\ge \\sum_{(u, v) \\in p} (x_v - x_u) = x_t - x_s.$$ But $x_s = 0$, so $x_t \\le \\delta(t)$. Note: Maximizing $x_t$ subject to the above inequalities solves the single-pair shortest-path problem when $t$ is reachable from $s$ and there are no negative-weight cycles. But if there's a negative-weight cycle, the inequalities have no feasible solution (as demonstrated in the proof of Theorem 24.9); and if $t$ is not reachable from $s$, then $x_t$ is unbounded.","title":"24.4-4"},{"location":"Chap24/24.4/#244-5","text":"Show how to modify the Bellman-Ford algorithm slightly so that when we use it to solve a system of difference constraints with $m$ inequalities on $n$ unknowns, the running time is $O(nm)$. We can follow the advice of problem 14.4-7 and solve the system of constraints on a modified constraint graph in which there is no new vertex $v_0$. This is simply done by initializing all of the vertices to have a $d$ value of $0$ before running the iterated relaxations of Bellman Ford. Since we don't add a new vertex and the $n$ edges going from it to to vertex corresponding to each variable, we are just running Bellman Ford on a graph with $n$ vertices and $m$ edges, and so it will have a runtime of $O(mn)$.","title":"24.4-5"},{"location":"Chap24/24.4/#244-6","text":"Suppose that in addition to a system of difference constraints, we want to handle equality constraints of the form $x_i = x_j + b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. To obtain the equality constraint $x_i = x_j + b_k$ we simply use the inequalities $x_i - x_j \\le b_k$ and $x_j - x_i \\le -bk$, then solve the problem as usual.","title":"24.4-6"},{"location":"Chap24/24.4/#244-7","text":"Show how to solve a system of difference constraints by a Bellman-Ford-like algorithm that runs on a constraint graph without the extra vertex $v_0$. Observe that after the first pass, all $d$ values are at most $0$, and that relaxing edges $(v_0, v_i)$ will never again change a $d$ value. Therefore, we can eliminate $v_0$ by running the Bellman-Ford algorithm on the constraint graph without the $v_0$ vertex but initializing all shortest path estimates to $0$ instead of $\\infty$.","title":"24.4-7"},{"location":"Chap24/24.4/#244-8-star","text":"Let $Ax \\le b$ be a system of $m$ difference constraints in $n$ unknowns. Show that the Bellman-Ford algorithm, when run on the corresponding constraint graph, maximizes $\\sum_{i = 1}^n x_i$ subject to $Ax \\le b$ and $x_i \\le 0$ for all $x_i$. Bellman-Ford correctly solves the system of difference constraints so $Ax \\le b$ is always satisfied. We also have that $x_i = \\delta(v_0, v_i) \\le w(v_0, v_i) = 0$ so $x_i \\le 0$ for all $i$. To show that $\\sum x_i$ is maximized, we'll show that for any feasible solution $(y_1, y_2, \\ldots, y_n)$ which satisfies the constraints we have $yi \\le \\delta(v_0, v_i) = x_i$. Let $v_0, v_{i_1}, \\ldots, v_{i_k}$ be a shortest path from $v_0$ to $v_i$ in the constraint graph. Then we must have the constraints $y_{i_2} - y_{i_1} \\le w(v_{i_1}, v_{i_2}), \\ldots, y_{i_k} - y_{i_{k - 1}} \\le w(v_{i_{k - 1}},v_{i_k})$. Summing these up we have $$y_i \\le y_i - y_1 \\le \\sum_{m = 2}^k w(v_{i_m}, v_{i_{m - 1}}) = \\delta(v_0, v_i) = x_i.$$","title":"24.4-8 $\\star$"},{"location":"Chap24/24.4/#244-9-star","text":"Show that the Bellman-Ford algorithm, when run on the constraint graph for a system $Ax \\le b$ of difference constraints, minimizes the quantity $(\\max\\{x_i\\} - \\min\\{x_i\\})$ subject to $Ax \\le b$. Explain how this fact might come in handy if the algorithm is used to schedule construction jobs. We can see that the Bellman-Ford algorithm run on the graph whose construction is described in this section causes the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ to be minimized. We know that the largest value assigned to any of the vertices in the constraint graph is a $0$. It is clear that it won't be greater than zero, since just the single edge path to each of the vertices has cost zero. We also know that we cannot have every vertex having a shortest path with negative weight. To see this, notice that this would mean that the pointer for each vertex has it's $p$ value going to some other vertex that is not the source. This means that if we follow the procedure for reconstructing the shortest path for any of the vertices, we have that it can never get back to the source, a contradiction to the fact that it is a shortest path from the source to that vertex. Next, we note that when we run Bellman-Ford, we are maximizing $\\min\\{x_i\\}$. The shortest distance in the constraint graphs is the bare minimum of what is required in order to have all the constraints satisfied, if we were to increase any of the values we would be violating a constraint. This could be in handy when scheduling construction jobs because the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ is equal to the difference in time between the last task and the first task. Therefore, it means that minimizing it would mean that the total time that all the jobs takes is also minimized. And, most people want the entire process of construction to take as short of a time as possible.","title":"24.4-9 $\\star$"},{"location":"Chap24/24.4/#244-10","text":"Suppose that every row in the matrix $A$ of a linear program $Ax \\le b$ corresponds to a difference constraint, a single-variable constraint of the form $x_i \\le b_k$, or a singlevariable constraint of the form $-x_i \\le b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. To allow for single-variable constraints, we add the variable $x_0$ and let it correspond to the source vertex $v_0$ of the constraint graph. The idea is that, if there are no negative-weight cycles containing $v_0$, we will find that $\\delta(v_0, v_0) = 0$. In this case, we set $x_0 = 0$, and so we can treat any single-variable constraint using $x_i$ as if it were a $2$-variable constraint with $x_0$ as the other variable. Specifically, we treat the constraint $x_i \\le b_k$ as if it were $x_i - x_0 \\le b_k$, and we add the edge $(v_0, v_i)$ with weight $b_k$ to the constraint graph. We treat the constraint $-x_i \\le b_k$ as if it were $x_0 - x_i \\le b_k$, and we add the edge $(v_i, v_0)$ with weight $b_k$ to the constraint graph. Once we find shortest-path weights from $v_0$, we set $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$; that is, we do as before but also include $x_0$ as one of the variables that we set to a shortest-path weight. Since $v_0$ is the source vertex, either $x_0 = 0$ or $x_0 < 0$. If $\\delta(v_0, v_0) = 0$, so that $x_0 = 0$, then setting $x_i = \\delta(v_0, v_i)$ for all $i = 0, 1, \\ldots, n$ gives a feasible solution for the system. The only new constraints beyond those in the text are those involving $x_0$. For constraints $x_i \\le b_k$, we use $x_i - x_0 \\le b_k$. By the triangle inequality, $\\delta(v_0, v_i) \\le \\delta(v_0, v_0) + w(v_0, v_i) = b_k$, and so $x_i \\le b_k$. For constraints $x_i \\le b_k$, we use $x_0 - x_i \\le b_k$. By the triangle inequality, $0 = \\delta(v_0, v_0) \\le \\delta(v_0, v_i) + w(v_i, v_0)$; thus, $0 \\le x_i + b_k$ or, equivalently, $-x_i \\le b_k$. If $\\delta(v_0, v_0) < 0$, so that $x_0 < 0$, then there is a negative-weight cycle containing $v_0$. The portion of the proof of Theorem 24.9 that deals with negative-weight cycles carries through but with $v_0$ on the negative-weight cycle, and we see that there is no feasible solution.","title":"24.4-10"},{"location":"Chap24/24.4/#244-11","text":"Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and all of the unknowns $x_i$ must be integers. To do this, just take the floor of (largest integer that is less than or equal to) each of the $b$ values and solve the resulting integer difference problem. These modified constraints will be admitting exactly the same set of assignments since we required that the solution have integer values assigned to the variables. This is because since the variables are integers, all of their differences will also be integers. For an integer to be less than or equal to a real number, it is necessary and sufficient for it to be less than or equal to the floor of that real number.","title":"24.4-11"},{"location":"Chap24/24.4/#244-12-star","text":"Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and a specified subset of some, but not necessarily all, of the unknowns $x_i$ must be integers. To solve the problem of $Ax \\le b$ where the elements of $b$ are real-valued we carry out the same procedure as before, running Bellman-Ford, but allowing our edge weights to be real-valued. To impose the integer condition on the $x_i$'s, we modify the $\\text{RELAX}$ procedure. Suppose we call $\\text{RELAX}(v_i, v_j, w)$ where $v_j$ is required to be integral valued. If $v_j.d > \\lfloor v_i.d + w(v_i, v_j) \\rfloor$, set $v_j.d = \\lfloor v_i.d + w(v_i, v_j) \\rfloor$. This guarantees that the condition that $v_j.d - v_i.d \\le w(v_i, v_j)$ as desired. It also ensures that $v_j$ is integer valued. Since the triangle inequality still holds, $x = (v_1.d, v_2.d, \\ldots, v_n.d)$ is a feasible solution for the system, provided that $G$ contains no negative weight cycles.","title":"24.4-12 $\\star$"},{"location":"Chap24/24.5/","text":"24.5-1 Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648) other than the two shown. Since the induced shortest path trees on $\\{s, t, y\\}$ and on $\\{t, x, y, z\\}$ are independent and have to possible configurations each, there are four total arising from that. So, we have the two not shown in the figure are the one consisting of the edges $\\{(s, t), (s, y), (y, x), (x, z)\\}$ and the one consisting of the edges $\\{(s, t), (t, y), (t, x), (y, z)\\}$. 24.5-2 Give an example of a weighted, directed graph $G = (V, E)$ with weight function $w: E \\rightarrow \\mathbb R$ and source vertex $s$ such that $G$ satisfies the following property: For every edge $(u, v) \\in E$, there is a shortest-paths tree rooted at $s$ that contains $(u, v)$ and another shortest-paths tree rooted at $s$ that does not contain $(u, v)$. Let $G$ have $3$ vertices $s$, $x$, and $y$. Let the edges be $(s, x)$, $(s, y)$, and $(x, y)$ with weights $1$, $1$, and $0$ respectively. There are $3$ possible trees on these vertices rooted at $s$, and each is a shortest paths tree which gives $\\delta(s, x) = \\delta(s, y) = 1$. 24.5-3 Embellish the proof of Lemma 24.10 to handle cases in which shortest-path weights are $\\infty$ or $-\\infty$. To modify Lemma 24.10 to allow for possible shortest path weights of $\\infty$ and $-\\infty$, we need to define our addition as $\\infty + c = \\infty$, and $-\\infty + c = -\\infty$. This will make the statement behave correctly, that is, we can take the shortest path from $s$ to $u$ and tack on the edge $(u, v)$ to the end. That is, if there is a negative weight cycle on your way to $u$ and there is an edge from $u$ to $v$, there is a negative weight cycle on our way to $v$. Similarly, if we cannot reach $v$ and there is an edge from $u$ to $v$, we cannot reach $u$. 24.5-4 Let $G = (V, E)$ be a weighted, directed graph with source vertex $s$, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that if a sequence of relaxation steps sets $s.\\pi$ to a non-$\\text{NIL}$ value, then $G$ contains a negative-weight cycle. Whenever $\\text{RELAX}$ sets $\\pi$ for some vertex, it also reduces the vertex's $d$ value. Thus if $s.\\pi$ gets set to a non-$\\text{NIL}$ value, $s.d$ is reduced from its initial value of $0$ to a negative number. But $s.d$ is the weight of some path from $s$ to $s$, which is a cycle including $s$. Thus, there is a negative-weight cycle. 24.5-5 Let $G = (V, E)$ be a weighted, directed graph with no negative-weight edges. Let $s \\in V$ be the source vertex, and suppose that we allow $v.\\pi$ to be the predecessor of $v$ on any shortest path to $v$ from source $s$ if $v \\in V - \\{s\\}$ is reachable from $s$, and $\\text{NIL}$ otherwise. Give an example of such a graph $G$ and an assignment of $\\pi$ values that produces a cycle in $G_\\pi$. (By Lemma 24.16, such an assignment cannot be produced by a sequence of relaxation steps.) Suppose that we have a grap hon three vertices $\\{s, u, v\\}$ and containing edges $(s, u), (s, v), (u, v), (v, u)$ all with weight $0$. Then, there is a shortest path from $s$ to $v$ of $s$, $u$, $v$ and a shortest path from $s$ to $u$ of $s$ $v$, $u$. Based off of these, we could set $v.\\pi = u$ and $u.\\pi = v$. This then means that there is a cycle consisting of $u, v$ in $G_\\pi$. 24.5-6 Let $G = (V, E)$ be a weighted, directed graph with weight function $w: E \\rightarrow \\mathbb R$ and no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that for every vertex $v \\in V_\\pi$, there exists a path from $s$ to $v$ in $G_\\pi$ and that this property is maintained as an invariant over any sequence of relaxations. We will prove this by induction on the number of relaxations performed. For the base-case, we have just called $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. The only vertex in $V_\\pi$ is $s$, and there is trivially a path from $s$ to itself. Now suppose that after any sequence of $n$ relaxations, for every vertex $v \\in V_\\pi$ there exists a path from $s$ to $v$ in $G_\\pi$. Consider the $(n + 1)$th relaxation. Suppose it is such that $v.d > u.d + w(u, v)$. When we relax $v$, we update $v.\\pi = u.\\pi$. By the induction hypothesis, there was a path from $s$ to $u$ in $G_\\pi$. Now $v$ is in $V_\\pi$, and the path from $s$ to $u$, followed by the edge $(u,v) = (v.\\pi, v)$ is a path from s to $v$ in $G_\\pi$, so the claim holds. 24.5-7 Let $G = (V, E)$ be a weighted, directed graph that contains no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that there exists a sequence of $|V| - 1$ relaxation steps that produces $v.d = \\delta(s, v)$ for all $v \\in V$. Suppose we have a shortest-paths tree $G_\\pi$. Relax edges in $G_\\pi$ according to the order in which a BFS would visit them. Then we are guaranteed that the edges along each shortest path are relaxed in order. By the path-relaxation property, we would then have $v.d = \\delta(s, v)$ for all $v \\in V$. Since $G_\\pi$ contains at most $|V| - 1$ edges, we need to relax only $|V| - 1$ edges to get $v.d = \\delta(s, v)$ for all $v \\in V$. 24.5-8 Let $G$ be an arbitrary weighted, directed graph with a negative-weight cycle reachable from the source vertex $s$. Show how to construct an infinite sequence of relaxations of the edges of $G$ such that every relaxation causes a shortest-path estimate to change. Suppose that there is a negative-weight cycle $c = \\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = v_k$, that is reachable from the source vertex $s$; thus, $w(c) < 0$. Without loss of generality, $c$ is simple. There must be an acyclic path from $s$ to some vertex of $c$ that uses no other vertices in $c$. Without loss of generality let this vertex of $c$ be $v_0$, and let this path from $s$ to $v_0$ be $p = \\langle u_0, u_1, \\ldots, u_l \\rangle$, where $u_0 = s$ and $u_l = v_0 = v_k$. (It may be the case that $u_l = s$, in which case path $p$ has no edges.) After the call to $\\text{INITIALIZE-SINGLE-SOURCE}$ sets $v.d = \\infty$ for all $v \\in V - \\{s\\}$, perform the following sequence of relaxations. First, relax every edge in path $p$, in order. Then relax every edge in cycle $c$, in order, and repeatedly relax the cycle. That is, we relax the edges $(u_0, u_1)$, $(u_1, u_2)$, $\\ldots$, $(u_{l - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $\\ldots$ We claim that every edge relaxation in this sequence reduces a shortest-path estimate. Clearly, the first time we relax an edge $(u_{i - 1}, u_i)$ or $(v_{j - 1}, v_j)$, for $i = 1, 2, \\ldots, l$ and $j = 1, 2, \\ldots, k - 1$ (note that we have not yet relaxed the last edge of cycle $c$), we reduce $u_i.d$ or $v_j.d$ from $\\infty$ to a finite value. Now consider the relaxation of any edge $(v_{j - 1}, v_j)$ after this opening sequence of relaxations. We use induction on the number of edge relaxations to show that this relaxation reduces $v_j.d$. Basis: The next edge relaxed after the opening sequence is $(v_{k - 1}, v_k)$. Before relaxation, $v_k.d = w(p)$, and after relaxation, $v_k.d = w(p) + w(c) < w(p)$, since $w(c) < 0$. Inductive step: Consider the relaxation of edge $(v_{j - 1}, v_j)$. Since $c$ is a simple cycle, the last time $v_j.d$ was updated was by a relaxation of this same edge. By the inductive hypothesis, $v_{j - 1}.d$ has just been reduced. Thus, $v_{j - 1}.d + w(v_{j - 1}, v_j) < v_j.d$, and so the relaxation will reduce the value of $v_j.d$.","title":"24.5 Proofs of shortest-paths properties"},{"location":"Chap24/24.5/#245-1","text":"Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648) other than the two shown. Since the induced shortest path trees on $\\{s, t, y\\}$ and on $\\{t, x, y, z\\}$ are independent and have to possible configurations each, there are four total arising from that. So, we have the two not shown in the figure are the one consisting of the edges $\\{(s, t), (s, y), (y, x), (x, z)\\}$ and the one consisting of the edges $\\{(s, t), (t, y), (t, x), (y, z)\\}$.","title":"24.5-1"},{"location":"Chap24/24.5/#245-2","text":"Give an example of a weighted, directed graph $G = (V, E)$ with weight function $w: E \\rightarrow \\mathbb R$ and source vertex $s$ such that $G$ satisfies the following property: For every edge $(u, v) \\in E$, there is a shortest-paths tree rooted at $s$ that contains $(u, v)$ and another shortest-paths tree rooted at $s$ that does not contain $(u, v)$. Let $G$ have $3$ vertices $s$, $x$, and $y$. Let the edges be $(s, x)$, $(s, y)$, and $(x, y)$ with weights $1$, $1$, and $0$ respectively. There are $3$ possible trees on these vertices rooted at $s$, and each is a shortest paths tree which gives $\\delta(s, x) = \\delta(s, y) = 1$.","title":"24.5-2"},{"location":"Chap24/24.5/#245-3","text":"Embellish the proof of Lemma 24.10 to handle cases in which shortest-path weights are $\\infty$ or $-\\infty$. To modify Lemma 24.10 to allow for possible shortest path weights of $\\infty$ and $-\\infty$, we need to define our addition as $\\infty + c = \\infty$, and $-\\infty + c = -\\infty$. This will make the statement behave correctly, that is, we can take the shortest path from $s$ to $u$ and tack on the edge $(u, v)$ to the end. That is, if there is a negative weight cycle on your way to $u$ and there is an edge from $u$ to $v$, there is a negative weight cycle on our way to $v$. Similarly, if we cannot reach $v$ and there is an edge from $u$ to $v$, we cannot reach $u$.","title":"24.5-3"},{"location":"Chap24/24.5/#245-4","text":"Let $G = (V, E)$ be a weighted, directed graph with source vertex $s$, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that if a sequence of relaxation steps sets $s.\\pi$ to a non-$\\text{NIL}$ value, then $G$ contains a negative-weight cycle. Whenever $\\text{RELAX}$ sets $\\pi$ for some vertex, it also reduces the vertex's $d$ value. Thus if $s.\\pi$ gets set to a non-$\\text{NIL}$ value, $s.d$ is reduced from its initial value of $0$ to a negative number. But $s.d$ is the weight of some path from $s$ to $s$, which is a cycle including $s$. Thus, there is a negative-weight cycle.","title":"24.5-4"},{"location":"Chap24/24.5/#245-5","text":"Let $G = (V, E)$ be a weighted, directed graph with no negative-weight edges. Let $s \\in V$ be the source vertex, and suppose that we allow $v.\\pi$ to be the predecessor of $v$ on any shortest path to $v$ from source $s$ if $v \\in V - \\{s\\}$ is reachable from $s$, and $\\text{NIL}$ otherwise. Give an example of such a graph $G$ and an assignment of $\\pi$ values that produces a cycle in $G_\\pi$. (By Lemma 24.16, such an assignment cannot be produced by a sequence of relaxation steps.) Suppose that we have a grap hon three vertices $\\{s, u, v\\}$ and containing edges $(s, u), (s, v), (u, v), (v, u)$ all with weight $0$. Then, there is a shortest path from $s$ to $v$ of $s$, $u$, $v$ and a shortest path from $s$ to $u$ of $s$ $v$, $u$. Based off of these, we could set $v.\\pi = u$ and $u.\\pi = v$. This then means that there is a cycle consisting of $u, v$ in $G_\\pi$.","title":"24.5-5"},{"location":"Chap24/24.5/#245-6","text":"Let $G = (V, E)$ be a weighted, directed graph with weight function $w: E \\rightarrow \\mathbb R$ and no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that for every vertex $v \\in V_\\pi$, there exists a path from $s$ to $v$ in $G_\\pi$ and that this property is maintained as an invariant over any sequence of relaxations. We will prove this by induction on the number of relaxations performed. For the base-case, we have just called $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. The only vertex in $V_\\pi$ is $s$, and there is trivially a path from $s$ to itself. Now suppose that after any sequence of $n$ relaxations, for every vertex $v \\in V_\\pi$ there exists a path from $s$ to $v$ in $G_\\pi$. Consider the $(n + 1)$th relaxation. Suppose it is such that $v.d > u.d + w(u, v)$. When we relax $v$, we update $v.\\pi = u.\\pi$. By the induction hypothesis, there was a path from $s$ to $u$ in $G_\\pi$. Now $v$ is in $V_\\pi$, and the path from $s$ to $u$, followed by the edge $(u,v) = (v.\\pi, v)$ is a path from s to $v$ in $G_\\pi$, so the claim holds.","title":"24.5-6"},{"location":"Chap24/24.5/#245-7","text":"Let $G = (V, E)$ be a weighted, directed graph that contains no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that there exists a sequence of $|V| - 1$ relaxation steps that produces $v.d = \\delta(s, v)$ for all $v \\in V$. Suppose we have a shortest-paths tree $G_\\pi$. Relax edges in $G_\\pi$ according to the order in which a BFS would visit them. Then we are guaranteed that the edges along each shortest path are relaxed in order. By the path-relaxation property, we would then have $v.d = \\delta(s, v)$ for all $v \\in V$. Since $G_\\pi$ contains at most $|V| - 1$ edges, we need to relax only $|V| - 1$ edges to get $v.d = \\delta(s, v)$ for all $v \\in V$.","title":"24.5-7"},{"location":"Chap24/24.5/#245-8","text":"Let $G$ be an arbitrary weighted, directed graph with a negative-weight cycle reachable from the source vertex $s$. Show how to construct an infinite sequence of relaxations of the edges of $G$ such that every relaxation causes a shortest-path estimate to change. Suppose that there is a negative-weight cycle $c = \\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = v_k$, that is reachable from the source vertex $s$; thus, $w(c) < 0$. Without loss of generality, $c$ is simple. There must be an acyclic path from $s$ to some vertex of $c$ that uses no other vertices in $c$. Without loss of generality let this vertex of $c$ be $v_0$, and let this path from $s$ to $v_0$ be $p = \\langle u_0, u_1, \\ldots, u_l \\rangle$, where $u_0 = s$ and $u_l = v_0 = v_k$. (It may be the case that $u_l = s$, in which case path $p$ has no edges.) After the call to $\\text{INITIALIZE-SINGLE-SOURCE}$ sets $v.d = \\infty$ for all $v \\in V - \\{s\\}$, perform the following sequence of relaxations. First, relax every edge in path $p$, in order. Then relax every edge in cycle $c$, in order, and repeatedly relax the cycle. That is, we relax the edges $(u_0, u_1)$, $(u_1, u_2)$, $\\ldots$, $(u_{l - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $(v_0, v_1)$, $(v_1, v_2)$, $\\ldots$, $(v_{k - 1}, v_0)$, $\\ldots$ We claim that every edge relaxation in this sequence reduces a shortest-path estimate. Clearly, the first time we relax an edge $(u_{i - 1}, u_i)$ or $(v_{j - 1}, v_j)$, for $i = 1, 2, \\ldots, l$ and $j = 1, 2, \\ldots, k - 1$ (note that we have not yet relaxed the last edge of cycle $c$), we reduce $u_i.d$ or $v_j.d$ from $\\infty$ to a finite value. Now consider the relaxation of any edge $(v_{j - 1}, v_j)$ after this opening sequence of relaxations. We use induction on the number of edge relaxations to show that this relaxation reduces $v_j.d$. Basis: The next edge relaxed after the opening sequence is $(v_{k - 1}, v_k)$. Before relaxation, $v_k.d = w(p)$, and after relaxation, $v_k.d = w(p) + w(c) < w(p)$, since $w(c) < 0$. Inductive step: Consider the relaxation of edge $(v_{j - 1}, v_j)$. Since $c$ is a simple cycle, the last time $v_j.d$ was updated was by a relaxation of this same edge. By the inductive hypothesis, $v_{j - 1}.d$ has just been reduced. Thus, $v_{j - 1}.d + w(v_{j - 1}, v_j) < v_j.d$, and so the relaxation will reduce the value of $v_j.d$.","title":"24.5-8"},{"location":"Chap24/Problems/24-1/","text":"Suppose that we order the edge relaxations in each pass of the Bellman-Ford algorithm as follows. Before the first pass, we assign an arbitrary linear order $v_1, v_2, \\ldots, v_{|V|}$ to the vertices of the input graph $G = (V, E)$. Then, we partition the edge set $E$ into $E_f \\cup E_b$, where $E_f = \\{(v_i, v_j) \\in E: i < j\\}$ and $E_b = \\{(v_i, v_j) \\in E: i > j\\}$. (Assume that $G$ contains no self-loops, so that every edge is in either $E_f$ or $E_b$.) Define $G_f = (V, E_f)$ and $G_b = (V, E_b)$. a. Prove that $G_f$ is acyclic with topological sort $\\langle v_1, v_2, \\ldots, v_{|V|} \\rangle$ and that $G_b$ is acyclic with topological sort $\\langle v_{|V|}, v_{|V| - 1}, \\ldots, v_1 \\rangle$. Suppose that we implement each pass of the Bellman-Ford algorithm in the following way. We visit each vertex in the order $v_1, v_2, \\ldots, v_{|V|}$, relaxing edges of $E_f$ that leave the vertex. We then visit each vertex in the order $v_{|V|}, v_{|V| - 1}, \\ldots, v_1$, relaxing edges of $E_b$ that leave the vertex. b. Prove that with this scheme, if $G$ contains no negative-weight cycles that are reachable from the source vertex $s$, then after only $\\lceil |V| / 2 \\rceil$ passes over the edges, $v.d = \\delta(s, v)$ for all vertices $v \\in V$. c. Does this scheme improve the asymptotic running time of the Bellman-Ford algorithm? a. Assume for the purpose contradiction that $G_f$ is not acyclic; thus $G_f$ has a cycle. A cycle must have at least one edge $(u, v)$ in which $u$ has higher index than $v$. This edge is not in $E_f$ (by the definition of $E_f$), in contradition to the assumption that $G_f$ has a cycle. Thus $G_f$ is acyclic. The sequence $\\langle v_1, v_2, \\ldots, v_{|V|} \\rangle$ is a topological sort for $G_f$, because from the definition of $E_f$ we know that all edges are directed from smaller indices to larger indices. The proof for $E_b$ is similar. b. For all vertices $v \\in V$, we know that either $\\delta(s, v) = \\infty$ or $\\delta(s, v)$ is finite. If $\\delta(s, v) = \\infty$, then $v.d$ will be $\\infty$. Thus, we need to consider only the case where $v.d$ is finite. There must be some shortest path from $s$ to $v$. Let $p = \\langle v_0, v_1, \\ldots, v_{k - 1}, v_k \\rangle$ be that path, where $v_0 = s$ and $v_k = v$. Let us now consider how many times there is a change in direction in $p$, that is, a situation in which $(v_{i - 1}, v_i) \\in E_f$ and $(v_i, v_{i + 1} \\in E_b$ or vice versa. There can be at most $|V| - 1$ edges in $p$, so there can be at most $|V| - 2$ changes in direction. Any portion of the path where there is no change in direction is computed with the correct $d$ values in the first or second half of a single pass once the vertex that begins the no-change-in-direction sequence has the correct $d$ value, because the edges are relaxed in the order of the direction of the sequence. Each change in direction requires a half pass in the new direction of the path. The following table shows the maximum number of passes needed depending on the parity of $|V| - 1$ and the direction of the first edge: $$ \\begin{array}{lll} |V| - 1 & \\text{first edge direction} & \\text{passes} \\\\ \\hline \\text{even} & \\text{forward} & (|V| - 1) / 2 \\\\ \\text{even} & \\text{backward} & (|V| - 1) / 2 + 1 \\\\ \\text{odd} & \\text{forward} & |V| / 2 \\\\ \\text{odd} & \\text{backward} & |V| / 2 \\end{array} $$ In any case, the maximum number of passes that we will need is $\\lceil |V| / 2 \\rceil$. c. This scheme does not affect the asymptotic running time of the algorithm because even though we perform only $\\lceil |V| / 2 \\rceil$ passes instead of $|V| - 1$ passes, it is still $O(V)$ passes. Each pass still takes $\\Theta(E)$ time, so the running time remains $O(VE)$.","title":"24-1 Yen's improvement to Bellman-Ford"},{"location":"Chap24/Problems/24-2/","text":"A $d$-dimensional box with dimensions $(x_1, x_2, \\ldots, x_d)$ nests within another box with dimensions $(y_1, y_2, \\ldots, y_d)$ if there exists a permutation $\\pi$ on $\\{1, 2, \\ldots, d\\}$ such that $x_{\\pi(1)} < y_1$, $x_{\\pi(2)} < y_2$, $\\ldots$, $x_{\\pi(d)} < y_d$. a. Argue that the nesting relation is transitive. b. Describe an efficient method to determine whether or not one $d$-dimensional box nests inside another. c. Suppose that you are given a set of $n$ $d$-dimensional boxes $\\{B_1, B_2, \\ldots, B_n\\}$. Give an efficient algorithm to find the longest sequence $\\langle B_{i_1}, B_{i_2}, \\ldots, B_{i_k} \\rangle$ of boxes such that $B_{i_j}$ nests within $B_{i_{j + 1}}$ for $j = 1, 2, \\ldots, k - 1$. Express the running time of your algorithm in terms of $n$ and $d$. a. Consider boxes with dimensions $x = (x_1, \\ldots, x_d)$, $y = (y_1, \\ldots, y_d)$, and $z = (z_1, \\ldots, z_d)$. Suppose there exists a permutation $\\pi$ such that $x_{\\pi(i)} < y_i$ for $i = 1, \\ldots, d$ and there exists a permutation $\\pi'$ such that $y_{\\pi'(i)} < z_i$ for $i = 1, \\ldots, d$, so that $x$ nests inside $y$ and $y$ nests inside $z$. Construct a permutation $\\pi''$, where $\\pi''(i) = \\pi'(\\pi(i))$. Then for $i = 1, \\ldots, d$, we have $x_{\\pi''(i)} = x_{\\pi'(\\pi(i))} < y_{\\pi'(i)} < z_i$, and so $x$ nests inside $z$. b. Sort the dimensions of each box from longest to shortest. A box $X$ with sorted dimensions $(x_1, x_2, \\ldots, x_d)$ nests inside a box $Y$ with sorted dimensions $(y_1, y_2, \\ldots, y_d)$ if and only if $x_i < y_i$ for $i = 1, 2, \\ldots, d$. The sorting can be done in $O(d\\lg d)$ time, and the test for nesting can be done in $O(d)$ time, and so the algorithm runs in $O(d\\lg d)$ time. This algorithm works because a $d$-dimensional box can be oriented so that every permutation of its dimensions is possible. (Experiment with a $3$-dimensional box if you are unsure of this). c. Construct a dag $G = (V, E)$, where each vertex $v_i$ corresponds to box $B_i$, and $(v_i, v_j) \\in E$ if and only if box $B_i$ nests inside box $B_j$. Graph $G$ is indeed a dag, because nesting is transitive and antireflexive (i.e., no box nests inside itself). The time to construct the dag is $O(dn^2 + dn\\lg d)$, from comparing each of the $\\binom{n}{2}$ pairs of boxes after sorting the dimensions of each. Add a supersource vertex $s$ and a supersink vertex $t$ to $G$, and add edges $(s, v_i)$ for all vertices $v_i$ with $in\\text-degree$ $0$ and $(v_j, t)$ for all vertices $v_j$ with outdegree $0$. Call the resulting dag $G'$. The time to do so is $O(n)$. Find a longest path from $s$ to $t$ in $G'$. (Section 24.2 discusses how to find a longest path in a dag.) This path corresponds to a longest sequence of nesting boxes. The time to find a longest path is $O(n^2)$, since $G'$ has $n + 2$ vertices and $O(n^2)$ edges. Overall, this algorithm runs in $O(dn^2 + dn\\lg d)$ time.","title":"24-2 Nesting boxes"},{"location":"Chap24/Problems/24-3/","text":"Arbitrage is the use of discrepancies in currency exchange rates to transform one unit of a currency into more than one unit of the same currency. For example, suppose that $1$ U.S. dollar buys $49$ Indian rupees, $1$ Indian rupee buys $2$ Japanese yen, and $1$ Japanese yen buys $0.0107$ U.S. dollars. Then, by converting currencies, a trader can start with $1$ U.S. dollar and buy $49 \\times 2 \\times 0.0107 = 1.0486$ U.S. dollars, thus turning a profit of $4.86$ percent. Suppose that we are given $n$ currencies $c_1, c_2, \\ldots, c_n$ and an $n \\times n$ table $R$ of exchange rates, such that one unit of currency $c_i$ buys $R[i, j]$ units of currency $c_j$. a. Give an efficient algorithm to determine whether or not there exists a sequence of currencies $\\langle c_{i_1}, c_{i_2}, \\ldots, c_{i_k} \\rangle$ such that $$R[i_1, i_2] \\cdot R[i_2, i_3] \\cdots R[i_{k - 1}, i_k] \\cdot R[i_k, i_1] > 1.$$ Analyze the running time of your algorithm. b. Give an efficient algorithm to print out such a sequence if one exists. Analyze the running time of your algorithm. a. We can use the Bellman-Ford algorithm on a suitable weighted, directed graph $G = (V, E)$, which we form as follows. There is one vertex in $V$ for each currency, and for each pair of currencies $c_i$ and $c_j$, there are directed edges $(v_i, v_j)$ and $(v_j , v_i)$. (Thus, $|V| = n$ and $|E| = n(n - 1)$.) We are looking for a cycle $\\langle i_1, i_2, i_3, \\ldots, i_k, i_1 \\rangle$ such that $$R[i_1, i_2] \\cdot R[i_2, i_3] \\ldots R[i_{k - 1}, i_k] \\cdot R[i_k, i_1] > 1.$$ Taking logarithms of both sides of this inequality gives $$\\lg R[i_1, i_2] + \\lg R[i_2, i_3] + \\cdots + \\lg R[i_{k - 1}, i_k] + \\lg R[i_k, i_1] > 0.$$ If we negate both sides, we get $$(-\\lg R[i_1, i_2]) + (-\\lg R[i_2, i_3]) + \\cdots + (-\\lg R[i_{k - 1}, i_k]) + (-\\lg R[i_k, i_1]) < 0,$$ and so we want to determine whether $G$ contains a negative-weight cycle with these edge weights. We can determine whether there exists a negative-weight cycle in $G$ by adding an extra vertex $v_0$ with $0$-weight edges $(v_0, v_i)$ for all $v_i \\in V$, running $\\text{BELLMAN-FORD}$ from $v_0$, and using the boolean result of $\\text{BELLMAN-FORD}$ (which is $\\text{TRUE}$ if there are no negative-weight cycles and $\\text{FALSE}$ if there is a negative-weight cycle) to guide our answer. That is, we invert the boolean result of $\\text{BELLMAN-FORD}$. This method works because adding the new vertex $v_0$ with $0$-weight edges from $v_0$ to all other vertices cannot introduce any new cycles, yet it ensures that all negative-weight cycles are reachable from $v_0$ . It takes $\\Theta(n^2)$ time to create $G$, which has $\\Theta(n^2)$ edges. Then it takes $O(n^3)$ time to run $\\text{BELLMAN-FORD}$. Thus, the total time is $O(n^3)$. Another way to determine whether a negative-weight cycle exists is to create $G$ and, without adding $v_0$ and its incident edges, run either of the all-pairs shortestpaths algorithms. If the resulting shortest-path distance matrix has any negative values on the diagonal, then there is a negative-weight cycle. b. Note: The solution to this part also serves as a solution to Exercise 24.1-6. Assuming that we ran $\\text{BELLMAN-FORD}$ to solve part (a), we only need to find the vertices of a negative-weight cycle. We can do so as follows. Go through the edges once again. Once we find an edge $(u, v)$ for which $u.d + w(u, v) < v.d$, then we know that either vertex $v$ is on a negative-weight cycle or is reachable from one. We can find a vertex on the negative-weight cycle by tracing back the $v$ values from $v$, keeping track of which vertices we've visited until we reach a vertex $x$ that we've visited before. Then we can trace back $v$ values from $x$ until we get back to $x$, and all vertices in between, along with $x$, will constitute a negative-weight cycle. We can use the recursive method given by the $\\text{PRINTPATH}$ procedure of Section 22.2, but stop it when it returns to vertex $x$. The running time is $O(n^3)$ to run $\\text{BELLMAN-FORD}$, plus $O(m)$ to check all the edges and $O(n)$ to print the vertices of the cycle, for a total of $O(n^3)$ time.","title":"24-3 Arbitrage"},{"location":"Chap24/Problems/24-4/","text":"A scaling algorithm solves a problem by initially considering only the highestorder bit of each relevant input value (such as an edge weight). It then refines the initial solution by looking at the two highest-order bits. It progressively looks at more and more high-order bits, refining the solution each time, until it has examined all bits and computed the correct solution. In this problem, we examine an algorithm for computing the shortest paths from a single source by scaling edge weights. We are given a directed graph $G = (V, E)$ with nonnegative integer edge weights $w$. Let $W = \\max_{(u, v) \\in E} \\{w(u, v)\\}$. Our goal is to develop an algorithm that runs in $O(E\\lg W)$ time. We assume that all vertices are reachable from the source. The algorithm uncovers the bits in the binary representation of the edge weights one at a time, from the most significant bit to the least significant bit. Specifically, let $k = \\lceil \\lg(W + 1) \\rceil$ be the number of bits in the binary representation of $W$, and for $i = 1, 2, \\ldots, k$, let $w_i(u, v) = \\lfloor w(u, v) / 2^{k - i} \\rfloor$. That is, $w_i(u, v)$ is the \"scaled-down\" version of $w(u, v)$ given by the $i$ most significant bits of $w(u, v)$. (Thus, $w_k(u, v) = w(u, v)$ for all $(u, v) \\in E$.) For example, if $k = 5$ and $w(u, v) = 25$, which has the binary representation $\\langle 11001 \\rangle$, then $w_3(u, v) = \\langle 110 \\rangle = 6$. As another example with $k = 5$, if $w(u, v) = \\langle 00100 \\rangle = 4$, then $w_3(u, v) = \\langle 001 \\rangle = 1$. Let us define $\\delta_i(u, v)$ as the shortest-path weight from vertex $u$ to vertex $v$ using weight function $w_i$. Thus, $\\delta_k(u, v) = \\delta(u, v)$ for all $u, v \\in V$. For a given source vertex $s$, the scaling algorithm first computes the shortest-path weights $\\delta_1(s, v)$ for all $v \\in V$, then computes $\\delta_2(s, v)$ for all $v \\in V$, and so on, until it computes $\\delta_k(s, v)$ for all $v \\in V$. We assume throughout that $|E| \\ge |V| - 1$, and we shall see that computing $\\delta_i$ from $\\delta_{i - 1}$ takes $O(E)$ time, so that the entire algorithm takes $O(kE) = O(E\\lg W)$ time. a. Suppose that for all vertices $v \\in V$, we have $\\delta(s, v) \\le |E|$. Show that we can compute $\\delta(s, v)$ for all $v \\in V$ in $O(E)$ time. b. Show that we can compute $\\delta_1(s, v)$ for all $v \\in V$ in $O(E)$ time. Let us now focus on computing $\\delta_i$ from $\\delta_{i - 1}$. c. Prove that for $i = 2, 3, \\ldots, k$, we have either $w_i(u, v) = 2w_{i - 1}(u, v)$ or $w_i(u, v) = 2w_{i - 1}(u, v) + 1$. Then, prove that $$2\\delta_{i - 1}(s, v) \\le \\delta_i(s, v) \\le 2\\delta_{i - 1}(s, v) + |V| - 1$$ for all $v \\in V$. d. Define for $i = 2, 3, \\ldots, k$ and all $(u, v) \\in E$, $$\\hat w_i = w_i(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v).$$ Prove that for $i = 2, 3, \\ldots, k$ and all $u, v \\in V$, the \"reweighted\" value $\\hat w_i(u, v)$ of edge $(u, v)$ is a nonnegative integer. e. Now, define $\\hat\\delta_i(s, v)$ as the shortest-path weight from $s$ to $v$ using the weight function $\\hat w_i$. Prove that for $i = 2, 3, \\ldots, k$ and all $v \\in V$, $$\\delta_i(s, v) = \\hat\\delta_i(s, v) + 2\\delta_{i - 1}(s, v)$$ and that $\\hat\\delta_i(s, v) \\le |E|$. f. Show how to compute $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ for all $v \\in V$ in $O(E)$ time, and conclude that we can compute $\\delta(s, v)$ for all $v \\in V$ in $O(E\\lg W)$ time. a. Since all weights are nonnegative, use Dijkstra's algorithm. Implement the priority queue as an array $Q[0..|E| + 1]$, where $Q[i]$ is a list of vertices $v$ for which $v.d = i$. Initialize $v.d$ for $v \\ne s$ to $|E| + 1$ instead of to $\\infty$, so that all vertices have a place in $Q$. (Any initial $v.d > \\delta(s, v)$ works in the algorithm, since $v.d$ decreases until it reaches $\\delta(s, v)$.) The $|V|$ $\\text{EXTRACT-MIN}$s can be done in $O(E)$ total time, and decreasing a $d$ value during relaxation can be done in $O(1)$ time, for a total running time of $O(E)$. When $v.d$ decreases, just add $v$ to the front of the list in $Q[v.d]$. $\\text{EXTRACT-MIN}$ removes the head of the list in the first nonempty slot of $Q$. To do $\\text{EXTRACT-MIN}$ without scanning all of $Q$, keep track of the smallest $i$ for which $Q[i]$ is not empty. The key point is that when $v.d$ decreases due to relaxation of edge $(u, v)$, $v.d$ remains $u.d$, so it never moves to an earlier slot of $Q$ than the one that had $u$, the previous minimum. Thus $\\text{EXTRACT-MIN}$ can always scan upward in the array, taking a total of $O(E)$ time for all $\\text{EXTRACT-MIN}$s. b. For all $(u, v) \\in E$, we have $w_1(u, v) \\in \\{0, 1\\}$, so $\\delta_1(s, v) \\le |V| - 1 \\le |E|$. Use part (a) to get the $O(E)$ time bound. c. To show that $w_i(u, v) = 2w_{i - 1}(u, v)$ or $w_i(u, v) = 2w_{i - 1}(u, v) + 1$, observe that the $i$ bits of $w_i(u, v)$ consist of the $i - 1$ bits of $w_{i - 1}(u, v)$ followed by one more bit. If that low-order bit is $0$, then $w_i(u, v) = 2w_{i - 1}(u, v)$; if it is $1$, then $w_i(u, v) = 2w_{i - 1}(u, v) + 1$. Notice the following two properties of shortest paths: If all edge weights are multiplied by a factor of $c$, then all shortest-path weights are multiplied by $c$. If all edge weights are increased by at most $c$, then all shortest-path weights are increased by at most $c(|V| - 1)$, since all shortest paths have at most $|V| - 1$ edges. The lowest possible value for $w_i(u, v)$ is $2w_{i - 1}(u, v)$, so by the first observation, the lowest possible value for $\\delta_i(s, v)$ is $2\\delta_{i - 1}(s, v)$. The highest possible value for $w_i(u, v)$ is $2w_{i - 1}(u, v) + 1$. Therefore, using the two observations together, the highest possible value for $\\delta_i(s, v)$ is $2\\delta_{i - 1}(s, v) + |V| - 1$ d. We have $$ \\begin{aligned} \\hat w_i(u, v) & = w_i(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v) \\\\ & \\ge 2w_{i - 1}(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v) \\\\ & \\ge 0. \\end{aligned} $$ The second line follows from part (c), and the third line follows from Lemma 24.10: $\\delta_{i - 1}(s, v) \\le \\delta_{i - 1}(s, u) + w_{i - 1}(u, v)$. e. Observe that if we compute $\\hat w_i(p)$ for any path $p:u \\leadsto v$, the terms $\\delta_{i - 1}(s, t)$ cancel for every intermediate vertex $t$ on the path. Thus, $$\\hat w_i(p) = w_i(p) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v).$$ (This relationship will be shown in detail in equation ($\\text{25.10}$) within the proof of Lemma 25.1.) The $\\delta_{i - 1}$ terms depend only on $u$, $v$, and $s$, but not on the path $p$; therefore the same paths will be of minimum $w_i$ weight and of minimum $\\hat w_i$ weight between $u$ and $v$. Letting $u = s$, we get $$ \\begin{aligned} \\hat\\delta_i(s, v) & = \\delta_i(s, v) + 2\\delta_{i - 1}(s, s) - 2\\delta_{i - 1}(s, v) \\\\ & = \\delta_i(s, v) - 2\\delta_{i - 1}(s, v). \\end{aligned} $$ Rewriting this result as $\\delta_i(s, v) = \\hat\\delta_i(s, v) + 2\\delta_{i - 1}(s, v)$ and combining it with $\\delta_i(s, v) \\le 2\\delta_{i - 1}(s, v) + |V| - 1$ (from part (c)) gives us $\\hat\\delta_i(s, v) \\le |V| - 1 \\le |E|$. f. To compute $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ for all $v \\in V$ in $O(E)$ time: Compute the weights $\\hat w_i(u, v)$ in $O(E)$ time, as shown in part (d). By part (e), $\\hat\\delta_i(s, v) \\le |E|$, so use part (a) to compute all $\\hat\\delta_i(s, v)$ in $O(E)$ time. Compute all $\\delta_i(s, v)$ from $\\hat\\delta_i(s, v)$ and $\\delta_{i - 1}(s, v)$ as shown in part (e), in $O(V)$ time. To compute all $\\delta(s, v)$ in $O(E\\lg W)$ time: Compute $\\delta_1(s, v)$ for all $v \\in V$. As shown in part (b), this takes $O(E)$ time. For each $i = 2, 3, \\ldots, k$, compute all $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ in $O(E)$ time as shown above. This procedure computes $\\delta(s, v) = \\delta_k(u, v)$ in time $O(Ek) = O(E\\lg W)$.","title":"24-4 Gabow's scaling algorithm for single-source shortest paths"},{"location":"Chap24/Problems/24-5/","text":"Let $G = (V, E)$ be a directed graph with weight function $w: E \\to \\mathbb R$, and let $n = |V|$. We define the mean weight of a cycle $c = \\langle e_1, e_2, \\ldots, e_k \\rangle$ of edges in $E$ to be $$\\mu(c) = \\frac{1}{k} \\sum_{i = 1}^k w(e_i).$$ Let $\\mu^* = \\min_c \\mu(c)$, where $c$ ranges over all directed cycles in $G$. We call a cycle $c$ for which $\\mu(c) = \\mu^*$ a minimum mean-weight cycle . This problem investigates an efficient algorithm for computing $\\mu^*$. Assume without loss of generality that every vertex $v \\in V$ is reachable from a source vertex $s \\in V$. Let $\\delta(s, v)$ be the weight of a shortest path from $s$ to $v$, and let $\\delta_k(s, v)$ be the weight of a shortest path from $s$ to $v$ consisting of exactly $k$ edges. If there is no path from $s$ to $v$ with exactly $k$ edges, then $\\delta_k(s, v) = \\infty$. a. Show that if $\\mu^* = 0$, then $G$ contains no negative-weight cycles and $\\delta(s, v) = \\min_{0 \\le k \\le n - 1} \\delta_k(s, v)$ for all vertices $v \\in V$. b. Show that if $\\mu^* = 0$, then $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\ge 0$$ for all vertices $v \\in V$. ($\\textit{Hint:}$ Use both properties from part (a).) c. Let $c$ be a $0$-weight cycle, and let $u$ and $v$ be any two vertices on $c$. Suppose that $\\mu^* = 0$ and that the weight of the simple path from $u$ to $v$ along the cycle is $x$. Prove that $\\delta(s, v) = \\delta(s, u) + x$. ($\\textit{Hint:}$ The weight of the simple path from $v$ to $u$ along the cycle is $-x$.) d. Show that if $\\mu^* = 0$, then on each minimum mean-weight cycle there exists a vertex $v$ such that $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$ ($\\textit{Hint:}$ Show how to extend a shortest path to any vertex on a minimum meanweight cycle along the cycle to make a shortest path to the next vertex on the cycle.) e. Show that if $\\mu^* = 0$, then $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$ f. Show that if we add a constant $t$ to the weight of each edge of $G$, then $\\mu^*$ increases by $t$. Use this fact to show that $$\\mu^* = \\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k}.$$ g. Give an $O(VE)$-time algorithm to compute $\\mu^*$. a. If $\\mu^* = 0$, then we have that the lowest that $\\frac{1}{k}_{i = 1}^k w(e_i)$ can be zero. This means that the lowest $\\sum_{i = 1}^k w(e_i)$ can be $0$. This means that no cycle can have negative weight. Also, we know that for any path from $s$ to $v$, we can make it simple by removing any cycles that occur. This means that it had a weight equal to some path that has at most $n - 1$ edges in it. Since we take the minimum over all possible number of edges, we have the minimum over all paths. b. To show that $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\ge 0,$$ we need to show that $$\\max_{0 \\le k \\le n - 1} \\delta_n(s, v) - \\delta_k(s, v) \\ge 0.$$ Since we have that $\\mu^* = 0$, there aren't any negative weight cycles. This means that we can't have the minimum cost of a path decrease as we increase the possible length of the path past $n - 1$. This means that there will be a path that at least ties for cheapest when we restrict to the path being less than length $n$. Note that there may also be cheapest path of longer length since we necessarily do have zero cost cycles. However, this isn't guaranteed since the zero cost cycle may not lie along a cheapest path from $s$ to $v$. c. Since the total cost of the cycle is $0$, and one part of it has cost $x$, in order to balance that out, the weight of the rest of the cycle has to be $-x$. So, suppose we have some shortest length path from $s$ to $u$, then, we could traverse the path from $u$ to $v$ along the cycle to get a path from $s$ to $u$ that has length $\\delta(s, u) + x$. This gets us that $\\delta(s, v) \\le \\delta(s, u) + x$. To see the converse inequality, suppose that we have some shortest length path from $s$ to $v$. Then, we can traverse the cycle going from $v$ to $u$. We already said that this part of the cycle had total cost $-x$. This gets us that $\\delta(s, u) \\le \\delta(s, v) - x$. Or, rearranging, we have $\\delta(s, u) + x \\le \\delta(s, v)$. Since we have inequalities both ways, we must have equality. d. To see this, we find a vertex $v$ and natural number $k \\le n - 1$ so that $\\delta_n(s, v) - \\delta_k(s, v) = 0$. To do this, we will first take any shortest length, smallest number of edges path from $s$ to any vertex on the cycle. Then, we will just keep on walking around the cycle until we've walked along $n$ edges. Whatever vertex we end up on at that point will be our $v$. Since we did not change the $d$ value of $v$ after looking at length $n$ paths, by part (a), we know that there was some length of this path, say $k$, which had the same cost. That is, we have $\\delta_n(s, v) = \\delta_k(s,v)$. e. This is an immediate result of the previous problem and part (b). Part (a) says that the inequality holds for all $v$, so, we have $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta(s, v)}{n - k} \\ge 0.$$ The previous part says that there is some $v$ on each minimum weight cycle so that $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta(s, v)}{n - k} = 0,$$ which means that $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\le 0.$$ Putting the two inequalities together, we have the desired equality. f. If we add $t$ to the weight of each edge, the mean weight of any cycle becomes $$\\mu(c) = \\frac{1}{k} \\sum_{i = 1}^k (w(e_i) + t) = \\frac{1}{k} \\Big(\\sum_i^k w(e_i) \\Big) + \\frac{kt}{k} = \\frac{1}{k} \\Big(\\sum_i^k w(e_i) \\Big) + t.$$ This is the original, unmodified mean weight cycle, plus $t$. Since this is how the mean weight of every cycle is changed, the lowest mean weight cycle stays the lowest mean weight cycle. This means that $\\mu^*$ will increase by $t$. Suppose that we first compute $\\mu^*$. Then, we subtract from every edge weight the value $\\mu^*$. This will make the new $\\mu^*$ equal zero, which by part (e) means that $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$ Since they are both equal to zero, they are both equal to each other. g. By the previous part, it suffices to compute the expression on the previ- ous line. We will start by creating a table that lists $\\delta_k(s, v)$ for every $k \\in \\{1, \\ldots, n\\}$ and $v \\in V$. This can be done in time $O(V(E + V))$ by creating a $|V|$ by $|V|$ table, where the $k$th row and vth column represent $\\delta)k(s, v)$ when wanting to compute a particular entry, we need look at a number of entries in the previous row equal to the in degree of the vertex we want to compute. So, summing over the computation required for each row, we need $O(E + V)$. Note that this total runtime can be bumped down to $O(VE)$ by not including in the table any isolated vertices, this will ensure that $E \\in \\Omega(V)$. So, $O(V(E + V))$ becomes $O(VE)$. Once we have this table of values computed, it is simple to just replace each row with the last row minus what it was, and divide each entry by $n - k$, then, find the min column in each row, and take the max of those numbers.","title":"24-5 Karp's minimum mean-weight cycle algorithm"},{"location":"Chap24/Problems/24-6/","text":"A sequence is bitonic if it monotonically increases and then monotonically decreases, or if by a circular shift it monotonically increases and then monotonically decreases. For example the sequences $\\langle 1, 4, 6, 8, 3, -2 \\rangle$, $\\langle 9, 2, -4, -10, -5 \\rangle$, and $\\langle 1, 2, 3, 4 \\rangle$ are bitonic, but $\\langle 1, 3, 12, 4, 2, 10 \\rangle$ is not bitonic. (See Problem 15-3 for the bitonic euclidean traveling-salesman problem.) Suppose that we are given a directed graph $G = (V, E)$ with weight function $w: E \\to \\mathbb R$, where all edge weights are unique, and we wish to find single-source shortest paths from a source vertex $s$. We are given one additional piece of information: for each vertex $v \\in V$, the weights of the edges along any shortest path from $s$ to $v$ form a bitonic sequence. Give the most efficient algorithm you can to solve this problem, and analyze its running time. Observe that a bitonic sequence can increase, then decrease, then increase, or it can decrease, then increase, then decrease. That is, there can be at most two changes of direction in a bitonic sequence. Any sequence that increases, then decreases, then increases, then decreases has a bitonic sequence as a subsequence. Now, let us suppose that we had an even stronger condition than the bitonic property given in the problem: for each vertex $v \\in V$, the weights of the edges along any shortest path from $s$ to $v$ are increasing. Then we could call $\\text{INITIALIZE-SINGLE-SOURCE}$ and then just relax all edges one time, going in increasing order of weight. Then the edges along every shortest path would be relaxed in order of their appearance on the path. (We rely on the uniqueness of edge weights to ensure that the ordering is correct.) The path-relaxation property (Lemma 24.15) would guarantee that we would have computed correct shortest paths from $s$ to each vertex. If we weaken the condition so that the weights of the edges along any shortest path increase and then decrease, we could relax all edges one time, in increasing order of weight, and then one more time, in decreasing order of weight. That order, along with uniqueness of edge weights, would ensure that we had relaxed the edges of every shortest path in order, and again the path-relaxation property would guarantee that we would have computed correct shortest paths. To make sure that we handle all bitonic sequences, we do as suggested above. That is, we perform four passes, relaxing each edge once in each pass. The first and third passes relax edges in increasing order of weight, and the second and fourth passes in decreasing order. Again, by the path-relaxation property and the uniqueness of edge weights, we have computed correct shortest paths. The total time is $O(V + E\\lg V)$, as follows. The time to sort $|E|$ edges by weight is $O(E\\lg E) = O(E\\lg V)$ (since $|E| = O(V^2)$). $\\text{INITIALIZE-SINGLE-SOURCE}$ takes $O(V)$ time. Each of the four passes takes $O(E)$ time. Thus, the total time is $O(E\\lg V + V + E) = O(V + E\\lg V)$.","title":"24-6 Bitonic shortest paths"},{"location":"Chap25/25.1/","text":"25.1-1 Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the weighted, directed graph of Figure 25.2, showing the matrices that result for each iteration of the loop. Then do the same for $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$. Initial: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & \\infty & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & 3 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ Slow: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 3$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -2 & -3 & 0 & -1 & 2 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ Fast: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 8$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ 25.1-2 Why do we require that $w_{ii} = 0$ for all $1 \\le i \\le n$? This is consistent with the fact that the shortest path from a vertex to itself is the empty path of weight $0$. If there were another path of weight less than $0$ then it must be a negative-weight cycle, since it starts and ends at $v_i$. 25.1-3 What does the matrix $$ L^{(0)} = \\begin{pmatrix} 0 & \\infty & \\infty & \\cdots & \\infty \\\\ \\infty & 0 & \\infty & \\cdots & \\infty \\\\ \\infty & \\infty & 0 & \\cdots & \\infty \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\infty & \\infty & \\infty & \\cdots & 0 \\end{pmatrix} $$ used in the shortest-paths algorithms correspond to in regular matrix multiplication? The matrix $L^{(0)}$ corresponds to the identity matrix $$ I = \\begin{pmatrix} 1 & 0 & 0 & \\cdots & 0 \\\\ 0 & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 \\end{pmatrix} $$ of regular matrix multiplication. Substitute $0$ (the identity for $+$) for $\\infty$ (the identity for $\\min$), and $1$ (the identity for $\\cdot$) for $0$ (the identity for $+$). 25.1-4 Show that matrix multiplication defined by $\\text{EXTEND-SHORTEST-PATHS}$ is associative. To verify associativity, we need to check that $(W^iW^j)W^p = W^i(W^jW^p)$ for all $i$, $j$ and $p$, where we use the matrix multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Consider entry $(a, b)$ of the left hand side. This is: $$ \\begin{aligned} \\min_{1 \\le k \\le n} [W^iW^j]_{a, k} + W_{k, b}^p & = \\min_{1 \\le k \\le n} \\min_{1 \\le q \\le n} W_{a, q}^i + W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + \\min_{1 \\le k \\le n} W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + [W^jW^p]_{q, b}, \\end{aligned} $$ which is precisely entry $(a, b)$ of the right hand side. 25.1-5 Show how to express the single-source shortest-paths problem as a product of matrices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1). The all-pairs shortest-paths algorithm in Section 25.1 computes $$L^{(n - 1)} = W^{n - 1} = L^{(0)} \\cdot W^{n - 1},$$ where $l_{ij}^{(n - 1)} = \\delta(i, j)$ and $L^{(0)}$ is the identity matrix. That is, the entry in the $i$th row and $j$th column of the matrix \"product\" is the shortest-path distance from vertex $i$ to vertex $j$, and row $i$ of the product is the solution to the single-source shortest-paths problem for vertex $i$. Notice that in a matrix \"product\" $C = A \\cdot B$, the $i$th row of $C$ is the $i$th row of $A$ \"multiplied\" by $B$. Since all we want is the $i$th row of $C$, we never need more than the $i$th row of $A$. Thus the solution to the single-source shortest-paths from vertex $i$ is $L_i^{(0)} \\cdot W^{n - 1}$, where $L_i^{(0)}$ is the $i$th row of $L^{(0)}$\u2014a vector whose $i$th entry is $0$ and whose other entries are $\\infty$. Doing the above \"multiplications\" starting from the left is essentially the same as the $\\text{BELLMAN-FORD}$ algorithm. The vector corresponds to the $d$ values in $\\text{BELLMAN-FORD}$\u2014the shortest-path estimates from the source to each vertex. The vector is initially $0$ for the source and $\\infty$ for all other vertices, the same as the values set up for $d$ by $\\text{INITIALIZE-SINGLE-SOURCE}$. Each \"multiplication\" of the current vector by $W$ relaxes all edges just as $\\text{BELLMAN-FORD}$ does. That is, a distance estimate in the row, say the distance to $v$, is updated to a smaller estimate, if any, formed by adding some $w(u, v)$ to the current estimate of the distance to $u$. The relaxation/multiplication is done $n - 1$ times. 25.1-6 Suppose we also wish to compute the vertices on shortest paths in the algorithms of this section. Show how to compute the predecessor matrix $\\prod$ from the completed matrix $L$ of shortest-path weights in $O(n^3)$ time. For each source vertex $v_i$ we need to compute the shortest-paths tree for $v_i$. To do this, we need to compute the predecessor for each $j \\ne i$. For fixed $i$ and $j$, this is the value of $k$ such that $L_{i, k} + w(k, j) = L[i, j]$. Since there are $n$ vertices whose trees need computing, $n$ vertices for each such tree whose predecessors need computing, and it takes $O(n)$ to compute this for each one (checking each possible $k$), the total time is $O(n^3)$. 25.1-7 We can also compute the vertices on shortest paths as we compute the shortestpath weights. Define $\\pi_{ij}^{(m)}$ as the predecessor of vertex $j$ on any minimum-weight path from $i$ to $j$ that contains at most $m$ edges. Modify the $\\text{EXTEND-SHORTESTPATHS}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ procedures to compute the matrices$\\prod^{(1)}, \\prod^{(2)}, \\ldots, \\prod^{(n - 1)}$ as the matrices $L^{(1)}, L^{(2)}, \\ldots, L^{(n - 1)}$ are computed. To have the procedure compute the predecessor along the shortest path, see the modified procedures, $\\text{EXTEND-SHORTEST-PATH-MOD}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD}$ 1 2 3 4 5 6 7 8 9 10 11 12 13 EXTEND - SHORTEST - PATH - MOD ( \u220f , L , W ) n = L . row let L ' = l ' [ i , j ] be a new n \u00d7 n matirx \u220f' = \u03c0' [ i , j ] is a new n \u00d7 n matrix for i = 1 to n for j = 1 to n l ' [ i , j ] = \u221e \u03c0 [ i , j ] = NIL for k = 1 to n if l [ i , k ] + l [ j , k ] < l [ i , j ] l [ i , j ] = l [ i , k ] + l [ j , k ] \u03c0' [ i , j ] = \u03c0 [ k , j ] return ( \u220f' , L ' ) 1 2 3 4 5 6 7 SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD(W) n = W.rows L(1) = W \u220f(1) = \u03c0[i, j](1) where \u03c0[i, j](1) = i if there is an edge from i to j, and NIL otherwise for m = 2 to n - 1 \u220f(m), L(m) = EXTEND-SHORTEST-PATH-MOD(\u220f(m - 1), L(m - 1), W) return (\u220f(n - 1), L(n - 1)) 25.1-8 The $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ procedure, as written, requires us to store $\\lceil \\lg(n - 1) \\rceil$ matrices, each with $n^2$ elements, for a total space requirement of $\\Theta(n^2\\lg n)$. Modify the procedure to require only $\\Theta(n^2)$ space by using only two $n \\times n$ matrices. We can overwrite matrices as we go. Let $A \\star B$ denote multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Then we modify $\\text{FASTER-ALL-EXTEND-SHORTEST-PATHS}(W)$. We initially create an $n$ by $n$ matrix $L$. Delete line 5 of the algorithm, and change line 6 to $L = W \\star W$, followed by $W = L$. 25.1-9 Modify $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ so that it can determine whether the graph contains a negative-weight cycle. For the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2k + 1)}$ where $2^k > n - 1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n - 1$ edges. However, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2k \\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm. 25.1-10 Give an efficient algorithm to find the length (number of edges) of a minimum-length negative-weight cycle in a graph. Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the graph. Look at the diagonal elements of $L^{(m)}$. Return the first value of $m$ for which one (or more) of the diagonal elements ($l_{ii}^{(m)}$) is negative. If $m$ reaches $n + 1$, then stop and declare that there are no negative-weight cycles. Let the number of edges in a minimum-length negative-weight cycle be $m^*$, where $m^* = \\infty$ if the graph has no negative-weight cycles. Correctness Let's assume that for some value $m^* \\le n$ and some value of $i$, we find that $l_{ii}^{m^*} < 0$. Then the graph has a cycle with $m^*$ edges that goes from vertex $i$ to itself, and this cycle has negative weight (stored in $l_{ii}^{m^*}$). This is the minimum-length negative-weight cycle because $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ computes all paths of $1$ edge, then all paths of $2$ edges, and so on, and all cycles shorter than $m^*$ edges were checked before and did not have negative weight. Now assume that for all $m \\le n$, there is no negative $l_{ii}^{(m)}$ element. Then, there is no negativeweight cycle in the graph, because all cycles have length at most $n$. Time $O(n^4)$. More precisely, $\\Theta(n^3 \\cdot \\min(n, m^*))$. Faster solution Run $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ on the graph until the first time that the matrix $L^{(m)}$ has one or more negative values on the diagonal, or until we have computed $L^{(m)}$ for some $m > n$. If we find any negative entries on the diagonal, we know that the minimum-length negative-weight cycle has more than $m / 2$ edges and at most $m$ edges. We just need to binary search for the value of $m^*$ in the range $m / 2 < m^* \\le m$. The key observation is that on our way to computing $L^{(m)}$ , we computed $L^{(1)}, L^{(2)}, L^{(4)}, L^{(8)}, \\ldots, L^{(m / 2)}$, and these matrices suffice to compute every matrix we'll need. Here's pseudocode: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 FIND - MIN - LENGTH - NEG - WEIGHT - CYCLE ( W ) n = W . rows L ( 1 ) = W m = 1 whiel m \u2264 n and no diagonal entries of L ( m ) are negative L ( 2 m ) = EXTEND - SHORTEST - PATHS ( L ( m ), L ( m )) m = 2 m if m > n and no diagonal entries of L ( m ) are negative return \"no negative-weight cycles\" eles if m \u2264 2 return m else low = m / 2 high = m d = m / 4 while d \u2265 1 s = low + d L ( s ) = EXTEND - SHORTEST - PATHS ( L ( low ), L ( d )) if L ( s ) has any negative entries on the diagonal high = s else low = s d = d / 2 return high Correctness If, after the first while loop, $m > n$ and no diagonal entries of $L^{(m)}$ are negative, then there is no negative-weight cycle. Otherwise, if $m \\le 2$, then either $m = 1$ or $m = 2$, and $L^{(m)}$ is the first matrix with a negative entry on the diagonal. Thus, the correct value to return is $m$. If $m > 2$, then we maintain an interval bracketed by the values $low$ and $high$, such that the correct value $m^*$ is in the range $low < m^* \\le high$. We use the following loop invariant: Loop invariant: At the start of each iteration of the \" while $d \\ge 1$\" loop, $d = 2^p$ for some integer $p \\ge -1$, $d = (high - low) / 2$, $low < m^* \\le high$. Initialization: Initially, $m$ is an integer power of $2$ and $m > 2$. Since $d = m / 4$, we have that $d$ is an integer power of $2$ and $d > 1 / 2$, so that $d = 2^p$ for some integer $p \\ge 0$. We also have $$(high - low) / 2 = (m - (m / 2)) / 2 = m / 4 = d.$$ Finally, $L^{(m)}$ has a negative entry on the diagonal and $L^{(m / 2)}$ does not. Since $low = m / 2$ and $high = m$, we have that $low < m^* \\le high$. Maintenance: We use $high$, $low$, and $d$ to denote variable values in a given iteration, and $high'$, $low'$, and $d'$ to denote the same variable values in the next iteration. Thus, we wish to show that $d = 2^p$ for some integer $p \\ge -1$ implies $d' = 2^p$ for some integer $p' \\ge -1$, that $d = (high - low) / 2$ implies $d' = (high' - low') / 2$, and that $low < m^* \\le high$ implies $low' < m^* \\le high'$. To see that $d' = 2^{p'}$, note that $d' = d / 2$, and so $d = 2^{p - 1}$. The condition that $d \\ge 1$ implies that $p \\ge 0$, and so $p' \\ge -1$. Within each iteration, $s$ is set to $low + d$, and one of the following actions occurs: If $L^{(s)}$ has any negative entries on the diagonal, then $high'$ is set to s and $d'$ is set to $d / 2$. Upon entering the next iteration, $$(high' - low') / 2 = (s - low') / 2 = ((low + d) - low) / 2 = d / 2 = d'.$$ Since $L^{(s)}$ has a negative diagonal entry, we know that $m^* \\le s$. Because $high' = s$ and $low'= low$, we have that $low' < m^* \\le high'$. If $L^{(s)}$ has no negative entries on the diagonal, then $low'$ is set to $s$, and $d'$ is set to $d / 2$. Upon entering the next iteration, $$(high' - low') / 2 = (high' - s) / 2 = (high - (low + d)) / 2 = (high - low) / 2 - d / 2 = d - d / 2 = d / 2 = d'.$$ Since $L^{(s)}$ has no negative diagonal entries, we know that $m^* > s$. Because $low' = s$ and $high' = high$, we have that $low' < m^* \\le high'$. Termination: At termination, $d < 1$. Since $d = 2^p$ for some integer $p \\ge -1$, we must have $p = -1$, so that $d = 1 / 2$. By the second part of the loop invariant, if we multiply both sides by $2$, we get that $high - low = 2d = 1$. By the third part of the loop invariant, we know that $low < m^* \\le high$. Since $high - low = 2d = 1$ and $m^* > low$, the only possible value for $m^*$ is high, which the procedure returns. Time If there is no negative-weight cycle, the first while loop iterates $\\Theta(\\lg n)$ times, and the total time is $\\Theta(n^3\\lg n)$. Now suppose that there is a negative-weight cycle. We claim that each time we call $\\text{EXTEND-SHORTEST-PATHS}(L^{(low)}, L^{(d)})$, we have already computed $L^{(low)}$ and $L^{(d)}$. Initially, since $low = m / 2$, we had already computed $L^{(low)}$ in the first while loop. In succeeding iterations of the second while loop, the only way that low changes is when it gets the value of $s$, and we have just computed $L^{(s)}$. As for $L^{(d)}$, observe that $d$ takes on the values $m / 4$, $m / 8$, $m / 16$, $\\ldots$, $1$, and again, we computed all of these $L$ matrices in the first while loop. Thus, the claim is proven. Each of the two while loops iterates $\\Theta(\\lg m^*)$ times. Since we have already computed the parameters to each call of $\\text{EXTEND-SHORTEST-PATHS}$, each iteration is dominated by the $\\Theta(n^3)$-time call to $\\text{EXTEND-SHORTEST-PATHS}$. Thus, the total time is $\\Theta(n^3\\lg m^*)$. In general, therefore, the running time is $\\Theta(n^3\\lg\\min(n, m^*))$. Space The slower algorithm needs to keep only three matrices at any time, and so its space requirement is $\\Theta(n^3)$. This faster algorithm needs to maintain $\\Theta(\\lg\\min(n, m^*))$ matrices, and so the space requirement increases to $\\Theta(n^3\\lg\\min(n, m^*))$.","title":"25.1 Shortest paths and matrix multiplication"},{"location":"Chap25/25.1/#251-1","text":"Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the weighted, directed graph of Figure 25.2, showing the matrices that result for each iteration of the loop. Then do the same for $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$. Initial: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & \\infty & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & 3 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ Slow: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 3$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -2 & -3 & 0 & -1 & 2 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ Fast: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 8$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$","title":"25.1-1"},{"location":"Chap25/25.1/#251-2","text":"Why do we require that $w_{ii} = 0$ for all $1 \\le i \\le n$? This is consistent with the fact that the shortest path from a vertex to itself is the empty path of weight $0$. If there were another path of weight less than $0$ then it must be a negative-weight cycle, since it starts and ends at $v_i$.","title":"25.1-2"},{"location":"Chap25/25.1/#251-3","text":"What does the matrix $$ L^{(0)} = \\begin{pmatrix} 0 & \\infty & \\infty & \\cdots & \\infty \\\\ \\infty & 0 & \\infty & \\cdots & \\infty \\\\ \\infty & \\infty & 0 & \\cdots & \\infty \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\infty & \\infty & \\infty & \\cdots & 0 \\end{pmatrix} $$ used in the shortest-paths algorithms correspond to in regular matrix multiplication? The matrix $L^{(0)}$ corresponds to the identity matrix $$ I = \\begin{pmatrix} 1 & 0 & 0 & \\cdots & 0 \\\\ 0 & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 \\end{pmatrix} $$ of regular matrix multiplication. Substitute $0$ (the identity for $+$) for $\\infty$ (the identity for $\\min$), and $1$ (the identity for $\\cdot$) for $0$ (the identity for $+$).","title":"25.1-3"},{"location":"Chap25/25.1/#251-4","text":"Show that matrix multiplication defined by $\\text{EXTEND-SHORTEST-PATHS}$ is associative. To verify associativity, we need to check that $(W^iW^j)W^p = W^i(W^jW^p)$ for all $i$, $j$ and $p$, where we use the matrix multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Consider entry $(a, b)$ of the left hand side. This is: $$ \\begin{aligned} \\min_{1 \\le k \\le n} [W^iW^j]_{a, k} + W_{k, b}^p & = \\min_{1 \\le k \\le n} \\min_{1 \\le q \\le n} W_{a, q}^i + W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + \\min_{1 \\le k \\le n} W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + [W^jW^p]_{q, b}, \\end{aligned} $$ which is precisely entry $(a, b)$ of the right hand side.","title":"25.1-4"},{"location":"Chap25/25.1/#251-5","text":"Show how to express the single-source shortest-paths problem as a product of matrices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1). The all-pairs shortest-paths algorithm in Section 25.1 computes $$L^{(n - 1)} = W^{n - 1} = L^{(0)} \\cdot W^{n - 1},$$ where $l_{ij}^{(n - 1)} = \\delta(i, j)$ and $L^{(0)}$ is the identity matrix. That is, the entry in the $i$th row and $j$th column of the matrix \"product\" is the shortest-path distance from vertex $i$ to vertex $j$, and row $i$ of the product is the solution to the single-source shortest-paths problem for vertex $i$. Notice that in a matrix \"product\" $C = A \\cdot B$, the $i$th row of $C$ is the $i$th row of $A$ \"multiplied\" by $B$. Since all we want is the $i$th row of $C$, we never need more than the $i$th row of $A$. Thus the solution to the single-source shortest-paths from vertex $i$ is $L_i^{(0)} \\cdot W^{n - 1}$, where $L_i^{(0)}$ is the $i$th row of $L^{(0)}$\u2014a vector whose $i$th entry is $0$ and whose other entries are $\\infty$. Doing the above \"multiplications\" starting from the left is essentially the same as the $\\text{BELLMAN-FORD}$ algorithm. The vector corresponds to the $d$ values in $\\text{BELLMAN-FORD}$\u2014the shortest-path estimates from the source to each vertex. The vector is initially $0$ for the source and $\\infty$ for all other vertices, the same as the values set up for $d$ by $\\text{INITIALIZE-SINGLE-SOURCE}$. Each \"multiplication\" of the current vector by $W$ relaxes all edges just as $\\text{BELLMAN-FORD}$ does. That is, a distance estimate in the row, say the distance to $v$, is updated to a smaller estimate, if any, formed by adding some $w(u, v)$ to the current estimate of the distance to $u$. The relaxation/multiplication is done $n - 1$ times.","title":"25.1-5"},{"location":"Chap25/25.1/#251-6","text":"Suppose we also wish to compute the vertices on shortest paths in the algorithms of this section. Show how to compute the predecessor matrix $\\prod$ from the completed matrix $L$ of shortest-path weights in $O(n^3)$ time. For each source vertex $v_i$ we need to compute the shortest-paths tree for $v_i$. To do this, we need to compute the predecessor for each $j \\ne i$. For fixed $i$ and $j$, this is the value of $k$ such that $L_{i, k} + w(k, j) = L[i, j]$. Since there are $n$ vertices whose trees need computing, $n$ vertices for each such tree whose predecessors need computing, and it takes $O(n)$ to compute this for each one (checking each possible $k$), the total time is $O(n^3)$.","title":"25.1-6"},{"location":"Chap25/25.1/#251-7","text":"We can also compute the vertices on shortest paths as we compute the shortestpath weights. Define $\\pi_{ij}^{(m)}$ as the predecessor of vertex $j$ on any minimum-weight path from $i$ to $j$ that contains at most $m$ edges. Modify the $\\text{EXTEND-SHORTESTPATHS}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ procedures to compute the matrices$\\prod^{(1)}, \\prod^{(2)}, \\ldots, \\prod^{(n - 1)}$ as the matrices $L^{(1)}, L^{(2)}, \\ldots, L^{(n - 1)}$ are computed. To have the procedure compute the predecessor along the shortest path, see the modified procedures, $\\text{EXTEND-SHORTEST-PATH-MOD}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD}$ 1 2 3 4 5 6 7 8 9 10 11 12 13 EXTEND - SHORTEST - PATH - MOD ( \u220f , L , W ) n = L . row let L ' = l ' [ i , j ] be a new n \u00d7 n matirx \u220f' = \u03c0' [ i , j ] is a new n \u00d7 n matrix for i = 1 to n for j = 1 to n l ' [ i , j ] = \u221e \u03c0 [ i , j ] = NIL for k = 1 to n if l [ i , k ] + l [ j , k ] < l [ i , j ] l [ i , j ] = l [ i , k ] + l [ j , k ] \u03c0' [ i , j ] = \u03c0 [ k , j ] return ( \u220f' , L ' ) 1 2 3 4 5 6 7 SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD(W) n = W.rows L(1) = W \u220f(1) = \u03c0[i, j](1) where \u03c0[i, j](1) = i if there is an edge from i to j, and NIL otherwise for m = 2 to n - 1 \u220f(m), L(m) = EXTEND-SHORTEST-PATH-MOD(\u220f(m - 1), L(m - 1), W) return (\u220f(n - 1), L(n - 1))","title":"25.1-7"},{"location":"Chap25/25.1/#251-8","text":"The $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ procedure, as written, requires us to store $\\lceil \\lg(n - 1) \\rceil$ matrices, each with $n^2$ elements, for a total space requirement of $\\Theta(n^2\\lg n)$. Modify the procedure to require only $\\Theta(n^2)$ space by using only two $n \\times n$ matrices. We can overwrite matrices as we go. Let $A \\star B$ denote multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Then we modify $\\text{FASTER-ALL-EXTEND-SHORTEST-PATHS}(W)$. We initially create an $n$ by $n$ matrix $L$. Delete line 5 of the algorithm, and change line 6 to $L = W \\star W$, followed by $W = L$.","title":"25.1-8"},{"location":"Chap25/25.1/#251-9","text":"Modify $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ so that it can determine whether the graph contains a negative-weight cycle. For the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2k + 1)}$ where $2^k > n - 1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n - 1$ edges. However, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2k \\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm.","title":"25.1-9"},{"location":"Chap25/25.1/#251-10","text":"Give an efficient algorithm to find the length (number of edges) of a minimum-length negative-weight cycle in a graph. Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the graph. Look at the diagonal elements of $L^{(m)}$. Return the first value of $m$ for which one (or more) of the diagonal elements ($l_{ii}^{(m)}$) is negative. If $m$ reaches $n + 1$, then stop and declare that there are no negative-weight cycles. Let the number of edges in a minimum-length negative-weight cycle be $m^*$, where $m^* = \\infty$ if the graph has no negative-weight cycles. Correctness Let's assume that for some value $m^* \\le n$ and some value of $i$, we find that $l_{ii}^{m^*} < 0$. Then the graph has a cycle with $m^*$ edges that goes from vertex $i$ to itself, and this cycle has negative weight (stored in $l_{ii}^{m^*}$). This is the minimum-length negative-weight cycle because $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ computes all paths of $1$ edge, then all paths of $2$ edges, and so on, and all cycles shorter than $m^*$ edges were checked before and did not have negative weight. Now assume that for all $m \\le n$, there is no negative $l_{ii}^{(m)}$ element. Then, there is no negativeweight cycle in the graph, because all cycles have length at most $n$. Time $O(n^4)$. More precisely, $\\Theta(n^3 \\cdot \\min(n, m^*))$. Faster solution Run $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ on the graph until the first time that the matrix $L^{(m)}$ has one or more negative values on the diagonal, or until we have computed $L^{(m)}$ for some $m > n$. If we find any negative entries on the diagonal, we know that the minimum-length negative-weight cycle has more than $m / 2$ edges and at most $m$ edges. We just need to binary search for the value of $m^*$ in the range $m / 2 < m^* \\le m$. The key observation is that on our way to computing $L^{(m)}$ , we computed $L^{(1)}, L^{(2)}, L^{(4)}, L^{(8)}, \\ldots, L^{(m / 2)}$, and these matrices suffice to compute every matrix we'll need. Here's pseudocode: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 FIND - MIN - LENGTH - NEG - WEIGHT - CYCLE ( W ) n = W . rows L ( 1 ) = W m = 1 whiel m \u2264 n and no diagonal entries of L ( m ) are negative L ( 2 m ) = EXTEND - SHORTEST - PATHS ( L ( m ), L ( m )) m = 2 m if m > n and no diagonal entries of L ( m ) are negative return \"no negative-weight cycles\" eles if m \u2264 2 return m else low = m / 2 high = m d = m / 4 while d \u2265 1 s = low + d L ( s ) = EXTEND - SHORTEST - PATHS ( L ( low ), L ( d )) if L ( s ) has any negative entries on the diagonal high = s else low = s d = d / 2 return high Correctness If, after the first while loop, $m > n$ and no diagonal entries of $L^{(m)}$ are negative, then there is no negative-weight cycle. Otherwise, if $m \\le 2$, then either $m = 1$ or $m = 2$, and $L^{(m)}$ is the first matrix with a negative entry on the diagonal. Thus, the correct value to return is $m$. If $m > 2$, then we maintain an interval bracketed by the values $low$ and $high$, such that the correct value $m^*$ is in the range $low < m^* \\le high$. We use the following loop invariant: Loop invariant: At the start of each iteration of the \" while $d \\ge 1$\" loop, $d = 2^p$ for some integer $p \\ge -1$, $d = (high - low) / 2$, $low < m^* \\le high$. Initialization: Initially, $m$ is an integer power of $2$ and $m > 2$. Since $d = m / 4$, we have that $d$ is an integer power of $2$ and $d > 1 / 2$, so that $d = 2^p$ for some integer $p \\ge 0$. We also have $$(high - low) / 2 = (m - (m / 2)) / 2 = m / 4 = d.$$ Finally, $L^{(m)}$ has a negative entry on the diagonal and $L^{(m / 2)}$ does not. Since $low = m / 2$ and $high = m$, we have that $low < m^* \\le high$. Maintenance: We use $high$, $low$, and $d$ to denote variable values in a given iteration, and $high'$, $low'$, and $d'$ to denote the same variable values in the next iteration. Thus, we wish to show that $d = 2^p$ for some integer $p \\ge -1$ implies $d' = 2^p$ for some integer $p' \\ge -1$, that $d = (high - low) / 2$ implies $d' = (high' - low') / 2$, and that $low < m^* \\le high$ implies $low' < m^* \\le high'$. To see that $d' = 2^{p'}$, note that $d' = d / 2$, and so $d = 2^{p - 1}$. The condition that $d \\ge 1$ implies that $p \\ge 0$, and so $p' \\ge -1$. Within each iteration, $s$ is set to $low + d$, and one of the following actions occurs: If $L^{(s)}$ has any negative entries on the diagonal, then $high'$ is set to s and $d'$ is set to $d / 2$. Upon entering the next iteration, $$(high' - low') / 2 = (s - low') / 2 = ((low + d) - low) / 2 = d / 2 = d'.$$ Since $L^{(s)}$ has a negative diagonal entry, we know that $m^* \\le s$. Because $high' = s$ and $low'= low$, we have that $low' < m^* \\le high'$. If $L^{(s)}$ has no negative entries on the diagonal, then $low'$ is set to $s$, and $d'$ is set to $d / 2$. Upon entering the next iteration, $$(high' - low') / 2 = (high' - s) / 2 = (high - (low + d)) / 2 = (high - low) / 2 - d / 2 = d - d / 2 = d / 2 = d'.$$ Since $L^{(s)}$ has no negative diagonal entries, we know that $m^* > s$. Because $low' = s$ and $high' = high$, we have that $low' < m^* \\le high'$. Termination: At termination, $d < 1$. Since $d = 2^p$ for some integer $p \\ge -1$, we must have $p = -1$, so that $d = 1 / 2$. By the second part of the loop invariant, if we multiply both sides by $2$, we get that $high - low = 2d = 1$. By the third part of the loop invariant, we know that $low < m^* \\le high$. Since $high - low = 2d = 1$ and $m^* > low$, the only possible value for $m^*$ is high, which the procedure returns. Time If there is no negative-weight cycle, the first while loop iterates $\\Theta(\\lg n)$ times, and the total time is $\\Theta(n^3\\lg n)$. Now suppose that there is a negative-weight cycle. We claim that each time we call $\\text{EXTEND-SHORTEST-PATHS}(L^{(low)}, L^{(d)})$, we have already computed $L^{(low)}$ and $L^{(d)}$. Initially, since $low = m / 2$, we had already computed $L^{(low)}$ in the first while loop. In succeeding iterations of the second while loop, the only way that low changes is when it gets the value of $s$, and we have just computed $L^{(s)}$. As for $L^{(d)}$, observe that $d$ takes on the values $m / 4$, $m / 8$, $m / 16$, $\\ldots$, $1$, and again, we computed all of these $L$ matrices in the first while loop. Thus, the claim is proven. Each of the two while loops iterates $\\Theta(\\lg m^*)$ times. Since we have already computed the parameters to each call of $\\text{EXTEND-SHORTEST-PATHS}$, each iteration is dominated by the $\\Theta(n^3)$-time call to $\\text{EXTEND-SHORTEST-PATHS}$. Thus, the total time is $\\Theta(n^3\\lg m^*)$. In general, therefore, the running time is $\\Theta(n^3\\lg\\min(n, m^*))$. Space The slower algorithm needs to keep only three matrices at any time, and so its space requirement is $\\Theta(n^3)$. This faster algorithm needs to maintain $\\Theta(\\lg\\min(n, m^*))$ matrices, and so the space requirement increases to $\\Theta(n^3\\lg\\min(n, m^*))$.","title":"25.1-10"},{"location":"Chap25/25.2/","text":"25.2-1 Run the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2. Show the matrix $D^{(k)}$ that results for each iteration of the outer loop. $k = 1$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ $k = 2$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & - 8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 3$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 4$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 6$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ 25.2-2 Show how to compute the transitive closure using the technique of Section 25.1. We set $w_{ij} = 1$ if $(i, j)$ is an edge, and $w_{ij} = 0$ otherwise. Then we replace line 7 of $\\text{EXTEND-SHORTEST-PATHS}(L, W)$ by $l''_{ij} = l''_{ij} \\lor (l_{ik} \\land w_{kj})$. Then run the $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ algorithm. 25.2-3 Modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\prod^{(k)}$ matrices according to equations $\\text{(25.6)}$ and $\\text{(25.7)}$. Prove rigorously that for all $i \\in V$, the predecessor subgraph $G_{\\pi, i}$ is a shortest-paths tree with root $i$. ($\\textit{Hint:}$ To show that $G_{\\pi, i}$ is acyclic, first show that $\\pi_{ij}^{(k)} = l$ implies $d_{ij}^{(k)} \\ge d_{il}^{(k)} + w_{lj}$, according to the definition of $\\pi_{ij}^{(k)}$. Then, adapt the proof of Lemma 23.16.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 MOD - FLOYD - WARSHALL ( W ) n = W . rows D ( 0 ) = W let \u03c0 ( 0 ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if i != j and D [ i , j ]( 0 ) < \u221e \u03c0 [ i , j ]( 0 ) = i for k = 1 to n let D ( k ) be a new n \u00d7 n matrix let \u03c0 ( k ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if d [ i , j ]( k - 1 ) \u2264 d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) d [ i , j ]( k ) = d [ i , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ i , j ]( k - 1 ) else d [ i , j ]( k ) = d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ k , j ]( k - 1 ) In order to have that $\\pi^{(k)}_{ij} = l$, we need that $d^{(k)}_{ij} \\ge d^{(k)}_{il} + w_{lj}$. To see this fact, we will note that having $\\pi^{(k)}_{ij} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \\le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality around, and if we ever get some cycle, $i_1, i_2, \\ldots, i_c$, then we would have that $d_{ii_1} \\le d_{ii_1} + w_{i_1i_2} + w_{i_2i_3} + \\cdots + w_{i_ci_1}$. So, if we subtract the common term sfrom both sides, we get that $0 \\le w_{i_ci_1} + \\sum_{q = 1}^{c - 1} w_{i_qi_{q + 1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation $\\text{(25.7)}$. 25.2-4 As it appears above, the Floyd-Warshall algorithm requires $\\Theta(n^3)$ space, since we compute $d_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$. Show that the following procedure, which simply drops all the superscripts, is correct, and thus only $\\Theta(n^2)$ space is required. 1 2 3 4 5 6 7 8 FLOYD - WARSHALL ' ( W ) n = W . rows D = W for k = 1 to n for i = 1 to n for j = 1 to n d [ i , j ] = min ( d [ i , j ], d [ i , k ] + d [ k , j ]) return D With the superscripts, the computation is $d_{ij}^{(k)} = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)})$. If, having dropped the superscripts, we were to compute and store $d_{ik}$ or $d_{kj}$ before using these values to compute $d_{ij}$, we might be computing one of the following: $$ \\begin{aligned} d_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k - 1)}), \\\\ d_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k)}), \\\\ d_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k)}), \\end{aligned} $$ In any of these scenarios, we're computing the weight of a shortest path from $i$ to $j$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. If we use $d_{ik}^{(k)}$, rather than $d_{ik}^{(k - 1)}$, in the computation, then we're using a subpath from $i$ to $k$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. But $k$ cannot be an intermediate vertex on a shortest path from $i$ to $k$, since otherwise there would be a cycle on this shortest path. Thus, $d_{ik}^{(k)} = d_{ik}^{(k - 1)}$. A similar argument applies to show that $d_{kj}^{(k)} = d_{kj}^{(k - 1)}$. Hence, we can drop the superscripts in the computation. 25.2-5 Suppose that we modify the way in which equation $\\text{(25.7)}$ handles equality: $$ \\pi_{ij}^{(k)} = \\begin{cases} \\pi_{ij}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} < d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}, \\\\ \\pi_{kj}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} \\ge d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}. \\end{cases} $$ Is this alternative definition of the predecessor matrix $\\prod$ correct? If we change the way that we handle the equality case, we will still be generating a the correct values for the $\\pi$ matrix. This is because updating the $\\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\\pi_{ij} = \\pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik} + d_{kj}$. 25.2-6 How can we use the output of the Floyd-Warshall algorithm to detect the presence of a negative-weight cycle? Here are two ways to detect negative-weight cycles: Check the main-diagonal entries of the result matrix for a negative value. There is a negative weight cycle if and only if $d_{ii}^{(n)} < 0$ for some vertex $i$: $d_{ii}^{(n)}$ is a path weight from $i$ to itself; so if it is negative, there is a path from $i$ to itself (i.e., a cycle), with negative weight. If there is a negative-weight cycle, consider the one with the fewest vertices. If it has just one vertex, then some $w_{ii} < 0$, so $d_{ii}$ starts out negative, and since $d$ values are never increased, it is also negative when the algorithm terminates. If it has at least two vertices, let $k$ be the highest-numbered vertex in the cycle, and let $i$ be some other vertex in the cycle. $d_{ik}^{(k - 1)}$ and $d_{ki}^{(k - 1)}$ have correct shortest-path weights, because they are not based on negativeweight cycles. (Neither $d_{ik}^{(k - 1)}$ nor $d_{ki}^{(k - 1)}$ can include $k$ as an intermediate vertex, and $i$ and $k$ are on the negative-weight cycle with the fewest vertices.) Since $i \\leadsto k \\leadsto i$ is a negative-weight cycle, the sum of those two weights is negative, so $d_{ii}^{(k)}$ will be set to a negative value. Since $d$ values are never increased, it is also negative when the algorithm terminates. In fact, it suffices to check whether $d_{ii}^{(n - 1)} < 0$ for some vertex $i$. Here's why. A negative-weight cycle containing vertex $i$ either contains vertex $n$ or it does not. If it does not, then clearly $d_{ii}^{(n - 1)} < 0$. If the negative-weight cycle contains vertex $n$, then consider $d_{nn}^{(n - 1)}$. This value must be negative, since the cycle, starting and ending at vertex $n$, does not include vertex $n$ as an intermediate vertex. Alternatively, one could just run the normal $\\text{FLOYD-WARSHALL}$ algorithm one extra iteration to see if any of the $d$ values change. If there are negative cycles, then some shortest-path cost will be cheaper. If there are no such cycles, then no $d$ values will change because the algorithm gives the correct shortest paths. 25.2-7 Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses values $\\phi_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$, where $\\phi_{ij}^{(k)}$ is the highest-numbered intermediate vertex of a shortest path from $i$ to $j$ in which all intermediate vertices are in the set $\\{1, 2, \\ldots, k \\}$. Give a recursive formulation for $\\phi_{ij}^{(k)}$, modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\phi_{ij}^{(k)}$ values, and rewrite the $\\text{PRINT-ALLPAIRS-SHORTEST-PATH}$ procedure to take the matrix $\\Phi = \\big(\\phi_{ij}^{(n)}\\big)$ as an input. How is the matrix $\\Phi$ like the $s$ table in the matrix-chain multiplication problem of Section 15.2? We can recursively compute the values of $\\phi_{ij}^{(k)}$ by, letting it be $\\phi_{ij}^{(k - 1)}$ if $d_{ik}^{(k)} + d_{kj}^{(k)} \\ge d_{ij}^{(k - 1)}$, and otherwise, let it be $k$. This works correctly because it perfectly captures whether we decided to use vertex $k$ when we were repeatedly allowing ourselves use of each vertex one at a time. To modify Floyd-Warshall to compute this, we would just need to stick within the innermost for loop, something that computes $\\phi(k)$ by this recursive rule, this would only be a constant amount of work in this innermost for loop, and so would not cause the asymptotic runtime to increase. It is similar to the s table in matrix-chain multiplication because it is computed by a similar recurrence. If we already have the $n^3$ values in $\\phi_{ij}^{(k)}$ provided, then we can reconstruct the shortest path from $i$ to $j$ because we know that the largest vertex in the path from $i$ to $j$ is $\\phi_{ij}^{(n)}$, call it $a_1$. Then, we know that the largest vertex in the path before $a_1$ will be $\\phi_{ia_1}^{(a_1 - 1)}$ and the largest after $a_1$ will be $\\phi_{a_1j}^{(a_1 - 1)}$. By continuing to recurse until we get that the largest element showing up at some point is $\\text{NIL}$, we will be able to continue subdividing the path until it is entirely constructed. 25.2-8 Give an $O(VE)$-time algorithm for computing the transitive closure of a directed graph $G = (V, E)$. Create an $n$ by $n$ matrix $A$ filled with $0$'s. We are done if we can determine the vertices reachable from a particular vertex in $O(E)$ time, since we can just compute this for each $v \\in V$. To do this, assign each edge weight $1$. Then we have $\\delta(v, u) \\le |E|$ for all $u \\in V$. By Problem 24-4(a) we can compute $\\delta(v, u)$ in $O(E)$ forall $u \\in V$. If $\\delta(v, u) < \\infty$, set $A_{ij} = 1$. Otherwise, leave it as $0$. 25.2-9 Suppose that we can compute the transitive closure of a directed acyclic graph in $f(|V|, |E|)$ time, where $f$ is a monotonically increasing function of $|V|$ and $|E|$. Show that the time to compute the transitive closure $G^* = (V, E^*)$ of a general directed graph $G = (V, E)$ is then $f(|V|, |E|) + O(V + E^*)$. First, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1, S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V + E')$. So, the total time required is $\\le f(|V|, |E|) + O(V + E)$.","title":"25.2 The Floyd-Warshall algorithm"},{"location":"Chap25/25.2/#252-1","text":"Run the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2. Show the matrix $D^{(k)}$ that results for each iteration of the outer loop. $k = 1$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ $k = 2$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & - 8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 3$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 4$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 6$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$","title":"25.2-1"},{"location":"Chap25/25.2/#252-2","text":"Show how to compute the transitive closure using the technique of Section 25.1. We set $w_{ij} = 1$ if $(i, j)$ is an edge, and $w_{ij} = 0$ otherwise. Then we replace line 7 of $\\text{EXTEND-SHORTEST-PATHS}(L, W)$ by $l''_{ij} = l''_{ij} \\lor (l_{ik} \\land w_{kj})$. Then run the $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ algorithm.","title":"25.2-2"},{"location":"Chap25/25.2/#252-3","text":"Modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\prod^{(k)}$ matrices according to equations $\\text{(25.6)}$ and $\\text{(25.7)}$. Prove rigorously that for all $i \\in V$, the predecessor subgraph $G_{\\pi, i}$ is a shortest-paths tree with root $i$. ($\\textit{Hint:}$ To show that $G_{\\pi, i}$ is acyclic, first show that $\\pi_{ij}^{(k)} = l$ implies $d_{ij}^{(k)} \\ge d_{il}^{(k)} + w_{lj}$, according to the definition of $\\pi_{ij}^{(k)}$. Then, adapt the proof of Lemma 23.16.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 MOD - FLOYD - WARSHALL ( W ) n = W . rows D ( 0 ) = W let \u03c0 ( 0 ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if i != j and D [ i , j ]( 0 ) < \u221e \u03c0 [ i , j ]( 0 ) = i for k = 1 to n let D ( k ) be a new n \u00d7 n matrix let \u03c0 ( k ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if d [ i , j ]( k - 1 ) \u2264 d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) d [ i , j ]( k ) = d [ i , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ i , j ]( k - 1 ) else d [ i , j ]( k ) = d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ k , j ]( k - 1 ) In order to have that $\\pi^{(k)}_{ij} = l$, we need that $d^{(k)}_{ij} \\ge d^{(k)}_{il} + w_{lj}$. To see this fact, we will note that having $\\pi^{(k)}_{ij} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \\le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality around, and if we ever get some cycle, $i_1, i_2, \\ldots, i_c$, then we would have that $d_{ii_1} \\le d_{ii_1} + w_{i_1i_2} + w_{i_2i_3} + \\cdots + w_{i_ci_1}$. So, if we subtract the common term sfrom both sides, we get that $0 \\le w_{i_ci_1} + \\sum_{q = 1}^{c - 1} w_{i_qi_{q + 1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation $\\text{(25.7)}$.","title":"25.2-3"},{"location":"Chap25/25.2/#252-4","text":"As it appears above, the Floyd-Warshall algorithm requires $\\Theta(n^3)$ space, since we compute $d_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$. Show that the following procedure, which simply drops all the superscripts, is correct, and thus only $\\Theta(n^2)$ space is required. 1 2 3 4 5 6 7 8 FLOYD - WARSHALL ' ( W ) n = W . rows D = W for k = 1 to n for i = 1 to n for j = 1 to n d [ i , j ] = min ( d [ i , j ], d [ i , k ] + d [ k , j ]) return D With the superscripts, the computation is $d_{ij}^{(k)} = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)})$. If, having dropped the superscripts, we were to compute and store $d_{ik}$ or $d_{kj}$ before using these values to compute $d_{ij}$, we might be computing one of the following: $$ \\begin{aligned} d_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k - 1)}), \\\\ d_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k - 1)} + d_{kj}^{(k)}), \\\\ d_{ij}^{(k)} & = \\min(d_{ij}^{(k - 1)}, d_{ik}^{(k)} + d_{kj}^{(k)}), \\end{aligned} $$ In any of these scenarios, we're computing the weight of a shortest path from $i$ to $j$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. If we use $d_{ik}^{(k)}$, rather than $d_{ik}^{(k - 1)}$, in the computation, then we're using a subpath from $i$ to $k$ with all intermediate vertices in $\\{1, 2, \\ldots, k\\}$. But $k$ cannot be an intermediate vertex on a shortest path from $i$ to $k$, since otherwise there would be a cycle on this shortest path. Thus, $d_{ik}^{(k)} = d_{ik}^{(k - 1)}$. A similar argument applies to show that $d_{kj}^{(k)} = d_{kj}^{(k - 1)}$. Hence, we can drop the superscripts in the computation.","title":"25.2-4"},{"location":"Chap25/25.2/#252-5","text":"Suppose that we modify the way in which equation $\\text{(25.7)}$ handles equality: $$ \\pi_{ij}^{(k)} = \\begin{cases} \\pi_{ij}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} < d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}, \\\\ \\pi_{kj}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} \\ge d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}. \\end{cases} $$ Is this alternative definition of the predecessor matrix $\\prod$ correct? If we change the way that we handle the equality case, we will still be generating a the correct values for the $\\pi$ matrix. This is because updating the $\\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\\pi_{ij} = \\pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik} + d_{kj}$.","title":"25.2-5"},{"location":"Chap25/25.2/#252-6","text":"How can we use the output of the Floyd-Warshall algorithm to detect the presence of a negative-weight cycle? Here are two ways to detect negative-weight cycles: Check the main-diagonal entries of the result matrix for a negative value. There is a negative weight cycle if and only if $d_{ii}^{(n)} < 0$ for some vertex $i$: $d_{ii}^{(n)}$ is a path weight from $i$ to itself; so if it is negative, there is a path from $i$ to itself (i.e., a cycle), with negative weight. If there is a negative-weight cycle, consider the one with the fewest vertices. If it has just one vertex, then some $w_{ii} < 0$, so $d_{ii}$ starts out negative, and since $d$ values are never increased, it is also negative when the algorithm terminates. If it has at least two vertices, let $k$ be the highest-numbered vertex in the cycle, and let $i$ be some other vertex in the cycle. $d_{ik}^{(k - 1)}$ and $d_{ki}^{(k - 1)}$ have correct shortest-path weights, because they are not based on negativeweight cycles. (Neither $d_{ik}^{(k - 1)}$ nor $d_{ki}^{(k - 1)}$ can include $k$ as an intermediate vertex, and $i$ and $k$ are on the negative-weight cycle with the fewest vertices.) Since $i \\leadsto k \\leadsto i$ is a negative-weight cycle, the sum of those two weights is negative, so $d_{ii}^{(k)}$ will be set to a negative value. Since $d$ values are never increased, it is also negative when the algorithm terminates. In fact, it suffices to check whether $d_{ii}^{(n - 1)} < 0$ for some vertex $i$. Here's why. A negative-weight cycle containing vertex $i$ either contains vertex $n$ or it does not. If it does not, then clearly $d_{ii}^{(n - 1)} < 0$. If the negative-weight cycle contains vertex $n$, then consider $d_{nn}^{(n - 1)}$. This value must be negative, since the cycle, starting and ending at vertex $n$, does not include vertex $n$ as an intermediate vertex. Alternatively, one could just run the normal $\\text{FLOYD-WARSHALL}$ algorithm one extra iteration to see if any of the $d$ values change. If there are negative cycles, then some shortest-path cost will be cheaper. If there are no such cycles, then no $d$ values will change because the algorithm gives the correct shortest paths.","title":"25.2-6"},{"location":"Chap25/25.2/#252-7","text":"Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses values $\\phi_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$, where $\\phi_{ij}^{(k)}$ is the highest-numbered intermediate vertex of a shortest path from $i$ to $j$ in which all intermediate vertices are in the set $\\{1, 2, \\ldots, k \\}$. Give a recursive formulation for $\\phi_{ij}^{(k)}$, modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\phi_{ij}^{(k)}$ values, and rewrite the $\\text{PRINT-ALLPAIRS-SHORTEST-PATH}$ procedure to take the matrix $\\Phi = \\big(\\phi_{ij}^{(n)}\\big)$ as an input. How is the matrix $\\Phi$ like the $s$ table in the matrix-chain multiplication problem of Section 15.2? We can recursively compute the values of $\\phi_{ij}^{(k)}$ by, letting it be $\\phi_{ij}^{(k - 1)}$ if $d_{ik}^{(k)} + d_{kj}^{(k)} \\ge d_{ij}^{(k - 1)}$, and otherwise, let it be $k$. This works correctly because it perfectly captures whether we decided to use vertex $k$ when we were repeatedly allowing ourselves use of each vertex one at a time. To modify Floyd-Warshall to compute this, we would just need to stick within the innermost for loop, something that computes $\\phi(k)$ by this recursive rule, this would only be a constant amount of work in this innermost for loop, and so would not cause the asymptotic runtime to increase. It is similar to the s table in matrix-chain multiplication because it is computed by a similar recurrence. If we already have the $n^3$ values in $\\phi_{ij}^{(k)}$ provided, then we can reconstruct the shortest path from $i$ to $j$ because we know that the largest vertex in the path from $i$ to $j$ is $\\phi_{ij}^{(n)}$, call it $a_1$. Then, we know that the largest vertex in the path before $a_1$ will be $\\phi_{ia_1}^{(a_1 - 1)}$ and the largest after $a_1$ will be $\\phi_{a_1j}^{(a_1 - 1)}$. By continuing to recurse until we get that the largest element showing up at some point is $\\text{NIL}$, we will be able to continue subdividing the path until it is entirely constructed.","title":"25.2-7"},{"location":"Chap25/25.2/#252-8","text":"Give an $O(VE)$-time algorithm for computing the transitive closure of a directed graph $G = (V, E)$. Create an $n$ by $n$ matrix $A$ filled with $0$'s. We are done if we can determine the vertices reachable from a particular vertex in $O(E)$ time, since we can just compute this for each $v \\in V$. To do this, assign each edge weight $1$. Then we have $\\delta(v, u) \\le |E|$ for all $u \\in V$. By Problem 24-4(a) we can compute $\\delta(v, u)$ in $O(E)$ forall $u \\in V$. If $\\delta(v, u) < \\infty$, set $A_{ij} = 1$. Otherwise, leave it as $0$.","title":"25.2-8"},{"location":"Chap25/25.2/#252-9","text":"Suppose that we can compute the transitive closure of a directed acyclic graph in $f(|V|, |E|)$ time, where $f$ is a monotonically increasing function of $|V|$ and $|E|$. Show that the time to compute the transitive closure $G^* = (V, E^*)$ of a general directed graph $G = (V, E)$ is then $f(|V|, |E|) + O(V + E^*)$. First, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1, S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V + E')$. So, the total time required is $\\le f(|V|, |E|) + O(V + E)$.","title":"25.2-9"},{"location":"Chap25/25.3/","text":"25.3-1 Use Johnson's algorithm to find the shortest paths between all pairs of vertices in the graph of Figure 25.2. Show the values of $h$ and $\\hat w$ computed by the algorithm. $$ \\begin{array}{c|c} v & h(v) \\\\ \\hline 1 & -5 \\\\ 2 & -3 \\\\ 3 & 0 \\\\ 4 & -1 \\\\ 5 & -6 \\\\ 6 & -8 \\end{array} $$ $$ \\begin{array}{ccc|ccc} u & v & \\hat w(u, v) & u & v & \\hat w(u, v) \\\\ \\hline 1 & 2 & \\text{NIL} & 4 & 1 & 0 \\\\ 1 & 3 & \\text{NIL} & 4 & 2 & \\text{NIL} \\\\ 1 & 4 & \\text{NIL} & 4 & 3 & \\text{NIL} \\\\ 1 & 5 & 0 & 4 & 5 & 8 \\\\ 1 & 6 & \\text{NIL} & 4 & 6 & \\text{NIL} \\\\ 2 & 1 & 3 & 5 & 1 & \\text{NIL} \\\\ 2 & 3 & \\text{NIL} & 5 & 2 & 4 \\\\ 2 & 4 & 0 & 5 & 3 & \\text{NIL} \\\\ 2 & 5 & \\text{NIL} & 5 & 4 & \\text{NIL} \\\\ 2 & 6 & \\text{NIL} & 5 & 6 & \\text{NIL} \\\\ 3 & 1 & \\text{NIL} & 6 & 1 & \\text{NIL} \\\\ 3 & 2 & 5 & 6 & 2 & 0 \\\\ 3 & 4 & \\text{NIL} & 6 & 3 & 18 \\\\ 3 & 5 & \\text{NIL} & 6 & 4 & \\text{NIL} \\\\ 3 & 6 & 0 & 6 & 5 & \\text{NIL} \\\\ \\end{array} $$ So, the $d_{ij}$ values that we get are $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} . $$ 25.3-2 What is the purpose of adding the new vertex $s$ to $V'$, yielding $V'$? This is only important when there are negative-weight cycles in the graph. Using a dummy vertex gets us around the problem of trying to compute $-\\infty + \\infty$ to find $\\hat w$. Moreover, if we had instead used a vertex $v$ in the graph instead of the new vertex $s$, then we run into trouble if a vertex fails to be reachable from $v$. 25.3-3 Suppose that $w(u, v) \\ge 0$ for all edges $(u, v) \\in E$. What is the relationship between the weight functions $w$ and $\\hat w$? If all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from s to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts $0$, and so we have that $w(u, v) = \\hat w(u, v)$ 25.3-4 Professor Greenstreet claims that there is a simpler way to reweight edges than the method used in Johnson's algorithm. Letting $w^* = \\min_{(u, v) \\in E} \\{w(u, v)\\}$, just define $\\hat w(u, v) = w(u, v) - w^*$ for all edges $(u, v) \\in E$. What is wrong with the professor's method of reweighting? It changes shortest paths. Consider the following graph. $V = \\{s, x, y, z\\}$, and there are 4 edges: $w(s, x) = 2$, $w(x, y) = 2$, $w(s, y) = 5$, and $w(s, z) = -10$. So we'd add $10$ to every weight to make $\\hat w$. With $w$, the shortest path from $s$ to $y$ is $s \\to x \\to y$, with weight $4$. With $\\hat w$, the shortest path from $s$ to $y$ is $s \\to y$, with weight $15$. (The path $s \\to x \\to y$ has weight $24$.) The problem is that by just adding the same amount to every edge, you penalize paths with more edges, even if their weights are low. 25.3-5 Suppose that we run Johnson's algorithm on a directed graph $G$ with weight function $w$. Show that if $G$ contains a $0$-weight cycle $c$, then $\\hat w(u, v) = 0$ for every edge $(u, v)$ in $c$. If $\\delta(s, v) - \\delta(s, u) \\le w(u, v)$, we have $$\\delta(s, u) \\le \\delta(s, v) + (0 - w(u, v)) < \\delta(s, u) + w(u, v) - w(u, v) = \\delta(s, u),$$ which is impossible, thus $\\delta(s, v) - \\delta(s, u) = w(u, v)$, $\\hat w(u, v) = w(u, v) + \\delta(s, u) - \\delta(s, v) = 0$. 25.3-6 Professor Michener claims that there is no need to create a new source vertex in line 1 of $\\text{JOHNSON}$. He claims that instead we can just use $G' = G$ and let $s$ be any vertex. Give an example of a weighted, directed graph $G$ for which incorporating the professor's idea into $\\text{JOHNSON}$ causes incorrect answers. Then show that if $G$ is strongly connected (every vertex is reachable from every other vertex), the results returned by $\\text{JOHNSON}$ with the professor's modification are correct. In this solution, we assume that $\\infty - \\infty$ is undefined, in particular, it's not $0$. Let $G = (V, E)$, where $V = {s, u}$, $E = \\{(u, s)\\}$, and $w(u, s) = 0$. There is only one edge, and it enters $s$. When we run Bellman-Ford from $s$, we get $h(s) = \\delta(s, s) = 0$ and $h(u) = \\delta(s, u) = \\infty$. When we reweight, we get $\\hat w(u, s) = 0 + \\infty - 0 = \\infty$. We compute $\\hat\\delta(u, s) = \\infty$, and so we compute $d_{us} = \\infty + 0 - \\infty \\ne 0$. Since $\\delta(u, s) = 0$, we get an incorrect answer. If the graph $G$ is strongly connected, then we get $h(v) = \\delta(s, v) < \\infty$ for all vertices $v \\in V$. Thus, the triangle inequality says that $h(v) \\le h(u) + w(u, v)$ for all edges $(u, v) \\in E$, and so $\\hat w(u, v) = w(u, v) + h(u) - h(v) \\ge 0$. Moreover, all edge weights $\\hat w(u, v)$ used in Lemma 25.1 are finite, and so the lemma holds. Therefore, the conditions we need in order to use Johnson's algorithm hold: that reweighting does not change shortest paths, and that all edge weights $\\hat w(u, v)$ are nonnegative. Again relying on $G$ being strongly connected, we get that $\\hat\\delta(u, v) < \\infty$ for all edges $(u, v) \\in E$, which means that $d_{uv} = \\hat\\delta(u, v) + h(v) - h(u)$ is finite and correct.","title":"25.3 Johnson's algorithm for sparse graphs"},{"location":"Chap25/25.3/#253-1","text":"Use Johnson's algorithm to find the shortest paths between all pairs of vertices in the graph of Figure 25.2. Show the values of $h$ and $\\hat w$ computed by the algorithm. $$ \\begin{array}{c|c} v & h(v) \\\\ \\hline 1 & -5 \\\\ 2 & -3 \\\\ 3 & 0 \\\\ 4 & -1 \\\\ 5 & -6 \\\\ 6 & -8 \\end{array} $$ $$ \\begin{array}{ccc|ccc} u & v & \\hat w(u, v) & u & v & \\hat w(u, v) \\\\ \\hline 1 & 2 & \\text{NIL} & 4 & 1 & 0 \\\\ 1 & 3 & \\text{NIL} & 4 & 2 & \\text{NIL} \\\\ 1 & 4 & \\text{NIL} & 4 & 3 & \\text{NIL} \\\\ 1 & 5 & 0 & 4 & 5 & 8 \\\\ 1 & 6 & \\text{NIL} & 4 & 6 & \\text{NIL} \\\\ 2 & 1 & 3 & 5 & 1 & \\text{NIL} \\\\ 2 & 3 & \\text{NIL} & 5 & 2 & 4 \\\\ 2 & 4 & 0 & 5 & 3 & \\text{NIL} \\\\ 2 & 5 & \\text{NIL} & 5 & 4 & \\text{NIL} \\\\ 2 & 6 & \\text{NIL} & 5 & 6 & \\text{NIL} \\\\ 3 & 1 & \\text{NIL} & 6 & 1 & \\text{NIL} \\\\ 3 & 2 & 5 & 6 & 2 & 0 \\\\ 3 & 4 & \\text{NIL} & 6 & 3 & 18 \\\\ 3 & 5 & \\text{NIL} & 6 & 4 & \\text{NIL} \\\\ 3 & 6 & 0 & 6 & 5 & \\text{NIL} \\\\ \\end{array} $$ So, the $d_{ij}$ values that we get are $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} . $$","title":"25.3-1"},{"location":"Chap25/25.3/#253-2","text":"What is the purpose of adding the new vertex $s$ to $V'$, yielding $V'$? This is only important when there are negative-weight cycles in the graph. Using a dummy vertex gets us around the problem of trying to compute $-\\infty + \\infty$ to find $\\hat w$. Moreover, if we had instead used a vertex $v$ in the graph instead of the new vertex $s$, then we run into trouble if a vertex fails to be reachable from $v$.","title":"25.3-2"},{"location":"Chap25/25.3/#253-3","text":"Suppose that $w(u, v) \\ge 0$ for all edges $(u, v) \\in E$. What is the relationship between the weight functions $w$ and $\\hat w$? If all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from s to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts $0$, and so we have that $w(u, v) = \\hat w(u, v)$","title":"25.3-3"},{"location":"Chap25/25.3/#253-4","text":"Professor Greenstreet claims that there is a simpler way to reweight edges than the method used in Johnson's algorithm. Letting $w^* = \\min_{(u, v) \\in E} \\{w(u, v)\\}$, just define $\\hat w(u, v) = w(u, v) - w^*$ for all edges $(u, v) \\in E$. What is wrong with the professor's method of reweighting? It changes shortest paths. Consider the following graph. $V = \\{s, x, y, z\\}$, and there are 4 edges: $w(s, x) = 2$, $w(x, y) = 2$, $w(s, y) = 5$, and $w(s, z) = -10$. So we'd add $10$ to every weight to make $\\hat w$. With $w$, the shortest path from $s$ to $y$ is $s \\to x \\to y$, with weight $4$. With $\\hat w$, the shortest path from $s$ to $y$ is $s \\to y$, with weight $15$. (The path $s \\to x \\to y$ has weight $24$.) The problem is that by just adding the same amount to every edge, you penalize paths with more edges, even if their weights are low.","title":"25.3-4"},{"location":"Chap25/25.3/#253-5","text":"Suppose that we run Johnson's algorithm on a directed graph $G$ with weight function $w$. Show that if $G$ contains a $0$-weight cycle $c$, then $\\hat w(u, v) = 0$ for every edge $(u, v)$ in $c$. If $\\delta(s, v) - \\delta(s, u) \\le w(u, v)$, we have $$\\delta(s, u) \\le \\delta(s, v) + (0 - w(u, v)) < \\delta(s, u) + w(u, v) - w(u, v) = \\delta(s, u),$$ which is impossible, thus $\\delta(s, v) - \\delta(s, u) = w(u, v)$, $\\hat w(u, v) = w(u, v) + \\delta(s, u) - \\delta(s, v) = 0$.","title":"25.3-5"},{"location":"Chap25/25.3/#253-6","text":"Professor Michener claims that there is no need to create a new source vertex in line 1 of $\\text{JOHNSON}$. He claims that instead we can just use $G' = G$ and let $s$ be any vertex. Give an example of a weighted, directed graph $G$ for which incorporating the professor's idea into $\\text{JOHNSON}$ causes incorrect answers. Then show that if $G$ is strongly connected (every vertex is reachable from every other vertex), the results returned by $\\text{JOHNSON}$ with the professor's modification are correct. In this solution, we assume that $\\infty - \\infty$ is undefined, in particular, it's not $0$. Let $G = (V, E)$, where $V = {s, u}$, $E = \\{(u, s)\\}$, and $w(u, s) = 0$. There is only one edge, and it enters $s$. When we run Bellman-Ford from $s$, we get $h(s) = \\delta(s, s) = 0$ and $h(u) = \\delta(s, u) = \\infty$. When we reweight, we get $\\hat w(u, s) = 0 + \\infty - 0 = \\infty$. We compute $\\hat\\delta(u, s) = \\infty$, and so we compute $d_{us} = \\infty + 0 - \\infty \\ne 0$. Since $\\delta(u, s) = 0$, we get an incorrect answer. If the graph $G$ is strongly connected, then we get $h(v) = \\delta(s, v) < \\infty$ for all vertices $v \\in V$. Thus, the triangle inequality says that $h(v) \\le h(u) + w(u, v)$ for all edges $(u, v) \\in E$, and so $\\hat w(u, v) = w(u, v) + h(u) - h(v) \\ge 0$. Moreover, all edge weights $\\hat w(u, v)$ used in Lemma 25.1 are finite, and so the lemma holds. Therefore, the conditions we need in order to use Johnson's algorithm hold: that reweighting does not change shortest paths, and that all edge weights $\\hat w(u, v)$ are nonnegative. Again relying on $G$ being strongly connected, we get that $\\hat\\delta(u, v) < \\infty$ for all edges $(u, v) \\in E$, which means that $d_{uv} = \\hat\\delta(u, v) + h(v) - h(u)$ is finite and correct.","title":"25.3-6"},{"location":"Chap25/Problems/25-1/","text":"Suppose that we wish to maintain the transitive closure of a directed graph $G = (V, E)$ as we insert edges into $E$. That is, after each edge has been inserted, we want to update the transitive closure of the edges inserted so far. Assume that the graph $G$ has no edges initially and that we represent the transitive closure as a boolean matrix. a. Show how to update the transitive closure $G^* = (V, E^*)$ of a graph $G = (V, E)$ in $O(V^2)$ time when a new edge is added to $G$. b. Give an example of a graph $G$ and an edge $e$ such that $\\Omega(V^2)$ time is required to update the transitive closure after the insertion of $e$ into $G$, no matter what algorithm is used. c. Describe an efficient algorithm for updating the transitive closure as edges are inserted into the graph. For any sequence of $n$ insertions, your algorithm should run in total time $\\sum_{i = 1}^n t_i = O(V^3)$, where $t_i$ is the time to update the transitive closure upon inserting the $i$th edge. Prove that your algorithm attains this time bound. a. Let $T = (t_{ij})$ be the $|V| \\times |V|$ matrix representing the transitive closure, such that $t_{ij}$ is $1$ if there is a path from $i$ to $j$, and $0$ otherwise. Initialize $T$ (when there are no edge in $G$) as follows: $$ t_{ij} = \\begin{cases} 1 & \\text{if $i = j$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ We update $T$ as follows when an edge $(u, v)$ is added to $G$: 1 2 3 4 5 6 TRANSITIVE - CLOSURE - UPDATE ( T , u , v ) let T be | V | \u00d7 | V | for i = 1 to | V | for j = 1 to | V | if t [ i , u ] == 1 and t [ v , j ] == 1 t [ i , j ] = 1 With this procedure, the effect of adding edge $(u, v)$ is to create a path (via the new edge) from every vertex that could already reach $u$ to every vertex that could already be reached from $v$. Note that the procedure sets $t_{uv} = 1$, because both $t_{uu}$ and $t_{vv}$ are initialized to $1$. This procedure takes $\\Theta(V^2)$ time because of the two nested loops. b. Consider inserting the edge $(v_{|V|}, v_1)$ into the straight-line graph $v_1 \\to v_2 \\to \\cdots \\to v_{|V|}$. Before this edge is inserted, only $|V|(|V| + 1) / 2$ entries in $T$ are $1$ (the entries on and above the main diagonal). After the edge is inserted, the graph is a cycle in which every vertex can reach every other vertex, so all $|V|^2$ entries in $T$ are $1$. Hence $|V|^2 - (|V|(|V| + 2) / 2) = \\Theta(V^2)$ entries must be changed in $T$, so any algorithm to update the transitive closure must take $\\Omega(V^2)$ time on this graph. c. The algorithm in part (a) would take $\\Theta(V^4)$ time to insert all possible $\\Theta(V^2)$ edges, so we need a more ef\ufb01cient algorithm in order for any sequence of insertions to take only $O(V^3)$ total time. To improve the algorithm, notice that the loop over $j$ is pointless when $t_{iv} = 1$. That is, if there is already a path $i \\leadsto v$, then adding the edge $(u, v)$ cannot make any new vertices reachable from $i$. The loop to set $t_{ij}$ to $1$ for $j$ such that there exists a path $v \\leadsto j$ is just setting entries that are already $1$. Eliminate this redundant processing as follows: 1 2 3 4 5 6 7 TRANSITIVE - CLOSURE - UPDATE ( T , u , v ) let T be | V | \u00d7 | V | for i = 1 to | V | if t [ i , u ] == 1 and t [ i , v ] == 0 for j = 1 to | V | if t [ v , j ] == 1 t [ i , j ] = 1 We show that this procedure takes $O(V^3)$ time to update the transitive closure for any sequence of $n$ insertions: There cannot be more than $|V|^2$ edges in $G$, so $n \\le |V|^2$. Summed over $n$ insertions, the time for the outer for loop header and the test for $t_{iu} == 1$ and $t_{iv} == 0$ is $O(nV) = O(V^3)$. The last three lines, which take $O(V^2)$ time, are executed only $O(V^2)$ times for $n$ insertions. To see why, notice that the last three lines are executed only when $t_{iv}$ equals $0$, and in that case, the last line sets $t_{iv} = 1$. Thus, the number of $0$ entries in $T$ is reduced by at least $1$ each time the last three lines run. Since there are only $|V|^2$ entries in $T$, these lines can run at most $|V|^2$ times. Hence, the total running time over $n$ insertions is $O(V^3)$.","title":"25-1 Transitive closure of a dynamic graph"},{"location":"Chap25/Problems/25-2/","text":"A graph $G = (V, E)$ is $\\epsilon$-dense if $|E| = \\Theta(V^{1 + \\epsilon})$ for some constant $\\epsilon$ in the range $0 < \\epsilon \\le 1$. By using $d$-ary min-heaps (see Problem 6-2) in shortest-paths algorithms on $\\epsilon$-dense graphs, we can match the running times of Fibonacci-heap-based algorithms without using as complicated a data structure. a. What are the asymptotic running times for $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$, as a function of $d$ and the number $n$ of elements in a $d$-ary min-heap? What are these running times if we choose $d = \\Theta(n^\\alpha)$ for some constant $0 < \\alpha \\le 1$? Compare these running times to the amortized costs of these operations for a Fibonacci heap. b. Show how to compute shortest paths from a single source on an $\\epsilon$-dense directed graph $G = (V, E)$ with no negative-weight edges in $O(E)$ time. ($\\textit{Hint:}$ Pick $d$ as a function of $\\epsilon$.) c. Show how to solve the all-pairs shortest-paths problem on an $\\epsilon$-dense directed graph $G = (V, E)$ with no negative-weight edges in $O(VE)$ time. d. Show how to solve the all-pairs shortest-paths problem in $O(VE)$ time on an $\\epsilon$-dense directed graph $G = (V, E)$ that may have negative-weight edges but has no negative-weight cycles. a. $\\text{INSERT}$: $\\Theta(\\log_d n) = \\Theta(1 / \\alpha)$. $\\text{EXTRACT-MIN}$: $\\Theta(d\\log_d n) = \\Theta(n^\\alpha / \\alpha)$. $\\text{DECREASE-KEY}$: $\\Theta(\\log_d n) = \\Theta(1 / \\alpha)$. b. Dijkstra, $O(d\\log_d V \\cdot V + \\log_d V \\cdot E)$, if $d = V^\\epsilon$, then $$ \\begin{aligned} O(d \\log_d V \\cdot V + \\log_d V \\cdot E) & = O(V^\\epsilon \\cdot V / \\epsilon + E / \\epsilon) \\\\ & = O((V^{1+\\epsilon} + E) / \\epsilon) \\\\ & = O((E + E) / \\epsilon) \\\\ & = O(E). \\end{aligned} $$ c. Run $|V|$ times Dijkstra, since the algorithm is $O(E)$ based on (b), the total time is $O(VE)$. d. Johnson's reweight is $O(VE)$.","title":"25-2 Shortest paths in epsilon-dense graphs"},{"location":"Chap26/26.1/","text":"26.1-1 Show that splitting an edge in a flow network yields an equivalent network. More formally, suppose that flow network $G$ contains edge $(u, v)$, and we create a new flow network $G'$ by creating a new vertex $x$ and replacing $(u, v)$ by new edges $(u, x)$ and $(x, v)$ with $c(u, x) = c(x, v) = c(u, v)$. Show that a maximum flow in $G'$ has the same value as a maximum flow in $G$. We will prove that for every flow in $G = (V, E)$, we can construct a flow in $G' = (V', E')$ that has the same value as that of the flow in $G$. The required result follows since a maximum flow in $G$ is also a flow. Let $f$ be a flow in $G$. By construction, $V' = V \\cup \\{x\\}$ and $E' = (E - \\{(u, v)\\}) \\cup \\{(u, x), (x, v)\\}$. Construct $f'$ in $G'$ as follows: $$ f'(y, z) = \\begin{cases} f(y, z) & \\text{if $(y, z) \\ne (u, x)$ and $(y, z) \\ne (x, v)$}, \\\\ f(u, v) & \\text{if $(y, z) = (u, x)$ or $(y, z) = (x, v)$}. \\end{cases} $$ Informally, $f'$ is the same as $f$, except that the flow $f(u, v)$ now passes through an intermediate vertex $x$. The vertex $x$ has incoming flow (if any) only from $u$, and has outgoing flow (if any) only to vertex $v$. We first prove that $f'$ satisfies the required properties of a flow. It is obvious that the capacity constraint is satisfied for every edge in $E'$ and that every vertex in $V' - \\{u, v, x\\}$ obeys flow conservation. To show that edges $(u, x)$ and $(x, v)$ obey the capacity constraint, we have $$ \\begin{aligned} f'(u, x) = f(u, v) & \\le c(u, v) = c(u, x), \\\\ f'(x, v) = f(u, v) & \\le c(u, v) = c(x, v). \\end{aligned} $$ We now prove flow conservation for $u$. Assuming that $u \\ne \\{s, t\\}$, we have $$ \\begin{aligned} \\sum_{y \\in V'} f'(u, y) & = \\sum_{y \\in V' - \\{x\\}} f'(u, y) + f'(u, x) \\\\ & = \\sum_{y \\in V - \\{v\\}} f(u, y) + f(u, v) \\\\ & = \\sum_{y \\in V} f(u, y) \\\\ & = \\sum_{y \\in V} f(y, u) \\quad \\text{(because $f$ obeys flow conservation)} \\\\ & = \\sum_{y \\in V'} f'(y, u). \\end{aligned} $$ For vertex $v$, a symmetric argument proves flow conservation. For vertex $x$, we have $$ \\begin{aligned} \\sum_{y \\in V'} f'(y, x) & = f'(u, x) \\\\ & = f'(x, v) \\\\ & = \\sum_{y \\in V'} f'(x, y). \\end{aligned} $$ Thus, $f'$ is a valid flow in $G'$. We now prove that the values of the flow in both cases are equal. If the source $s$ is not in $\\{u, v\\}$, the proof is trivial, since our construction assigns the same flows to incoming and outgoing edges of $s$. If $s = u$, then $$ \\begin{aligned} |f'| & = \\sum_{y \\in V'} f'(u, y) - \\sum_{y \\in V'} f'(y, u) \\\\ & = \\sum_{y \\in V' - \\{x\\}} f'(u, y) - \\sum_{y \\in V'} f'(y, u) + f'(u, x) \\\\ & = \\sum_{y \\in V - \\{v\\}} f(u, y) - \\sum_{y \\in V} f(y, u) + f(u, v) \\\\ & = \\sum_{y \\in V} f(u, y) - \\sum_{y \\in V} f(y, u) \\\\ & = |f|. \\end{aligned} $$ The case when $s = v$ is symmetric. We conclude that $f'$ is a valid flow in $G'$ with $|f'| = |f|$. 26.1-2 Extend the flow properties and definitions to the multiple-source, multiple-sink problem. Show that any flow in a multiple-source, multiple-sink flow network corresponds to a flow of identical value in the single-source, single-sink network obtained by adding a supersource and a supersink, and vice versa. Capacity constraint: for all $u, v \\in V$, we require $0 \\le f(u, v) \\le c(u, v)$. Flow conservation: for all $u \\in V - S - T$, we require $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$. 26.1-3 Suppose that a flow network $G = (V, E)$ violates the assumption that the network contains a path $s \\leadsto v \\leadsto t$ for all vertices $v \\in V$. Let $u$ be a vertex for which there is no path $s \\leadsto u \\leadsto t$. Show that there must exist a maximum flow $f$ in $G$ such that $f(u, v) = f(v, u) = 0$ for all vertices $v \\in V$. We show that, given any flow $f'$ in the flow network $G = (V, E)$, we can construct a flow $f$ as stated in the exercise. The result will follow when $f'$ is a maximum flow. The idea is that even if there is a path from $s$ to the connected component of $u$, no flow can enter the component, since the flow has no path to reach $t$. Thus, all the flow inside the component must be cyclic, which can be made zero without affecting the net value of the flow. Two cases are possible: where $u$ is not connected to $t$, and where $u$ is not connected to $s$. We only analyze the former case. The analysis for the latter case is similar. Let $Y$ be the set of all vertices that have no path to $t$. Our roadmap will be to first prove that no flow can leave $Y$. We use this result and flow conservation to prove that no flow can enter $Y$. We shall then constuct the flow $f$, which has the required properties, and prove that $|f| = |f'|$. The first step is to prove that there can be no flow from a vertex $y \\in Y$ to a vertex $v \\in V - Y$. That is, $f'(y, v) = 0$. This is so, because there are no edges $(y, v)$ in $E$. If there were an edge $(y, v) \\in E$, then there would be a path from $y$ to $t$, which contradicts how we defined the set $Y$. We will now prove that $f'(v, y) = 0$, too. We will do so by applying flow conservation to each vertex in $Y$ and taking the sum over $Y$. By flow conservation, we have $$\\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V} f'(v, y).$$ Partitioning $V$ into $Y$ and $V - Y$ gives $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) + \\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) + \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y). \\tag{*}$$ But we also have $$\\sum_{y \\in Y} \\sum_{v \\in Y} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y),$$ since the left-hand side is the same as the right-hand side, except for a change of variable names $v$ and $y$. We also have $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) = 0,$$ since $f'(y, v) = 0$ for each $y \\in Y$ and $v \\in V - Y$. Thus, equation $(*)$ simplifies to $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) = 0.$$ Because the flow function is nonnegative, $f(v, y) = 0$ for each $v \\in V$ and $y \\in Y$. We conclude that there can be no flow between any vertex in $Y$ and any vertex in $V - Y$. The same technique can show that if there is a path from $u$ to $t$ but not from $s$ to $u$, and we define $Z$ as the set of vertices that do not have have a path from $s$ to $u$, then there can be no flow between any vertex in $Z$ and any vertex in $V - Z$. Let $X = Y \\cup Z$. We thus have $f'(v, x) = f'(x, v) = 0$ if $x \\in X$ and $v \\ne X$. We are now ready to construct flow $f$: $$ f(u, v) = \\begin{cases} f'(u, v) & \\text{if $u, v \\ne X$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ We note that $f$ satisfies the requirements of the exercise. We now prove that $f$ also satisfies the requirements of a flow function. The capacity constraint is satisfied, since whenever $f(u, v) = f'(u, v)$, we have $f(u, v) = f'(u, v) \\le c(u, v)$ and whenever $f(u, v) = 0$, we have $f(u, v) = 0 \\le c(u, v)$. For flow conservation, let $x$ be some vertex other than $s$ or $t$. If $x \\in X$, then from the construction of $f$, we have $$\\sum_{v \\in V} f(x, v) = \\sum_{v \\in V} f(v, x) = 0.$$ Otherwise, if $x \\ne X$, note that $f(x, v) = f'(x, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$. Thus, $$ \\begin{aligned} \\sum_{v \\in V} f(x, v) & = \\sum_{v \\in V} f'(x, v) \\\\ & = \\sum_{v \\in V} f'(v, x) \\quad \\text{(because $f'$ obeys flow conservation)} \\\\ & = \\sum_{v \\in V} f(v, x). \\end{aligned} $$ Finally, we prove that the value of the flow remains the same. Since $s \\ne X$, we have $f(s, v) = f'(s, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$, and so $$ \\begin{aligned} |f| & = \\sum_{v \\in V} f(s, v) - \\sum_{v \\in V} f(v, s) \\\\ & = \\sum_{v \\in V} f'(s, v) - \\sum_{v \\in V} f'(v, s) \\\\ & = |f'|. \\end{aligned} $$ 26.1-4 Let $f$ be a flow in a network, and let $\\alpha$ be a real number. The scalar flow product , denoted $\\alpha f$, is a function from $V \\times V$ to $\\mathbb{R}$ defined by $$(\\alpha f)(u, v) = \\alpha \\cdot f(u, v).$$ Prove that the flows in a network form a convex set . That is, show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ in the range $0 \\le \\alpha \\le 1$. To see that the flows form a convex set, we show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ such that $0 \\le \\alpha \\le 1$. For the capacity constraint, first observe that $\\alpha \\le 1$ implies that $1 - \\alpha \\ge 0$. Thus, for any $u, v \\in V$, we have $$ \\begin{aligned} \\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v) & \\ge 0 \\cdot f_1(u, v) + 0 \\cdot (1 - \\alpha) f_2(u, v) \\\\ & = 0. \\end{aligned} $$ Since $f_1(u, v) \\le c(u, v)$ and $f_2(u, v) \\le c(u, v)$, we also have $$ \\begin{aligned} \\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v) & \\le \\alpha c(u, v) + (1 - \\alpha) c(u, v) \\\\ & = (\\alpha + (1 - \\alpha))c(u, v) \\\\ & = c(u, v). \\end{aligned} $$ For flow conservation, observe that since $f_1$ and $f_2$ obey flow conservation, we have $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ and $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ for any $u \\in V - \\{s, t\\}$. We need to show that $$\\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u)) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v))$$ for any $u \\in V - \\{s, t\\}$. We multiply both sides of the equality for $f_1$ by $\\alpha$, multiply both sides of the equality for $f_2$ by $1 - \\alpha$, and add the left-hand and right-hand sides of the resulting equalities to get $$\\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) = \\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v).$$ Observing that $$ \\begin{aligned} \\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) & = \\sum_{v \\in V} \\alpha f_1(v, u) + \\sum_{v \\in V} (1 - \\alpha) f_2(v, u) \\\\ & = \\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u)) \\end{aligned} $$ and, likewise, that $$\\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha)f_2(u, v))$$ completes the proof that flow conservation holds, and thus that flows form a convex set. 26.1-5 State the maximum-flow problem as a linear-programming problem. $$ \\begin{array}{ll} \\max & \\sum\\limits_{v \\in V} f(s, v) - \\sum\\limits_{v \\in V} f(v, s) \\\\ s.t. & 0 \\le f(u, v) \\le c(u, v) \\\\ & \\sum\\limits_{v \\in V} f(v, u) - \\sum\\limits_{v \\in V} f(u, v) = 0 \\end{array} $$ 26.1-6 Professor Adam has two children who, unfortunately, dislike each other. The problem is so severe that not only do they refuse to walk to school together, but in fact each one refuses to walk on any block that the other child has stepped on that day. The children have no problem with their paths crossing at a corner. Fortunately both the professor's house and the school are on corners, but beyond that he is not sure if it is going to be possible to send both of his children to the same school. The professor has a map of his town. Show how to formulate the problem of determining whether both his children can go to the same school as a maximum-flow problem. Create a vertex for each corner, and if there is a street between corners $u$ and $v$, create directed edges $(u, v)$ and $(v, u)$. Set the capacity of each edge to $1$. Let the source be corner on which the professor's house sits, and let the sink be the corner on which the school is located. We wish to find a flow of value $2$ that also has the property that $f(u, v)$ is an integer for all vertices $u$ and $v$. Such a flow represents two edge-disjoint paths from the house to the school. 26.1-7 Suppose that, in addition to edge capacities, a flow network has vertex capacities . That is each vertex $v$ has a limit $l(v)$ on how much flow can pass though $v$. Show how to transform a flow network $G = (V, E)$ with vertex capacities into an equivalent flow network $G' = (V', E')$ without vertex capacities, such that a maximum flow in $G'$ has the same value as a maximum flow in $G$. How many vertices and edges does $G'$ have? We will construct $G'$ by splitting each vertex $v$ of $G$ into two vertices $v_1$, $v_2$, joined by an edge of capacity $l(v)$. All incoming edges of $v$ are now incoming edges to $v_1$. All outgoing edges from $v$ are now outgoing edges from $v_2$. More formally, construct $G' = (V', E')$ with capacity function $c'$ as follows. For every $v \\in V$, create two vertices $v_1$, $v_2$ in $V'$. Add an edge $(v_1, v_2)$ in $E'$ with $c'(v_1, v_2) = l(v)$. For every edge $(u, v) \\in E$, create an edge $(u_2, v_1)$ in $E'$ with capacity $c'(u_2, v_1) = c(u, v)$. Make $s_1$ and $t_2$ as the new source and target vertices in $G'$. Clearly, $|V'| = 2|V|$ and $|E'| = |E| + |V|$. Let $f$ be a flow in $G$ that respects vertex capacities. Create a flow function $f'$ in $G'$ as follows. For each edge $(u, v) \\in G$, let $f'(u_2, v_1) = f(u, v)$. For each vertex $u \\in V - \\{t\\}$, let $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v)$. Let $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t)$. We readily see that there is a one-to-one correspondence between flows that respect vertex capacities in $G$ and flows in $G'$. For the capacity constraint, every edge in $G'$ of the form $(u_2, v_1)$ has a corresponding edge in $G$ with a corresponding capacity and flow and thus satisfies the capacity constraint. For edges in $E'$ of the form $(u_1, u_2)$, the capacities reflect the vertex capacities in $G$. Therefore, for $u \\in V - \\{s, t\\}$, we have $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v) \\le l(u) = c'(u_1, u_2)$. We also have $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t) \\le l(t) = c'(t_1, t_2)$. Note that this constraint also enforces the vertex capacities in $G$. Now, we prove flow conservation. By construction, every vertex of the form $u_1$ in $G'$ has exactly one outgoing edge $(u_1, u_2)$, and every incoming edge to $u_1$ corresponds to an incoming edge of $u \\in G$. Thus, for all vertices $u \\in V - \\{s, t\\}$, we have $$ \\begin{aligned} \\text{incoming flow to $u_1$} & = \\sum_{v \\in V'} f'(v, u_1) \\\\ & = \\sum_{v \\in V} f(v, u) \\\\ & = \\sum_{v \\in V} f(u, v) \\qquad \\text{(because $f$ obeys flow conservation)} \\\\ & = f'(u_1, u_2) \\\\ & = \\text{outgoing flow from $u_1$}. \\end{aligned} $$ For $t_1$, we have $$ \\begin{aligned} \\text{incoming flow} & = \\sum_{v \\in V'} f'(v, t_1) \\\\ & = \\sum_{v \\in V} f(v, u) \\\\ & = f'(t_1, t_2) \\\\ & = \\text{outgoing flow}. \\end{aligned} $$ Vertices of the form $u_2$ have exactly one incoming edge $(u_1, u_2)$, and every outgoing edge of $u_2$ corresponds to an outgoing edge of $u \\in G$. Thus, for $u_2 \\ne t_2$, $$ \\begin{aligned} \\text{incoming flow} & = f'(u_1, u_2) \\\\ & = \\sum_{v \\in V} f(u, v) \\\\ & = \\sum_{v \\in V'} f'(u_2, v) \\\\ & = \\text{outgoing flow}. \\end{aligned} $$ Finally, we prove that $|f'| = |f|$: $$ \\begin{aligned} |f'| & = \\sum_{v \\in V'} f'(s_1, v) \\\\ & = f'(s_1, s_2) \\qquad \\text{(because there are no other outgoing edges from $s_1$)} \\\\ & = \\sum_{v \\in V} f(s, v) \\\\ & = |f|. \\end{aligned} $$","title":"26.1 Flow networks"},{"location":"Chap26/26.1/#261-1","text":"Show that splitting an edge in a flow network yields an equivalent network. More formally, suppose that flow network $G$ contains edge $(u, v)$, and we create a new flow network $G'$ by creating a new vertex $x$ and replacing $(u, v)$ by new edges $(u, x)$ and $(x, v)$ with $c(u, x) = c(x, v) = c(u, v)$. Show that a maximum flow in $G'$ has the same value as a maximum flow in $G$. We will prove that for every flow in $G = (V, E)$, we can construct a flow in $G' = (V', E')$ that has the same value as that of the flow in $G$. The required result follows since a maximum flow in $G$ is also a flow. Let $f$ be a flow in $G$. By construction, $V' = V \\cup \\{x\\}$ and $E' = (E - \\{(u, v)\\}) \\cup \\{(u, x), (x, v)\\}$. Construct $f'$ in $G'$ as follows: $$ f'(y, z) = \\begin{cases} f(y, z) & \\text{if $(y, z) \\ne (u, x)$ and $(y, z) \\ne (x, v)$}, \\\\ f(u, v) & \\text{if $(y, z) = (u, x)$ or $(y, z) = (x, v)$}. \\end{cases} $$ Informally, $f'$ is the same as $f$, except that the flow $f(u, v)$ now passes through an intermediate vertex $x$. The vertex $x$ has incoming flow (if any) only from $u$, and has outgoing flow (if any) only to vertex $v$. We first prove that $f'$ satisfies the required properties of a flow. It is obvious that the capacity constraint is satisfied for every edge in $E'$ and that every vertex in $V' - \\{u, v, x\\}$ obeys flow conservation. To show that edges $(u, x)$ and $(x, v)$ obey the capacity constraint, we have $$ \\begin{aligned} f'(u, x) = f(u, v) & \\le c(u, v) = c(u, x), \\\\ f'(x, v) = f(u, v) & \\le c(u, v) = c(x, v). \\end{aligned} $$ We now prove flow conservation for $u$. Assuming that $u \\ne \\{s, t\\}$, we have $$ \\begin{aligned} \\sum_{y \\in V'} f'(u, y) & = \\sum_{y \\in V' - \\{x\\}} f'(u, y) + f'(u, x) \\\\ & = \\sum_{y \\in V - \\{v\\}} f(u, y) + f(u, v) \\\\ & = \\sum_{y \\in V} f(u, y) \\\\ & = \\sum_{y \\in V} f(y, u) \\quad \\text{(because $f$ obeys flow conservation)} \\\\ & = \\sum_{y \\in V'} f'(y, u). \\end{aligned} $$ For vertex $v$, a symmetric argument proves flow conservation. For vertex $x$, we have $$ \\begin{aligned} \\sum_{y \\in V'} f'(y, x) & = f'(u, x) \\\\ & = f'(x, v) \\\\ & = \\sum_{y \\in V'} f'(x, y). \\end{aligned} $$ Thus, $f'$ is a valid flow in $G'$. We now prove that the values of the flow in both cases are equal. If the source $s$ is not in $\\{u, v\\}$, the proof is trivial, since our construction assigns the same flows to incoming and outgoing edges of $s$. If $s = u$, then $$ \\begin{aligned} |f'| & = \\sum_{y \\in V'} f'(u, y) - \\sum_{y \\in V'} f'(y, u) \\\\ & = \\sum_{y \\in V' - \\{x\\}} f'(u, y) - \\sum_{y \\in V'} f'(y, u) + f'(u, x) \\\\ & = \\sum_{y \\in V - \\{v\\}} f(u, y) - \\sum_{y \\in V} f(y, u) + f(u, v) \\\\ & = \\sum_{y \\in V} f(u, y) - \\sum_{y \\in V} f(y, u) \\\\ & = |f|. \\end{aligned} $$ The case when $s = v$ is symmetric. We conclude that $f'$ is a valid flow in $G'$ with $|f'| = |f|$.","title":"26.1-1"},{"location":"Chap26/26.1/#261-2","text":"Extend the flow properties and definitions to the multiple-source, multiple-sink problem. Show that any flow in a multiple-source, multiple-sink flow network corresponds to a flow of identical value in the single-source, single-sink network obtained by adding a supersource and a supersink, and vice versa. Capacity constraint: for all $u, v \\in V$, we require $0 \\le f(u, v) \\le c(u, v)$. Flow conservation: for all $u \\in V - S - T$, we require $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$.","title":"26.1-2"},{"location":"Chap26/26.1/#261-3","text":"Suppose that a flow network $G = (V, E)$ violates the assumption that the network contains a path $s \\leadsto v \\leadsto t$ for all vertices $v \\in V$. Let $u$ be a vertex for which there is no path $s \\leadsto u \\leadsto t$. Show that there must exist a maximum flow $f$ in $G$ such that $f(u, v) = f(v, u) = 0$ for all vertices $v \\in V$. We show that, given any flow $f'$ in the flow network $G = (V, E)$, we can construct a flow $f$ as stated in the exercise. The result will follow when $f'$ is a maximum flow. The idea is that even if there is a path from $s$ to the connected component of $u$, no flow can enter the component, since the flow has no path to reach $t$. Thus, all the flow inside the component must be cyclic, which can be made zero without affecting the net value of the flow. Two cases are possible: where $u$ is not connected to $t$, and where $u$ is not connected to $s$. We only analyze the former case. The analysis for the latter case is similar. Let $Y$ be the set of all vertices that have no path to $t$. Our roadmap will be to first prove that no flow can leave $Y$. We use this result and flow conservation to prove that no flow can enter $Y$. We shall then constuct the flow $f$, which has the required properties, and prove that $|f| = |f'|$. The first step is to prove that there can be no flow from a vertex $y \\in Y$ to a vertex $v \\in V - Y$. That is, $f'(y, v) = 0$. This is so, because there are no edges $(y, v)$ in $E$. If there were an edge $(y, v) \\in E$, then there would be a path from $y$ to $t$, which contradicts how we defined the set $Y$. We will now prove that $f'(v, y) = 0$, too. We will do so by applying flow conservation to each vertex in $Y$ and taking the sum over $Y$. By flow conservation, we have $$\\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V} f'(v, y).$$ Partitioning $V$ into $Y$ and $V - Y$ gives $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) + \\sum_{y \\in Y} \\sum_{v \\in V} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) + \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y). \\tag{*}$$ But we also have $$\\sum_{y \\in Y} \\sum_{v \\in Y} f'(y, v) = \\sum_{y \\in Y} \\sum_{v \\in Y} f'(v, y),$$ since the left-hand side is the same as the right-hand side, except for a change of variable names $v$ and $y$. We also have $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(y, v) = 0,$$ since $f'(y, v) = 0$ for each $y \\in Y$ and $v \\in V - Y$. Thus, equation $(*)$ simplifies to $$\\sum_{y \\in Y} \\sum_{v \\in V - Y} f'(v, y) = 0.$$ Because the flow function is nonnegative, $f(v, y) = 0$ for each $v \\in V$ and $y \\in Y$. We conclude that there can be no flow between any vertex in $Y$ and any vertex in $V - Y$. The same technique can show that if there is a path from $u$ to $t$ but not from $s$ to $u$, and we define $Z$ as the set of vertices that do not have have a path from $s$ to $u$, then there can be no flow between any vertex in $Z$ and any vertex in $V - Z$. Let $X = Y \\cup Z$. We thus have $f'(v, x) = f'(x, v) = 0$ if $x \\in X$ and $v \\ne X$. We are now ready to construct flow $f$: $$ f(u, v) = \\begin{cases} f'(u, v) & \\text{if $u, v \\ne X$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ We note that $f$ satisfies the requirements of the exercise. We now prove that $f$ also satisfies the requirements of a flow function. The capacity constraint is satisfied, since whenever $f(u, v) = f'(u, v)$, we have $f(u, v) = f'(u, v) \\le c(u, v)$ and whenever $f(u, v) = 0$, we have $f(u, v) = 0 \\le c(u, v)$. For flow conservation, let $x$ be some vertex other than $s$ or $t$. If $x \\in X$, then from the construction of $f$, we have $$\\sum_{v \\in V} f(x, v) = \\sum_{v \\in V} f(v, x) = 0.$$ Otherwise, if $x \\ne X$, note that $f(x, v) = f'(x, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$. Thus, $$ \\begin{aligned} \\sum_{v \\in V} f(x, v) & = \\sum_{v \\in V} f'(x, v) \\\\ & = \\sum_{v \\in V} f'(v, x) \\quad \\text{(because $f'$ obeys flow conservation)} \\\\ & = \\sum_{v \\in V} f(v, x). \\end{aligned} $$ Finally, we prove that the value of the flow remains the same. Since $s \\ne X$, we have $f(s, v) = f'(s, v)$ and $f(v, x) = f'(v, x)$ for all vertices $v \\in V$, and so $$ \\begin{aligned} |f| & = \\sum_{v \\in V} f(s, v) - \\sum_{v \\in V} f(v, s) \\\\ & = \\sum_{v \\in V} f'(s, v) - \\sum_{v \\in V} f'(v, s) \\\\ & = |f'|. \\end{aligned} $$","title":"26.1-3"},{"location":"Chap26/26.1/#261-4","text":"Let $f$ be a flow in a network, and let $\\alpha$ be a real number. The scalar flow product , denoted $\\alpha f$, is a function from $V \\times V$ to $\\mathbb{R}$ defined by $$(\\alpha f)(u, v) = \\alpha \\cdot f(u, v).$$ Prove that the flows in a network form a convex set . That is, show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ in the range $0 \\le \\alpha \\le 1$. To see that the flows form a convex set, we show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ such that $0 \\le \\alpha \\le 1$. For the capacity constraint, first observe that $\\alpha \\le 1$ implies that $1 - \\alpha \\ge 0$. Thus, for any $u, v \\in V$, we have $$ \\begin{aligned} \\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v) & \\ge 0 \\cdot f_1(u, v) + 0 \\cdot (1 - \\alpha) f_2(u, v) \\\\ & = 0. \\end{aligned} $$ Since $f_1(u, v) \\le c(u, v)$ and $f_2(u, v) \\le c(u, v)$, we also have $$ \\begin{aligned} \\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v) & \\le \\alpha c(u, v) + (1 - \\alpha) c(u, v) \\\\ & = (\\alpha + (1 - \\alpha))c(u, v) \\\\ & = c(u, v). \\end{aligned} $$ For flow conservation, observe that since $f_1$ and $f_2$ obey flow conservation, we have $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ and $\\sum_{v \\in V} f_1(v, u) = \\sum_{v \\in V} f_1(u, v)$ for any $u \\in V - \\{s, t\\}$. We need to show that $$\\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u)) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha) f_2(u, v))$$ for any $u \\in V - \\{s, t\\}$. We multiply both sides of the equality for $f_1$ by $\\alpha$, multiply both sides of the equality for $f_2$ by $1 - \\alpha$, and add the left-hand and right-hand sides of the resulting equalities to get $$\\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) = \\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v).$$ Observing that $$ \\begin{aligned} \\alpha \\sum_{v \\in V} f_1(v, u) + (1 - \\alpha) \\sum_{v \\in V} f_2(v, u) & = \\sum_{v \\in V} \\alpha f_1(v, u) + \\sum_{v \\in V} (1 - \\alpha) f_2(v, u) \\\\ & = \\sum_{v \\in V} (\\alpha f_1(v, u) + (1 - \\alpha) f_2(v, u)) \\end{aligned} $$ and, likewise, that $$\\alpha \\sum_{v \\in V} f_1(u, v) + (1 - \\alpha) \\sum_{v \\in V} f_2(u, v) = \\sum_{v \\in V} (\\alpha f_1(u, v) + (1 - \\alpha)f_2(u, v))$$ completes the proof that flow conservation holds, and thus that flows form a convex set.","title":"26.1-4"},{"location":"Chap26/26.1/#261-5","text":"State the maximum-flow problem as a linear-programming problem. $$ \\begin{array}{ll} \\max & \\sum\\limits_{v \\in V} f(s, v) - \\sum\\limits_{v \\in V} f(v, s) \\\\ s.t. & 0 \\le f(u, v) \\le c(u, v) \\\\ & \\sum\\limits_{v \\in V} f(v, u) - \\sum\\limits_{v \\in V} f(u, v) = 0 \\end{array} $$","title":"26.1-5"},{"location":"Chap26/26.1/#261-6","text":"Professor Adam has two children who, unfortunately, dislike each other. The problem is so severe that not only do they refuse to walk to school together, but in fact each one refuses to walk on any block that the other child has stepped on that day. The children have no problem with their paths crossing at a corner. Fortunately both the professor's house and the school are on corners, but beyond that he is not sure if it is going to be possible to send both of his children to the same school. The professor has a map of his town. Show how to formulate the problem of determining whether both his children can go to the same school as a maximum-flow problem. Create a vertex for each corner, and if there is a street between corners $u$ and $v$, create directed edges $(u, v)$ and $(v, u)$. Set the capacity of each edge to $1$. Let the source be corner on which the professor's house sits, and let the sink be the corner on which the school is located. We wish to find a flow of value $2$ that also has the property that $f(u, v)$ is an integer for all vertices $u$ and $v$. Such a flow represents two edge-disjoint paths from the house to the school.","title":"26.1-6"},{"location":"Chap26/26.1/#261-7","text":"Suppose that, in addition to edge capacities, a flow network has vertex capacities . That is each vertex $v$ has a limit $l(v)$ on how much flow can pass though $v$. Show how to transform a flow network $G = (V, E)$ with vertex capacities into an equivalent flow network $G' = (V', E')$ without vertex capacities, such that a maximum flow in $G'$ has the same value as a maximum flow in $G$. How many vertices and edges does $G'$ have? We will construct $G'$ by splitting each vertex $v$ of $G$ into two vertices $v_1$, $v_2$, joined by an edge of capacity $l(v)$. All incoming edges of $v$ are now incoming edges to $v_1$. All outgoing edges from $v$ are now outgoing edges from $v_2$. More formally, construct $G' = (V', E')$ with capacity function $c'$ as follows. For every $v \\in V$, create two vertices $v_1$, $v_2$ in $V'$. Add an edge $(v_1, v_2)$ in $E'$ with $c'(v_1, v_2) = l(v)$. For every edge $(u, v) \\in E$, create an edge $(u_2, v_1)$ in $E'$ with capacity $c'(u_2, v_1) = c(u, v)$. Make $s_1$ and $t_2$ as the new source and target vertices in $G'$. Clearly, $|V'| = 2|V|$ and $|E'| = |E| + |V|$. Let $f$ be a flow in $G$ that respects vertex capacities. Create a flow function $f'$ in $G'$ as follows. For each edge $(u, v) \\in G$, let $f'(u_2, v_1) = f(u, v)$. For each vertex $u \\in V - \\{t\\}$, let $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v)$. Let $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t)$. We readily see that there is a one-to-one correspondence between flows that respect vertex capacities in $G$ and flows in $G'$. For the capacity constraint, every edge in $G'$ of the form $(u_2, v_1)$ has a corresponding edge in $G$ with a corresponding capacity and flow and thus satisfies the capacity constraint. For edges in $E'$ of the form $(u_1, u_2)$, the capacities reflect the vertex capacities in $G$. Therefore, for $u \\in V - \\{s, t\\}$, we have $f'(u_1, u_2) = \\sum_{v \\in V} f(u, v) \\le l(u) = c'(u_1, u_2)$. We also have $f'(t_1, t_2) = \\sum_{v \\in V} f(v, t) \\le l(t) = c'(t_1, t_2)$. Note that this constraint also enforces the vertex capacities in $G$. Now, we prove flow conservation. By construction, every vertex of the form $u_1$ in $G'$ has exactly one outgoing edge $(u_1, u_2)$, and every incoming edge to $u_1$ corresponds to an incoming edge of $u \\in G$. Thus, for all vertices $u \\in V - \\{s, t\\}$, we have $$ \\begin{aligned} \\text{incoming flow to $u_1$} & = \\sum_{v \\in V'} f'(v, u_1) \\\\ & = \\sum_{v \\in V} f(v, u) \\\\ & = \\sum_{v \\in V} f(u, v) \\qquad \\text{(because $f$ obeys flow conservation)} \\\\ & = f'(u_1, u_2) \\\\ & = \\text{outgoing flow from $u_1$}. \\end{aligned} $$ For $t_1$, we have $$ \\begin{aligned} \\text{incoming flow} & = \\sum_{v \\in V'} f'(v, t_1) \\\\ & = \\sum_{v \\in V} f(v, u) \\\\ & = f'(t_1, t_2) \\\\ & = \\text{outgoing flow}. \\end{aligned} $$ Vertices of the form $u_2$ have exactly one incoming edge $(u_1, u_2)$, and every outgoing edge of $u_2$ corresponds to an outgoing edge of $u \\in G$. Thus, for $u_2 \\ne t_2$, $$ \\begin{aligned} \\text{incoming flow} & = f'(u_1, u_2) \\\\ & = \\sum_{v \\in V} f(u, v) \\\\ & = \\sum_{v \\in V'} f'(u_2, v) \\\\ & = \\text{outgoing flow}. \\end{aligned} $$ Finally, we prove that $|f'| = |f|$: $$ \\begin{aligned} |f'| & = \\sum_{v \\in V'} f'(s_1, v) \\\\ & = f'(s_1, s_2) \\qquad \\text{(because there are no other outgoing edges from $s_1$)} \\\\ & = \\sum_{v \\in V} f(s, v) \\\\ & = |f|. \\end{aligned} $$","title":"26.1-7"},{"location":"Chap26/26.2/","text":"26.2-1 Prove that the summations in equation $\\text{(26.6)}$ equal the summations in equation $\\text{(26.7)}$. Lemma If $v \\notin V_1$, then $f(s, v) = 0$. If $v \\notin V_2$, then $f(v, s) = 0$. If $v \\notin V_1 \\cup V_2$, then $f'(s, v) = 0$. If $v \\notin V_1 \\cup V_2$, then $f'(v, s) = 0$. Proof Let $v \\notin V_1$ be some vertex. From the definition of $V_1$, there is no edge from $s$ to $v$. Thus, $f(s, v) = 0$. Let $v \\notin V_2$ be some vertex. From the definition of $V_2$, there is no edge from $v$ to $s$. Thus, $f(v, s) = 0$. Let $v \\notin V_1 \\cup V_2$ be some vertex. From the definition of $V_1$ and $V_2$, neither $(s, v)$ nor $(v, s)$ exists. Therefore, the third condition of the definition of residual capacity (equation $\\text{(26.2)}$) applies, and $c_f(s, v) = 0$. Thus, $f'(s, v) = 0$. Let $v \\notin V_1 \\cup V_2$ be some vertex. By equation $\\text{(26.2)}$, we have that $c_f(v, s) = 0$ and thus $f'(v, s) = 0$. 26.2-2 In Figure $\\text{26.1}$(b), what is the flow across the cut $(\\{s, v_2, v_4\\}, \\{v_1, v_3, t\\})$? What is the capacity of this cut? $$ \\begin{aligned} f(S, T) & = f(s, v_1) + f(v_2, v_1) + f(v_4, v_3) + f(v_4, t) - f(v_3, v_2) = 11 + 1 + 7 + 4 - 4 = 19, \\\\ c(S, T) & = c(s, v_1) + c(v_2, v_1) + c(v_4, v_3) + c(v_4, t) = 16 + 4 + 7 + 4 = 31. \\end{aligned} $$ 26.2-3 Show the execution of the Edmonds-Karp algorithm on the flow network of Figure 26.1(a). If we perform a breadth first search where we consider the neighbors of a vertex as they appear in the ordering $\\{s, v_1, v_2, v_3, v_4, t\\}$, the first path that we will find is $s, v_1, v_3, t$. The min capacity of this augmenting path is $12$, so we send $12$ units along it. We perform a $\\text{BFS}$ on the resulting residual network. This gets us the path $s, v_2, v_4, t$. The min capacity along this path is $4$, so we send $4$ units along it. Then, the only path remaining in the residual network is $\\{s, v_2, v_4, v_3\\}$ which has a min capacity of $7$, since that's all that's left, we find it in our $\\text{BFS}$. Putting it all together, the total flow that we have found has a value of $23$. 26.2-4 In the example of Figure 26.6, what is the minimum cut corresponding to the maximum flow shown? Of the augmenting paths appearing in the example, which one cancels flow? A minimum cut corresponding to the maximum flow is $S = \\{s, v_1, v_2, v_4\\}$ and $T = \\{v_3, t\\}$. The augmenting path in part (c) cancels flow on edge $(v_3, v_2)$. 26.2-5 Recall that the construction in Section 26.1 that converts a flow network with multiple sources and sinks into a single-source, single-sink network adds edges with infinite capacity. Prove that any flow in the resulting network has a finite value if the edges of the original network with multiple sources and sinks have finite capacity. Since the only edges that have infinite value are those going from the supersource or to the supersink, as long as we pick a cut that has the supersource and all the original sources on one side, and the other side has the supersink as well as all the original sinks, then it will only cut through edges of finite capacity. Then, by Corollary 26.5, we have that the value of the flow is bounded above by the value of any of these types of cuts, which is finite. 26.2-6 Suppose that each source $s_i$ in a flow network with multiple sources and sinks produces exactly $p_i$ units of flow, so that $\\sum_{v \\in V} f(s_i, v) = p_i$. Suppose also that each sink $t_j$ consumes exactly $q_j$ units, so that $\\sum_{v \\in V} f(v, t_j) = q_j$, where $\\sum_i p_i = \\sum_j q_j$. Show how to convert the problem of finding a flow $f$ that obeys these additional constraints into the problem of finding a maximum flow in a single-source, single-sink flow network. $c(s, s_i) = p_i$, $c(t_j, t) = q_j$. 26.2-7 Prove Lemma 26.2. To check that $f_p$ is a flow, we make sure that it satisfies both the capacity constraints and the flow constraints. First, the capacity constraints. To see this, we recall our definition of $c_f(p)$, that is, it is the smallest residual capacity of any of the edges along the path $p$. Since we have that the residual capacity is always less than or equal to the initial capacity, we have that each value of the flow is less than the capacity. Second, we check the flow constraints, Since the only edges that are given any flow are along a path, we have that at each vertex interior to the path, the flow in from one edge is immediately canceled by the flow out to the next vertex in the path. Lastly, we can check that its value is equal to $c_f(p)$ because, while $s$ may show up at spots later on in the path, it will be canceled out as it leaves to go to the next vertex. So, the only net flow from s is the initial edge along the path, since it (along with all the other edges) is given flow $c_f(p)$, that is the value of the flow $f_p$. 26.2-8 Suppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow. Let $G_f$ be the residual network just before an iteration of the while loop of $\\text{FORD-FULKERSON}$, and let $E_s$ be the set of residual edges of $G_f$ into $s$. We'll show that the augmenting path $p$ chosen by $\\text{FORD-FULKERSON}$ does not include an edge in $E_s$. Thus, even if we redefine $G_f$ to disallow edges in $E_s$, the path $p$ still remains an augmenting path in the redefined network. Since $p$ remains unchanged, an iteration of the while loop of $\\text{FORD-FULKERSON}$ updates the flow in the same way as before the redefinition. Furthermore, by disallowing some edges, we do not introduce any new augmenting paths. Thus, $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow. Now, we prove that $\\text{FORD-FULKERSON}$ never chooses an augmenting path $p$ that includes an edge $(v, s) \\in E_s$. Why? The path $p$ always starts from $s$, and if $p$ included an edge $(v, s)$, the vertex $s$ would be repeated twice in the path. Thus, $p$ would no longer be a simple path. Since $\\text{FORD-FULKERSON}$ chooses only simple paths, $p$ cannot include $(v, s)$. 26.2-9 Suppose that both $f$ and $f'$ are flows in a network $G$ and we compute flow $f \\uparrow f'$. Does the augmented flow satisfy the flow conservation property? Does it satisfy the capacity constraint? The augmented flow $f \\uparrow f'$ satisfies the flow conservation property but not the capacity constraint property. First, we prove that $f \\uparrow f'$ satisfies the flow conservation property. We note that if edge $(u, v) \\in E$, then $(v, u) \\ne E$ and $f'(v, u) = 0$. Thus, we can rewrite the definition of flow augmentation (equation $\\text{(26.4)}$), when applied to two flows, as $$ (f \\uparrow f')(u, v) = \\begin{cases} f(u, v) + f'(u, v) & \\text{if $(u, v) \\in E$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ The definition implies that the new flow on each edge is simply the sum of the two flows on that edge. We now prove that in $f \\uparrow f'$, the net incoming flow for each vertex equals the net outgoing flow. Let $u \\ne {s, t}$ be any vertex of $G$. We have $$ \\begin{aligned} \\sum_{v \\in V} (f \\uparrow f') (v, u) & = \\sum_{v \\in V} (f(v, u) + f'(v, u)) \\\\ & = \\sum_{v \\in V} f(v, u) + \\sum_{v \\in V} f'(v, u) \\\\ & = \\sum_{v \\in V} f(u, v) + \\sum_{v \\in V} f'(u, v) \\quad \\text{(because $f$, $f'$ obey flow conservation)} \\\\ & = \\sum_{v \\in V} (f(u, v) + f'(u, v)) \\\\ & = \\sum_{v \\in V} (f \\uparrow f') (u, v). \\end{aligned} $$ We conclude that $f \\uparrow f'$ satisfies flow conservation. We now show that $f \\uparrow f'$ need not satisfy the capacity constraint by giving a simple counterexample. Let the flow network $G$ have just a source and a target vertex, with a single edge $(s, t)$ having $c(s, t) = 1$. Define the flows $f$ and $f'$ to have $f(s, t) = f'(s, t) = 1$. Then, we have $(f \\uparrow f')(s, t) = 2 > c(s, t)$. We conclude that $f \\uparrow f'$ need not satisfy the capacity constraint. 26.2-10 Show how to find a maximum flow in a network $G = (V, E)$ by a sequence of at most $|E|$ augmenting paths. ($\\textit{Hint:}$ Determine the paths after finding the maximum flow.) Suppose we already have a maximum flow $f$. Consider a new graph $G$ where we set the capacity of edge $(u, v)$ to $f(u, v)$. Run Ford-Fulkerson, with the modification that we remove an edge if its flow reaches its capacity. In other words, if $f(u, v) = c(u, v)$ then there should be no reverse edge appearing in residual network. This will still produce correct output in our case because we never exceed the actual maximum flow through an edge, so it is never advantageous to cancel flow. The augmenting paths chosen in this modified version of Ford-Fulkerson are precisely the ones we want. There are at most $|E|$ because every augmenting path produces at least one edge whose flow is equal to its capacity, which we set to be the actual flow for the edge in a maximum flow, and our modification prevents us from ever destroying this progress. 26.2-11 The edge connectivity of an undirected graph is the minimum number $k$ of edges that must be removed to disconnect the graph. For example, the edge connectivity of a tree is $1$, and the edge connectivity of a cyclic chain of vertices is $2$. Show how to determine the edge connectivity of an undirected graph $G = (V, E)$ by running a maximum-flow algorithm on at most $|V|$ flow networks, each having $O(V)$ vertices and $O(E)$ edges. For any two vertices $u$ and $v$ in $G$, we can define a flow network $G_{uv}$ consisting of the directed version of $G$ with $s = u$, $t = v$, and all edge capacities set to $1$. (The flow network $G_{uv}$ has $V$ vertices and $2|E|$ edges, so that it has $O(V)$ vertices and $O(E)$ edges, as required. We want all capacities to be $1$ so that the number of edges of $G$ crossing a cut equals the capacity of the cut in $G_{uv}$.) Let $f_{uv}$ denote a maximum flow in $G_{uv}$. We claim that for any $u \\in V$, the edge connectivity $k$ equals $\\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$. We'll show below that this claim holds. Assuming that it holds, we can find $k$ as follows: 1 2 3 4 5 6 7 8 EDGE - CONNECTIVITY ( G ) k = \u221e select any vertex u \u2208 G . V for each vertex v \u2208 G . V - { u } set up the flow network G [ u , v ] as described above find the maximum flow f [ u , v ] on G [ u , v ] k = min ( k , | f [ u , v ] | ) return k The claim follows from the max-flow min-cut theorem and how we chose capacities so that the capacity of a cut is the number of edges crossing it. We prove that $k = \\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$ for any $u \\in V$ by showing separately that $k$ is at least this minimum and that $k$ is at most this minimum. Proof that $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$: Let $m = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. Suppose we remove only $m - 1$ edges from $G$. For any vertex $v$, by the max-flow min-cut theorem, $u$ and $v$ are still connected. (The max flow from $u$ to $v$ is at least $m$, hence any cut separating $u$ from $v$ has capacity at least $m$, which means at least $m$ edges cross any such cut. Thus at least one edge is left crossing the cut when we remove $m - 1$ edges.) Thus every vertex is connected to $u$, which implies that the graph is still connected. So at least $m$ edges must be removed to disconnect the graph\u2014i.e., $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. Proof that $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$: Consider a vertex $v$ with the minimum $|f_{uv}|$. By the max-flow min-cut theorem, there is a cut of capacity $|f_{uv}|$ separating $u$ and $v$. Since all edge capacities are $1$, exactly $|f_{uv}|$ edges cross this cut. If these edges are removed, there is no path from $u$ to $v$, and so our graph becomes disconnected. Hence $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$ Thus, the claim that $k = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$ for any $u \\in V$ is true. 26.2-12 Suppose that you are given a flow network $G$, and $G$ has edges entering the source $s$. Let $f$ be a flow in $G$ in which one of the edges $(v, s)$ entering the source has $f(v, s) = 1$. Prove that there must exist another flow $f'$ with $f'(v, s) = 0$ such that $|f| = |f'|$. Give an $O(E)$-time algorithm to compute $f'$, given $f$, and assuming that all edge capacities are integers. The idea of the proof is that if $f(v, s) = 1$, then there must exist a cycle containing the edge $(v, s)$ and for which each edge carries one unit of flow. If we reduce the flow on each edge in the cycle by one unit, we can reduce $f(v, s)$ to $0$ without affecting the value of the flow. Given the flow network $G$ and the flow $f$, we say that vertex $y$ is flow-connected to vertex $z$ if there exists a path $p$ from $y$ to $z$ such that each edge of $p$ has a positive flow on it. We also define $y$ to be flow-connected to itself. In particular, $s$ is flow-connected to $s$. We start by proving the following lemma: Lemma Let $G = (V, E)$ be a flow network and $f$ be a flow in $G$. If $s$ is not flow-connected to $v$, then $f(v, s) = 0$. Proof The idea is that since $s$ is not flow-connected to $v$, there cannot be any flow from $s$ to $v$. By using flow conservation, we will prove that there cannot be any flow from $v$ to $s$ either, and thus that $f(v, s) = 0$. Let $Y$ be the set of all vertices $y$ such that $s$ is flow-connected to $y$. By applying flow conservation to vertices in $V - Y$ and taking the sum, we obtain $$\\sum_{z \\in V - Y} \\sum_{x \\in V} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V} f(z, x).$$ Partitioning $V$ into $Y$ and $V - Y$ gives $$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x). \\tag{*}$$ But we have $$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x),$$ since the left-hand side is the same as the right-hand side, except for a change of variable names $x$ and $z$. We also have $$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = 0,$$ since the flow from any vertex in $Y$ to any vertex in $V - Y$ must be $0$. Thus, equation $(*)$ simplifies to $$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x) = 0.$$ The above equation implies that $f(z, x) = 0$ for each $z \\in V - Y$ and $x \\in Y$. In particular, since $v \\in V - Y$ and $s \\in Y$, we have that $f(v, s) = 0$. Now, we show how to construct the required flow $f'$. By the contrapositive of the lemma, $f(v, s) > 0$ implies that $s$ is flow-connected to $v$ through some path $p$. Let path $p'$ be the path $s \\overset{p}{\\leadsto} v \\to s$. Path $p'$ is a cycle that has positive flow on each edge. Because we assume that all edge capacities are integers, the flow on each edge of $p'$ is at least $1$. If we subtract $1$ from each edge of the cycle to obtain a flow $f'$, then $f'$ still satisfies the properties of a flow network and has the same value as $|f|$. Because edge $(v, s)$ is in the cycle, we have that $f'(v, s) = f(v, s) - 1 = 0$. 26.2-13 Suppose that you wish to find, among all minimum cuts in a flow network $G$ with integral capacities, one that contains the smallest number of edges. Show how to modify the capacities of $G$ to create a new flow network $G'$ in which any minimum cut in $G'$ is a minimum cut with the smallest number of edges in $G$. Let $(S, T)$ and $(X, Y)$ be two cuts in $G$ (and $G'$). Let $c'$ be the capacity function of $G'$. One way to define $c'$ is to add a small amount $\\delta$ to the capacity of each edge in $G$. That is, if $u$ and $v$ are two vertices, we set $$c'(u, v) = c(u, v) + \\delta.$$ Thus, if $c(S, T) = c(X, Y)$ and $(S, T)$ has fewer edges than $(X, Y)$, then we would have $c'(S, T) < c'(X, Y)$. We have to be careful and choose a small $\\delta$, lest we change the relative ordering of two unequal capacities. That is, if $c(S, T) < c(X, Y)$, then no matter many more edges $(S, T)$ has than $(X, Y)$, we still need to have $c'(S, T) < c'(X, Y)$. With this definition of $c'$, a minimum cut in $G'$ will be a minimum cut in $G$ that has the minimum number of edges. How should we choose the value of $\\delta$? Let $m$ be the minimum difference between capacities of two unequal-capacity cuts in $G$. Choose $\\delta = m / (2|E|)$. For any cut $(S, T)$, since the cut can have at most $|E|$ edges, we can bound $c'(S, T)$ by $$c(S, T) \\le c'(S, T) \\le c(S, T) + |E| \\cdot \\delta.$$ Let $c(S, T) < c(X, Y)$. We need to prove that $c'(S, T) < c'(X, Y)$. We have $$ \\begin{aligned} c'(S, T) & \\le c(S, T) + |E| \\cdot \\delta \\\\ & = c(S, T) + m / 2 \\\\ & < c(X, Y) \\qquad \\text{(since $c(X, Y) - c(S, T) \\ge m$)} \\\\ & \\le c'(X, Y). \\end{aligned} $$ Because all capacities are integral, we can choose $m = 1$, obtaining $\\delta = 1 / 2|E|$. To avoid dealing with fractional values, we can scale all capacities by $2|E|$ to obtain $$c'(u, v) = 2|E| \\cdot c(u, v) + 1.$$","title":"26.2 The Ford-Fulkerson method"},{"location":"Chap26/26.2/#262-1","text":"Prove that the summations in equation $\\text{(26.6)}$ equal the summations in equation $\\text{(26.7)}$. Lemma If $v \\notin V_1$, then $f(s, v) = 0$. If $v \\notin V_2$, then $f(v, s) = 0$. If $v \\notin V_1 \\cup V_2$, then $f'(s, v) = 0$. If $v \\notin V_1 \\cup V_2$, then $f'(v, s) = 0$. Proof Let $v \\notin V_1$ be some vertex. From the definition of $V_1$, there is no edge from $s$ to $v$. Thus, $f(s, v) = 0$. Let $v \\notin V_2$ be some vertex. From the definition of $V_2$, there is no edge from $v$ to $s$. Thus, $f(v, s) = 0$. Let $v \\notin V_1 \\cup V_2$ be some vertex. From the definition of $V_1$ and $V_2$, neither $(s, v)$ nor $(v, s)$ exists. Therefore, the third condition of the definition of residual capacity (equation $\\text{(26.2)}$) applies, and $c_f(s, v) = 0$. Thus, $f'(s, v) = 0$. Let $v \\notin V_1 \\cup V_2$ be some vertex. By equation $\\text{(26.2)}$, we have that $c_f(v, s) = 0$ and thus $f'(v, s) = 0$.","title":"26.2-1"},{"location":"Chap26/26.2/#262-2","text":"In Figure $\\text{26.1}$(b), what is the flow across the cut $(\\{s, v_2, v_4\\}, \\{v_1, v_3, t\\})$? What is the capacity of this cut? $$ \\begin{aligned} f(S, T) & = f(s, v_1) + f(v_2, v_1) + f(v_4, v_3) + f(v_4, t) - f(v_3, v_2) = 11 + 1 + 7 + 4 - 4 = 19, \\\\ c(S, T) & = c(s, v_1) + c(v_2, v_1) + c(v_4, v_3) + c(v_4, t) = 16 + 4 + 7 + 4 = 31. \\end{aligned} $$","title":"26.2-2"},{"location":"Chap26/26.2/#262-3","text":"Show the execution of the Edmonds-Karp algorithm on the flow network of Figure 26.1(a). If we perform a breadth first search where we consider the neighbors of a vertex as they appear in the ordering $\\{s, v_1, v_2, v_3, v_4, t\\}$, the first path that we will find is $s, v_1, v_3, t$. The min capacity of this augmenting path is $12$, so we send $12$ units along it. We perform a $\\text{BFS}$ on the resulting residual network. This gets us the path $s, v_2, v_4, t$. The min capacity along this path is $4$, so we send $4$ units along it. Then, the only path remaining in the residual network is $\\{s, v_2, v_4, v_3\\}$ which has a min capacity of $7$, since that's all that's left, we find it in our $\\text{BFS}$. Putting it all together, the total flow that we have found has a value of $23$.","title":"26.2-3"},{"location":"Chap26/26.2/#262-4","text":"In the example of Figure 26.6, what is the minimum cut corresponding to the maximum flow shown? Of the augmenting paths appearing in the example, which one cancels flow? A minimum cut corresponding to the maximum flow is $S = \\{s, v_1, v_2, v_4\\}$ and $T = \\{v_3, t\\}$. The augmenting path in part (c) cancels flow on edge $(v_3, v_2)$.","title":"26.2-4"},{"location":"Chap26/26.2/#262-5","text":"Recall that the construction in Section 26.1 that converts a flow network with multiple sources and sinks into a single-source, single-sink network adds edges with infinite capacity. Prove that any flow in the resulting network has a finite value if the edges of the original network with multiple sources and sinks have finite capacity. Since the only edges that have infinite value are those going from the supersource or to the supersink, as long as we pick a cut that has the supersource and all the original sources on one side, and the other side has the supersink as well as all the original sinks, then it will only cut through edges of finite capacity. Then, by Corollary 26.5, we have that the value of the flow is bounded above by the value of any of these types of cuts, which is finite.","title":"26.2-5"},{"location":"Chap26/26.2/#262-6","text":"Suppose that each source $s_i$ in a flow network with multiple sources and sinks produces exactly $p_i$ units of flow, so that $\\sum_{v \\in V} f(s_i, v) = p_i$. Suppose also that each sink $t_j$ consumes exactly $q_j$ units, so that $\\sum_{v \\in V} f(v, t_j) = q_j$, where $\\sum_i p_i = \\sum_j q_j$. Show how to convert the problem of finding a flow $f$ that obeys these additional constraints into the problem of finding a maximum flow in a single-source, single-sink flow network. $c(s, s_i) = p_i$, $c(t_j, t) = q_j$.","title":"26.2-6"},{"location":"Chap26/26.2/#262-7","text":"Prove Lemma 26.2. To check that $f_p$ is a flow, we make sure that it satisfies both the capacity constraints and the flow constraints. First, the capacity constraints. To see this, we recall our definition of $c_f(p)$, that is, it is the smallest residual capacity of any of the edges along the path $p$. Since we have that the residual capacity is always less than or equal to the initial capacity, we have that each value of the flow is less than the capacity. Second, we check the flow constraints, Since the only edges that are given any flow are along a path, we have that at each vertex interior to the path, the flow in from one edge is immediately canceled by the flow out to the next vertex in the path. Lastly, we can check that its value is equal to $c_f(p)$ because, while $s$ may show up at spots later on in the path, it will be canceled out as it leaves to go to the next vertex. So, the only net flow from s is the initial edge along the path, since it (along with all the other edges) is given flow $c_f(p)$, that is the value of the flow $f_p$.","title":"26.2-7"},{"location":"Chap26/26.2/#262-8","text":"Suppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow. Let $G_f$ be the residual network just before an iteration of the while loop of $\\text{FORD-FULKERSON}$, and let $E_s$ be the set of residual edges of $G_f$ into $s$. We'll show that the augmenting path $p$ chosen by $\\text{FORD-FULKERSON}$ does not include an edge in $E_s$. Thus, even if we redefine $G_f$ to disallow edges in $E_s$, the path $p$ still remains an augmenting path in the redefined network. Since $p$ remains unchanged, an iteration of the while loop of $\\text{FORD-FULKERSON}$ updates the flow in the same way as before the redefinition. Furthermore, by disallowing some edges, we do not introduce any new augmenting paths. Thus, $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow. Now, we prove that $\\text{FORD-FULKERSON}$ never chooses an augmenting path $p$ that includes an edge $(v, s) \\in E_s$. Why? The path $p$ always starts from $s$, and if $p$ included an edge $(v, s)$, the vertex $s$ would be repeated twice in the path. Thus, $p$ would no longer be a simple path. Since $\\text{FORD-FULKERSON}$ chooses only simple paths, $p$ cannot include $(v, s)$.","title":"26.2-8"},{"location":"Chap26/26.2/#262-9","text":"Suppose that both $f$ and $f'$ are flows in a network $G$ and we compute flow $f \\uparrow f'$. Does the augmented flow satisfy the flow conservation property? Does it satisfy the capacity constraint? The augmented flow $f \\uparrow f'$ satisfies the flow conservation property but not the capacity constraint property. First, we prove that $f \\uparrow f'$ satisfies the flow conservation property. We note that if edge $(u, v) \\in E$, then $(v, u) \\ne E$ and $f'(v, u) = 0$. Thus, we can rewrite the definition of flow augmentation (equation $\\text{(26.4)}$), when applied to two flows, as $$ (f \\uparrow f')(u, v) = \\begin{cases} f(u, v) + f'(u, v) & \\text{if $(u, v) \\in E$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ The definition implies that the new flow on each edge is simply the sum of the two flows on that edge. We now prove that in $f \\uparrow f'$, the net incoming flow for each vertex equals the net outgoing flow. Let $u \\ne {s, t}$ be any vertex of $G$. We have $$ \\begin{aligned} \\sum_{v \\in V} (f \\uparrow f') (v, u) & = \\sum_{v \\in V} (f(v, u) + f'(v, u)) \\\\ & = \\sum_{v \\in V} f(v, u) + \\sum_{v \\in V} f'(v, u) \\\\ & = \\sum_{v \\in V} f(u, v) + \\sum_{v \\in V} f'(u, v) \\quad \\text{(because $f$, $f'$ obey flow conservation)} \\\\ & = \\sum_{v \\in V} (f(u, v) + f'(u, v)) \\\\ & = \\sum_{v \\in V} (f \\uparrow f') (u, v). \\end{aligned} $$ We conclude that $f \\uparrow f'$ satisfies flow conservation. We now show that $f \\uparrow f'$ need not satisfy the capacity constraint by giving a simple counterexample. Let the flow network $G$ have just a source and a target vertex, with a single edge $(s, t)$ having $c(s, t) = 1$. Define the flows $f$ and $f'$ to have $f(s, t) = f'(s, t) = 1$. Then, we have $(f \\uparrow f')(s, t) = 2 > c(s, t)$. We conclude that $f \\uparrow f'$ need not satisfy the capacity constraint.","title":"26.2-9"},{"location":"Chap26/26.2/#262-10","text":"Show how to find a maximum flow in a network $G = (V, E)$ by a sequence of at most $|E|$ augmenting paths. ($\\textit{Hint:}$ Determine the paths after finding the maximum flow.) Suppose we already have a maximum flow $f$. Consider a new graph $G$ where we set the capacity of edge $(u, v)$ to $f(u, v)$. Run Ford-Fulkerson, with the modification that we remove an edge if its flow reaches its capacity. In other words, if $f(u, v) = c(u, v)$ then there should be no reverse edge appearing in residual network. This will still produce correct output in our case because we never exceed the actual maximum flow through an edge, so it is never advantageous to cancel flow. The augmenting paths chosen in this modified version of Ford-Fulkerson are precisely the ones we want. There are at most $|E|$ because every augmenting path produces at least one edge whose flow is equal to its capacity, which we set to be the actual flow for the edge in a maximum flow, and our modification prevents us from ever destroying this progress.","title":"26.2-10"},{"location":"Chap26/26.2/#262-11","text":"The edge connectivity of an undirected graph is the minimum number $k$ of edges that must be removed to disconnect the graph. For example, the edge connectivity of a tree is $1$, and the edge connectivity of a cyclic chain of vertices is $2$. Show how to determine the edge connectivity of an undirected graph $G = (V, E)$ by running a maximum-flow algorithm on at most $|V|$ flow networks, each having $O(V)$ vertices and $O(E)$ edges. For any two vertices $u$ and $v$ in $G$, we can define a flow network $G_{uv}$ consisting of the directed version of $G$ with $s = u$, $t = v$, and all edge capacities set to $1$. (The flow network $G_{uv}$ has $V$ vertices and $2|E|$ edges, so that it has $O(V)$ vertices and $O(E)$ edges, as required. We want all capacities to be $1$ so that the number of edges of $G$ crossing a cut equals the capacity of the cut in $G_{uv}$.) Let $f_{uv}$ denote a maximum flow in $G_{uv}$. We claim that for any $u \\in V$, the edge connectivity $k$ equals $\\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$. We'll show below that this claim holds. Assuming that it holds, we can find $k$ as follows: 1 2 3 4 5 6 7 8 EDGE - CONNECTIVITY ( G ) k = \u221e select any vertex u \u2208 G . V for each vertex v \u2208 G . V - { u } set up the flow network G [ u , v ] as described above find the maximum flow f [ u , v ] on G [ u , v ] k = min ( k , | f [ u , v ] | ) return k The claim follows from the max-flow min-cut theorem and how we chose capacities so that the capacity of a cut is the number of edges crossing it. We prove that $k = \\min\\limits_{v \\in V - \\{u\\}}\\{|f_{uv}|\\}$ for any $u \\in V$ by showing separately that $k$ is at least this minimum and that $k$ is at most this minimum. Proof that $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$: Let $m = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. Suppose we remove only $m - 1$ edges from $G$. For any vertex $v$, by the max-flow min-cut theorem, $u$ and $v$ are still connected. (The max flow from $u$ to $v$ is at least $m$, hence any cut separating $u$ from $v$ has capacity at least $m$, which means at least $m$ edges cross any such cut. Thus at least one edge is left crossing the cut when we remove $m - 1$ edges.) Thus every vertex is connected to $u$, which implies that the graph is still connected. So at least $m$ edges must be removed to disconnect the graph\u2014i.e., $k \\ge \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$. Proof that $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$: Consider a vertex $v$ with the minimum $|f_{uv}|$. By the max-flow min-cut theorem, there is a cut of capacity $|f_{uv}|$ separating $u$ and $v$. Since all edge capacities are $1$, exactly $|f_{uv}|$ edges cross this cut. If these edges are removed, there is no path from $u$ to $v$, and so our graph becomes disconnected. Hence $k \\le \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$ Thus, the claim that $k = \\min\\limits_{v \\in V - \\{u\\}} \\{|f_{uv}|\\}$ for any $u \\in V$ is true.","title":"26.2-11"},{"location":"Chap26/26.2/#262-12","text":"Suppose that you are given a flow network $G$, and $G$ has edges entering the source $s$. Let $f$ be a flow in $G$ in which one of the edges $(v, s)$ entering the source has $f(v, s) = 1$. Prove that there must exist another flow $f'$ with $f'(v, s) = 0$ such that $|f| = |f'|$. Give an $O(E)$-time algorithm to compute $f'$, given $f$, and assuming that all edge capacities are integers. The idea of the proof is that if $f(v, s) = 1$, then there must exist a cycle containing the edge $(v, s)$ and for which each edge carries one unit of flow. If we reduce the flow on each edge in the cycle by one unit, we can reduce $f(v, s)$ to $0$ without affecting the value of the flow. Given the flow network $G$ and the flow $f$, we say that vertex $y$ is flow-connected to vertex $z$ if there exists a path $p$ from $y$ to $z$ such that each edge of $p$ has a positive flow on it. We also define $y$ to be flow-connected to itself. In particular, $s$ is flow-connected to $s$. We start by proving the following lemma: Lemma Let $G = (V, E)$ be a flow network and $f$ be a flow in $G$. If $s$ is not flow-connected to $v$, then $f(v, s) = 0$. Proof The idea is that since $s$ is not flow-connected to $v$, there cannot be any flow from $s$ to $v$. By using flow conservation, we will prove that there cannot be any flow from $v$ to $s$ either, and thus that $f(v, s) = 0$. Let $Y$ be the set of all vertices $y$ such that $s$ is flow-connected to $y$. By applying flow conservation to vertices in $V - Y$ and taking the sum, we obtain $$\\sum_{z \\in V - Y} \\sum_{x \\in V} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V} f(z, x).$$ Partitioning $V$ into $Y$ and $V - Y$ gives $$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x) + \\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x). \\tag{*}$$ But we have $$\\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(x, z) = \\sum_{z \\in V - Y} \\sum_{x \\in V - Y} f(z, x),$$ since the left-hand side is the same as the right-hand side, except for a change of variable names $x$ and $z$. We also have $$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(x, z) = 0,$$ since the flow from any vertex in $Y$ to any vertex in $V - Y$ must be $0$. Thus, equation $(*)$ simplifies to $$\\sum_{z \\in V - Y} \\sum_{x \\in Y} f(z, x) = 0.$$ The above equation implies that $f(z, x) = 0$ for each $z \\in V - Y$ and $x \\in Y$. In particular, since $v \\in V - Y$ and $s \\in Y$, we have that $f(v, s) = 0$. Now, we show how to construct the required flow $f'$. By the contrapositive of the lemma, $f(v, s) > 0$ implies that $s$ is flow-connected to $v$ through some path $p$. Let path $p'$ be the path $s \\overset{p}{\\leadsto} v \\to s$. Path $p'$ is a cycle that has positive flow on each edge. Because we assume that all edge capacities are integers, the flow on each edge of $p'$ is at least $1$. If we subtract $1$ from each edge of the cycle to obtain a flow $f'$, then $f'$ still satisfies the properties of a flow network and has the same value as $|f|$. Because edge $(v, s)$ is in the cycle, we have that $f'(v, s) = f(v, s) - 1 = 0$.","title":"26.2-12"},{"location":"Chap26/26.2/#262-13","text":"Suppose that you wish to find, among all minimum cuts in a flow network $G$ with integral capacities, one that contains the smallest number of edges. Show how to modify the capacities of $G$ to create a new flow network $G'$ in which any minimum cut in $G'$ is a minimum cut with the smallest number of edges in $G$. Let $(S, T)$ and $(X, Y)$ be two cuts in $G$ (and $G'$). Let $c'$ be the capacity function of $G'$. One way to define $c'$ is to add a small amount $\\delta$ to the capacity of each edge in $G$. That is, if $u$ and $v$ are two vertices, we set $$c'(u, v) = c(u, v) + \\delta.$$ Thus, if $c(S, T) = c(X, Y)$ and $(S, T)$ has fewer edges than $(X, Y)$, then we would have $c'(S, T) < c'(X, Y)$. We have to be careful and choose a small $\\delta$, lest we change the relative ordering of two unequal capacities. That is, if $c(S, T) < c(X, Y)$, then no matter many more edges $(S, T)$ has than $(X, Y)$, we still need to have $c'(S, T) < c'(X, Y)$. With this definition of $c'$, a minimum cut in $G'$ will be a minimum cut in $G$ that has the minimum number of edges. How should we choose the value of $\\delta$? Let $m$ be the minimum difference between capacities of two unequal-capacity cuts in $G$. Choose $\\delta = m / (2|E|)$. For any cut $(S, T)$, since the cut can have at most $|E|$ edges, we can bound $c'(S, T)$ by $$c(S, T) \\le c'(S, T) \\le c(S, T) + |E| \\cdot \\delta.$$ Let $c(S, T) < c(X, Y)$. We need to prove that $c'(S, T) < c'(X, Y)$. We have $$ \\begin{aligned} c'(S, T) & \\le c(S, T) + |E| \\cdot \\delta \\\\ & = c(S, T) + m / 2 \\\\ & < c(X, Y) \\qquad \\text{(since $c(X, Y) - c(S, T) \\ge m$)} \\\\ & \\le c'(X, Y). \\end{aligned} $$ Because all capacities are integral, we can choose $m = 1$, obtaining $\\delta = 1 / 2|E|$. To avoid dealing with fractional values, we can scale all capacities by $2|E|$ to obtain $$c'(u, v) = 2|E| \\cdot c(u, v) + 1.$$","title":"26.2-13"},{"location":"Chap26/26.3/","text":"26.3-1 Run the Ford-Fulkerson algorithm on the flow network in Figure 26.8 (c) and show the residual network after each flow augmentation. Number the vertices in $L$ top to bottom from $1$ to $5$ and in $R$ top to bottom from $6$ to $9$. For each iteration, pick the augmenting path that is lexicographically smallest. First, we pick an augmenting path that passes through vertices 1 and 6. Then, we pick the path going through 2 and 8. Then, we pick the path going through 3 and 7. Then, the resulting residual graph has no path from $s$ to $t$. So, we know that we are done, and that we are pairing up vertices $(1, 6)$, $(2, 8)$, and $(3, 7)$. This number of unit augmenting paths agrees with the value of the cut where you cut the edges $(s, 3)$, $(6, t)$, and $(7, t)$. 26.3-2 Prove Theorem 26.10. We proceed by induction on the number of iterations of the while loop of Ford-Fulkerson. After the first iteration, since $c$ only takes on integer values and $(u, v).f$ is set to $0$, $c_f$ only takes on integer values. Thus, lines 7 and 8 of Ford-Fulkerson only assign integer values to $(u, v).f$. Assume that $(u, v).f \\in \\mathbb Z$ for all $(u, v)$ after the $n$th iteration. On the $(n + 1)$th iteration $c_f(p)$ is set to the minimum of $c_f(u, v)$ which is an integer by the induction hypothesis. Lines 7 and 8 compute $(u, v).f$ or $(v, u).f$. Either way, these the the sum or difference of integers by assumption, so after the $(n + 1)$th iteration we have that $(u, v).f$ is an integer for all $(u, v) \\in E$. Since the value of the flow is a sum of flows of edges, we must have $|f| \\in \\mathbb Z$ as well. 26.3-3 Let $G = (V, E)$ be a bipartite graph with vertex partition $V = L \\cup R$, and let $G'$ be its corresponding flow network. Give a good upper bound on the length of any augmenting path found in $G'$ during the execution of $\\text{FORD-FULKERSON}$. By definition, an augmenting path is a simple path $s \\leadsto t$ in the residual network $G_f'$. Since $G$ has no edges between vertices in $L$ and no edges between vertices in $R$, neither does the flow network $G'$ and hence neither does $G_f'$. Also, the only edges involving $s$ or $t$ connect $s$ to $L$ and $R$ to $t$. Note that although edges in $G'$ can go only from $L$ to $R$, edges in $G_f'$ can also go from $R$ to $L$. Thus any augmenting path must go $$s \\to L \\to R \\to \\cdots \\to L \\to R \\to t,$$ crossing back and forth between $L$ and $R$ at most as many times as it can do so without using a vertex twice. It contains $s$, $t$, and equal numbers of distinct vertices from $L$ and $R$\u2014at most $2 + 2 \\cdot \\min(|L|, |R|)$ vertices in all. The length of an augmenting path (i.e., its number of edges) is thus bounded above by $2 \\cdot \\min(|L|, |R|) + 1$. 26.3-4 $\\star$ A perfect matching is a matching in which every vertex is matched. Let $G = (V, E)$ be an undirected bipartite graph with vertex partition $V = L \\cup R$, where $|L| = |R|$. For any $X \\subseteq V$, define the neighborhood of $X$ as $$N(X) = \\{y \\in V: (x, y) \\in E \\text{ for some } x \\in X\\},$$ that is, the set of vertices adjacent to some member of $X$. Prove Hall's theorem : there exists a perfect matching in $G$ if and only if $|A| \\le |N(A)|$ for every subset $A \\subseteq L$. First suppose there exists a perfect matching in $G$. Then for any subset $A \\subseteq L$, each vertex of $A$ is matched with a neighbor in $R$, and since it is a matching, no two such vertices are matched with the same vertex in $R$. Thus, there are at least $|A|$ vertices in the neighborhood of $A$. Now suppose that $|A| \\le |N(A)|$ for all $A \\subseteq L$. Run Ford-Fulkerson on the corresponding flow network. The flow is increased by $1$ each time an augmenting path is found, so it will suffice to show that this happens $|L|$ times. Suppose the while loop has run fewer than $L$ times, but there is no augmenting path. Then fewer than $L$ edges from $L$ to $R$ have flow $1$. Let $v_1 \\in L$ be such that no edge from $v_1$ to a vertex in $R$ has nonzero flow. By assumption, $v_1$ has at least one neighbor $v_1' \\in R$. If any of $v_1$'s neighbors are connected to $t$ in $G_f$ then there is a path, so assume this is not the case. Thus, there must be some edge $(v_2, v_1)$ with flow $1$. By assumption, $N(\\{v_1, v_2\\}) \\ge 2$, so there must exist $v_2' \\ne v_1'$ such that $v_2'\\in N(\\{v_1, v_2 \\})$. If $(v_2', t)$ is an edge in the residual network we're done since $v_2'$ must be a neighbor of $v_2$, so $s$, $v_1$, $v_1'$, $v_2$, $v_2'$, and $t$ is a path in $G_f$. Otherwise $v_2'$ must have a neighbor $v_3 \\in L$ such that $(v_3, v_2')$ is in $G_f$. Specifically, $v_3 \\ne v_1$ since $(v_3, v_2')$ has flow $1$, and $v_3 \\ne v_2$ since $(v_2, v_1')$ has flow $1$, so no more flow can leave $v_2$ without violating conservation of flow. Again by our hypothesis, $N(\\{v_1, v_2, v_3\\}) \\ge 3$, so there is another neighbor $v_3' \\in R$. Continuing in this fashion, we keep building up the neighborhood $v_i'$, expanding each time we find that $(v_i', t)$ is not an edge in $G_f$. This cannot happen $L$ times, since we have run the Ford-Fulkerson while-loop fewer than $|L|$ times, so there still exist edges into $t$ in $G_f$. Thus, the process must stop at some vertex $v_k'$, and we obtain an augmenting path $$s, v_1, v_1', v_2, v_2', v_3, \\ldots, v_k, v_k', t,$$ contradicting our assumption that there was no such path. Therefore the while loop runs at least $|L|$ times. By Corollary 26.3 the flow strictly increases each time by $f_p$. By Theorem 26.10 $f_p$ is an integer. In particular, it is equal to $1$. This implies that $|f| \\ge |L|$. It is clear that $|f| \\le |L|$, so we must have $|f| = |L|$. By Corollary 26.11 this is the cardinality of a maximum matching. Since $|L| = |R|$, any maximum matching must be a perfect matching. 26.3-5 $\\star$ We say that a bipartite graph $G = (V, E)$, where $V = L \\cup R$, is $d$-regular if every vertex $v \\in V$ has degree exactly $d$. Every $d$-regular bipartite graph has $|L| = |R|$. Prove that every $d$-regular bipartite graph has a matching of cardinality $|L|$ by arguing that a minimum cut of the corresponding flow network has capacity $|L|$. We convert the bipartite graph into a flow problem by making a new vertex for the source which has an edge of unit capacity going to each of the vertices in $L$, and a new vertex for the sink that has an edge from each of the vertices in $R$, each with unit capacity. We want to show that the number of edge between the two parts of the cut is at least $L$, this would get us by the max-flow-min-cut theorem that there is a flow of value at least $|L|$. The, we can apply the integrality theorem that all of the flow values are integers, meaning that we are selecting $|L|$ disjoint edges between $L$ and $R$. To see that every cut must have capacity at lest $|L|$, let $S_1$ be the side of the cut containing the source and let $S_2$ be the side of the cut containing the sink. Then, look at $L \\cap S_1$. The source has an edge going to each of $L \\cap (S_1)^c$, and there is an edge from $R \\cap S_1$ to the sink that will be cut. This means that we need that there are at least $|L \\cap S_1| - |R \\cap S_1|$ many edges going from $L \\cap S_1$ to $R \\cap S_2$. If we look at the set of all neighbors of $L \\cap S_1$, we get that there must be at least the same number of neighbors in $R$, because otherwise we could sum up the degrees going from $L \\cap S_1$ to $R$ on both sides, and get that some of the vertices in $R$ would need to have a degree higher than $d$. This means that the number of neighbors of $L \\cap S_1$ is at least $L \\cap S_1$, but we have that they are in $S_1$, but there are only $|R \\cap S_1|$ of those, so, we have that the size of the set of neighbors of $L \\cap S_1$ that are in $S_2$ is at least $|L \\cap S_1| - |R \\cap S_1|$. Since each of these neighbors has an edge crossing the cut, we have that the total number of edges that the cut breaks is at least $$(|L| - |L \\cap S_1|) + (|L \\cap S_1| - |R \\cap S_1|) + |R \\cap S_1| = |L|.$$ Since each of these edges is unit valued, the value of the cut is at least $|L|$.","title":"26.3 Maximum bipartite matching"},{"location":"Chap26/26.3/#263-1","text":"Run the Ford-Fulkerson algorithm on the flow network in Figure 26.8 (c) and show the residual network after each flow augmentation. Number the vertices in $L$ top to bottom from $1$ to $5$ and in $R$ top to bottom from $6$ to $9$. For each iteration, pick the augmenting path that is lexicographically smallest. First, we pick an augmenting path that passes through vertices 1 and 6. Then, we pick the path going through 2 and 8. Then, we pick the path going through 3 and 7. Then, the resulting residual graph has no path from $s$ to $t$. So, we know that we are done, and that we are pairing up vertices $(1, 6)$, $(2, 8)$, and $(3, 7)$. This number of unit augmenting paths agrees with the value of the cut where you cut the edges $(s, 3)$, $(6, t)$, and $(7, t)$.","title":"26.3-1"},{"location":"Chap26/26.3/#263-2","text":"Prove Theorem 26.10. We proceed by induction on the number of iterations of the while loop of Ford-Fulkerson. After the first iteration, since $c$ only takes on integer values and $(u, v).f$ is set to $0$, $c_f$ only takes on integer values. Thus, lines 7 and 8 of Ford-Fulkerson only assign integer values to $(u, v).f$. Assume that $(u, v).f \\in \\mathbb Z$ for all $(u, v)$ after the $n$th iteration. On the $(n + 1)$th iteration $c_f(p)$ is set to the minimum of $c_f(u, v)$ which is an integer by the induction hypothesis. Lines 7 and 8 compute $(u, v).f$ or $(v, u).f$. Either way, these the the sum or difference of integers by assumption, so after the $(n + 1)$th iteration we have that $(u, v).f$ is an integer for all $(u, v) \\in E$. Since the value of the flow is a sum of flows of edges, we must have $|f| \\in \\mathbb Z$ as well.","title":"26.3-2"},{"location":"Chap26/26.3/#263-3","text":"Let $G = (V, E)$ be a bipartite graph with vertex partition $V = L \\cup R$, and let $G'$ be its corresponding flow network. Give a good upper bound on the length of any augmenting path found in $G'$ during the execution of $\\text{FORD-FULKERSON}$. By definition, an augmenting path is a simple path $s \\leadsto t$ in the residual network $G_f'$. Since $G$ has no edges between vertices in $L$ and no edges between vertices in $R$, neither does the flow network $G'$ and hence neither does $G_f'$. Also, the only edges involving $s$ or $t$ connect $s$ to $L$ and $R$ to $t$. Note that although edges in $G'$ can go only from $L$ to $R$, edges in $G_f'$ can also go from $R$ to $L$. Thus any augmenting path must go $$s \\to L \\to R \\to \\cdots \\to L \\to R \\to t,$$ crossing back and forth between $L$ and $R$ at most as many times as it can do so without using a vertex twice. It contains $s$, $t$, and equal numbers of distinct vertices from $L$ and $R$\u2014at most $2 + 2 \\cdot \\min(|L|, |R|)$ vertices in all. The length of an augmenting path (i.e., its number of edges) is thus bounded above by $2 \\cdot \\min(|L|, |R|) + 1$.","title":"26.3-3"},{"location":"Chap26/26.3/#263-4-star","text":"A perfect matching is a matching in which every vertex is matched. Let $G = (V, E)$ be an undirected bipartite graph with vertex partition $V = L \\cup R$, where $|L| = |R|$. For any $X \\subseteq V$, define the neighborhood of $X$ as $$N(X) = \\{y \\in V: (x, y) \\in E \\text{ for some } x \\in X\\},$$ that is, the set of vertices adjacent to some member of $X$. Prove Hall's theorem : there exists a perfect matching in $G$ if and only if $|A| \\le |N(A)|$ for every subset $A \\subseteq L$. First suppose there exists a perfect matching in $G$. Then for any subset $A \\subseteq L$, each vertex of $A$ is matched with a neighbor in $R$, and since it is a matching, no two such vertices are matched with the same vertex in $R$. Thus, there are at least $|A|$ vertices in the neighborhood of $A$. Now suppose that $|A| \\le |N(A)|$ for all $A \\subseteq L$. Run Ford-Fulkerson on the corresponding flow network. The flow is increased by $1$ each time an augmenting path is found, so it will suffice to show that this happens $|L|$ times. Suppose the while loop has run fewer than $L$ times, but there is no augmenting path. Then fewer than $L$ edges from $L$ to $R$ have flow $1$. Let $v_1 \\in L$ be such that no edge from $v_1$ to a vertex in $R$ has nonzero flow. By assumption, $v_1$ has at least one neighbor $v_1' \\in R$. If any of $v_1$'s neighbors are connected to $t$ in $G_f$ then there is a path, so assume this is not the case. Thus, there must be some edge $(v_2, v_1)$ with flow $1$. By assumption, $N(\\{v_1, v_2\\}) \\ge 2$, so there must exist $v_2' \\ne v_1'$ such that $v_2'\\in N(\\{v_1, v_2 \\})$. If $(v_2', t)$ is an edge in the residual network we're done since $v_2'$ must be a neighbor of $v_2$, so $s$, $v_1$, $v_1'$, $v_2$, $v_2'$, and $t$ is a path in $G_f$. Otherwise $v_2'$ must have a neighbor $v_3 \\in L$ such that $(v_3, v_2')$ is in $G_f$. Specifically, $v_3 \\ne v_1$ since $(v_3, v_2')$ has flow $1$, and $v_3 \\ne v_2$ since $(v_2, v_1')$ has flow $1$, so no more flow can leave $v_2$ without violating conservation of flow. Again by our hypothesis, $N(\\{v_1, v_2, v_3\\}) \\ge 3$, so there is another neighbor $v_3' \\in R$. Continuing in this fashion, we keep building up the neighborhood $v_i'$, expanding each time we find that $(v_i', t)$ is not an edge in $G_f$. This cannot happen $L$ times, since we have run the Ford-Fulkerson while-loop fewer than $|L|$ times, so there still exist edges into $t$ in $G_f$. Thus, the process must stop at some vertex $v_k'$, and we obtain an augmenting path $$s, v_1, v_1', v_2, v_2', v_3, \\ldots, v_k, v_k', t,$$ contradicting our assumption that there was no such path. Therefore the while loop runs at least $|L|$ times. By Corollary 26.3 the flow strictly increases each time by $f_p$. By Theorem 26.10 $f_p$ is an integer. In particular, it is equal to $1$. This implies that $|f| \\ge |L|$. It is clear that $|f| \\le |L|$, so we must have $|f| = |L|$. By Corollary 26.11 this is the cardinality of a maximum matching. Since $|L| = |R|$, any maximum matching must be a perfect matching.","title":"26.3-4 $\\star$"},{"location":"Chap26/26.3/#263-5-star","text":"We say that a bipartite graph $G = (V, E)$, where $V = L \\cup R$, is $d$-regular if every vertex $v \\in V$ has degree exactly $d$. Every $d$-regular bipartite graph has $|L| = |R|$. Prove that every $d$-regular bipartite graph has a matching of cardinality $|L|$ by arguing that a minimum cut of the corresponding flow network has capacity $|L|$. We convert the bipartite graph into a flow problem by making a new vertex for the source which has an edge of unit capacity going to each of the vertices in $L$, and a new vertex for the sink that has an edge from each of the vertices in $R$, each with unit capacity. We want to show that the number of edge between the two parts of the cut is at least $L$, this would get us by the max-flow-min-cut theorem that there is a flow of value at least $|L|$. The, we can apply the integrality theorem that all of the flow values are integers, meaning that we are selecting $|L|$ disjoint edges between $L$ and $R$. To see that every cut must have capacity at lest $|L|$, let $S_1$ be the side of the cut containing the source and let $S_2$ be the side of the cut containing the sink. Then, look at $L \\cap S_1$. The source has an edge going to each of $L \\cap (S_1)^c$, and there is an edge from $R \\cap S_1$ to the sink that will be cut. This means that we need that there are at least $|L \\cap S_1| - |R \\cap S_1|$ many edges going from $L \\cap S_1$ to $R \\cap S_2$. If we look at the set of all neighbors of $L \\cap S_1$, we get that there must be at least the same number of neighbors in $R$, because otherwise we could sum up the degrees going from $L \\cap S_1$ to $R$ on both sides, and get that some of the vertices in $R$ would need to have a degree higher than $d$. This means that the number of neighbors of $L \\cap S_1$ is at least $L \\cap S_1$, but we have that they are in $S_1$, but there are only $|R \\cap S_1|$ of those, so, we have that the size of the set of neighbors of $L \\cap S_1$ that are in $S_2$ is at least $|L \\cap S_1| - |R \\cap S_1|$. Since each of these neighbors has an edge crossing the cut, we have that the total number of edges that the cut breaks is at least $$(|L| - |L \\cap S_1|) + (|L \\cap S_1| - |R \\cap S_1|) + |R \\cap S_1| = |L|.$$ Since each of these edges is unit valued, the value of the cut is at least $|L|$.","title":"26.3-5 $\\star$"},{"location":"Chap26/26.4/","text":"26.4-1 Prove that, after the procedure $\\text{INITIALIZE-PREFLOW}(G, S)$ terminates, we have $s.e \\le -|f^*|$, where $f^*$ is a maximum flow for $G$. We apply the definition of excess flow (equation $\\text{(26.14)}$) to the initial preflow $f$ created by $\\text{INITIALIZE-PREFLOW}$ (equation $\\text{(26.15)}$) to obtain $$ \\begin{aligned} e(s) & = \\sum_{v \\in V} f(v, s) - \\sum_{v \\in V} f(s, v) \\\\ & = 0 - \\sum_{v \\in V} c(s, v) \\\\ & = -\\sum_{v \\in V} c(s, v). \\end{aligned} $$ Now, $$ \\begin{aligned} -|f^*| & = \\sum_{v \\in V} f^*(v, s) - \\sum_{v \\in V} f^*(s, v) \\\\ & \\ge 0 - \\sum_{v \\in V} c(s, v) \\qquad \\text{(since $f^*(v, s) \\ge 0$ and $f^*(s, v) \\le c(s, v)$)} \\\\ & = e(s). \\end{aligned} $$ 26.4-2 Show how to implement the generic push-relabel algorithm using $O(V)$ time per relabel operation, $O(1)$ time per push, and $O(1)$ time to select an applicable operation, for a total time of $O(V^2E)$. We must select an appropriate data structure to store all the information which will allow us to select a valid operation in constant time. To do this, we will need to maintain a list of overflowing vertices. By Lemma 26.14, a push or a relabel operation always applies to an overflowing vertex. To determine which operation to perform, we need to determine whether $u.h = v.h + 1$ for some $v \\in N(u)$. We'll do this by maintaining a list $u.high$ of all neighbors of $u$ in $G_f$ which have height greater than or equal to $u$. We'll update these attributes in the $\\text{PUSH}$ and $\\text{RELABEL}$ functions. It is clear from the pseudocode given for $\\text{PUSH}$ that we can execute it in constant time, provided we have maintain the attributes $\\delta_f(u, v)$, $u.e$, $c_f(u, v)$, $(u, v).f$ and $u.h$. Each time we call $\\text{PUSH}(u, v)$ the result is that $u$ is no longer overflowing, so we must remove it from the list. Maintain a pointer $u.overflow$ to $u$'s position in the overflow list. If a vertex $u$ is not overflowing, set $u.overflow = \\text{NIL}$. Next, check if $v$ became overflowing. If so, set $v.overflow$ equal to the head of the overflow list. Since we can update the pointer in constant time and delete from a linked list given a pointer to the element to be deleted in constant time, we can maintain the list in $O(1)$. The $\\text{RELABEL}$ operation takes $O(V)$ because we need to compute the minimum $v.h$ from among all $(u, v) \\in E_f$, and there could be $|V| - 1$ many such $v$. We will also need to update $u.high$ during $\\text{RELABEL}$. When $\\text{RELABEL}(u)$ is called, set $u.high$ equal to the empty list and for each vertex $v$ which is adjacent to $u$, if $v.h = u.h + 1$, add $u$ to the list $v.high$. Since this takes constant time per adjacent vertex we can maintain the attribute in $O(V)$ per call to relabel. 26.4-3 Prove that the generic push-relabel algorithm spends a total of only $O(VE)$ time in performing all the $O(V^2)$ relabel operations. Each time we call $\\text{RELABEL}(u)$, we examine all edges $(u, v) \\in E_f$. Since the number of relabel operations is at most $2|V| - 1$ per vertex, edge $(u, v)$ will be examined during relabel operations at most $4|V| - 2 = O(V)$ times (at most $2|V| - 1$ times during calls to $\\text{RELABEL}(u)$ and at most $2|V| - 1$ times during calls to $\\text{RELABEL}(v)$). Summing up over all the possible residual edges, of which there are at most $2|E| = O(E)$, we see that the total time spent relabeling vertices is $O(VE)$. 26.4-4 Suppose that we have found a maximum flow in a flow network $G = (V, E)$ using a push-relabel algorithm. Give a fast algorithm to find a minimum cut in $G$. We can find a minimum cut, given a maximum flow found in $G = (V, E)$ by a push-relabel algorithm, in $O(V)$ time. First, find a height $\\hat h$ such that $0 < \\hat h < |V|$ and there is no vertex whose height equals $\\hat h$ at termination of the algorithm. We need consider only $|V| - 2$ vertices, since $s.h = |V|$ and $t.h = 0$. Because $\\hat h$ can be one of at most $|V| - 1$ possible values, we know that for at least one number in $1, 2, \\ldots, |V| - 1$, there will be no vertex of that height. Hence, $\\hat h$ is well defined, and it is easy to find in $O(V)$ time by using a simple boolean array indexed by heights $1, 2, \\ldots, |V| - 1$. Let $S = \\{u \\in V: u.h > \\hat h\\}$ and $T = \\{v \\in V: v.h < \\hat h\\}$. Because we know that $s.h = |V| > \\hat h$, we have $s \\in S$, and because $t.h = 0 < \\hat h$, y we have $t \\in T$, as required for a cut. We need to show that $f(u, v) = c(u, v)$, i.e., that $(u, v) \\notin E_f$, for all $u \\in S$ and $v \\in T$. Once we do that, we have that $f(S, T) = c(S, T)$, and by Corollary 26.5, $(S, T)$ is a minimum cut. Suppose for the purpose of contradiction that there exist vertices $u \\in S$ and $v \\in T$ such that $(u, v) \\in E_f$. Because $h$ is always maintained as a height function (Lemma 26.16), we have that $u.h \\le v.h + 1$. But we also have $v.h < \\hat h < u.h$, and because all values are integer, $v.h \\le u.h - 2$. Thus, we have $u.h \\le v.h + 1 \\le u.h - 2 + 1 = u.h - 1$, which gives the contradiction that $u.height \\le u.height - 1$. Thus, $(S, T)$ is a minimum cut. 26.4-5 Give an efficient push-relabel algorithm to find a maximum matching in a bipartite graph. Analyze your algorithm. First, construct the flow network for the bipartite graph as in the previous section. Then, we relabel everything in $L$. Then, we push from every vertex in $L$ to a vertex in $R$, so long as it is possible. Keeping track of those that vertices of $L$ that are still overflowing can be done by a simple bit vector. Then, we relabel everything in R and push to the last vertex. Once these operations have been done, The only possible valid operations are to relabel the vertices of $L$ that weren't able to find an edge that they could push their flow along, so could possibly have to get a push back from $R$ to $L$. This continues until there are no more operations to do. This takes time of $O(V(E + V))$. 26.4-6 Suppose that all edge capacities in a flow network $G = (V, E)$ are in the set $\\{1, 2, \\ldots, k\\}$. Analyze the running time of the generic push-relabel algorithm in terms of $|V|$, $|E|$, and $k$. ($\\textit{Hint:}$ How many times can each edge support a nonsaturating push before it becomes saturated?) The number of relabel operations and saturating pushes is the same as before. An edge can handle at most $k$ nonsaturating pushes before it becomes saturated, so the number of nonsaturating pushes is at most $2k|V||E|$. Thus, the total number of basic operations is at most $2|V|^2 + 2|V||E| + 2k|V||E| = O(kVE)$. 26.4-7 Show that we could change line 6 of $\\text{INITIALIZE-PREFLOW}$ to 1 6 s . h = | G . V | - 2 without affecting the correctness or asymptotic performance of the generic pushrelabel algorithm. If we set $s.h = |V| - 2$, we have to change our definition of a height function to allow $s.h = |V| - 2$, rather than $s.h = |V|$. The only change we need to make to the proof of correctness is to update the proof of Lemma 26.17. The original proof derives the contradiction that $s.h \\le k < |V|$, which is at odds with $s.h = |V|$. When $s.h = |V| - 2$, there is no contradiction. As in the original proof, let us suppose that we have a simple augmenting path $\\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = s$ and $v_k = t$, so that $k < |V|$. How could $(s, v_1)$ be a residual edge? It had been saturated in $\\text{INITIALIZE-PREFLOW}$, which means that we had to have pushed some flow from $v_1$ to $s$. In order for that to have happened, we must have had $v_1.h = s.h + 1$. If we set $s.h = |V| - 2$, then $v_1.h$ was $|V| - 1$ at the time. Since then, $v_1.h$ did not decrease, and so we have $v_1.h \\ge |V| - 1$. Working backwards over our augmenting path, we have $v_{k - i}.h \\le t.h + i$ for $i = 0, 1, \\ldots, k$. As before, because the augmenting path is simple, $k < |V|$. Letting $i = k - 1$, we have $v_1.h \\le t.h + k - 1 < 0 + |V| - 1$. We now have the contradiction that $v_1.h \\ge |V| - 1$ and $v_1.h < |V| - 1$, which shows that Lemma 26.17 still holds. Nothing in the analysis changes asymptotically. 26.4-8 Let $\\delta_f(u, v)$ be the distance (number of edges) from $u$ to $v$ in the residual network $G_f$. Show that the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the properties that $u.h < |V|$ implies $u.h \\le \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| \\le \\delta_f(u, s)$. We'll prove the claim by induction on the number of push and relabel operations. Initially, we have $u.h = |V|$ if $u = s$ and $0$ otherwise. We have $s.h - |V| = 0 \\le \\delta_f(s, s) = 0$ and $u.h = 0 \\le \\delta_f(u, t)$ for all $u \\ne s$, so the claim holds prior to the first iteration of the while loop on line 2 of the $\\text{GENERIC-PUSH-RELABEL}$ algorithm. Suppose that the properties have been maintained thus far. If the next iteration is a nonsaturating push then the properties are maintained because the heights and existence of edges in the residual network are preserved. If it is a saturating push then edge $(u, v)$ is removed from the residual network, which increases both $\\delta_f(u, t)$ and $\\delta_f(u, s)$, so the properties are maintained regardless of the height of $u$. Now suppose that the next iteration causes a relabel of vertex $u$. For all $v$ such that $(u, v) \\in E_f$ we must have $u.h \\le v.h$. Let $v' = \\min\\{v.h \\mid (u,v) \\in E_f\\}$. There are two cases to consider. First, suppose that $v.h < |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + \\min_{(u, v)} \\in E_f \\delta_f(v, t) = \\delta_f(u, t).$$ Second, suppose that $v'.h \\ge |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + |V| + \\min_{(u, v)} \\in E_f \\delta_f(v, s) = \\delta_f(u, s) + |V|,$$ which implies that $u.h - |V| \\le \\delta_f(u, s)$. Therefore, the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the desired properties. 26.4-9 $\\star$ As in the previous exercise, let $\\delta_f(u, v)$ be the distance from $u$ to $v$ in the residual network $G_f$. Show how to modify the generic push-relabel algorithm to maintain the property that $u.h < |V|$ implies $u.h = \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| = \\delta_f(u, s)$. The total time that your implementation dedicates to maintaining this property should be $O(VE)$. What we should do is to, for successive backwards neighborhoods of $t$, relabel everything in that neighborhood. This will only take at most $O(VE)$ time (see 26.4-3). This also has the upshot of making it so that once we are done with it, every vertex's height is equal to the quantity $\\delta_f(u, t)$. Then, since we begin with equality, after doing this, the inductive step we had in the solution to the previous exercise shows that this equality is preserved. 26.4-10 Show that the number of nonsaturating pushes executed by the $\\text{GENERIC-PUSH-RELABEL}$ procedure on a flow network $G = (V, E)$ is at most $4|V|^2|E|$ for $|V| \\ge 4$. Each vertex has maximum height $2|V| - 1$. Since heights don't decrease, and there are $|V| - 2$ vertices which can be overflowing, the maximum contribution of relabels to $\\Phi$ over all vertices is $(2|V| - 1)(|V| - 2)$. A saturating push from $u$ to $v$ increases $\\Phi$ by at most $v.h \\le 2|V| - 1$, and there are at most $2|V||E|$ saturating pushes, so the total contribution over all saturating pushes to $\\Phi$ is at most $(2|V| - 1)(2|V||E|)$. Since each nonsaturating push decrements $\\Phi$ by at least on and $\\Phi$ must equal zero upon termination, we must have that the number of nonsaturating pushes is at most $$(2|V| - 1)(|V| - 2) + (2|V| - 1)(2|V||E|) = 4|V|^2|E| + 2|V|^2 - 5|V| + 3 - 2|V||E|.$$ Using the fact that $|E| \\ge |V| - 1$ and $|V| \\ge 4$ we can bound the number of saturating pushes by $4|V|^2|E|$.","title":"26.4 Push-relabel algorithms"},{"location":"Chap26/26.4/#264-1","text":"Prove that, after the procedure $\\text{INITIALIZE-PREFLOW}(G, S)$ terminates, we have $s.e \\le -|f^*|$, where $f^*$ is a maximum flow for $G$. We apply the definition of excess flow (equation $\\text{(26.14)}$) to the initial preflow $f$ created by $\\text{INITIALIZE-PREFLOW}$ (equation $\\text{(26.15)}$) to obtain $$ \\begin{aligned} e(s) & = \\sum_{v \\in V} f(v, s) - \\sum_{v \\in V} f(s, v) \\\\ & = 0 - \\sum_{v \\in V} c(s, v) \\\\ & = -\\sum_{v \\in V} c(s, v). \\end{aligned} $$ Now, $$ \\begin{aligned} -|f^*| & = \\sum_{v \\in V} f^*(v, s) - \\sum_{v \\in V} f^*(s, v) \\\\ & \\ge 0 - \\sum_{v \\in V} c(s, v) \\qquad \\text{(since $f^*(v, s) \\ge 0$ and $f^*(s, v) \\le c(s, v)$)} \\\\ & = e(s). \\end{aligned} $$","title":"26.4-1"},{"location":"Chap26/26.4/#264-2","text":"Show how to implement the generic push-relabel algorithm using $O(V)$ time per relabel operation, $O(1)$ time per push, and $O(1)$ time to select an applicable operation, for a total time of $O(V^2E)$. We must select an appropriate data structure to store all the information which will allow us to select a valid operation in constant time. To do this, we will need to maintain a list of overflowing vertices. By Lemma 26.14, a push or a relabel operation always applies to an overflowing vertex. To determine which operation to perform, we need to determine whether $u.h = v.h + 1$ for some $v \\in N(u)$. We'll do this by maintaining a list $u.high$ of all neighbors of $u$ in $G_f$ which have height greater than or equal to $u$. We'll update these attributes in the $\\text{PUSH}$ and $\\text{RELABEL}$ functions. It is clear from the pseudocode given for $\\text{PUSH}$ that we can execute it in constant time, provided we have maintain the attributes $\\delta_f(u, v)$, $u.e$, $c_f(u, v)$, $(u, v).f$ and $u.h$. Each time we call $\\text{PUSH}(u, v)$ the result is that $u$ is no longer overflowing, so we must remove it from the list. Maintain a pointer $u.overflow$ to $u$'s position in the overflow list. If a vertex $u$ is not overflowing, set $u.overflow = \\text{NIL}$. Next, check if $v$ became overflowing. If so, set $v.overflow$ equal to the head of the overflow list. Since we can update the pointer in constant time and delete from a linked list given a pointer to the element to be deleted in constant time, we can maintain the list in $O(1)$. The $\\text{RELABEL}$ operation takes $O(V)$ because we need to compute the minimum $v.h$ from among all $(u, v) \\in E_f$, and there could be $|V| - 1$ many such $v$. We will also need to update $u.high$ during $\\text{RELABEL}$. When $\\text{RELABEL}(u)$ is called, set $u.high$ equal to the empty list and for each vertex $v$ which is adjacent to $u$, if $v.h = u.h + 1$, add $u$ to the list $v.high$. Since this takes constant time per adjacent vertex we can maintain the attribute in $O(V)$ per call to relabel.","title":"26.4-2"},{"location":"Chap26/26.4/#264-3","text":"Prove that the generic push-relabel algorithm spends a total of only $O(VE)$ time in performing all the $O(V^2)$ relabel operations. Each time we call $\\text{RELABEL}(u)$, we examine all edges $(u, v) \\in E_f$. Since the number of relabel operations is at most $2|V| - 1$ per vertex, edge $(u, v)$ will be examined during relabel operations at most $4|V| - 2 = O(V)$ times (at most $2|V| - 1$ times during calls to $\\text{RELABEL}(u)$ and at most $2|V| - 1$ times during calls to $\\text{RELABEL}(v)$). Summing up over all the possible residual edges, of which there are at most $2|E| = O(E)$, we see that the total time spent relabeling vertices is $O(VE)$.","title":"26.4-3"},{"location":"Chap26/26.4/#264-4","text":"Suppose that we have found a maximum flow in a flow network $G = (V, E)$ using a push-relabel algorithm. Give a fast algorithm to find a minimum cut in $G$. We can find a minimum cut, given a maximum flow found in $G = (V, E)$ by a push-relabel algorithm, in $O(V)$ time. First, find a height $\\hat h$ such that $0 < \\hat h < |V|$ and there is no vertex whose height equals $\\hat h$ at termination of the algorithm. We need consider only $|V| - 2$ vertices, since $s.h = |V|$ and $t.h = 0$. Because $\\hat h$ can be one of at most $|V| - 1$ possible values, we know that for at least one number in $1, 2, \\ldots, |V| - 1$, there will be no vertex of that height. Hence, $\\hat h$ is well defined, and it is easy to find in $O(V)$ time by using a simple boolean array indexed by heights $1, 2, \\ldots, |V| - 1$. Let $S = \\{u \\in V: u.h > \\hat h\\}$ and $T = \\{v \\in V: v.h < \\hat h\\}$. Because we know that $s.h = |V| > \\hat h$, we have $s \\in S$, and because $t.h = 0 < \\hat h$, y we have $t \\in T$, as required for a cut. We need to show that $f(u, v) = c(u, v)$, i.e., that $(u, v) \\notin E_f$, for all $u \\in S$ and $v \\in T$. Once we do that, we have that $f(S, T) = c(S, T)$, and by Corollary 26.5, $(S, T)$ is a minimum cut. Suppose for the purpose of contradiction that there exist vertices $u \\in S$ and $v \\in T$ such that $(u, v) \\in E_f$. Because $h$ is always maintained as a height function (Lemma 26.16), we have that $u.h \\le v.h + 1$. But we also have $v.h < \\hat h < u.h$, and because all values are integer, $v.h \\le u.h - 2$. Thus, we have $u.h \\le v.h + 1 \\le u.h - 2 + 1 = u.h - 1$, which gives the contradiction that $u.height \\le u.height - 1$. Thus, $(S, T)$ is a minimum cut.","title":"26.4-4"},{"location":"Chap26/26.4/#264-5","text":"Give an efficient push-relabel algorithm to find a maximum matching in a bipartite graph. Analyze your algorithm. First, construct the flow network for the bipartite graph as in the previous section. Then, we relabel everything in $L$. Then, we push from every vertex in $L$ to a vertex in $R$, so long as it is possible. Keeping track of those that vertices of $L$ that are still overflowing can be done by a simple bit vector. Then, we relabel everything in R and push to the last vertex. Once these operations have been done, The only possible valid operations are to relabel the vertices of $L$ that weren't able to find an edge that they could push their flow along, so could possibly have to get a push back from $R$ to $L$. This continues until there are no more operations to do. This takes time of $O(V(E + V))$.","title":"26.4-5"},{"location":"Chap26/26.4/#264-6","text":"Suppose that all edge capacities in a flow network $G = (V, E)$ are in the set $\\{1, 2, \\ldots, k\\}$. Analyze the running time of the generic push-relabel algorithm in terms of $|V|$, $|E|$, and $k$. ($\\textit{Hint:}$ How many times can each edge support a nonsaturating push before it becomes saturated?) The number of relabel operations and saturating pushes is the same as before. An edge can handle at most $k$ nonsaturating pushes before it becomes saturated, so the number of nonsaturating pushes is at most $2k|V||E|$. Thus, the total number of basic operations is at most $2|V|^2 + 2|V||E| + 2k|V||E| = O(kVE)$.","title":"26.4-6"},{"location":"Chap26/26.4/#264-7","text":"Show that we could change line 6 of $\\text{INITIALIZE-PREFLOW}$ to 1 6 s . h = | G . V | - 2 without affecting the correctness or asymptotic performance of the generic pushrelabel algorithm. If we set $s.h = |V| - 2$, we have to change our definition of a height function to allow $s.h = |V| - 2$, rather than $s.h = |V|$. The only change we need to make to the proof of correctness is to update the proof of Lemma 26.17. The original proof derives the contradiction that $s.h \\le k < |V|$, which is at odds with $s.h = |V|$. When $s.h = |V| - 2$, there is no contradiction. As in the original proof, let us suppose that we have a simple augmenting path $\\langle v_0, v_1, \\ldots, v_k \\rangle$, where $v_0 = s$ and $v_k = t$, so that $k < |V|$. How could $(s, v_1)$ be a residual edge? It had been saturated in $\\text{INITIALIZE-PREFLOW}$, which means that we had to have pushed some flow from $v_1$ to $s$. In order for that to have happened, we must have had $v_1.h = s.h + 1$. If we set $s.h = |V| - 2$, then $v_1.h$ was $|V| - 1$ at the time. Since then, $v_1.h$ did not decrease, and so we have $v_1.h \\ge |V| - 1$. Working backwards over our augmenting path, we have $v_{k - i}.h \\le t.h + i$ for $i = 0, 1, \\ldots, k$. As before, because the augmenting path is simple, $k < |V|$. Letting $i = k - 1$, we have $v_1.h \\le t.h + k - 1 < 0 + |V| - 1$. We now have the contradiction that $v_1.h \\ge |V| - 1$ and $v_1.h < |V| - 1$, which shows that Lemma 26.17 still holds. Nothing in the analysis changes asymptotically.","title":"26.4-7"},{"location":"Chap26/26.4/#264-8","text":"Let $\\delta_f(u, v)$ be the distance (number of edges) from $u$ to $v$ in the residual network $G_f$. Show that the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the properties that $u.h < |V|$ implies $u.h \\le \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| \\le \\delta_f(u, s)$. We'll prove the claim by induction on the number of push and relabel operations. Initially, we have $u.h = |V|$ if $u = s$ and $0$ otherwise. We have $s.h - |V| = 0 \\le \\delta_f(s, s) = 0$ and $u.h = 0 \\le \\delta_f(u, t)$ for all $u \\ne s$, so the claim holds prior to the first iteration of the while loop on line 2 of the $\\text{GENERIC-PUSH-RELABEL}$ algorithm. Suppose that the properties have been maintained thus far. If the next iteration is a nonsaturating push then the properties are maintained because the heights and existence of edges in the residual network are preserved. If it is a saturating push then edge $(u, v)$ is removed from the residual network, which increases both $\\delta_f(u, t)$ and $\\delta_f(u, s)$, so the properties are maintained regardless of the height of $u$. Now suppose that the next iteration causes a relabel of vertex $u$. For all $v$ such that $(u, v) \\in E_f$ we must have $u.h \\le v.h$. Let $v' = \\min\\{v.h \\mid (u,v) \\in E_f\\}$. There are two cases to consider. First, suppose that $v.h < |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + \\min_{(u, v)} \\in E_f \\delta_f(v, t) = \\delta_f(u, t).$$ Second, suppose that $v'.h \\ge |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + |V| + \\min_{(u, v)} \\in E_f \\delta_f(v, s) = \\delta_f(u, s) + |V|,$$ which implies that $u.h - |V| \\le \\delta_f(u, s)$. Therefore, the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the desired properties.","title":"26.4-8"},{"location":"Chap26/26.4/#264-9-star","text":"As in the previous exercise, let $\\delta_f(u, v)$ be the distance from $u$ to $v$ in the residual network $G_f$. Show how to modify the generic push-relabel algorithm to maintain the property that $u.h < |V|$ implies $u.h = \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| = \\delta_f(u, s)$. The total time that your implementation dedicates to maintaining this property should be $O(VE)$. What we should do is to, for successive backwards neighborhoods of $t$, relabel everything in that neighborhood. This will only take at most $O(VE)$ time (see 26.4-3). This also has the upshot of making it so that once we are done with it, every vertex's height is equal to the quantity $\\delta_f(u, t)$. Then, since we begin with equality, after doing this, the inductive step we had in the solution to the previous exercise shows that this equality is preserved.","title":"26.4-9 $\\star$"},{"location":"Chap26/26.4/#264-10","text":"Show that the number of nonsaturating pushes executed by the $\\text{GENERIC-PUSH-RELABEL}$ procedure on a flow network $G = (V, E)$ is at most $4|V|^2|E|$ for $|V| \\ge 4$. Each vertex has maximum height $2|V| - 1$. Since heights don't decrease, and there are $|V| - 2$ vertices which can be overflowing, the maximum contribution of relabels to $\\Phi$ over all vertices is $(2|V| - 1)(|V| - 2)$. A saturating push from $u$ to $v$ increases $\\Phi$ by at most $v.h \\le 2|V| - 1$, and there are at most $2|V||E|$ saturating pushes, so the total contribution over all saturating pushes to $\\Phi$ is at most $(2|V| - 1)(2|V||E|)$. Since each nonsaturating push decrements $\\Phi$ by at least on and $\\Phi$ must equal zero upon termination, we must have that the number of nonsaturating pushes is at most $$(2|V| - 1)(|V| - 2) + (2|V| - 1)(2|V||E|) = 4|V|^2|E| + 2|V|^2 - 5|V| + 3 - 2|V||E|.$$ Using the fact that $|E| \\ge |V| - 1$ and $|V| \\ge 4$ we can bound the number of saturating pushes by $4|V|^2|E|$.","title":"26.4-10"},{"location":"Chap26/26.5/","text":"26.5-1 Illustrate the execution of $\\text{RELABEL-TO-FRONT}$ in the manner of Figure 26.10 for the flow network in Figure 26.1(a). Assume that the initial ordering of vertices in $L$ is $\\langle v_1, v_2, v_3, v_4 \\rangle$ and that the neighbor lists are $$ \\begin{aligned} v_1.N & = \\langle s, v_2, v_3 \\rangle, \\\\ v_2.N & = \\langle s, v_1, v_3, v_4 \\rangle, \\\\ v_3.N & = \\langle v_1, v_2, v_4, t \\rangle, \\\\ v_4.N & = \\langle v_2, v_3, t \\rangle. \\end{aligned} $$ When we initialize the preflow, we have $26$ units of flow leaving $s$. Then, we consider $v_1$ since it is the first element in the $L$ list. When we discharge it, we increase it's height to $1$ so that it can dump $12$ of it's excess along its edge to vertex $v_3$, to discharge the rest of it, it has to increase it's height to $|V| + 1$ to discharge it back to $s$. It was already at the front, so, we consider $v_2$. We increase its height to $1$. Then, we send all of its excess along its edge to $v_4$. We move it to the front, which means we next consider $v_1$, and do nothing because it is not overflowing. Up next is vertex $v_3$. After increasing its height to $1$, it can send all of its excess to $t$. This puts $v_3$ at the front, and we consider the non-overflowing vertices $v_2$ and $v_1$. Then, we consider $v_4$, it increases its height to $1$, then sends $4$ units to $t$. Since it still has an excess of $10$ units, it increases its height once again. Then it becomes valid for it to send flow back to $v_2$ or to $v_3$. It considers $v_4$ first because of the ordering of its neighbor list. This means that $10$ units of flow are pushed back to $v_2$. Since $v_4.h$ increased, it moves to the front of the list Then, we consider $v_2$ since it is the only still overflowing vertex. We increase its height to $3$. Then, it is overflowing by $10$ so it increases its height to $3$ to send $6$ units to $v_4$. It's height increased so it goes to the of the list. Then, we consider $v_4$, which is overflowing. it increases its height to $3$, then it sends $6$ units to $v_3$. Again, it goes to the front of the list. Up next is $v_2$ which is not overflowing, $v_3$ which is, so it increases it's height by $1$ to send $4$ units of flow to $t$. Then sends $2$ units to $v_4$ after increasing in height. The excess flow keeps bobbing around the four vertices, each time requiring them to increase their height a bit to discharge to a neighbor only to have that neighbor increase to discharge it back until $v_2$ has increased in height enough to send all of it's excess back to s, this completes and gives us a maximum flow of $23$. 26.5-2 $\\star$ We would like to implement a push-relabel algorithm in which we maintain a firstin, first-out queue of overflowing vertices. The algorithm repeatedly discharges the vertex at the head of the queue, and any vertices that were not overflowing before the discharge but are overflowing afterward are placed at the end of the queue. After the vertex at the head of the queue is discharged, it is removed. When the queue is empty, the algorithm terminates. Show how to implement this algorithm to compute a maximum flow in $O(V^3)$ time. Initially, the vertices adjacent to $s$ are the only ones which are overflowing. The implementation is as follows: 1 2 3 4 5 6 7 8 PUSH - RELABEL - QUEUE ( G , s ) INITIALIZE - PREFLOW ( G , s ) let q be a new empty queue for v \u2208 G . Adj [ s ] PUSH ( q , v ) while q . head != NIL DISCHARGE ( q . head ) POP ( q ) Note that we need to modify the $\\text{DISCHARGE}$ algorithm to push vertices $v$ onto the queue if $v$ was not overflowing before a discharge but is overflowing after one. Between lines 7 and 8 of $\\text{DISCHARGE}(u)$, add the line \"if $v.e > 0$, $\\text{PUSH}(q, v)$.\" This is an implementation of the generic push-relabel algorithm, so we know it is correct. The analysis of runtime is almost identical to that of Theorem 26.30. We just need to verify that there are at most $|V|$ calls to $\\text{DISCHARGE}$ between two consecutive relabel operations. Observe that after calling $\\text{PUSH}(u, v)$, Corollary 26.28 tells us that no admissible edges are entering $v$. Thus, once $v$ is put into the queue because of the push, it can't be added again until it has been relabeled. Thus, at most $|V|$ vertices are added to the queue between relabel operations. 26.5-3 Show that the generic algorithm still works if $\\text{RELABEL}$ updates $u.h$ by simply computing $u.h = u.h + 1$. How would this change affect the analysis of $\\text{RELABEL-TO-FRONT}$? If we change relabel to just increment the value of $u$, we will not be ruining the correctness of the Algorithm. This is because since it only applies when $u.h \\le v.h$, we won't be every creating a graph where $h$ ceases to be a height function, since $u.h$ will only ever be increasing by exactly $1$ whenever relabel is called, ensuring that $u.h + 1 \\le v.h$. This means that Lemmatae 26.15 and 26.16 will still hold. Even Corollary 26.21 holds since all it counts on is that relabel causes some vertex's $h$ value to increase by at least $1$, it will still work when we have all of the operations causing it to increase by exactly $1$. However, Lemma 26.28 will no longer hold. That is, it may require more than a single relabel operation to cause an admissible edge to appear, if for example, $u.h$ was strictly less than the $h$ values of all its neighbors. However, this lemma is not used in the proof of Exercise 26.4-3, which bounds the number of relabel operations. Since the number of relabel operations still have the same bound, and we know that we can simulate the old relabel operation by doing (possibly many) of these new relabel operations, we have the same bound as in the original algorithm with this different relabel operation. 26.5-4 $\\star$ Show that if we always discharge a highest overflowing vertex, we can make the push-relabel method run in $O(V^3)$ time. We'll keep track of the heights of the overflowing vertices using an array and a series of doubly linked lists. In particular, let $A$ be an array of size $|V|$, and let $A[i]$ store a list of the elements of height $i$. Now we create another list $L$, which is a list of lists. The head points to the list containing the vertices of highest height. The next pointer of this list points to the next nonempty list stored in $A$, and so on. This allows for constant time insertion of a vertex into $A$, and also constant time access to an element of largest height, and because all lists are doubly linked, we can add and delete elements in constant time. Essentially, we are implementing the algorithm of Exercise 26.5-2, but with the queue replaced by a priority queue with constant time operations. As before, it will suffice to show that there are at most $|V|$ calls to discharge between consecutive relabel operations. Consider what happens when a vertex $v$ is put into the priority queue. There must exist a vertex $u$ for which we have called $\\text{PUSH}(u, v)$. After this, no ad- missible edge is entering $v$, so it can't be added to the priority queue again until after a relabel operation has occurred on $v$. Moreover, every call to $\\text{DISCHARGE}$ terminates with a $\\text{PUSH}$, so for every call to $\\text{DISCHARGE}$ there is another vertex which can't be added until a relabel operation occurs. After $|V|$ $\\text{DISCHARGE}$ operations and no relabel operations, there are no remaining valid $\\text{PUSH}$ operations, so either the algorithm terminates, or there is a valid relabel operation which is performed. Thus, there are $O(V^3)$ calls to $\\text{DISCHARGE}$. By carrying out the rest of the analysis of Theorem 26.30, we conclude that the runtime is $O(V^3)$. 26.5-5 Suppose that at some point in the execution of a push-relabel algorithm, there exists an integer $0 < k \\le |V| - 1$ for which no vertex has $v.h = k$. Show that all vertices with $v.h > k$ are on the source side of a minimum cut. If such a $k$ exists, the gap heuristic updates every vertex $v \\in V - \\{s\\}$ for which $v.h > k$, to set $v.h = \\max(v.h, |V| + 1)$. Show that the resulting attribute $h$ is a height function. (The gap heuristic is crucial in making implementations of the push-relabel method perform well in practice.) Suppose to try and obtain a contradiction that there were some minimum cut for which a vertex that had $v.h > k$ were on the sink side of that cut. For that minimum cut, there is a residual flow network for which that cut is saturated. Then, if there were any vertices that were also on the sink side of the cut which had an edge going to $v$ in this residual flow network, since it's $h$ value cannot be equal to $k$, we know that it must be greater than $k$ since it could be only at most one less than $v$. We can continue in this way to let $S$ be the set of vertices on the sink side of the graph which have an $h$ value greater than $k$. Suppose that there were some simple path from a vertex in $S$ to $s$. Then, at each of these steps, the height could only decrease by at most $1$, since it cannot get from above $k$ to $0$ without going through $k$, we know that there is no path in the residual flow network going from a vertex in $S$ to $s$. Since a minimal cut corresponds to disconnected parts of the residual graph for a maximum flow, and we know there is no path from $S$ to $s$, there is a minimum cut for which $S$ lies entirely on the source side of the cut. This was a contradiction to how we selected $v$, and so have shown the first claim. Now we show that after updating the $h$ values as suggested, we are still left with a height function. Suppose we had an edge $(u, v)$ in the residual graph. We knew from before that $u.h \\le v.h + 1$. However, this means that if $u.h > k$, so must be $v.h$. So, if both were above $k$, we would be making them equal, causing the inequality to still hold. Also, if just $v.k$ were above $k$, then we have not decreased it's $h$ value, meaning that the inequality also still must hold. Since we have not changed the value of $s.h$, and $t.h$, we have all the required properties to have a height function after modifying the $h$ values as described.","title":"26.5 The relabel-to-front algorithm"},{"location":"Chap26/26.5/#265-1","text":"Illustrate the execution of $\\text{RELABEL-TO-FRONT}$ in the manner of Figure 26.10 for the flow network in Figure 26.1(a). Assume that the initial ordering of vertices in $L$ is $\\langle v_1, v_2, v_3, v_4 \\rangle$ and that the neighbor lists are $$ \\begin{aligned} v_1.N & = \\langle s, v_2, v_3 \\rangle, \\\\ v_2.N & = \\langle s, v_1, v_3, v_4 \\rangle, \\\\ v_3.N & = \\langle v_1, v_2, v_4, t \\rangle, \\\\ v_4.N & = \\langle v_2, v_3, t \\rangle. \\end{aligned} $$ When we initialize the preflow, we have $26$ units of flow leaving $s$. Then, we consider $v_1$ since it is the first element in the $L$ list. When we discharge it, we increase it's height to $1$ so that it can dump $12$ of it's excess along its edge to vertex $v_3$, to discharge the rest of it, it has to increase it's height to $|V| + 1$ to discharge it back to $s$. It was already at the front, so, we consider $v_2$. We increase its height to $1$. Then, we send all of its excess along its edge to $v_4$. We move it to the front, which means we next consider $v_1$, and do nothing because it is not overflowing. Up next is vertex $v_3$. After increasing its height to $1$, it can send all of its excess to $t$. This puts $v_3$ at the front, and we consider the non-overflowing vertices $v_2$ and $v_1$. Then, we consider $v_4$, it increases its height to $1$, then sends $4$ units to $t$. Since it still has an excess of $10$ units, it increases its height once again. Then it becomes valid for it to send flow back to $v_2$ or to $v_3$. It considers $v_4$ first because of the ordering of its neighbor list. This means that $10$ units of flow are pushed back to $v_2$. Since $v_4.h$ increased, it moves to the front of the list Then, we consider $v_2$ since it is the only still overflowing vertex. We increase its height to $3$. Then, it is overflowing by $10$ so it increases its height to $3$ to send $6$ units to $v_4$. It's height increased so it goes to the of the list. Then, we consider $v_4$, which is overflowing. it increases its height to $3$, then it sends $6$ units to $v_3$. Again, it goes to the front of the list. Up next is $v_2$ which is not overflowing, $v_3$ which is, so it increases it's height by $1$ to send $4$ units of flow to $t$. Then sends $2$ units to $v_4$ after increasing in height. The excess flow keeps bobbing around the four vertices, each time requiring them to increase their height a bit to discharge to a neighbor only to have that neighbor increase to discharge it back until $v_2$ has increased in height enough to send all of it's excess back to s, this completes and gives us a maximum flow of $23$.","title":"26.5-1"},{"location":"Chap26/26.5/#265-2-star","text":"We would like to implement a push-relabel algorithm in which we maintain a firstin, first-out queue of overflowing vertices. The algorithm repeatedly discharges the vertex at the head of the queue, and any vertices that were not overflowing before the discharge but are overflowing afterward are placed at the end of the queue. After the vertex at the head of the queue is discharged, it is removed. When the queue is empty, the algorithm terminates. Show how to implement this algorithm to compute a maximum flow in $O(V^3)$ time. Initially, the vertices adjacent to $s$ are the only ones which are overflowing. The implementation is as follows: 1 2 3 4 5 6 7 8 PUSH - RELABEL - QUEUE ( G , s ) INITIALIZE - PREFLOW ( G , s ) let q be a new empty queue for v \u2208 G . Adj [ s ] PUSH ( q , v ) while q . head != NIL DISCHARGE ( q . head ) POP ( q ) Note that we need to modify the $\\text{DISCHARGE}$ algorithm to push vertices $v$ onto the queue if $v$ was not overflowing before a discharge but is overflowing after one. Between lines 7 and 8 of $\\text{DISCHARGE}(u)$, add the line \"if $v.e > 0$, $\\text{PUSH}(q, v)$.\" This is an implementation of the generic push-relabel algorithm, so we know it is correct. The analysis of runtime is almost identical to that of Theorem 26.30. We just need to verify that there are at most $|V|$ calls to $\\text{DISCHARGE}$ between two consecutive relabel operations. Observe that after calling $\\text{PUSH}(u, v)$, Corollary 26.28 tells us that no admissible edges are entering $v$. Thus, once $v$ is put into the queue because of the push, it can't be added again until it has been relabeled. Thus, at most $|V|$ vertices are added to the queue between relabel operations.","title":"26.5-2 $\\star$"},{"location":"Chap26/26.5/#265-3","text":"Show that the generic algorithm still works if $\\text{RELABEL}$ updates $u.h$ by simply computing $u.h = u.h + 1$. How would this change affect the analysis of $\\text{RELABEL-TO-FRONT}$? If we change relabel to just increment the value of $u$, we will not be ruining the correctness of the Algorithm. This is because since it only applies when $u.h \\le v.h$, we won't be every creating a graph where $h$ ceases to be a height function, since $u.h$ will only ever be increasing by exactly $1$ whenever relabel is called, ensuring that $u.h + 1 \\le v.h$. This means that Lemmatae 26.15 and 26.16 will still hold. Even Corollary 26.21 holds since all it counts on is that relabel causes some vertex's $h$ value to increase by at least $1$, it will still work when we have all of the operations causing it to increase by exactly $1$. However, Lemma 26.28 will no longer hold. That is, it may require more than a single relabel operation to cause an admissible edge to appear, if for example, $u.h$ was strictly less than the $h$ values of all its neighbors. However, this lemma is not used in the proof of Exercise 26.4-3, which bounds the number of relabel operations. Since the number of relabel operations still have the same bound, and we know that we can simulate the old relabel operation by doing (possibly many) of these new relabel operations, we have the same bound as in the original algorithm with this different relabel operation.","title":"26.5-3"},{"location":"Chap26/26.5/#265-4-star","text":"Show that if we always discharge a highest overflowing vertex, we can make the push-relabel method run in $O(V^3)$ time. We'll keep track of the heights of the overflowing vertices using an array and a series of doubly linked lists. In particular, let $A$ be an array of size $|V|$, and let $A[i]$ store a list of the elements of height $i$. Now we create another list $L$, which is a list of lists. The head points to the list containing the vertices of highest height. The next pointer of this list points to the next nonempty list stored in $A$, and so on. This allows for constant time insertion of a vertex into $A$, and also constant time access to an element of largest height, and because all lists are doubly linked, we can add and delete elements in constant time. Essentially, we are implementing the algorithm of Exercise 26.5-2, but with the queue replaced by a priority queue with constant time operations. As before, it will suffice to show that there are at most $|V|$ calls to discharge between consecutive relabel operations. Consider what happens when a vertex $v$ is put into the priority queue. There must exist a vertex $u$ for which we have called $\\text{PUSH}(u, v)$. After this, no ad- missible edge is entering $v$, so it can't be added to the priority queue again until after a relabel operation has occurred on $v$. Moreover, every call to $\\text{DISCHARGE}$ terminates with a $\\text{PUSH}$, so for every call to $\\text{DISCHARGE}$ there is another vertex which can't be added until a relabel operation occurs. After $|V|$ $\\text{DISCHARGE}$ operations and no relabel operations, there are no remaining valid $\\text{PUSH}$ operations, so either the algorithm terminates, or there is a valid relabel operation which is performed. Thus, there are $O(V^3)$ calls to $\\text{DISCHARGE}$. By carrying out the rest of the analysis of Theorem 26.30, we conclude that the runtime is $O(V^3)$.","title":"26.5-4 $\\star$"},{"location":"Chap26/26.5/#265-5","text":"Suppose that at some point in the execution of a push-relabel algorithm, there exists an integer $0 < k \\le |V| - 1$ for which no vertex has $v.h = k$. Show that all vertices with $v.h > k$ are on the source side of a minimum cut. If such a $k$ exists, the gap heuristic updates every vertex $v \\in V - \\{s\\}$ for which $v.h > k$, to set $v.h = \\max(v.h, |V| + 1)$. Show that the resulting attribute $h$ is a height function. (The gap heuristic is crucial in making implementations of the push-relabel method perform well in practice.) Suppose to try and obtain a contradiction that there were some minimum cut for which a vertex that had $v.h > k$ were on the sink side of that cut. For that minimum cut, there is a residual flow network for which that cut is saturated. Then, if there were any vertices that were also on the sink side of the cut which had an edge going to $v$ in this residual flow network, since it's $h$ value cannot be equal to $k$, we know that it must be greater than $k$ since it could be only at most one less than $v$. We can continue in this way to let $S$ be the set of vertices on the sink side of the graph which have an $h$ value greater than $k$. Suppose that there were some simple path from a vertex in $S$ to $s$. Then, at each of these steps, the height could only decrease by at most $1$, since it cannot get from above $k$ to $0$ without going through $k$, we know that there is no path in the residual flow network going from a vertex in $S$ to $s$. Since a minimal cut corresponds to disconnected parts of the residual graph for a maximum flow, and we know there is no path from $S$ to $s$, there is a minimum cut for which $S$ lies entirely on the source side of the cut. This was a contradiction to how we selected $v$, and so have shown the first claim. Now we show that after updating the $h$ values as suggested, we are still left with a height function. Suppose we had an edge $(u, v)$ in the residual graph. We knew from before that $u.h \\le v.h + 1$. However, this means that if $u.h > k$, so must be $v.h$. So, if both were above $k$, we would be making them equal, causing the inequality to still hold. Also, if just $v.k$ were above $k$, then we have not decreased it's $h$ value, meaning that the inequality also still must hold. Since we have not changed the value of $s.h$, and $t.h$, we have all the required properties to have a height function after modifying the $h$ values as described.","title":"26.5-5"},{"location":"Chap26/Problems/26-1/","text":"A$n \\times n$ grid is an undirected graph consisting of $n$ rows and $n$ columns of vertices, as shown in Figure 26.11. We denote the vertex in the $i$th row and the $j$th column by $(i, j)$. All vertices in a grid have exactly four neighbors, except for the boundary vertices, which are the points $(i, j)$ for which $i = 1$, $i = n$, $j = 1$, or $j = n$. Given $m \\le n^2$ starting points $(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)$ in the grid, the escape problem is to determine whether or not there are $m$ vertex-disjoint paths from the starting points to any $m$ different points on the boundary. For example, the grid in Figure 26.11(a) has an escape, but the grid in Figure 26.11(b) does not. a. Consider a flow network in which vertices, as well as edges, have capacities. That is, the total positive flow entering any given vertex is subject to a capacity constraint. Show that determining the maximum flow in a network with edge and vertex capacities can be reduced to an ordinary maximum-flow problem on a flow network of comparable size. b. Describe an efficient algorithm to solve the escape problem, and analyze its running time. a. This problem is identical to exercise 26.1-7. b. Construct a vertex constrained flow network from the instance of the escape problem by letting our flow network have a vertex (each with unit capacity) for each intersection of grid lines, and have a bidirectional edge with unit capacity for each pair of vertices that are adjacent in the grid. Then, we will put a unit capacity edge going from $s$ to each of the distinguished vertices, and a unit capacity edge going from each vertex on the sides of the grid to $t$. Then, we know that a solution to this problem will correspond to a solution to the escape problem because all of the augmenting paths will be a unit flow, because every edge has unit capacity. This means that the flows through the grid will be the paths taken. This gets us the escaping paths if the total flow is equal to $m$ (we know it cannot be greater than $m$ by looking at the cut which has $s$ by itself). And, if the max flow is less than $m$, we know that the escape problem is not solvable, because otherwise we could construct a flow with value $m$ from the list of disjoint paths that the people escaped along.","title":"26-1 Escape problem"},{"location":"Chap26/Problems/26-2/","text":"A path cover of a directed graph $G = (V, E)$ is a set $P$ of vertex-disjoint paths such that every vertex in $V$ is included in exactly one path in $P$. Paths may start and end anywhere, and they may be of any length, including $0$. A minimum path cover of $G$ is a path cover containing the fewest possible paths. a. Give an efficient algorithm to find a minimum path cover of a directed acyclic graph $G = (V, E)$. ($\\textit{Hint:}$ Assuming that $V = \\{1, 2, \\ldots, n\\}$, construct the graph $G' = (V', E')$, where $$ \\begin{aligned} V' & = \\{x_0, x_1, \\ldots, x_n\\} \\cup \\{y_0, y_1, \\ldots, y_n\\}, \\\\ E' & = \\{(x_0, x_i): i \\in V\\} \\cup \\{(y_i, y_0): i \\in V\\} \\cup \\{(x_i, y_j): (i, j) \\in E\\}, \\end{aligned} $$ and run a maximum-flow algorithm.) b. Does your algorithm work for directed graphs that contain cycles? Explain. a. The idea is to use a maximum-flow algorithm to find a maximum bipartite matching that selects the edges to use in a minimum path cover. We must show how to formulate the max-flow problem and how to construct the path cover from the resulting matching, and we must prove that the algorithm indeed finds a minimum path cover. Define $G'$ as suggested, with directed edges. Make $G'$ into a flow network with source $x_0$ and sink $y_0$ by defining all edge capacities to be $1$. $G'$ is the flow network corresponding to a bipartite graph $G''$ in which $L = \\{x_1, \\ldots, x_n\\}$, $R = \\{y_1, \\ldots, y_n\\}$, and the edges are the (undirected version of the) subset of $E'$ that doesn't involve $x_0$ or $y_0$ . The relationship of $G$ to the bipartite graph $G''$ is that every vertex $i$ in $G$ is represented by two vertices, $x_i$ and $y_i$, in $G''$. Edge $(i, j)$ in $G$ corresponds to edge $(x_i, y_j)$ in $G''$. That is, an edge $(x_i, y_j)$ in $G''$ means that an edge in $G$ leaves $i$ and enters $j$. Vertex $x_i$ tells us about edges leaving $i$, and $y_i$ tells us about edges entering $i$. The edges in a bipartite matching in $G''$ can be used in a path cover of $G$, for the following reasons: In a bipartite matching, no vertex is used more than once. In a bipartite matching in $G''$, since no $x_i$ is used more than once, at most one edge in the matching leaves any vertex $i$ in $G$. Similarly, since no $y_j$ is used more than once, at most one edge in the matching enters any vertex $j$ in $G$. In a path cover, since no vertex appears in more than one path, at most one path edge enters each vertex and at most one path edge leaves each vertex. We can construct a path cover $P$ from any bipartite matching $M$ (not just a maximum matching) by moving from some $x_i$ to its matching $y_j$ (if any), then from $x_j$ to its matching $y_k$, and so on, as follows: Start a new path containing a vertex $i$ that has not yet been placed in a path. If $x_i$ is unmatched, the path can't go any farther; just add it to $P$. If $x_i$ is matched to some $y_j$, add $j$ to the current path. If $j$ has already been placed in a path (i.e., though we've just entered $j$ by processing $y_j$, we've already built a path that leaves $j$ by processing $x_j$), combine this path with that one and go back to step 1. Otherwise go to step 2 to process $x_j$. This algorithm constructs a path cover, for the following reasons: Every vertex is put into some path, because we keep picking an unused vertex from which to start a path until there are no unused vertices. No vertex is put into two paths, because every $x_i$ is matched to at most one $y_j$, and vice versa. That is, at most one candidate edge leaves each vertex, and at most one candidate edge enters each vertex. When building a path, we start or enter a vertex and then leave it, building a single path. If we ever enter a vertex that was left earlier, it must have been the start of another path, since there are no cycles, and we combine those paths so that the vertex is entered and left on a single path. Every edge in $M$ is used in some path because we visit every $x_i$, and we incorporate the single edge, if any, from each visited $x_i$. Thus, there is a one-to-one correspondence between edges in the matching and edges in the constructed path cover. We now show that the path cover $P$ constructed above has the fewest possible paths when the matching is maximum. Let $f$ be the flow corresonding to the bipartite matching $M$. $$ \\begin{aligned} |V| & = \\sum_{p \\in P} \\text{(\\# vertices in $p$)} & \\text{(every vertex is on exactly 1 path)} \\\\ & = \\sum_{p \\in P} \\text{(1 + \\# edges in $p$)} \\\\ & = \\sum_{p \\in P} 1 + \\sum_{p \\in P} \\text{(\\# edges in $p$)} \\\\ & = |P| + |M| & \\text{(by 1-to-1 correspondence)} \\\\ & = |P| + |f|. & \\text{(by Lemma 26.9)} \\end{aligned} $$ Thus, for the fixed set $V$ in our graph $G$, $|P|$ (the number of paths) is minimized when the flow $f$ is maximized. The overall algorithm is as follows: Use $\\text{FORD-FULKERSON}$ to find a maximum flow in $G'$ and hence a maximum bipartite matching $M$ in $G''$. Construct the path cover as described above. Time $O(VE)$ total: $O(V + E)$ to set up $G'$, $O(VE)$ to find the maximum bipartite matching, $O(E)$ to trace the paths, because each edge $\\in M$ is traversed only once and there are $O(E)$ edges in $M$. b. The algorithm does not work if there are cycles. Consider a graph $G$ with $4$ vertices, consisting of a directed triangle and an edge pointing to the triangle: $$E = \\{(1, 2), (2, 3), (3, 1), (4, 1)\\}.$$ $G$ can be covered with a single path: $4 \\to 1 \\to 2 \\to 3$, but our algorithm might find only a $2$-path cover. In the bipartite graph $G'$, the edges $(x_i, y_j)$ are $$(x_1, y_2), (x_2, y_3), (x_3, y_1), (x_4, y_1).$$ There are $4$ edges from an $x_i$ to a $y_j$, but $2$ of them lead to $y_1$, so a maximum bipartite matching can have only $3$ edges (and the maximum flow in $G'$ has value $3$). In fact, there are $2$ possible maximum matchings. It is always possible to match $(x_1, y_2)$ and $(x_2, y_3)$, and then either $(x_3, y_1)$ or $(x_4, y_1)$ can be chosen, but not both. The maximum flow found by one of our max-flow algorithms could find the flow corresponding to either of these matchings, since both are maximal. If it finds the matching with edge $(x_3, x_1)$, then the matching would not contain $(x_4, x_1)$; given that matching, our path algorithm is forced to produce $2$ paths, one of which contains just the vertex $4$.","title":"26-2 Minimum path cover"},{"location":"Chap26/Problems/26-3/","text":"Professor Gore wants to open up an algorithmic consulting company. He has identified n important subareas of algorithms (roughly corresponding to different portions of this textbook), which he represents by the set $A = \\{A_1, A_2, \\ldots, A_n\\}$. In each subarea $A_k$, he can hire an expert in that area for $c_k$ dollars. The consulting company has lined up a set $J = \\{J_1, J_2, \\ldots, J_m\\}$ of potential jobs. In order to perform job $J_i$, the company needs to have hired experts in a subset $R_i \\subseteq A$ of subareas. Each expert can work on multiple jobs simultaneously. If the company chooses to accept job $J_i$, it must have hired experts in all subareas in $R_i$, and it will take in revenue of $p_i$ dollars. Professor Gore's job is to determine which subareas to hire experts in and which jobs to accept in order to maximize the net revenue, which is the total income from jobs accepted minus the total cost of employing the experts. Consider the following flow network $G$. It contains a source vertex $s$, vertices $A_1, A_2, \\ldots, A_n$, vertices $J_1, J_2, \\ldots, J_m$, and a sink vertex $t$. For $k = 1, 2, \\ldots, n$, the flow network contains an edge $(s, A_k)$ with capacity $c(s, A_k) = c_k$, and for $i = 1, 2, \\ldots, m$, the flow network contains an edge $(J_i, t)$ with capacity $c(J_i, t) = p_i$. For $k = 1, 2, \\ldots, n$ and $i = 1, 2, \\ldots, m$, if $A_k \\in R_i$, then $G$ contains an edge $(A_k, J_i)$ with capacity $c(A_k, J_i) = \\infty$. a. Show that if $J_i \\in T$ for a finite-capacity cut $(S, T)$ of $G$, then $A_k \\in T$ for each $A_k \\in R_i$. b. Show how to determine the maximum net revenue from the capacity of a minimum cut of $G$ and the given $p_i$ values. c. Give an efficient algorithm to determine which jobs to accept and which experts to hire. Analyze the running time of your algorithm in terms of $m$, $n$, and $r = \\sum_{i = 1}^m |R_i|$. a. Assume for the sake of contradiction that $A_k \\notin T$ for some $A_k \\in R_i$. Since $A_k \\notin T$, we must have $A_k \\in S$. On the other hand, we have $J_i \\in T$. Thus, the edge $(A_k, J_i)$ crosses the cut $(S, T)$. But $c(A_k, J_i) = \\infty$ by construction, which contradicts the assumption that $(S, T)$ is a finite-capacity cut. b. Let us define a project-plan as a set of jobs to accept and experts to hire. Let $P$ be a project-plan. We assume that $P$ has two attributes. The attribute $P.J$ denotes the set of accepted jobs, and $P.A$ denotes the set of hired experts. A valid project-plan is one in which we have hired all experts that are required by the accepted jobs. Specifically, let $P$ be a valid project plan. If $J_i \\in P.J$, then $A_k \\in P.A$ for each $A_k \\in R_i$. Note that Professor Gore might decide to hire more experts than those that are actually required. We define the revenue of a project-plan as the total profit from the accepted jobs minus the total cost of the hired experts. The problem asks us to find a valid project plan with maximum revenue. We start by proving the following lemma, which establishes the relationship between the capacity of a cut in flow network $G$ and the revenue of a valid project-plan. Lemma (Min-cut max-revenue) There exists a finite-capacity cut $(S, T)$ of $G$ with capacity $c(S, T)$ if and only if there exists a valid project-plan with net revenue $(\\sum_{J_i \\in J} p_i) - c(S, T)$. Proof Let $(S, T)$ be a finite-capacity cut of $G$ with capacity $c(S, T)$. We prove one direction of the lemma by constructing the required project-plan. Construct the project-plan $P$ by including $J_i$ in $P.J$ if and only if $J_i \\in T$ and including $A_k$ in $P.A$ if and only if $A_k \\in T$. From part (a), $P$ is a valid project-plan, since, for every $J_i \\in P.J$, we have $A_k \\in P.A$ for each $A_k \\in R_i$. Since the capacity of the cut is finite, there cannot be any edges of the form $(A_k, J_i)$ crossing the cut, where $A_k \\in S$ and $J_i \\in T$. All edges going from a vertex in $S$ to a vertex in $T$ must be either of the form $(s, A_k)$ or of the form $(J_i, t)$. Let $E_A$ be the set of edges of the form $(s, A_k)$ that cross the cut, and let $E_J$ be the set of edges of the form $(J_i, t)$ that cross the cut, so that $$c(S, T) = \\sum_{(s, A_k) \\in E_A} c(s, A_k) + \\sum_{(J_i, j) \\in E_J} c(J_i, t).$$ Consider edges of the form $(s, A_k)$. We have $$ \\begin{aligned} (s, A_k) \\in E_A & \\text{ if and only if $A_k \\in T$} \\\\ & \\text{ if and only if $A_k \\in P.A$}. \\end{aligned} $$ By construction, $c(s, A_k) = c_k$. Taking summations over $E_A$ and over $P.A$, we obtain $$\\sum_{(s, A_k) \\in E_A} c(s, A_k) = \\sum_{A_k \\in P.A} c_k.$$ Similarly, consider edges of the form $(J_i, t)$. We have $$ \\begin{aligned} (J_i, t) \\in E_J & \\text{ if and only if $J_i \\in S$} \\\\ & \\text{ if and only if $J_i \\notin T$} \\\\ & \\text{ if and only if $J_i \\notin P.J$}. \\end{aligned} $$ By construction, $c(J_i, t) = p_i$. Taking summations over $E_J$ and over $P.J$, we obtain $$\\sum_{(J_i, t) \\in E_J} c(J_i, t) = \\sum_{J_i \\notin P.J} p_i.$$ Let $v$ be the net revenue of $P$. Then, we have $$ \\begin{aligned} v & = \\sum_{J_i \\in P.J} p_i - \\sum_{A_k \\in P.A} c_k \\\\ & = \\Bigg( \\sum_{J_i \\in J} p_i - \\sum_{J_i \\notin P.J} p_i \\Bigg) - \\sum_{A_k \\in P.A} c_k \\\\ & = \\sum_{J_i \\in J} p_i - \\Bigg( \\sum_{J_i \\notin P.J} p_i + \\sum_{A_k \\in P.A} c_k \\Bigg) \\\\ & = \\sum_{J_i \\in J} p_i - \\Bigg( \\sum_{(J_i, t) \\in E_J} c(J_i, t) + \\sum_{(s, A_k) \\in E_A} c(s, A_k) \\Bigg) \\\\ & = \\Bigg( \\sum_{J_i \\in J} p_i \\Bigg) - c(S, T). \\end{aligned} $$ Now, we prove the other direction of the lemma by constructing the required cut from a valid project-plan. Construct the cut $(S, T)$ as follows. For every $J_i \\in P.J$, let $J_i \\in T$. For every $A_k \\in P.A$, let $A_k \\in T$. First, we prove that the cut $(S, T)$ is a finite-capacity cut. Since edges of the form $(A_k, J_i)$ are the only infinite-capacity edges, it suffices to prove that there are no edges $(A_k, J_i)$ such that $A_k \\in S$ and $J_i \\in T$. For the purpose of contradiction, assume there is an edge $(A_k, J_i)$ such that $A_k \\in S$ and $J_i \\in T$. By our constuction, we must have $J_i \\in P.J$ and $A_k \\notin P.A$. But since the edge $(A_k, J_i)$ exists, we have $A_k \\in R_i$. Since $P$ is a valid project-plan, we derive the contradiction that $A_k$ must have been in $P.A$. From here on, the analysis is the same as the previous direction. In particular, the last equation from the previous analysis holds: the net revenue $v$ equals $(\\sum_{J_i \\in J} p_i) - c(S, T)$. We conclude that the problem of finding a maximum-revenue project-plan reduces to the problem of finding a minimum cut in $G$. Let $(S, T)$ be a minimum cut. From the lemma, the maximum net revenue is given by $$\\Bigg( \\sum_{j_i \\in J} p_i \\Bigg) - c(S, T).$$ c. Construct the flow network $G$ as shown in the problem statement. Obtain a minimum cut $(S, T)$ by running any of the maximum-flow algorithms (say, Edmonds-Karp). Construct the project plan $P$ as follows: add $J_i$ to $P.J$ if and only if $J_i \\in T$. Add $A_k$ to $P.A$ if and only if $A_k \\in T$. First, we note that the number of vertices in $G$ is $|V| = m + n + 2$, and the number of edges in $G$ is $|E| = r + m + n$. Constructing $G$ and recovering the project-plan from the minimum cut are clearly linear-time operations. The running time of our algorithm is thus asymptotically the same as the running time of the algorithm used to find the minimum cut. If we use Edmonds-Karp to find the minimum cut, the running time is $O(VE^2)$.","title":"26-3 Algorithmic consulting"},{"location":"Chap26/Problems/26-4/","text":"Let $G = (V, E)$ be a flow network with source $s$, sink $t$, and integer capacities. Suppose that we are given a maximum flow in $G$. a. Suppose that we increase the capacity of a single edge $(u, v) \\in E$ by $1$. Give an $O(V + E)$-time algorithm to update the maximum flow. b. Suppose that we decrease the capacity of a single edge $(u, v) \\in E$ by $1$. Give an $O(V + E)$-time algorithm to update the maximum flow. a. Just execute one iteration of the Ford-Fulkerson algorithm. The edge $(u, v)$ in $E$ with increased capacity ensures that the edge $(u, v)$ is in the residual network. So look for an augmenting path and update the flow if a path is found. Time $O(V + E) = O(E)$ if we find the augmenting path with either depth-first or breadth-first search. To see that only one iteration is needed, consider separately the cases in which $(u, v)$ is or is not an edge that crosses a minimum cut. If $(u, v)$ does not cross a minimum cut, then increasing its capacity does not change the capacity of any minimum cut, and hence the value of the maximum flow does not change. If $(u, v)$ does cross a minimum cut, then increasing its capacity by $1$ increases the capacity of that minimum cut by $1$, and hence possibly the value of the maximum flow by $1$. In this case, there is either no augmenting path (in which case there was some other minimum cut that $(u, v)$ does not cross), or the augmenting path increases flow by $1$. No matter what, one iteration of Ford-Fulkerson suffices. b. Let $f$ be the maximum flow before reducing $c(u, v)$. If $f(u, v) = 0$, we don't need to do anything. If $f(u, v) > 0$, we will need to update the maximum flow. Assume from now on that $f(u, v) > 0$, which in turn implies that $f(u, v) \\ge 1$. Define $f'(x, y) = f(x, y)$ for all $x, y \\in V$, except that $f'(u, v) = f(u, v) - 1$. Although $f'$ obeys all capacity contraints, even after $c(u, v)$ has been reduced, it is not a legal flow, as it violates flow conservation at $u$ (unless $u = s$) and $v$ (unless $v = t$). $f'$ has one more unit of flow entering $u$ than leaving $u$, and it has one more unit of flow leaving $v$ than entering $v$. The idea is to try to reroute this unit of flow so that it goes out of $u$ and into $v$ via some other path. If that is not possible, we must reduce the flow from $s$ to $u$ and from $v$ to $t$ by one unit. Look for an augmenting path from $u$ to $v$ (note: not from $s$ to $t$). If there is such a path, augment the flow along that path. If there is no such path, reduce the flow from $s$ to $u$ by augmenting the flow from $u$ to $s$. That is, find an augmenting path $u \\leadsto s$ and augment the flow along that path. (There definitely is such a path, because there is flow from $s$ to $u$.) Similarly, reduce the flow from $v$ to $t$ by finding an augmenting path $t \\leadsto v$ and augmenting the flow along that path. Time $O(V + E) = O(E)$ if we find the paths with either $\\text{DFS}$ or $\\text{BFS}$.","title":"26-4 Updating maximum flow"},{"location":"Chap26/Problems/26-5/","text":"Let $G = (V, E)$ be a flow network with source $s$, sink $t$, and an integer capacity $c(u, v)$ on each edge $(u, v) \\in E$. Let $C = \\max_{(u, v) \\in E} c(u, v)$. a. Argue that a minimum cut of $G$ has capacity at most $C|E|$. b. For a given number $K$, show how to find an augmenting path of capacity at least $K$ in $O(E)$ time, if such a path exists. We can use the following modification of $\\text{FORD-FULKERSON-METHOD}$ to compute a maximum flow in $G$: 1 2 3 4 5 6 7 8 MAX - FLOW - BY - SCALING ( G , s , t ) C = max_ {( u , v ) \u2208 E } c ( u , v ) initialize flow f to 0 K = 2 ^ { floor ( lg C )} while K \u2265 1 while there exists an augmenting path p of capacity at least K augment flow f along p K = K / 2 return f c. Argue that $\\text{MAX-FLOW-BY-SCALING}$ returns a maximum flow. d. Show that the capacity of a minimum cut of the residual network $G_f$ is at most $2K|E|$ each time line 4 is executed. e. Argue that the inner while loop of lines 5\u20136 executes $O(E)$ times for each value of $K$. f. Conclude that $\\text{MAX-FLOW-BY-SCALING}$ can be implemented so that it runs in $O(E^2\\lg C)$ time. a. The capacity of a cut is defined to be the sum of the capacities of the edges crossing it. Since the number of such edges is at most $|E|$, and the capacity of each edge is at most $C$, the capacity of any cut of $G$ is at most $C|E|$. b. The capacity of an augmenting path is the minimum capacity of any edge on the path, so we are looking for an augmenting path whose edges all have capacity at least $K$. Do a breadth-first search or depth-first-search as usual to find the path, considering only edges with residual capacity at least $K$. (Treat lower-capacity edges as though they don't exist.) This search takes $O(V + E) = O(E)$ time. (Note that $|V| = O(E)$ in a flow network.) c. $\\text{MAX-FLOW-BY-SCALING}$ uses the Ford-Fulkerson method. It repeatedly augments the flow along an augmenting path until there are no augmenting paths with capacity at least $1$. Since all the capacities are integers, and the capacity of an augmenting path is positive, when there are no augmenting paths with capacity at least $1$, there must be no augmenting paths whatsoever in the residual network. Thus, by the max-flow min-cut theorem, $\\text{MAX-FLOW-BY-SCALING}$ returns a maximum flow. d. The first time line 4 is executed, the capacity of any edge in $G_f$ equals its capacity in G, and by part (a) the capacity of a minimum cut of $G$ is at most $C|E|$. Initially $K = 2^{\\lfloor \\lg C \\rfloor}$, and so $2K = 2 \\cdot 2^{\\lfloor \\lg C \\rfloor + 1} > 2^{\\lg C} = C$. Thus, the capacity of a minimum cut of $G_f$ is initially less than $2K|E|$. The other times line 4 is executed, $K$ has just been halved, and so the capacity of a cut of $G_f$ is at most $2K|E|$ at line 4 if and only if that capacity was at most $K|E|$ when the while loop of lines 5\u20136 last terminated. Thus, we want to show that when line 7 is reached, the capacity of a minimum cut of $G_f$ is at most $K|E|$. Let $G_f$ be the residual network when line 7 is reached. When we reach line 7, $G_f$ contains no augmenting path with capacity at least $K$. Therefore, a maximum flow $f'$ in $G_f$ has value $|f'| < K|E|$. Then, by the max-flow min-cut theorem, a minimum cut in $G_f$ has capacity less than $K|E|$. e. By part (d), when line 4 is reached, the capacity of a minimum cut of $G_f$ is at most $2K|E|$, and thus the maximum flow in $G_f$ is at most $2K|E|$. The following lemma shows that the value of a maximum flow in $G$ equals the value of the current flow $f$ in $G$ plus the value of a maximum flow in $G_f$. Lemma Let $f$ be a flow in flow network $G$, and $f'$ be a maximum flow in the residual network $G_f$. Then $f \\uparrow f'$ is a maximum flow in $G$. Proof By the max-flow min-cut theorem, $|f'| = c_f(S, T)$ for some cut $(S, T)$ of $G_f$, which is also a cut of $G$. By Lemma 26.4, $|f| = f(S, T)$. By Lemma 26.1, $f \\uparrow f'$ is a flow in $G$ with value $|f \\uparrow f'| = |f| + |f'|$. We will show that $|f| + |f'| = c(S, T)$ which, by the max-flow min-cut theorem, will prove that $f \\uparrow f'$ is a maximum flow in $G$. We have $$ \\begin{aligned} |f| + |f'| & = f(S, T) + c_f(S, T) \\\\ & = \\Bigg( \\sum_{u \\in S} \\sum_{v \\in T} f(u, v) - \\sum_{u \\in S} \\sum_{v \\in T} f(v, u) \\Bigg) + \\sum_{u \\in S} \\sum_{v \\in T} c_f(u, v) \\\\ & = \\Bigg( \\sum_{u \\in S, v \\in T} f(u, v) - \\sum_{u \\in S, v \\in T} f(v, u) \\Bigg) + \\Bigg( \\sum_{u \\in S, v \\in T, (u, v) \\in E} c(u, v) - \\sum_{u \\in S, v \\in T, (u, v) \\in E} f(u, v) + \\sum_{u \\in S, v \\in T, (v, u) \\in E} f(v, u) \\Bigg). \\end{aligned} $$ Noting that $(u, v) \\notin E$ implies $f(u, v) = 0$, we have that $$\\sum_{u \\in S, v \\in T} f(u, v) = \\sum_{u \\in S, v \\in T, (u, v) \\in E} f(u, v).$$ Similarly, $$\\sum_{u \\in S, v \\in T} f(v, u) = \\sum_{u \\in S, v \\in T, (v, u) \\in E} f(v, u).$$ Thus, the summations of $f(u, v)$ cancel each other out, as do the summations of $f(v, u)$. Therefore, $$ \\begin{aligned} |f| + |f'| & = \\sum_{u \\in S, v \\in T, (u, v) \\in E} c(u, v) \\\\ & = \\sum_{u \\in S} \\sum_{v \\in T} c(u, v) \\\\ & = c(S, T). \\end{aligned} $$ By this lemma, we see that the value of a maximum flow in $G$ is at most $2K|E|$ more than the value of the current flow $f$ in $G$. Every time the inner while loop finds an augmenting path of capacity at least $K$, the flow in $G$ increases by at least $K$. Since the flow cannot increase by more than $2K|E|$, the loop executes at most $(2K|E|) / K = 2|E|$ times. f. The time complexity is dominated by the while loop of lines 4\u20137. (The lines outside the loop take $O(E)$ time.) The outer while loop executes $O(\\lg C)$ times, since $K$ is initially $O(C)$ and is halved on each iteration, until $K < 1$. By part (e), the inner while loop executes $O(E)$ times for each value of $K$, and by part (b), each iteration takes $O(E)$ time. Thus, the total time is $O(E^2 \\lg C)$.","title":"26-5 Maximum flow by scaling"},{"location":"Chap26/Problems/26-6/","text":"In this problem, we describe a faster algorithm, due to Hopcroft and Karp, for $p$ finding a maximum matching in a bipartite graph. The algorithm runs in $O(\\sqrt V E)$ time. Given an undirected, bipartite graph $G = (V, E)$, where $V = L \\cup R$ and all edges have exactly one endpoint in $L$, let $M$ be a matching in $G$. We say that a simple path $P$ in $G$ is an augmenting path with respect to $M$ if it starts at an unmatched vertex in $L$, ends at an unmatched vertex in $R$, and its edges belong alternately to $M$ and $E - M$. (This definition of an augmenting path is related to, but different from, an augmenting path in a flow network.) In this problem, we treat a path as a sequence of edges, rather than as a sequence of vertices. A shortest augmenting path with respect to a matching $M$ is an augmenting path with a minimum number of edges. Given two sets $A$ and $B$, the symmetric difference $A \\oplus B$ is defined as $(A - B) \\cup (B - A)$, that is, the elements that are in exactly one of the two sets. a. Show that if $M$ is a matching and $P$ is an augmenting path with respect to $M$, then the symmetric difference $M \\oplus P$ is a matching and $|M \\oplus P| = |M| + 1$. Show that if $P_1, P_2, \\ldots, P_k$ are vertex-disjoint augmenting paths with respect to $M$, then the symmetric difference $M \\oplus (P_1 \\cup P_2 \\cup \\cdots \\cup P_k)$ is a matching with cardinality $|M| + k$. The general structure of our algorithm is the following: 1 2 3 4 5 6 7 HOPCROPFT - KARP ( G ) M = \u00d8 repeat let P = { P [ 1 ], P [ 2 ], ..., P [ k ]} be a maximal set of vertex - disjoint shortest augmenting paths with respect to M M = M \u2a01 ( P [ 1 ] \u222a P [ 2 ] \u222a ... \u222a P [ k ]) until P == \u00d8 return M The remainder of this problem asks you to analyze the number of iterations in the algorithm (that is, the number of iterations in the repeat loop) and to describe an implementation of line 3. b. Given two matchings $M$ and $M^*$ in $G$, show that every vertex in the graph $G' = (V, M \\oplus M^*)$ has degree at most $2$. Conclude that $G'$ is a disjoint union of simple paths or cycles. Argue that edges in each such simple path or cycle belong alternately to $M$ or $M^*$. Prove that if $|M| \\le |M^*|$, then $M \\oplus M^*$ contains at least $|M^*| - |M|$ vertex-disjoint augmenting paths with respect to $M$. Let $l$ be the length of a shortest augmenting path with respect to a matching $M$, and let $P_1, P_2, \\ldots, P_k$ be a maximal set of vertex-disjoint augmenting paths of length $l$ with respect to $M$. Let $M' = M \\oplus (P_1 \\cup \\cdots \\cup P_k)$, and suppose that $P$ is a shortest augmenting path with respect to $M'$. c. Show that if $P$ is vertex-disjoint from $P_1, P_2, \\ldots, P_k$ , then $P$ has more than $l$ edges. d. Now suppose that $P$ is not vertex-disjoint from $P_1, P_2, \\ldots, P_k$ . Let $A$ be the set of edges $(M \\oplus M') \\oplus P$. Show that $A = (P_1 \\cup P_2 \\cup \\cdots \\cup P_k) \\oplus P$ and that $|A| \\ge (k + 1)l$. Conclude that $P$ has more than $l$ edges. e. Prove that if a shortest augmenting path with respect to $M$ has $l$ edges, the size of the maximum matching is at most $|M| + |V| / (l + 1)$. f. Show that the number of repeat loop iterations in the algorithm is at most $2\\sqrt{|V|}$. ($\\textit{Hint:}$ By how much can $M$ grow after iteration number $\\sqrt{|V|}$?) g. Give an algorithm that runs in $O(E)$ time to find a maximal set of vertexdisjoint shortest augmenting paths $P_1, P_2, \\ldots, P_k$ for a given matching $M$. Conclude that the total running time of $\\text{HOPCROFT-KARP}$ is $O(\\sqrt V E)$. a. Suppose $M$ is a matching and $P$ is an augmenting path with respect to $M$. Then $P$ consists of $k$ edges in $M$, and $k + 1$ edges not in $M$. This is because the first edge of $P$ touches an unmatched vertex in $L$, so it cannot be in $M$. Similarly, the last edge in $P$ touches an unmatched vertex in $R$, so the last edge cannot be in $M$. Since the edges alternate being in or not in $M$, there must be exactly one more edge not in $M$ than in $M$. This implies that $$|M \\oplus P| = |M| + |P| - 2k = |M| + 2k + 1 - 2k = |M| + 1,$$ since we must remove each edge of $M$ which is in $P$ from both $M$ and $P$. Now suppose $P_1, P_2, \\ldots, P_k$ are vertex-disjoint augmenting paths with respect to $M$. Let $k_i$ be the number of edges in $P_i$ which are in $M$, so that $|P_i| = 2k + i + 1$. Then we have $$M \\oplus (P_1 \\cup P_2 \\cup \\cdots \\cup P_k) = |M| + |P_1| + \\cdots + |P_k| - 2k_1 - 2k_2 - \\cdots - 2k_k = |M| + k.$$ To see that we in fact get a matching, suppose that there was some vertex $v$ which had at least $2$ incident edges $e$ and $e'$. They cannot both come from $M$, since $M$ is a matching. They cannot both come from $P$ since $P$ is simple and every other edge of $P$ is removed. Thus, $e \\in M$ and $e' \\in P \\backslash M$. However, if $e \\in M$ then $e \\in P$, so $e \\notin M \\oplus P$, a contradiction. A similar argument gives the case of $M \\oplus (P_1 \\cup \\cdots \\cup P_k)$. b. Suppose some vertex in $G'$ has degree at least $3$. Since the edges of $G'$ come from $M \\oplus M^*$, at least $2$ of these edges come from the same matching. However, a matching never contains two edges with the same endpoint, so this is impossible. Thus every vertex has degree at most $2$, so $G'$ is a disjoint union of simple paths and cycles. If edge $(u, v)$ is followed by edge $(z, w)$ in a simple path or cycle then we must have $v = z$. Since two edges with the same endpoint cannot appear in a matching, they must belong alternately to $M$ and $M^*$. Since edges alternate, every cycle has the same number of edges in each matching and every path has at most one more edge in one matching than in the other. Thus, if $|M| \\le |M^*|$ there must be at least $|M^*| - |M|$ vertex-disjoint augmenting paths with respect to $M$. c. Every vertex matched by $M$ must be incident with some edge in $M'$. Since $P$ is augmenting with respect to $M$\u2032, the left endpoint of the first edge of $P$ isn't incident to a vertex touched by an edge in $M'$. In particular, $P$ starts at a vertex in $L$ which is unmatched by $M$ since every vertex of $M$ is incident with an edge in $M'$. Since $P$ is vertex disjoint from $P_1, P_2, \\ldots, P_k$, any edge of $P$ which is in $M'$ must in fact be in $M$ and any edge of $P$ which is not in $M'$ cannot be in $M$. Since $P$ has edges alternately in $M'$ and $E - M'$, $P$ must in fact have edges alternately in $M$ and $E - M$. Finally, the last edge of $P$ must be incident to a vertex in $R$ which is unmatched by $M'$. Any vertex unmatched by $M'$ is also unmatched by $M$, so $P$ is an augmenting path for $M$. $P$ must have length at least $l$ since $l$ is the length of the shortest augmenting path with respect to $M$. If $P$ had length exactly $l$, then this would contradict the fact that $P_1 \\cup \\cdots \\cup P_k$ is a maximal set of vertex disjoint paths of length $l$ because we could add $P$ to the set. Thus $P$ has more than $l$ edges. d. Any edge in $M \\oplus M'$ is in exactly one of $M$ or $M'$. Thus, the only possible contributing edges from $M'$ are from $P_1 \\cup \\cdots \\cup P_k$. An edge from $M$ can contribute if and only if it is not in exactly one of $M$ and $P_1 \\cup \\cdots \\cup P_k$, which means it must be in both. Thus, the edges from $M$ are redundant so $M \\oplus M' = (P_1 \\cup \\cdots \\cup P_k)$ which implies $A = (P_1 \\cup \\cdots \\cup P_k) \\oplus P$. Now we'll show that $P$ is edge disjoint from each $P_i$. Suppose that an edge $e$ of $P$ is also an edge of $P_i$ for some $i$. Since $P$ is an augmenting path with respect to $M'$ either $e \\in M'$ or $e \\in E - M'$. Suppose $e \\in M'$. Since $P$ is also augmenting with respect to $M$, we must have $e \\in M$. However, if $e$ is in $M$ and $M'$, then $e$ cannot be in any of the $P_i$'s by the definition of $M'$. Now suppose $e \\in E - M'$. Then $e \\in E - M$ since $P$ is augmenting with respect to $M$. Since $e$ is an edge of $P_i$, $e \\in E - M'$ implies that $e \\in M$, a contradiction. Since $P$ has edges alternately in $M'$ and $E - M'$ and is edge disjoint from $P_1 \\cup \\cdots \\cup P_k$, $P$ is also an augmenting path for $M$, which implies $|P| \\ge l$. Since every edge in $A$ is disjoint we conclude that $|A| \\ge (k + 1)l$. e. Suppose $M^*$ is a matching with strictly more than $|M| + |V| / (l + 1)$ edges. By part (b) there are strictly more than $|V| / (l + 1)$ vertex-disjoint augmenting paths with respect to $M$. Each one of these contains at least $l$ edges, so it is incident on $l + 1$ vertices. Since the paths are vertex disjoint, there are strictly more than $|V|(l + 1) / (l + 1)$ distinct vertices incident with these paths, a contradiction. Thus, the size of the maximum matching is at most $|M| + |V| / (l + 1)$. f. Consider what happens after iteration number $\\sqrt{|V|}$. Let $M^*$ be a maximal matching in $G$. Then $|M^*| \\ge |M|$ so by part (b), $M \\oplus M^*$ contains at least $|M^*| - |M|$ vertex disjoint augmenting paths with respect to $M$. By part (c), each of these is also a an augmenting path for $M$. Since each has length $\\sqrt{|V|}$, there can be at most $\\sqrt{|V|}$ such paths, so $|M^*| - |M| \\le \\sqrt{|V|}$. Thus, only $\\sqrt{|V|}$ additional iterations of the repeat loop can occur, so there are at most $2\\sqrt{|V|}$ iterations in total. g. For each unmatched vertex in $L$ we can perform a modified $\\text{BFS}$ to find the length of the shortest path to an unmatched vertex in $R$. Modify the $\\text{BFS}$ to ensure that we only traverse an edge if it causes the path to alternate between an edge in $M$ and an edge in $E - M$. The first time an unmatched vertex in $R$ is reached we know the length $k$ of a shortest augmenting path. We can use this to stop our search early if at any point we have traversed more than that number of edges. To find disjoint paths, start at the vertices of $R$ which were found at distance $k$ in the $\\text{BFS}$. Run a $\\text{DFS}$ backwards from these, which maintains the property that the next vertex we pick has distance one fewer, and the edges alternate between being in $M$ and $E - M$. As we build up a path, mark the vertices as used so that we never traverse them again. This takes $O(E)$, so by part (f) the total runtime is $O(\\sqrt VE)$.","title":"26-6 The Hopcroft-Karp bipartite matching algorithm"},{"location":"Chap27/27.1/","text":"27.1-1 Suppose that we spawn $\\text{P-FIB}(n - 2)$ in line 4 of $\\text{P-FIB}$, rather than calling it as is done in the code. What is the impact on the asymptotic work, span, and parallelism? There will be no change in the asymptotic work, span, or parallelism of $\\text{P-FIB}$ even if we were to spawn the recursive call to $\\text{P-FIB}(n - 2)$. The serialization of $\\text{P-FIB}$ under consideration would yield the same recurrence as that for $\\text{FIB}$; we can, therefore, calculate the work as $T_1(n) = \\Theta(\\phi^n)$. Similarly, because the spawned calls to $\\text{P-FIB}(n - 1)$ and $\\text{P-FIB}(n - 2)$ can run in parallel, we can calculate the span in exactly the same way as in the text, $T_\\infty(n) = \\Theta(n)$, resulting in $\\Theta(\\phi^n / n)$ parallelism. 27.1-2 Draw the computation dag that results from executing $\\text{P-FIB}(5)$. Assuming that each strand in the computation takes unit time, what are the work, span, and parallelism of the computation? Show how to schedule the dag on 3 processors using greedy scheduling by labeling each strand with the time step in which it is executed. Work: $T_1 = 29$. Span: $T_\\infty = 10$. Parallelism: $T_1 / T_\\infty \\approx 2.9$. 27.1-3 Prove that a greedy scheduler achieves the following time bound, which is slightly stronger than the bound proven in Theorem 27.1: $$T_P \\le \\frac{T_1 - T_\\infty}{P} + T_\\infty. \\tag{27.5}$$ Suppose that there are x incomplete steps in a run of the program. Since each of these steps causes at least one unit of work to be done, we have that there is at most $(T_1 - x)$ units of work done in the complete steps. Then, we suppose by contradiction that the number of complete steps is strictly greater than $\\lfloor (T_1 - x) / P \\rfloor$. Then, we have that the total amount of work done during the complete steps is $$P \\cdot (\\lfloor (T_1 - x) / P \\rfloor + 1) = P \\lfloor (T_1 - x) / P = (T_1 - x) - ((T_1 - x) \\mod P) + P > T_1 - x.$$ This is a contradiction because there are only $(T_1 - x)$ units of work done during complete steps, which is less than the amount we would be doing. Notice that since $T_\\infty$ is abound on the total number of both kinds of steps, it is a bound on the number of incomplete steps, $x$, so, $$T_P \\le \\lfloor (T_1 - x) / P \\rfloor + x \\le \\lfloor (T_1 - T_\\infty) / P \\rfloor + T_\\infty.$$ Where the second inequality comes by noting that the middle expression, as a function of $x$ is monotonically increasing, and so is bounded by the largest value of $x$ that is possible, namely $T_\\infty$. 27.1-4 Construct a computation dag for which one execution of a greedy scheduler can take nearly twice the time of another execution of a greedy scheduler on the same number of processors. Describe how the two executions would proceed. The computation is given in the image below. Let vertex $u$ have degree $k$, and assume that there are $m$ vertices in each vertical chain. Assume that this is executed on $k$ processors. In one execution, each strand from among the $k$ on the left is executed concurrently, and then the $m$ strands on the right are executed one at a time. If each strand takes unit time to execute, then the total computation takes $2m$ time. On the other hand, suppose that on each time step of the computation, $k - 1$ strands from the left (descendants of $u$) are executed, and one from the right (a descendant of $v$), is executed. If each strand take unit time to executed, the total computation takes $m + m / k$. Thus, the ratio of times is $2m / (m + m / k) = 2 / (1 + 1 / k)$. As $k$ gets large, this approaches $2$ as desired. 27.1-5 Professor Karan measures her deterministic multithreaded algorithm on $4$, $10$, and $64$ processors of an ideal parallel computer using a greedy scheduler. She claims that the three runs yielded $T_4 = 80$ seconds, $T_{10} = 42$ seconds, and $T_{64} = 10$ seconds. Argue that the professor is either lying or incompetent. ($\\textit{Hint:}$ Use the work law $\\text{(27.2)}$, the span law $\\text{(27.3)}$, and inequality $\\text{(27.5)}$ from Exercise 27.1-3.) By the work law for $P = 4$, we have $80 = T_4 \\ge T_1 / 4$, or $T_1 \\le 320$. By the span law for $P = 64$, we have $T_\\infty \\le T_{64} = 10$. Now we will use inequality $\\text{(27.5)}$ from Exercise 27.1-3 to derive a contradiction. For $P = 10$, we have $$ \\begin{aligned} 42 & = T_{10} \\\\ & \\le \\frac{320 - T_\\infty}{10} + T_\\infty \\\\ & = 32 + \\frac{9}{10} T_\\infty \\end{aligned} $$ or, equivalently, $$ \\begin{aligned} T_\\infty & \\ge \\frac{10}{9} \\cdot 10 \\\\ & > 10, \\end{aligned} $$ which contradicts $T_\\infty \\le 10$. Therefore, the running times reported by the professor are suspicious. 27.1-6 Give a multithreaded algorithm to multiply an $n \\times n$ matrix by an $n$-vector that achieves $\\Theta(n^2 / \\lg n)$ parallelism while maintaining $\\Theta(n^2)$ work. 1 2 3 4 5 6 7 8 FAST - MAT - VEC ( A , x ) n = A . rows let y be a new vector of length n parallel for i = 1 to n y [ i ] = 0 parallel for i = 1 to n y [ i ] = MAT - SUB - LOOP ( A , x , i , 1 , n ) return y 1 2 3 4 5 6 7 8 MAT - SUB - LOOP ( A , x , i , j , j ' ) if j == j ' return a [ i , j ] * x [ j ] else mid = floor (( j + j ' ) / 2 ) lhalf = spawn MAT - SUB - LOOP ( A , x , i , j , mid ) uhalf = MAT - SUB - LOOP ( A , x , i , mid + 1 , j ' ) sync return lhalf + uhalf We calculate the work $T_1(n)$ of $\\text{FAST-MAT-VEC}$ by computing the running time of its serialization, i.e., by replacing the parallel for loop by an ordinary for loop. Therefore, we have $T_1(n) = n T_1'(n)$, where $T_1'(n)$ denotes the work of $\\text{MAT-SUB-LOOP}$ to compute a given output entry $y_i$. The work of $\\text{MAT-SUB-LOOP}$ is given by the recurrence $$T_1'(n) = 2T_1'(n / 2) + \\Theta(1).$$ By applying case 1 of the master theorem, we have $T_1'(n) = \\Theta(n)$. Therefore, $T_1(n) = \\Theta(n^2)$. To calculate the span, we use $$T_\\infty(n) = \\Theta(\\lg n) + \\max_{1 \\le i \\le n} iter_\\infty (i).$$ Note that each iteration of the second parallel for loop calls procedure $\\text{MAT-SUB-LOOP}$ with the same parameters, except for the index $i$. Because $\\text{MAT-SUB-LOOP}$ recursively halves the space between its last two parameters ($1$ and $n$), does constant-time work in the base case, and spawns one of the recursive calls in parallel with the other, it has span $\\Theta(\\lg n)$. The procedure $\\text{FAST-MAT-VEC}$, therefore, has a span of $\\Theta(\\lg n)$ and $\\Theta(n^2 / \\lg n)$ parallelism. 27.1-7 Consider the following multithreaded pseudocode for transposing an $n \\times n$ matrix $A$ in place: 1 2 3 4 5 P - TRANSPOSE ( A ) n = A . rows parallel for j = 2 to n parallel for i = 1 to j - 1 exchange a [ i , j ] with a [ j , i ] Analyze the work, span, and parallelism of this algorithm. We analyze the work of $\\text{P-TRANSPOSE}$, as usual, by computing the running time of its serialization, where we replace both the parallel for loops with simple for loops. We can compute the work of $\\text{P-TRANSPOSE}$ using the summation $$ \\begin{aligned} T_1(n) & = \\Theta \\Bigg( \\sum_{j = 2}^n (j - 1) \\Bigg) \\\\ & = \\Theta \\Bigg( \\sum_{j = 1}^{n - 1} j \\Bigg) \\\\ & = \\Theta(n^2). \\end{aligned} $$ The span of $\\text{P-TRANSPOSE}$ is determined by the span of the doubly nested parallel for loops. Although the number of iterations of the inner loop depends on the value of the variable $j$ of the outer loop, each iteration of the inner loop does constant work. Let $iter_\\infty(j)$ denote the span of the $j$th iteration of the outer loop and $iter'_\\infty(i)$ denote the span of the ith iteration of the inner loop. We characterize the span $T_\\infty(n)$ of $\\text{P-TRANSPOSE}$ as $$T_\\infty(n) = \\Theta(\\lg n) + \\max_{2 \\le j \\le n} iter_\\infty(j).$$ The maximum occurs when $j = n$, and in this case, $$iter_\\infty(n) = \\Theta(\\lg n) + \\max_{1 \\le i \\le n - 1} iter'_\\infty(i).$$ As we noted, each iteration of the inner loop does constant work, and therefore $iter'_\\infty(i) = \\Theta(1)$ for all $i$. Thus, we have $$ \\begin{aligned} \\Theta(\\lg n) & = \\Theta(\\lg n) + \\Theta(\\lg n) + \\Theta(1) \\\\ & = \\Theta(\\lg n). \\end{aligned} $$ Since the work $\\text{P-TRANSPOSE}$ is $\\Theta(n^2)$ and its span is $\\Theta(\\lg n)$, the parallelism of $\\text{P-TRANSPOSE}$ is $\\Theta(n^2 / \\lg n)$. 27.1-8 Suppose that we replace the parallel for loop in line 3 of $\\text{P-TRANSPOSE}$ (see Exercise 27.1-7) with an ordinary for loop. Analyze the work, span, and parallelism of the resulting algorithm. If we were to replace the inner parallel for loop of $\\text{P-TRANSPOSE}$ with an ordinary for loop, the work would still remain $\\Theta(n^2)$. The span, however, would increase to $\\Theta(n)$ because the last iteration of the parallel for loop, which dominates the span of the computation, would lead to $(n - 1)$ iterations of the inner, serial for loop. The parallelism, therefore, would reduce to $\\Theta(n^2) / \\Theta(n) = \\Theta(n)$. 27.1-9 For how many processors do the two versions of the chess programs run equally fast, assuming that $T_P = T_1 / P + T_\\infty$? Based on the values of work and span given for the two versions of the chess program, we solve for $P$ using $$\\frac{2048}{P} + 1 = \\frac{1024}{P} + 8.$$ The solution gives $P$ between $146$ and $147$.","title":"27.1 The basics of dynamic multithreading"},{"location":"Chap27/27.1/#271-1","text":"Suppose that we spawn $\\text{P-FIB}(n - 2)$ in line 4 of $\\text{P-FIB}$, rather than calling it as is done in the code. What is the impact on the asymptotic work, span, and parallelism? There will be no change in the asymptotic work, span, or parallelism of $\\text{P-FIB}$ even if we were to spawn the recursive call to $\\text{P-FIB}(n - 2)$. The serialization of $\\text{P-FIB}$ under consideration would yield the same recurrence as that for $\\text{FIB}$; we can, therefore, calculate the work as $T_1(n) = \\Theta(\\phi^n)$. Similarly, because the spawned calls to $\\text{P-FIB}(n - 1)$ and $\\text{P-FIB}(n - 2)$ can run in parallel, we can calculate the span in exactly the same way as in the text, $T_\\infty(n) = \\Theta(n)$, resulting in $\\Theta(\\phi^n / n)$ parallelism.","title":"27.1-1"},{"location":"Chap27/27.1/#271-2","text":"Draw the computation dag that results from executing $\\text{P-FIB}(5)$. Assuming that each strand in the computation takes unit time, what are the work, span, and parallelism of the computation? Show how to schedule the dag on 3 processors using greedy scheduling by labeling each strand with the time step in which it is executed. Work: $T_1 = 29$. Span: $T_\\infty = 10$. Parallelism: $T_1 / T_\\infty \\approx 2.9$.","title":"27.1-2"},{"location":"Chap27/27.1/#271-3","text":"Prove that a greedy scheduler achieves the following time bound, which is slightly stronger than the bound proven in Theorem 27.1: $$T_P \\le \\frac{T_1 - T_\\infty}{P} + T_\\infty. \\tag{27.5}$$ Suppose that there are x incomplete steps in a run of the program. Since each of these steps causes at least one unit of work to be done, we have that there is at most $(T_1 - x)$ units of work done in the complete steps. Then, we suppose by contradiction that the number of complete steps is strictly greater than $\\lfloor (T_1 - x) / P \\rfloor$. Then, we have that the total amount of work done during the complete steps is $$P \\cdot (\\lfloor (T_1 - x) / P \\rfloor + 1) = P \\lfloor (T_1 - x) / P = (T_1 - x) - ((T_1 - x) \\mod P) + P > T_1 - x.$$ This is a contradiction because there are only $(T_1 - x)$ units of work done during complete steps, which is less than the amount we would be doing. Notice that since $T_\\infty$ is abound on the total number of both kinds of steps, it is a bound on the number of incomplete steps, $x$, so, $$T_P \\le \\lfloor (T_1 - x) / P \\rfloor + x \\le \\lfloor (T_1 - T_\\infty) / P \\rfloor + T_\\infty.$$ Where the second inequality comes by noting that the middle expression, as a function of $x$ is monotonically increasing, and so is bounded by the largest value of $x$ that is possible, namely $T_\\infty$.","title":"27.1-3"},{"location":"Chap27/27.1/#271-4","text":"Construct a computation dag for which one execution of a greedy scheduler can take nearly twice the time of another execution of a greedy scheduler on the same number of processors. Describe how the two executions would proceed. The computation is given in the image below. Let vertex $u$ have degree $k$, and assume that there are $m$ vertices in each vertical chain. Assume that this is executed on $k$ processors. In one execution, each strand from among the $k$ on the left is executed concurrently, and then the $m$ strands on the right are executed one at a time. If each strand takes unit time to execute, then the total computation takes $2m$ time. On the other hand, suppose that on each time step of the computation, $k - 1$ strands from the left (descendants of $u$) are executed, and one from the right (a descendant of $v$), is executed. If each strand take unit time to executed, the total computation takes $m + m / k$. Thus, the ratio of times is $2m / (m + m / k) = 2 / (1 + 1 / k)$. As $k$ gets large, this approaches $2$ as desired.","title":"27.1-4"},{"location":"Chap27/27.1/#271-5","text":"Professor Karan measures her deterministic multithreaded algorithm on $4$, $10$, and $64$ processors of an ideal parallel computer using a greedy scheduler. She claims that the three runs yielded $T_4 = 80$ seconds, $T_{10} = 42$ seconds, and $T_{64} = 10$ seconds. Argue that the professor is either lying or incompetent. ($\\textit{Hint:}$ Use the work law $\\text{(27.2)}$, the span law $\\text{(27.3)}$, and inequality $\\text{(27.5)}$ from Exercise 27.1-3.) By the work law for $P = 4$, we have $80 = T_4 \\ge T_1 / 4$, or $T_1 \\le 320$. By the span law for $P = 64$, we have $T_\\infty \\le T_{64} = 10$. Now we will use inequality $\\text{(27.5)}$ from Exercise 27.1-3 to derive a contradiction. For $P = 10$, we have $$ \\begin{aligned} 42 & = T_{10} \\\\ & \\le \\frac{320 - T_\\infty}{10} + T_\\infty \\\\ & = 32 + \\frac{9}{10} T_\\infty \\end{aligned} $$ or, equivalently, $$ \\begin{aligned} T_\\infty & \\ge \\frac{10}{9} \\cdot 10 \\\\ & > 10, \\end{aligned} $$ which contradicts $T_\\infty \\le 10$. Therefore, the running times reported by the professor are suspicious.","title":"27.1-5"},{"location":"Chap27/27.1/#271-6","text":"Give a multithreaded algorithm to multiply an $n \\times n$ matrix by an $n$-vector that achieves $\\Theta(n^2 / \\lg n)$ parallelism while maintaining $\\Theta(n^2)$ work. 1 2 3 4 5 6 7 8 FAST - MAT - VEC ( A , x ) n = A . rows let y be a new vector of length n parallel for i = 1 to n y [ i ] = 0 parallel for i = 1 to n y [ i ] = MAT - SUB - LOOP ( A , x , i , 1 , n ) return y 1 2 3 4 5 6 7 8 MAT - SUB - LOOP ( A , x , i , j , j ' ) if j == j ' return a [ i , j ] * x [ j ] else mid = floor (( j + j ' ) / 2 ) lhalf = spawn MAT - SUB - LOOP ( A , x , i , j , mid ) uhalf = MAT - SUB - LOOP ( A , x , i , mid + 1 , j ' ) sync return lhalf + uhalf We calculate the work $T_1(n)$ of $\\text{FAST-MAT-VEC}$ by computing the running time of its serialization, i.e., by replacing the parallel for loop by an ordinary for loop. Therefore, we have $T_1(n) = n T_1'(n)$, where $T_1'(n)$ denotes the work of $\\text{MAT-SUB-LOOP}$ to compute a given output entry $y_i$. The work of $\\text{MAT-SUB-LOOP}$ is given by the recurrence $$T_1'(n) = 2T_1'(n / 2) + \\Theta(1).$$ By applying case 1 of the master theorem, we have $T_1'(n) = \\Theta(n)$. Therefore, $T_1(n) = \\Theta(n^2)$. To calculate the span, we use $$T_\\infty(n) = \\Theta(\\lg n) + \\max_{1 \\le i \\le n} iter_\\infty (i).$$ Note that each iteration of the second parallel for loop calls procedure $\\text{MAT-SUB-LOOP}$ with the same parameters, except for the index $i$. Because $\\text{MAT-SUB-LOOP}$ recursively halves the space between its last two parameters ($1$ and $n$), does constant-time work in the base case, and spawns one of the recursive calls in parallel with the other, it has span $\\Theta(\\lg n)$. The procedure $\\text{FAST-MAT-VEC}$, therefore, has a span of $\\Theta(\\lg n)$ and $\\Theta(n^2 / \\lg n)$ parallelism.","title":"27.1-6"},{"location":"Chap27/27.1/#271-7","text":"Consider the following multithreaded pseudocode for transposing an $n \\times n$ matrix $A$ in place: 1 2 3 4 5 P - TRANSPOSE ( A ) n = A . rows parallel for j = 2 to n parallel for i = 1 to j - 1 exchange a [ i , j ] with a [ j , i ] Analyze the work, span, and parallelism of this algorithm. We analyze the work of $\\text{P-TRANSPOSE}$, as usual, by computing the running time of its serialization, where we replace both the parallel for loops with simple for loops. We can compute the work of $\\text{P-TRANSPOSE}$ using the summation $$ \\begin{aligned} T_1(n) & = \\Theta \\Bigg( \\sum_{j = 2}^n (j - 1) \\Bigg) \\\\ & = \\Theta \\Bigg( \\sum_{j = 1}^{n - 1} j \\Bigg) \\\\ & = \\Theta(n^2). \\end{aligned} $$ The span of $\\text{P-TRANSPOSE}$ is determined by the span of the doubly nested parallel for loops. Although the number of iterations of the inner loop depends on the value of the variable $j$ of the outer loop, each iteration of the inner loop does constant work. Let $iter_\\infty(j)$ denote the span of the $j$th iteration of the outer loop and $iter'_\\infty(i)$ denote the span of the ith iteration of the inner loop. We characterize the span $T_\\infty(n)$ of $\\text{P-TRANSPOSE}$ as $$T_\\infty(n) = \\Theta(\\lg n) + \\max_{2 \\le j \\le n} iter_\\infty(j).$$ The maximum occurs when $j = n$, and in this case, $$iter_\\infty(n) = \\Theta(\\lg n) + \\max_{1 \\le i \\le n - 1} iter'_\\infty(i).$$ As we noted, each iteration of the inner loop does constant work, and therefore $iter'_\\infty(i) = \\Theta(1)$ for all $i$. Thus, we have $$ \\begin{aligned} \\Theta(\\lg n) & = \\Theta(\\lg n) + \\Theta(\\lg n) + \\Theta(1) \\\\ & = \\Theta(\\lg n). \\end{aligned} $$ Since the work $\\text{P-TRANSPOSE}$ is $\\Theta(n^2)$ and its span is $\\Theta(\\lg n)$, the parallelism of $\\text{P-TRANSPOSE}$ is $\\Theta(n^2 / \\lg n)$.","title":"27.1-7"},{"location":"Chap27/27.1/#271-8","text":"Suppose that we replace the parallel for loop in line 3 of $\\text{P-TRANSPOSE}$ (see Exercise 27.1-7) with an ordinary for loop. Analyze the work, span, and parallelism of the resulting algorithm. If we were to replace the inner parallel for loop of $\\text{P-TRANSPOSE}$ with an ordinary for loop, the work would still remain $\\Theta(n^2)$. The span, however, would increase to $\\Theta(n)$ because the last iteration of the parallel for loop, which dominates the span of the computation, would lead to $(n - 1)$ iterations of the inner, serial for loop. The parallelism, therefore, would reduce to $\\Theta(n^2) / \\Theta(n) = \\Theta(n)$.","title":"27.1-8"},{"location":"Chap27/27.1/#271-9","text":"For how many processors do the two versions of the chess programs run equally fast, assuming that $T_P = T_1 / P + T_\\infty$? Based on the values of work and span given for the two versions of the chess program, we solve for $P$ using $$\\frac{2048}{P} + 1 = \\frac{1024}{P} + 8.$$ The solution gives $P$ between $146$ and $147$.","title":"27.1-9"},{"location":"Chap27/27.2/","text":"27.2-1 Draw the computation dag for computing $\\text{P-SQUARE-MATRIX-MULTIPLY}$ on $2 \\times 2$ matrices, labeling how the vertices in your diagram correspond to strands in the execution of the algorithm. Use the convention that spawn and call edges point downward, continuation edges point horizontally to the right, and return edges point upward. Assuming that each strand takes unit time, analyze the work, span, and parallelism of this computation. (Omit!) 27.2-2 Repeat Exercise 27.2-1 for $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. (Omit!) 27.2-3 Give pseudocode for a multithreaded algorithm that multiplies two $n \\times n$ matrices with work $\\Theta(n^3)$ but span only $\\Theta(\\lg n)$. Analyze your algorithm. 1 2 3 4 5 6 7 P - FAST - MATRIX - MULTIPLY ( A , B ) n = A . rows let C be a new n \u00d7 n matrix parallel for i = 1 to n parallel for j = 1 to n c [ i , j ] = MATRIX - MULT - SUBLOOP ( A , B , i , j , 1 , n ) return C 1 2 3 4 5 6 7 8 MATRIX - MULT - SUBLOOP ( A , B , i , j , k , k ' ) if k == k ' return a [ i , k ] * b [ k , j ] else mid = floor (( k + k ' ) / 2 ) lhalf = spawn MATRIX - MULT - SUBLOOP ( A , B , i , j , k , mid ) uhalf = MATRIX - MULT - SUBLOOP ( A , B , i , j , mid + 1 , k ' ) sync return lhalf + uhalf We calculate the work $T_1(n)$ of $\\text{P-FAST-MATRIX-MULTIPLY}$ by computing the running time of its serialization, i.e., by replacing the parallel for loops by ordinary for loops. Therefore, we have $T_1(n) = n^2T_1'(n)$, where $T_1'(n)$ denotes the work of $\\text{MATRIX-MULT-SUBLOOP}$ to compute a given output entry $c_{ij}$. The work of $\\text{MATRIX-MULT-SUBLOOP}$ is given by the recurrence $$T_1'(n) = 2T_1'(n / 2) + \\Theta(1).$$ By applying case 1 of the master theorem, we have $T_1'(n) = \\Theta(n)$. Therefore, $$T_1(n) = \\Theta(n^3).$$ To calculate the span, we use $$T_\\infty(n) = \\Theta(\\lg n) + \\max_{1 \\le i \\le n} iter_\\infty(i).$$ Note that each iteration of the outer parallel for loop does the same amount of work: it calls the inner parallel for loop. Similarly, each iteration of the inner parallel for loop calls procedure $\\text{MATRIX-MULT-SUBLOOP}$ with the same parameters, except for the indices $i$ and $j$. Because $\\text{MATRIX-MULT-SUBLOOP}$ recursively halves the space between its last two parameters ($1$ and $n$), does constanttime work in the base case, and spawns one of the recursive calls in parallel with the other, it has span $\\Theta(\\lg n)$. Since each iteration of the inner parallel for loop, which has $n$ iterations, has span $\\Theta(\\lg n)$, the inner parallel for loop has span $\\Theta(\\lg n)$. By similar logic, the outer parallel for loop, and hence procedure $\\text{P-FAST-MATRIX-MULTIPLY}$, has span $\\Theta(\\lg n)$ and $\\Theta(n^3 / \\lg n)$ parallelism. 27.2-4 Give pseudocode for an efficient multithreaded algorithm that multiplies a $p \\times q$ matrix by a $q \\times r$ matrix. Your algorithm should be highly parallel even if any of $p$, $q$, and $r$ are $1$. Analyze your algorithm. We can efficiently multiply a $p \\times q$ matrix by a $q \\times r$ matrix in parallel by using the solution to Exercise 27.2-3 as a base. We will replace the upper limits of the nested parallel for loops with $p$ and $r$ respectively and we will pass $q$ as the last argument to the call of $\\text{MATRIX-MULT-SUBLOOP}$. We present the pseudocode for a multithreaded algorithm for multiplying a $p \\times q$ matrix by a $q \\times r$ matrix in procedure $\\text{P-GEN-MATRIX-MULTIPLY}$ below. Because the pseudocode for procedure $\\text{MATRIX-MULT-SUBLOOP}$ (which $\\text{P-GEN-MATRIX-MULTIPLY}$ calls) remains the same as was presented in the solution to Exercise 27.2-3, we do not repeat it here. 1 2 3 4 5 6 7 8 9 P - GEN - MATRIX - MULTIPLY ( A , B ) p = A . rows q = A . columns r = B . columns let C be a new p \u00d7 r matrix parallel for i = 1 to p parallel for j = 1 to r c [ i , j ] = MATRIX - MULT - SUBLOOP ( A , B , i , j , 1 , q ) return C To calculate the work for $\\text{P-GEN-MATRIX-MULTIPLY}$, we replace the parallel for loops with ordinary for loops. As before, we can calculate the work of $\\text{MATRIX-MULT-SUBLOOP}$ to be $\\Theta(q)$ (because the input size to the procedure is $q$ here). Therefore, the work of $\\text{P-GEN-MATRIX-MULTIPLY}$ is $T_1 = \\Theta(pqr)$. We can analyze the span of $\\text{P-GEN-MATRIX-MULTIPLY}$ as we did in the solution to Exercise 27.2-3, but we must take into account the different number of loop iterations. Each of the $p$ iterations of the outer parallel for loop executes the inner parallel for loop, and each of the $r$ iterations of the inner parallel for loop calls $\\text{MATRIX-MULT-SUBLOOP}$, whose span is given by $\\Theta(\\lg q)$. We know that, in general, the span of a parallel for loop with $n$ iterations, where the $i$th iteration has span $iter_\\infty(i)$ is given by $$T_\\infty = \\Theta(\\lg n) + \\max_{1 \\le i \\le n} iter_\\infty(i).$$ Based on the above observations, we can calculate the span of $\\text{P-GEN-MATRIX-MULTIPLY}$ as $$ \\begin{aligned} T_\\infty & = \\Theta(\\lg p) + \\Theta(\\lg r) + \\Theta(\\lg q) \\\\ & = \\Theta(\\lg(pqr)). \\end{aligned} $$ The parallelism of the procedure is, therefore, given by $\\Theta(pqr / \\lg(pqr))$. To check whether this analysis is consistent with Exercise 27.2-3, we note that if $p = q = r = n$, then the parallelism of $\\text{P-GEN-MATRIX-MULTIPLY}$ would be $\\Theta(n^3 / \\lg n^3) = \\Theta(n^3 / \\lg n)$. 27.2-5 Give pseudocode for an efficient multithreaded algorithm that transposes an $n \\times n$ matrix in place by using divide-and-conquer to divide the matrix recursively into four $n / 2 \\times n / 2$ submatrices. Analyze your algorithm. 1 2 3 4 5 6 7 8 9 P - MATRIX - TRANSPOSE - RECURSIVE ( A , r , c , s ) // Transpose the s \u00d7 s submatrix starting at a[r, c]. if s == 1 return else s ' = floor ( s / 2 ) spawn P - MATRIX - TRANSPOSE - RECURSIVE ( A , r , c , s ' ) spawn P - MATRIX - TRANSPOSE - RECURSIVE ( A , r + s ' , c + s ' , s - s ' ) P - MATRIX - TRANSPOSE - SWAP ( A , r , c + s ' , r + s ' , c , s ' , s - s ' ) sync 1 2 3 4 5 6 7 8 9 10 P - MATRIX - TRANSPOSE - SWAP ( A , r [ 1 ], c [ 1 ], r [ 2 ], c [ 2 ], s [ 1 ], s [ 2 ]) // Transpose the s[1] \u00d7 s[2] submatrix starting at a[r[1], c[1]] with the s[2] \u00d7 s[1] submatrix starting at a[r[2], c[2]]. if s [ 1 ] < s [ 2 ] P - MATRIX - TRANSPOSE - SWAP ( A , r [ 2 ], c [ 2 ], r [ 1 ], c [ 1 ], s [ 2 ], s [ 1 ]) else if s [ 1 ] == 1 // since s[1] \u2265 s[2], must have that s[2] equals 1 exchange a [ r [ 1 ], c [ 1 ]] with a [ r [ 2 ], c [ 2 ]] else s ' = floor ( s [ 1 ] / 2 ) spawn P - MATRIX - TRANSPOSE - SWAP ( A , r [ 2 ], c [ 2 ], r [ 1 ], c [ 1 ], s [ 2 ], s ' ) P - MATRIX - TRANSPOSE - SWAP ( A , r [ 2 ], c [ 2 ] + s ' , r [ 1 ] + s ' , c [ 1 ], s [ 2 ], s [ 1 ] - s ' ) sync In order to transpose an $n \\times n$ matrix $A$, we call $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}(A, 1, 1, n)$. Let us first calculate the work and span of $\\text{P-MATRIX-TRANSPOSE-SWAP}$ so that we can plug in these values into the work and span calculations of $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$. The work $T_1'(N)$ of $\\text{P-MATRIX-TRANSPOSE-SWAP}$ on an $N$-element matrix is the running time of its serialization. We have the recurrence $$ \\begin{aligned} T_1'(N) & = 2T_1'(N / 2) + \\Theta(1) \\\\ & = \\Theta(N). \\end{aligned} $$ The span $T_\\infty'(N)$ is similarly described by the recurrence $$ \\begin{aligned} T_\\infty'(N) & = T_\\infty'(N / 2) + \\Theta(1) \\\\ & = \\Theta(\\lg N). \\end{aligned} $$ In order to calculate the work of $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$, we calculate the running time of its serialization. Let $T_1(N)$ be the work of the algorithm on an $N$-element matrix, where $N = n^2$, and assume for simplicity that $n$ is an exact power of $2$. Because the procedure makes two recursive calls with square submatrices of sizes $n / 2 \\times n / 2 = N / 4$ and because it does $\\Theta(n^2) = \\Theta(N)$ work to swap all the elements of the other two submatrices of size $n / 2 \\times n / 2$, its work is given by the recurrence $$ \\begin{aligned} T_1(N) & = 2T_1(N / 4) + \\Theta(N) \\\\ & = \\Theta(N). \\end{aligned} $$ The two parallel recursive calls in $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ execute on matrices of size $n / 2 \\times n / 2$. The span of the procedure is given by maximum of the span of one of these two recursive calls and the $\\Theta(\\lg N)$ span of $\\text{P-MATRIX-TRANSPOSE-SWAP}$, plus $\\Theta(1)$. Since the recurrence $$T_\\infty(N) = T_\\infty(N / 4) + \\Theta(1)$$ has the solution $T_\\infty(N) = \\Theta(\\lg N)$ by case 2 of Theorem 4.1, the span of the recursive call is asymptotically the same as the span of $\\text{P-MATRIX-TRANSPOSE-SWAP}$, and hence the span of $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ is $\\Theta(\\lg N)$. Thus, $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ has parallelism $\\Theta(N / \\lg N) = \\Theta(n^2 / \\lg n)$. 27.2-6 Give pseudocode for an efficient multithreaded implementation of the Floyd-Warshall algorithm (see Section 25.2), which computes shortest paths between all pairs of vertices in an edge-weighted graph. Analyze your algorithm. 1 2 3 4 5 6 7 8 9 10 P - FLOYD - WARSHALL ( W ) n = W . rows parallel for i = 1 to n parallel for j = 1 to n d [ i , j ] = w [ i , j ] for k = 1 to n parallel for i = 1 to n parallel for j = 1 to n d [ i , j ] = min ( d [ i , j ], d [ i , k ] + d [ k , j ]) return D By Exercise 25.2-4, we can compute all the $d_{ij}$ values in parallel. The work of $\\text{P-FLOYD-WARSHALL}$ is the same as the running time of its serialization, which we computed as $\\Theta(n^3)$ in Section 25.2. The span of the doubly nested parallel for loops, which do constant work inside, is $\\Theta(\\lg n)$. Note, however, that the second set of doubly nested parallel for loops is executed within each of the $n$ iterations of the outermost serial for loop. Therefore, $\\text{P-FLOYD-WARSHALL}$ has span $\\Theta(n\\lg n)$ and $\\Theta(n^2 / \\lg n)$ parallelism.","title":"27.2 Multithreaded matrix multiplication"},{"location":"Chap27/27.2/#272-1","text":"Draw the computation dag for computing $\\text{P-SQUARE-MATRIX-MULTIPLY}$ on $2 \\times 2$ matrices, labeling how the vertices in your diagram correspond to strands in the execution of the algorithm. Use the convention that spawn and call edges point downward, continuation edges point horizontally to the right, and return edges point upward. Assuming that each strand takes unit time, analyze the work, span, and parallelism of this computation. (Omit!)","title":"27.2-1"},{"location":"Chap27/27.2/#272-2","text":"Repeat Exercise 27.2-1 for $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. (Omit!)","title":"27.2-2"},{"location":"Chap27/27.2/#272-3","text":"Give pseudocode for a multithreaded algorithm that multiplies two $n \\times n$ matrices with work $\\Theta(n^3)$ but span only $\\Theta(\\lg n)$. Analyze your algorithm. 1 2 3 4 5 6 7 P - FAST - MATRIX - MULTIPLY ( A , B ) n = A . rows let C be a new n \u00d7 n matrix parallel for i = 1 to n parallel for j = 1 to n c [ i , j ] = MATRIX - MULT - SUBLOOP ( A , B , i , j , 1 , n ) return C 1 2 3 4 5 6 7 8 MATRIX - MULT - SUBLOOP ( A , B , i , j , k , k ' ) if k == k ' return a [ i , k ] * b [ k , j ] else mid = floor (( k + k ' ) / 2 ) lhalf = spawn MATRIX - MULT - SUBLOOP ( A , B , i , j , k , mid ) uhalf = MATRIX - MULT - SUBLOOP ( A , B , i , j , mid + 1 , k ' ) sync return lhalf + uhalf We calculate the work $T_1(n)$ of $\\text{P-FAST-MATRIX-MULTIPLY}$ by computing the running time of its serialization, i.e., by replacing the parallel for loops by ordinary for loops. Therefore, we have $T_1(n) = n^2T_1'(n)$, where $T_1'(n)$ denotes the work of $\\text{MATRIX-MULT-SUBLOOP}$ to compute a given output entry $c_{ij}$. The work of $\\text{MATRIX-MULT-SUBLOOP}$ is given by the recurrence $$T_1'(n) = 2T_1'(n / 2) + \\Theta(1).$$ By applying case 1 of the master theorem, we have $T_1'(n) = \\Theta(n)$. Therefore, $$T_1(n) = \\Theta(n^3).$$ To calculate the span, we use $$T_\\infty(n) = \\Theta(\\lg n) + \\max_{1 \\le i \\le n} iter_\\infty(i).$$ Note that each iteration of the outer parallel for loop does the same amount of work: it calls the inner parallel for loop. Similarly, each iteration of the inner parallel for loop calls procedure $\\text{MATRIX-MULT-SUBLOOP}$ with the same parameters, except for the indices $i$ and $j$. Because $\\text{MATRIX-MULT-SUBLOOP}$ recursively halves the space between its last two parameters ($1$ and $n$), does constanttime work in the base case, and spawns one of the recursive calls in parallel with the other, it has span $\\Theta(\\lg n)$. Since each iteration of the inner parallel for loop, which has $n$ iterations, has span $\\Theta(\\lg n)$, the inner parallel for loop has span $\\Theta(\\lg n)$. By similar logic, the outer parallel for loop, and hence procedure $\\text{P-FAST-MATRIX-MULTIPLY}$, has span $\\Theta(\\lg n)$ and $\\Theta(n^3 / \\lg n)$ parallelism.","title":"27.2-3"},{"location":"Chap27/27.2/#272-4","text":"Give pseudocode for an efficient multithreaded algorithm that multiplies a $p \\times q$ matrix by a $q \\times r$ matrix. Your algorithm should be highly parallel even if any of $p$, $q$, and $r$ are $1$. Analyze your algorithm. We can efficiently multiply a $p \\times q$ matrix by a $q \\times r$ matrix in parallel by using the solution to Exercise 27.2-3 as a base. We will replace the upper limits of the nested parallel for loops with $p$ and $r$ respectively and we will pass $q$ as the last argument to the call of $\\text{MATRIX-MULT-SUBLOOP}$. We present the pseudocode for a multithreaded algorithm for multiplying a $p \\times q$ matrix by a $q \\times r$ matrix in procedure $\\text{P-GEN-MATRIX-MULTIPLY}$ below. Because the pseudocode for procedure $\\text{MATRIX-MULT-SUBLOOP}$ (which $\\text{P-GEN-MATRIX-MULTIPLY}$ calls) remains the same as was presented in the solution to Exercise 27.2-3, we do not repeat it here. 1 2 3 4 5 6 7 8 9 P - GEN - MATRIX - MULTIPLY ( A , B ) p = A . rows q = A . columns r = B . columns let C be a new p \u00d7 r matrix parallel for i = 1 to p parallel for j = 1 to r c [ i , j ] = MATRIX - MULT - SUBLOOP ( A , B , i , j , 1 , q ) return C To calculate the work for $\\text{P-GEN-MATRIX-MULTIPLY}$, we replace the parallel for loops with ordinary for loops. As before, we can calculate the work of $\\text{MATRIX-MULT-SUBLOOP}$ to be $\\Theta(q)$ (because the input size to the procedure is $q$ here). Therefore, the work of $\\text{P-GEN-MATRIX-MULTIPLY}$ is $T_1 = \\Theta(pqr)$. We can analyze the span of $\\text{P-GEN-MATRIX-MULTIPLY}$ as we did in the solution to Exercise 27.2-3, but we must take into account the different number of loop iterations. Each of the $p$ iterations of the outer parallel for loop executes the inner parallel for loop, and each of the $r$ iterations of the inner parallel for loop calls $\\text{MATRIX-MULT-SUBLOOP}$, whose span is given by $\\Theta(\\lg q)$. We know that, in general, the span of a parallel for loop with $n$ iterations, where the $i$th iteration has span $iter_\\infty(i)$ is given by $$T_\\infty = \\Theta(\\lg n) + \\max_{1 \\le i \\le n} iter_\\infty(i).$$ Based on the above observations, we can calculate the span of $\\text{P-GEN-MATRIX-MULTIPLY}$ as $$ \\begin{aligned} T_\\infty & = \\Theta(\\lg p) + \\Theta(\\lg r) + \\Theta(\\lg q) \\\\ & = \\Theta(\\lg(pqr)). \\end{aligned} $$ The parallelism of the procedure is, therefore, given by $\\Theta(pqr / \\lg(pqr))$. To check whether this analysis is consistent with Exercise 27.2-3, we note that if $p = q = r = n$, then the parallelism of $\\text{P-GEN-MATRIX-MULTIPLY}$ would be $\\Theta(n^3 / \\lg n^3) = \\Theta(n^3 / \\lg n)$.","title":"27.2-4"},{"location":"Chap27/27.2/#272-5","text":"Give pseudocode for an efficient multithreaded algorithm that transposes an $n \\times n$ matrix in place by using divide-and-conquer to divide the matrix recursively into four $n / 2 \\times n / 2$ submatrices. Analyze your algorithm. 1 2 3 4 5 6 7 8 9 P - MATRIX - TRANSPOSE - RECURSIVE ( A , r , c , s ) // Transpose the s \u00d7 s submatrix starting at a[r, c]. if s == 1 return else s ' = floor ( s / 2 ) spawn P - MATRIX - TRANSPOSE - RECURSIVE ( A , r , c , s ' ) spawn P - MATRIX - TRANSPOSE - RECURSIVE ( A , r + s ' , c + s ' , s - s ' ) P - MATRIX - TRANSPOSE - SWAP ( A , r , c + s ' , r + s ' , c , s ' , s - s ' ) sync 1 2 3 4 5 6 7 8 9 10 P - MATRIX - TRANSPOSE - SWAP ( A , r [ 1 ], c [ 1 ], r [ 2 ], c [ 2 ], s [ 1 ], s [ 2 ]) // Transpose the s[1] \u00d7 s[2] submatrix starting at a[r[1], c[1]] with the s[2] \u00d7 s[1] submatrix starting at a[r[2], c[2]]. if s [ 1 ] < s [ 2 ] P - MATRIX - TRANSPOSE - SWAP ( A , r [ 2 ], c [ 2 ], r [ 1 ], c [ 1 ], s [ 2 ], s [ 1 ]) else if s [ 1 ] == 1 // since s[1] \u2265 s[2], must have that s[2] equals 1 exchange a [ r [ 1 ], c [ 1 ]] with a [ r [ 2 ], c [ 2 ]] else s ' = floor ( s [ 1 ] / 2 ) spawn P - MATRIX - TRANSPOSE - SWAP ( A , r [ 2 ], c [ 2 ], r [ 1 ], c [ 1 ], s [ 2 ], s ' ) P - MATRIX - TRANSPOSE - SWAP ( A , r [ 2 ], c [ 2 ] + s ' , r [ 1 ] + s ' , c [ 1 ], s [ 2 ], s [ 1 ] - s ' ) sync In order to transpose an $n \\times n$ matrix $A$, we call $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}(A, 1, 1, n)$. Let us first calculate the work and span of $\\text{P-MATRIX-TRANSPOSE-SWAP}$ so that we can plug in these values into the work and span calculations of $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$. The work $T_1'(N)$ of $\\text{P-MATRIX-TRANSPOSE-SWAP}$ on an $N$-element matrix is the running time of its serialization. We have the recurrence $$ \\begin{aligned} T_1'(N) & = 2T_1'(N / 2) + \\Theta(1) \\\\ & = \\Theta(N). \\end{aligned} $$ The span $T_\\infty'(N)$ is similarly described by the recurrence $$ \\begin{aligned} T_\\infty'(N) & = T_\\infty'(N / 2) + \\Theta(1) \\\\ & = \\Theta(\\lg N). \\end{aligned} $$ In order to calculate the work of $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$, we calculate the running time of its serialization. Let $T_1(N)$ be the work of the algorithm on an $N$-element matrix, where $N = n^2$, and assume for simplicity that $n$ is an exact power of $2$. Because the procedure makes two recursive calls with square submatrices of sizes $n / 2 \\times n / 2 = N / 4$ and because it does $\\Theta(n^2) = \\Theta(N)$ work to swap all the elements of the other two submatrices of size $n / 2 \\times n / 2$, its work is given by the recurrence $$ \\begin{aligned} T_1(N) & = 2T_1(N / 4) + \\Theta(N) \\\\ & = \\Theta(N). \\end{aligned} $$ The two parallel recursive calls in $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ execute on matrices of size $n / 2 \\times n / 2$. The span of the procedure is given by maximum of the span of one of these two recursive calls and the $\\Theta(\\lg N)$ span of $\\text{P-MATRIX-TRANSPOSE-SWAP}$, plus $\\Theta(1)$. Since the recurrence $$T_\\infty(N) = T_\\infty(N / 4) + \\Theta(1)$$ has the solution $T_\\infty(N) = \\Theta(\\lg N)$ by case 2 of Theorem 4.1, the span of the recursive call is asymptotically the same as the span of $\\text{P-MATRIX-TRANSPOSE-SWAP}$, and hence the span of $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ is $\\Theta(\\lg N)$. Thus, $\\text{P-MATRIX-TRANSPOSE-RECURSIVE}$ has parallelism $\\Theta(N / \\lg N) = \\Theta(n^2 / \\lg n)$.","title":"27.2-5"},{"location":"Chap27/27.2/#272-6","text":"Give pseudocode for an efficient multithreaded implementation of the Floyd-Warshall algorithm (see Section 25.2), which computes shortest paths between all pairs of vertices in an edge-weighted graph. Analyze your algorithm. 1 2 3 4 5 6 7 8 9 10 P - FLOYD - WARSHALL ( W ) n = W . rows parallel for i = 1 to n parallel for j = 1 to n d [ i , j ] = w [ i , j ] for k = 1 to n parallel for i = 1 to n parallel for j = 1 to n d [ i , j ] = min ( d [ i , j ], d [ i , k ] + d [ k , j ]) return D By Exercise 25.2-4, we can compute all the $d_{ij}$ values in parallel. The work of $\\text{P-FLOYD-WARSHALL}$ is the same as the running time of its serialization, which we computed as $\\Theta(n^3)$ in Section 25.2. The span of the doubly nested parallel for loops, which do constant work inside, is $\\Theta(\\lg n)$. Note, however, that the second set of doubly nested parallel for loops is executed within each of the $n$ iterations of the outermost serial for loop. Therefore, $\\text{P-FLOYD-WARSHALL}$ has span $\\Theta(n\\lg n)$ and $\\Theta(n^2 / \\lg n)$ parallelism.","title":"27.2-6"},{"location":"Chap27/27.3/","text":"27.3-1 Explain how to coarsen the base case of $\\text{P-MERGE}$. Replace the condition on line 2 with a check that $n < k$ for some base case size $k$. And instead of just copying over the particular element of $A$ to the right spot in $B$, you would call a serial sort on the remaining segment of $A$ and copy the result of that over into the right spots in $B$. 27.3-2 Instead of finding a median element in the larger subarray, as $\\text{P-MERGE}$ does, consider a variant that finds a median element of all the elements in the two sorted subarrays using the result of Exercise 9.3-8. Give pseudocode for an efficient multithreaded merging procedure that uses this median-finding procedure. Analyze your algorithm. By a slight modification of exercise 9.3-8 we can find we can find the median of all elements in two sorted arrays of total length $n$ in $O(\\lg n)$ time. We'll modify $\\text{P-MERGE}$ to use this fact. Let $\\text{MEDIAN}(T, p_1, r_1, p_2, r_2)$ be the function which returns a pair, $q$, where $q.pos$ is the position of the median of all the elements $T$ which lie between positions $p_1$ and $r_1$, and between positions $p_2$ and $r_2$, and $q.arr$ is $1$ if the position is between $p_1$ and $r_1$, and $2$ otherwise. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 P - MEDIAN - MERGE ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ], A , p [ 3 ]) n [ 1 ] = r [ 1 ] - p [ 1 ] + 1 n [ 2 ] = r [ 2 ] - p [ 2 ] + 1 if n [ 1 ] < n [ 2 ] // ensure that n[1] \u2265 n[2] exchange p [ 1 ] with p [ 2 ] exchange r [ 1 ] with r [ 2 ] exchange n [ 1 ] with n [ 2 ] if n [ 1 ] == 0 // both empty? return q = MEDIAN ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ]) if q . arr == 1 q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 2 ], r [ 2 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 1 ] + q [ 2 ] - p [ 2 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q . pos - 1 , p [ 2 ], q [ 2 ] - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q . pos + 1 , r [ 1 ], q [ 2 ] + 1 , r [ 2 ], A , p [ 3 ]) sync else q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 1 ], r [ 1 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 2 ] + q [ 2 ] - p [ 1 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q [ 2 ] - 1 , p [ 2 ], q . pos - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q [ 2 ] + 1 , r [ 1 ], q . pos + 1 , r [ 2 ], A , p [ 3 ]) sync The work is characterized by the recurrence $T_1(n) = O(\\lg n) + 2T_1(n / 2)$, whose solution tells us that $T_1(n) = O(n)$. The work is at least $\\Omega(n)$ since we need to examine each element, so the work is $\\Theta(n)$. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = O(\\lg n) + O(\\lg n / 2) + T_\\infty(n / 2) \\\\ & = O(\\lg n) + T_\\infty(n / 2) \\\\ & = \\Theta(\\lg^2 n), \\end{aligned} $$ by exercise 4.6-2. 27.3-3 Give an efficient multithreaded algorithm for partitioning an array around a pivot, as is done by the $\\text{PARTITION}$ procedure on page 171. You need not partition the array in place. Make your algorithm as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ You may need an auxiliary array and may need to make more than one pass over the input elements.) Suppose that there are $c$ different processors, and the array has length $n$ and you are going to use its last element as a pivot. Then, look at each chunk of size $\\lceil \\frac{n}{c} \\rceil$ of entries before the last element, give one to each processor. Then, each counts the number of elements that are less than the pivot. Then, we compute all the running sums of these values that are returned. This can be done easily by considering all of the subarrays placed along the leaves of a binary tree, and then summing up adjacent pairs. This computation can be done in time $\\lg(\\min\\{c, n\\})$ since it's the log of the number of leaves. From there, we can compute all the running sums for each of the subarrays also in logarithmic time. This is by keeping track of the sum of all more left cousins of each internal node, which is found by adding the left sibling's sum vale to the left cousin value of the parent, with the root's left cousin value initiated to $0$. This also just takes time the depth of the tree, so is $\\lg(\\min\\{c, n\\})$. Once all of these values are computed at the root, it is the index that the subarray's elements less than the pivot should be put. To find the position where the subarray's elements larger than the root should be put, just put it at twice the sum value of the root minus the left cousin value for that subarray. Then, the time taken is just $O(\\frac{n}{c})$. By doing this procedure, the total work is just $O(n)$, and the span is $O(\\lg n)$, and so has parallelization of $O(\\frac{n}{\\lg n})$. This whole process is split across the several algoithms appearing here. 27.3-4 Give a multithreaded version of $\\text{RECURSIVE-FFT}$ on page 911. Make your implementation as parallel as possible. Analyze your algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 P - RECURSIVE - FFT ( a ) n = a . length if n == 1 return a w [ n ] = e ^ { 2 * \u03c0 * i / n } w = 1 a ( 0 ) = [ a [ 0 ], a [ 2 ].. a [ n - 2 ]] a ( 1 ) = [ a [ 1 ], a [ 3 ].. a [ n - 1 ]] y ( 0 ) = spawn P - RECURSIVE - FFT ( a [ 0 ]) y ( 1 ) = P - RECURSIVE - FFT ( a [ 1 ]) sync parallel for k = 0 to n / 2 - 1 y [ k ] = y [ k ]( 0 ) + w * y [ k ]( 1 ) y [ k + n / 2 ] = y [ k ]( 0 ) - w * y [ k ]( 1 ) w = w * w [ n ] return y $\\text{P-RECURSIVE-FFT}$ parallelized over the two recursive calls, having a parallel for works because each of the iterations of the for loop touch independent sets of variables. The span of the procedure is only $\\Theta(\\lg n)$ giving it a parallelization of $\\Theta(n)$. 27.3-5 $\\star$ Give a multithreaded version of $\\text{RANDOMIZED-SELECT}$ on page 216. Make your implementation as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ Use the partitioning algorithm from Exercise 27.3-3.) Randomly pick a pivot element, swap it with the last element, so that it is in the correct format for running the procedure described in 27.3-3. Run partition from problem 27.3-3. As an intermediate step, in that procedure, we compute the number of elements less than the pivot ($T.root.sum$), so keep track of that value after the end of partition. Then, if we have that it is less than $k$, recurse on the subarray that was greater than or equal to the pivot, decreasing the order statistic of the element to be selected by $T.root.sum$. If it is larger than the order statistic of the element to be selected, then leave it unchanged and recurse on the subarray that was formed to be less than the pivot. A lot of the analysis in section 9.2 still applies, except replacing the timer needed for partitioning with the runtime of the algorithm in problem 27.3-3. The work is unchanged from the serial case because when $c = 1$, the algorithm reduces to the serial algorithm for partitioning. For span, the $O(n)$ term in the equation half way down page 218 can be replaced with an $O(\\lg n)$ term. It can be seen with the substitution method that the solution to this is logarithmic $$E[T(n)] \\le \\frac{2}{n} \\sum_{k = \\lfloor n / 2 \\rfloor}^{n - 1} C\\lg k + O(\\lg n) \\le O(\\lg n).$$ So, the total span of this algorithm will still just be $O(\\lg n)$. 27.3-6 $\\star$ Show how to multithread $\\text{SELECT}$ from Section 9.3. Make your implementation as parallel as possible. Analyze your algorithm. Let $\\text{MEDIAN}(A)$ denote a brute force method which returns the median element of the array $A$. We will only use this to find the median of small arrays, in particular, those of size at most $5$, so it will always run in constant time. We also let $A[i..j]$ denote the array whose elements are $A[i], A[i + 1], \\ldots, A[j]$. The function $\\text{P-PARTITION}(A, x)$ is a multithreaded function which partitions $A$ around the input element $x$ and returns the number of elements in $A$ which are less than or equal to $x$. Using a parallel for loop, its span is logarithmic in the number of elements in $A$. The work is the same as the serialization, which is $\\Theta(n)$ according to section 9.3. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = \\Theta(lg n / 5) + T_\\infty(n / 5) + \\Theta(\\lg n) + T_\\infty(7n / 10 + 6) \\\\ & \\le \\Theta(\\lg n) + T_\\infty(n / 5) + T_\\infty(7n / 10 + 6). \\end{aligned} $$ Using the substitution method we can show that $T_\\infty(n) = O(n^\\epsilon)$ for some $\\epsilon < 1$. In particular, $\\epsilon = 0.9$ works. This gives a parallelization of $\\Omega(n^0.1)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 P - SELECT ( A , i ) if n == 1 return A [ 1 ] let T [ 1. . floor ( n / 5 )] be a new array parallel for i = 0 to floor ( n / 5 ) - 1 T [ i + 1 ] = MEDIAN ( A [ i * floor ( n / 5 ).. i * floor ( n / 5 ) + 4 ]) if n / 5 is not an integer T [ floor ( n / 5 )] = MEDIAN ( A [ 5 * floor ( n / 5 ).. n ]) x = P - SELECT ( T , ceil ( n / 5 )) k = P - PARTITION ( A , x ) if k == i return x else if i < k P - SELECT ( A [ 1. . k - 1 ], i ) else P - SELECT ( A [ k + 1. . n ], i - k )","title":"27.3 Multithreaded merge sort"},{"location":"Chap27/27.3/#273-1","text":"Explain how to coarsen the base case of $\\text{P-MERGE}$. Replace the condition on line 2 with a check that $n < k$ for some base case size $k$. And instead of just copying over the particular element of $A$ to the right spot in $B$, you would call a serial sort on the remaining segment of $A$ and copy the result of that over into the right spots in $B$.","title":"27.3-1"},{"location":"Chap27/27.3/#273-2","text":"Instead of finding a median element in the larger subarray, as $\\text{P-MERGE}$ does, consider a variant that finds a median element of all the elements in the two sorted subarrays using the result of Exercise 9.3-8. Give pseudocode for an efficient multithreaded merging procedure that uses this median-finding procedure. Analyze your algorithm. By a slight modification of exercise 9.3-8 we can find we can find the median of all elements in two sorted arrays of total length $n$ in $O(\\lg n)$ time. We'll modify $\\text{P-MERGE}$ to use this fact. Let $\\text{MEDIAN}(T, p_1, r_1, p_2, r_2)$ be the function which returns a pair, $q$, where $q.pos$ is the position of the median of all the elements $T$ which lie between positions $p_1$ and $r_1$, and between positions $p_2$ and $r_2$, and $q.arr$ is $1$ if the position is between $p_1$ and $r_1$, and $2$ otherwise. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 P - MEDIAN - MERGE ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ], A , p [ 3 ]) n [ 1 ] = r [ 1 ] - p [ 1 ] + 1 n [ 2 ] = r [ 2 ] - p [ 2 ] + 1 if n [ 1 ] < n [ 2 ] // ensure that n[1] \u2265 n[2] exchange p [ 1 ] with p [ 2 ] exchange r [ 1 ] with r [ 2 ] exchange n [ 1 ] with n [ 2 ] if n [ 1 ] == 0 // both empty? return q = MEDIAN ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ]) if q . arr == 1 q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 2 ], r [ 2 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 1 ] + q [ 2 ] - p [ 2 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q . pos - 1 , p [ 2 ], q [ 2 ] - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q . pos + 1 , r [ 1 ], q [ 2 ] + 1 , r [ 2 ], A , p [ 3 ]) sync else q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 1 ], r [ 1 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 2 ] + q [ 2 ] - p [ 1 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q [ 2 ] - 1 , p [ 2 ], q . pos - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q [ 2 ] + 1 , r [ 1 ], q . pos + 1 , r [ 2 ], A , p [ 3 ]) sync The work is characterized by the recurrence $T_1(n) = O(\\lg n) + 2T_1(n / 2)$, whose solution tells us that $T_1(n) = O(n)$. The work is at least $\\Omega(n)$ since we need to examine each element, so the work is $\\Theta(n)$. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = O(\\lg n) + O(\\lg n / 2) + T_\\infty(n / 2) \\\\ & = O(\\lg n) + T_\\infty(n / 2) \\\\ & = \\Theta(\\lg^2 n), \\end{aligned} $$ by exercise 4.6-2.","title":"27.3-2"},{"location":"Chap27/27.3/#273-3","text":"Give an efficient multithreaded algorithm for partitioning an array around a pivot, as is done by the $\\text{PARTITION}$ procedure on page 171. You need not partition the array in place. Make your algorithm as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ You may need an auxiliary array and may need to make more than one pass over the input elements.) Suppose that there are $c$ different processors, and the array has length $n$ and you are going to use its last element as a pivot. Then, look at each chunk of size $\\lceil \\frac{n}{c} \\rceil$ of entries before the last element, give one to each processor. Then, each counts the number of elements that are less than the pivot. Then, we compute all the running sums of these values that are returned. This can be done easily by considering all of the subarrays placed along the leaves of a binary tree, and then summing up adjacent pairs. This computation can be done in time $\\lg(\\min\\{c, n\\})$ since it's the log of the number of leaves. From there, we can compute all the running sums for each of the subarrays also in logarithmic time. This is by keeping track of the sum of all more left cousins of each internal node, which is found by adding the left sibling's sum vale to the left cousin value of the parent, with the root's left cousin value initiated to $0$. This also just takes time the depth of the tree, so is $\\lg(\\min\\{c, n\\})$. Once all of these values are computed at the root, it is the index that the subarray's elements less than the pivot should be put. To find the position where the subarray's elements larger than the root should be put, just put it at twice the sum value of the root minus the left cousin value for that subarray. Then, the time taken is just $O(\\frac{n}{c})$. By doing this procedure, the total work is just $O(n)$, and the span is $O(\\lg n)$, and so has parallelization of $O(\\frac{n}{\\lg n})$. This whole process is split across the several algoithms appearing here.","title":"27.3-3"},{"location":"Chap27/27.3/#273-4","text":"Give a multithreaded version of $\\text{RECURSIVE-FFT}$ on page 911. Make your implementation as parallel as possible. Analyze your algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 P - RECURSIVE - FFT ( a ) n = a . length if n == 1 return a w [ n ] = e ^ { 2 * \u03c0 * i / n } w = 1 a ( 0 ) = [ a [ 0 ], a [ 2 ].. a [ n - 2 ]] a ( 1 ) = [ a [ 1 ], a [ 3 ].. a [ n - 1 ]] y ( 0 ) = spawn P - RECURSIVE - FFT ( a [ 0 ]) y ( 1 ) = P - RECURSIVE - FFT ( a [ 1 ]) sync parallel for k = 0 to n / 2 - 1 y [ k ] = y [ k ]( 0 ) + w * y [ k ]( 1 ) y [ k + n / 2 ] = y [ k ]( 0 ) - w * y [ k ]( 1 ) w = w * w [ n ] return y $\\text{P-RECURSIVE-FFT}$ parallelized over the two recursive calls, having a parallel for works because each of the iterations of the for loop touch independent sets of variables. The span of the procedure is only $\\Theta(\\lg n)$ giving it a parallelization of $\\Theta(n)$.","title":"27.3-4"},{"location":"Chap27/27.3/#273-5-star","text":"Give a multithreaded version of $\\text{RANDOMIZED-SELECT}$ on page 216. Make your implementation as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ Use the partitioning algorithm from Exercise 27.3-3.) Randomly pick a pivot element, swap it with the last element, so that it is in the correct format for running the procedure described in 27.3-3. Run partition from problem 27.3-3. As an intermediate step, in that procedure, we compute the number of elements less than the pivot ($T.root.sum$), so keep track of that value after the end of partition. Then, if we have that it is less than $k$, recurse on the subarray that was greater than or equal to the pivot, decreasing the order statistic of the element to be selected by $T.root.sum$. If it is larger than the order statistic of the element to be selected, then leave it unchanged and recurse on the subarray that was formed to be less than the pivot. A lot of the analysis in section 9.2 still applies, except replacing the timer needed for partitioning with the runtime of the algorithm in problem 27.3-3. The work is unchanged from the serial case because when $c = 1$, the algorithm reduces to the serial algorithm for partitioning. For span, the $O(n)$ term in the equation half way down page 218 can be replaced with an $O(\\lg n)$ term. It can be seen with the substitution method that the solution to this is logarithmic $$E[T(n)] \\le \\frac{2}{n} \\sum_{k = \\lfloor n / 2 \\rfloor}^{n - 1} C\\lg k + O(\\lg n) \\le O(\\lg n).$$ So, the total span of this algorithm will still just be $O(\\lg n)$.","title":"27.3-5 $\\star$"},{"location":"Chap27/27.3/#273-6-star","text":"Show how to multithread $\\text{SELECT}$ from Section 9.3. Make your implementation as parallel as possible. Analyze your algorithm. Let $\\text{MEDIAN}(A)$ denote a brute force method which returns the median element of the array $A$. We will only use this to find the median of small arrays, in particular, those of size at most $5$, so it will always run in constant time. We also let $A[i..j]$ denote the array whose elements are $A[i], A[i + 1], \\ldots, A[j]$. The function $\\text{P-PARTITION}(A, x)$ is a multithreaded function which partitions $A$ around the input element $x$ and returns the number of elements in $A$ which are less than or equal to $x$. Using a parallel for loop, its span is logarithmic in the number of elements in $A$. The work is the same as the serialization, which is $\\Theta(n)$ according to section 9.3. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = \\Theta(lg n / 5) + T_\\infty(n / 5) + \\Theta(\\lg n) + T_\\infty(7n / 10 + 6) \\\\ & \\le \\Theta(\\lg n) + T_\\infty(n / 5) + T_\\infty(7n / 10 + 6). \\end{aligned} $$ Using the substitution method we can show that $T_\\infty(n) = O(n^\\epsilon)$ for some $\\epsilon < 1$. In particular, $\\epsilon = 0.9$ works. This gives a parallelization of $\\Omega(n^0.1)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 P - SELECT ( A , i ) if n == 1 return A [ 1 ] let T [ 1. . floor ( n / 5 )] be a new array parallel for i = 0 to floor ( n / 5 ) - 1 T [ i + 1 ] = MEDIAN ( A [ i * floor ( n / 5 ).. i * floor ( n / 5 ) + 4 ]) if n / 5 is not an integer T [ floor ( n / 5 )] = MEDIAN ( A [ 5 * floor ( n / 5 ).. n ]) x = P - SELECT ( T , ceil ( n / 5 )) k = P - PARTITION ( A , x ) if k == i return x else if i < k P - SELECT ( A [ 1. . k - 1 ], i ) else P - SELECT ( A [ k + 1. . n ], i - k )","title":"27.3-6 $\\star$"},{"location":"Chap27/Problems/27-1/","text":"Consider the following multithreaded algorithm for performing pairwise addition on $n$-element arrays $A[1..n]$ and $B[1..n]$, storing the sums in $C[1..n]$: 1 2 3 SUM - ARRAYS ( A , B , C ) parallel for i = 1 to A . length C [ i ] = A [ i ] + B [ i ] a. Rewrite the parallel loop in $\\text{SUM-ARRAYS}$ using nested parallelism ( spawn and sync ) in the manner of $\\text{MAT-VEC-MAIN-LOOP}$. Analyze the parallelism of your implementation. Consider the following alternative implementation of the parallel loop, which contains a value $grain\\text-size$ to be specified: 1 2 3 4 5 6 7 SUM - ARRAYS ' ( A , B , C ) n = A . length grain - size = ? // to be determined r = ceil ( n / grain - size ) for k = 0 to r - 1 spawn ADD - SUBARRAY ( A , B , C , k * grain - size + 1 , min (( k + 1 ) * grain - size , n )) sync 1 2 3 ADD - SUBARRAY ( A , B , C , i , j ) for k = i to j C [ k ] = A [ k ] + B [ k ] b. Suppose that we set $grain\\text -size = 1$. What is the parallelism of this implementation? c. Give a formula for the span of $\\text{SUM-ARRAYS}'$ in terms of $n$ and $grain\\text-size$. Derive the best value for grain-size to maximize parallelism. a. Similar to $\\text{MAT-VEC-MAIN-LOOP}$, the required procedure, which we name $\\text{NESTED-SUM-ARRAYS}$, will take parameters $i$ and $j$ to specify the range of the array that is being computed in parallel. In order to perform the pairwise addition of two $n$-element arrays $A$ and $B$ and store the result into array $C$, we call $\\text{NESTED-SUM-ARRAYS}(A, B, C, 1, A.length)$. 1 2 3 4 5 6 NESTED - SUM - ARRAYS ( A , B , C , i , j ) if i == j C [ i ] = A [ i ] + B [ i ] else k = floor (( i + j ) / 2 ) spawn NESTED - SUM - ARRAYS ( A , B , C , i , k ) NESTED - SUM - ARRAYS ( A , B , C , k + 1 , j ) sync The work of $\\text{NESTED-SUM-ARRAYS}$ is given by the recurrence $$ \\begin{aligned} T_1(n) & = 2T_1(n / 2) + \\Theta(1) \\\\ & = \\Theta(n), \\end{aligned} $$ by case 1 of the master theorem. The span of the procedure is given by the recurrence $$ \\begin{aligned} T_\\infty(n) & = T_\\infty(n / 2) + \\Theta(1) \\\\ & = \\Theta(\\lg n), \\end{aligned} $$ by case 2 of the master theorem. Therefore, the above algorithm has $\\Theta(n / \\lg n)$ parallelism. b. Because $\\text{ADD-SUBARRAY}$ is serial, we can calculate both its work and span to be $\\Theta(j - i + 1)$, which based on the arguments from the call in $\\text{SUM-ARRAYS}'$ is $\\Theta(grain\\text-size)$, for all but the last call (which is $O(grain\\text-size)$). If $grain\\text-size = 1$, the procedure $\\text{SUM-ARRAYS}'$ calculates $r$ to be $n$, and each of the $n$ iterations of the serial for loop spawns $\\text{ADD-SUBARRAY}$ with the same value, $k + 1$, for the last two arguments. For example, when $k = 0$, the last two arguments to $\\text{ADD-SUBARRAY}$ are $1$, when $k = 1$, the last two arguments are $2$, and so on. That is, in each call to $\\text{ADD-SUBARRAY}$, its for loop iterates once and calculates a single value in the array $C$. When $grain\\text-size = 1$, the for loop in $\\text{SUM-ARRAYS}'$ iterates $n$ times and each iteration takes $\\Theta(1)$ time, resulting in $\\Theta(n)$ work. Although the for loop in $\\text{SUM-ARRAYS}'$ looks serial, note that each iteration spawns the call to $\\text{ADD-SUBARRAY}$ and the procedure waits for all its spawned children at the end of the for loop. That is, all loop iterations of $\\text{SUM-ARRAYS}'$ execute in parallel. Therefore, one might be tempted to say that the span of $\\text{SUM-ARRAYS}'$ is equal to the span of a single call to $\\text{ADD-SUBARRAY}$ plus the constant work done by the \ufb01rst three lines in $\\text{SUM-ARRAYS}'$, giving $\\Theta(1)$ span and $\\Theta(n)$ parallelism. This calculation of span and parallelism would be wrong, however, because there are $r$ spawns of $\\text{ADD-SUBARRAY}$ in $\\text{SUM-ARRAYS}'$, where $r$ is not a constant. Hence, we must add a $\\Theta(r)$ term to the span of $\\text{SUM-ARRAYS}'$ in order to account for the overhead of spawning $r$ calls to $\\text{ADD-SUBARRAY}$. Based on the above discussion, the span of $\\text{SUM-ARRAYS}'$ is $\\Theta(r) + \\Theta(grain\\text-size) + \\Theta(1)$. When $grain\\text-size = 1$, we get $r = n$; therefore, $\\text{SUM-ARRAYS}'$ has $\\Theta(n)$ span and $\\Theta(1)$ parallelism. c. For a general $grain\\text-size$, each iteration of the for loop in $\\text{SUM-ARRAYS}'$ except for the last results in $grain\\text-size$ iterations of the for loop in $\\text{ADD-SUBARRAY}$. In the last iteration of $\\text{SUM-ARRAYS}'$, the for loop in $\\text{ADD-SUBARRAY}$ iterates $n \\mod grain\\text-size$ times. Therefore, we can claim that the span of $\\text{ADD-SUBARRAY}$ is $$\\Theta(\\max(grain\\text-size, n \\mod grain\\text-size)) = \\Theta(grain\\text-size).$$ $\\text{SUM-ARRAYS}'$ achieves maximum parallelism when its span, given by $\\Theta(r) + \\Theta(grain\\text-size) + \\Theta(1)$, is minimum. Since $r = \\lceil n / grain\\text-size \\rceil$, the minimum occurs when $r \\approx grain\\text-size$, i.e., when $grain\\text-size \\approx \\sqrt n$.","title":"27-1 Implementing parallel loops using nested parallelism"},{"location":"Chap27/Problems/27-2/","text":"The $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$ procedure has the disadvantage that it must allocate a temporary matrix $T$ of size $n \\times n$, which can adversely affect the constants hidden by the $\\Theta$-notation. The $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$ procedure does have high parallelism, however. For example, ignoring the constants in the $\\Theta$-notation, the parallelism for multiplying $1000 \\times 1000$ matrices comes to approximately $1000^3 / 10^2 = 10^7$, since $\\lg 1000 \\approx 10$. Most parallel computers have far fewer than 10 million processors. a. Describe a recursive multithreaded algorithm that eliminates the need for the temporary matrix $T$ at the cost of increasing the span to $\\Theta(n)$. ($\\textit{Hint:}$ Compute $C = C + AB$ following the general strategy of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$, but initialize $C$ in parallel and insert a sync in a judiciously chosen location.) b. Give and solve recurrences for the work and span of your implementation. c. Analyze the parallelism of your implementation. Ignoring the constants in the $\\Theta$-notation, estimate the parallelism on $1000 \\times 1000$ matrices. Compare with the parallelism of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. a. We initialize the output matrix $C$ using doubly nested parallel for loops and then call $\\text{P-MATRIX-MULTIPLY-RECURSIVE}'$, defined below. 1 2 3 4 5 6 P - MATRIX - MULTIPLY - LESS - MEM ( C , A , B ) n = A . rows parallel for i = 1 to n parallel for j = 1 to n c [ i , j ] = 0 P - MATRIX - MULTIPLY - RECURSIVE ' ( C , A , B ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 P - MATRIX - MULTIPLY - RECURSIVE ' ( C , A , B ) n = A . rows if n == 1 c [ 1 , 1 ] = c [ 1 , 1 ] + a [ 1 , 1 ] * b [ 1 , 1 ] else partition A , B , and C into n / 2 \u00d7 n / 2 submatrices A [ 1 , 1 ], A [ 1 , 2 ], A [ 2 , 1 ], A [ 2 , 2 ]; B [ 1 , 1 ], B [ 1 , 2 ], B [ 2 , 1 ], B [ 2 , 2 ]; and C [ 1 , 1 ], C [ 1 , 2 ], C [ 2 , 1 ], C [ 2 , 2 ] spwan P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 1 , 2 ], A [ 1 , 1 ], B [ 1 , 1 ]) spwan P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 1 , 2 ], A [ 1 , 1 ], B [ 1 , 2 ]) spwan P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 2 , 1 ], A [ 2 , 1 ], B [ 1 , 1 ]) P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 2 , 2 ], A [ 2 , 1 ], B [ 1 , 2 ]) sync spwan P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 1 , 1 ], A [ 1 , 2 ], B [ 2 , 1 ]) spwan P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 1 , 2 ], A [ 1 , 2 ], B [ 2 , 2 ]) spwan P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 2 , 1 ], A [ 2 , 2 ], B [ 2 , 1 ]) P - MATRIX - MULTIPLY - RECURSIVE ' ( C [ 2 , 2 ], A [ 2 , 2 ], B [ 2 , 2 ]) sync b. The procedure $\\text{P-MATRIX-MULTIPLY-LESS-MEM}$ performs $\\Theta(n^2)$ work in the doubly nested parallel for loops, and then it calls the procedure $\\text{P-MATRIX-MULTIPLY-RECURSIVE}'$. The recurrence for the work $M_1'(n)$ of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}'$ is $8M_1'(n / 2) + \\Theta(1)$, which gives us $M_1'(n) = \\Theta(n^3)$. Therefore, $T_1(n) = \\Theta(n^3)$. The span of the doubly nested parallel for loops that initialize the output array $C$ is $\\Theta(\\lg n)$. In $\\text{P-MATRIX-MULTIPLY-RECURSIVE}'$, there are two groups of spawned recursive calls; therefore, the span $M_\\infty'(n)$ of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}'$ is given by the recurrence $M_\\infty'(n) = 2M_\\infty'(n / 2) + \\Theta(1)$, which gives us $M_\\infty'(n) = \\Theta(n)$. Because the span $\\Theta(n)$ of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}'$ dominates, we have $T_\\infty(n) = \\Theta(n)$. c. The parallelism of $\\text{P-MATRIX-MULTIPLY-LESS-MEM}$ is $\\Theta(n^3 / n) = \\Theta(n^2)$. Ignoring the constants in the $\\Theta$-notation, the parallelism for multiplying $1000 \\times 1000$ matrices is $1000^2 = 10^6$, which is only a factor of $10$ less than that of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. Although the parallelism of the new procedure is much less than that of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$, the algorithm still scales well for a large number of processors.","title":"27-2 Saving temporary space in matrix multiplication"},{"location":"Chap27/Problems/27-3/","text":"a. Parallelize the $\\text{LU-DECOMPOSITION}$ procedure on page 821 by giving pseudocode for a multithreaded version of this algorithm. Make your implementation as parallel as possible, and analyze its work, span, and parallelism. b. Do the same for $\\text{LUP-DECOMPOSITION}$ on page 824. c. Do the same for $\\text{LUP-SOLVE}$ on page 817. d. Do the same for a multithreaded algorithm based on equation $\\text{(28.13)}$ for inverting a symmetric positive-definite matrix. a. For the algorithm $\\text{LU-DECOMPOSITION}(A)$ on page 821, the inner for loops can be parallelized, since they never update values that are read on later runs of those loops. However, the outermost for loop cannot be parallelized because across iterations of it the changes to the matrices from previous runs are used to affect the next. This means that the span will be $\\Theta(n \\lg n)$, work will still be $\\Theta(n^3)$ and, so, the parallelization will be $\\Theta(\\frac{n^3}{n\\lg n}) = \\Theta(\\frac{n^2}{\\lg n})$. b. The for loop on lines 7-10 is taking the max of a set of things, while recording the index that that max occurs. This for loop can therefor be replaced with a $\\lg n$ span parallelized procedure in which we arrange the $n$ elements into the leaves of an almost balanced binary tree, and we let each internal node be the max of its two children. Then, the span will just be the depth of this tree. This procedure can gracefully scale with the number of processors to make the span be linear, though even if it is $\\Theta(n\\lg n)$ it will be less than the $\\Theta(n^2)$ work later. The for loop on line 14-15 and the implicit for loop on line 15 have no concurrent editing, and so, can be made parallel to have a span of $lg n$. While the for loop on lines 18-19 can be made parallel, the one containing it cannot without creating data races. Therefore, the total span of the naive parallelized algorithm will be $\\Theta(n^2\\lg n)$, with a work of $\\Theta(n^3)$. So, the parallelization will be $\\Theta(\\frac{n}{\\lg n})$. Not as parallized as part (a), but still a significant improvement. c. We can parallelize the computing of the sums on lines 4 and 6, but cannot also parallize the for loops containing them without creating an issue of concurrently modifying data that we are reading. This means that the span will be $\\Theta(n\\lg n)$, work will still be $\\Theta(n^2)$, and so the parallelization will be $\\Theta(\\frac{n}{\\lg n})$. d. The recurrence governing the amount of work of implementing this procedure is given by $$I(n) \\le 2I(n / 2) + 4M(n / 2) + O(n^2).$$ However, the two inversions that we need to do are independent, and the span of parallelized matrix multiply is just $O(\\lg n)$. Also, the $n^2$ work of having to take a transpose and subtract and add matrices has a span of only $O(\\lg n)$. Therefore, the span satisfies the recurrence $$I_\\infty(n) \\le I_\\infty(n / 2) + O(\\lg n).$$ This recurrence has the solution $I_\\infty(n) \\in \\Theta(\\lg^2 n)$ by exercise 4.6-2. Therefore, the span of the inversion algorithm obtained by looking at the procedure detailed on page 830. This makes the parallelization of it equal to $\\Theta(M(n) / \\lg^2 n)$ where $M(n)$ is the time to compute matrix products.","title":"27-3 Multithreaded matrix algorithms"},{"location":"Chap27/Problems/27-4/","text":"A $\\otimes$-reduction of an array $x[1 \\ldots n]$, where $\\otimes$ is an associative operator, is the value $$y = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[n].$$ The following procedure computes the $\\otimes$-reduction of a subarray $x[i \\ldots j]$ serially. 1 2 3 4 5 REDUCE ( x , i , j ) y = x [ i ] for k = i + 1 to j y = y \u2297 x [ k ] return y a. Use nested parallelism to implement a multithreaded algorithm $\\text{P-REDUCE}$, which performs the same function with $\\Theta(n)$ work and $\\Theta(\\lg n)$ span. Analyze your algorithm. A related problem is that of computing a $\\otimes$-prefix computation , sometimes called a $\\otimes$-scan , on an array $x[1 \\ldots n]$, where $\\otimes$ is once again an associative operator. The $\\otimes$-scan produces the array $y[1 \\ldots n]$ given by $$ \\begin{aligned} y[1] & = x[1], \\\\ y[2] & = x[1] \\otimes x[2], \\\\ y[3] & = x[1] \\otimes x[2] \\otimes x[3], \\\\ & \\vdots \\\\ y[n] & = x[1] \\otimes x[2] \\otimes x[3] \\otimes \\cdots \\otimes x[n], \\end{aligned} $$ that is, all prefixes of the array $x$ \"summed\" using $\\otimes$ operator. The following serial procedure $\\text{SCAN}$ performs a $\\otimes$-prefix computation: 1 2 3 4 5 6 7 SCAN ( x ) n = x . length let y [ 1. . n ] be a new array y [ 1 ] = x [ 1 ] for i = 2 to n y [ i ] = y [ i - 1 ] \u2297 x [ i ] return y Unfortunately, multithreading $\\text{SCAN}$ is not straightforward. For example, changing the for loop to a parallel for loop would create races, since each iteration of the loop body depends on the previous iteration. The following procedure $\\text{P-SCAN-1}$ performs the $\\otimes$-prefix computation in parallel, albeit inefficiently. 1 2 3 4 5 P - SCAN - 1 ( x ) n = x . length let y [ 1. . n ] be a new array P - SCAN - 1 - AUX ( x , y , 1 , n ) return y 1 2 3 P - SCAN - 1 - AUX ( x , y , i , j ) parallel for l = i to j y [ l ] = P - REDUCE ( x , 1 , l ) b. Analyze the work, span, and parallelism of $\\text{P-SCAN-1}$. By using nested parallelism, we can obtain a more efficient $\\otimes$-prefix computation: 1 2 3 4 5 P - SCAN - 2 ( x ) n = x . length let y [ 1. . n ] be a new array P - SCAN - 2 - AUX ( x , y , 1 , n ) return y 1 2 3 4 5 6 7 8 9 P - SCAN - 2 - AUX ( x , y , i , j ) if i == j y [ i ] = x [ i ] else k = floor (( i + j ) / 2 ) spawn P - SCAN - 2 - AUX ( x , y , i , k ) P - SCAN - 2 - AUX ( x , y , k + 1 , j ) sync parallel for l = k + 1 to j y [ l ] = y [ k ] \u2297 y [ l ] c. Argue that $\\text{P-SCAN-2}$ is correct, and analyze its work, span, and parallelism. We can improve on both $\\text{P-SCAN-1}$ and $\\text{P-SCAN-2}$ by performing the $\\otimes$-prefix computation in two distinct passes over the data. On the first pass, we gather the terms for various contiguous subarrays of $x$ into a temporary array $t$, and on the second pass we use the terms in $t$ to compute the final result $y$. The following pseudocode implements this strategy, but certain expressions have been omitted: 1 2 3 4 5 6 7 8 P - SCAN - 3 ( x ) n = x . length let y [ 1. . n ] and t [ 1. . n ] be new arrays y [ 1 ] = x [ 1 ] if n > 1 P - SCAN - UP ( x , t , 2 , n ) P - SCAN - DOWN ( x [ 1 ], x , t , y , 2 , n ) return y 1 2 3 4 5 6 7 8 9 P - SCAN - UP ( x , t , i , j ) if i == j return x [ i ] else k = floor (( i + j ) / 2 ) t [ k ] = spawn P - SCAN - UP ( x , t , i , k ) right = P - SCAN - UP ( x , t , k + 1 , j ) sync return ____ // fill in the blank 1 2 3 4 5 6 7 8 P - SCAN - DOWN ( v , x , t , y , i , j ) if i == j y [ i ] = v \u2297 x [ i ] else k = floor (( i + j ) / 2 ) spawn P - SCAN - DOWN ( ____ , x , t , y , i , k ) // fill in the blank P - SCAN - DOWN ( ____ , x , t , y , k + 1 , j ) // fill in the blank sync d. Fill in the three missing expressions in line 8 of $\\text{P-SCAN-UP}$ and lines 5 and 6 of $\\text{P-SCAN-DOWN}$. Argue that with expressions you supplied, $\\text{P-SCAN-3}$ is correct. ($\\textit{Hint:}$ Prove that the value $v$ passed to $\\text{P-SCAN-DOWN}(v, x, t, y, i, j)$ satisfies $v = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[i - 1]$.) e. Analyze the work, span, and parallelism of $\\text{P-SCAN-3}$. a. Here is a multithreaded $\\otimes$-reduction algorithm: 1 2 3 4 5 6 7 8 P - REDUCE ( x , i , j ) if i == j return x [ i ] else mid = floor (( i + j ) / 2 ) lh = spawn P - REDUCE ( x , i , mid ) rh = P - REDUCE ( x , mid + 1 , j ) sync return lh \u2297 rh If we denote the length $j - i + 1$ of the subarray $x[i..j]$ by $n$, then the work for the above algorithm is given by the recurrence $T_1(n) = 2T_1(n / 2) + \\Theta(1) = \\Theta(n)$. Because one of the recursive calls to $\\text{P-REDUCE}$ is spawned and the procedure does constant work following the recursive calls and in the base case, the span is given by the recurrence $T_\\infty(n) = T_\\infty(n / 2) + \\Theta(1) = \\Theta(\\lg n)$. b. The work and span of $\\text{P-SCAN-1-AUX}$ dominate the work and span of $\\text{P-SCAN-1}$. We can calculate the work of $\\text{P-SCAN-1-AUX}$ by replacing the parallel for loop with an ordinary for loop and noting that in each iteration, the running time of $\\text{P-REDUCE}$ will be equal to $\\Theta(l)$. Since $\\text{P-SCAN-1}$ calls $\\text{P-SCAN-1-AUX}$ with $1$ and $n$ as the last two arguments, the running time of $\\text{P-SCAN-1}$, and hence its work, is $\\Theta(1 + 2 + \\cdots + n) = \\Theta(n^2)$. As we noted earlier, the parallel for loop in $\\text{P-SCAN-1-AUX}$ undergoes $n$ iterations; therefore, the span of $\\text{P-SCAN-1-AUX}$ is given by $\\Theta(\\lg n)$ for the recursive splitting of the loop iterations plus the span of the iteration that has maximum span. Among the loop iterations, the call to $\\text{P-REDUCE}$ in the last iteration (when $l = n$) has the maximum span, equal to $\\Theta(\\lg n)$. Thus, $\\text{P-SCAN-1}$ has $\\Theta(\\lg n)$ span and $\\Theta(n^2 / \\lg n)$ parallelism. c. In $\\text{P-SCAN-2-AUX}$, before the parallel for loop in lines 7 and 8 executes, the following invariant is satisfied: $y[l] = x[i] \\otimes x[i + 1] \\otimes \\cdots \\otimes x[l]$ for $l = i, i + 1, \\ldots, k$ and $y[l] = x[k + 1] \\otimes x[k + 2] \\otimes \\cdots \\otimes x[k]$ for $l = k + 1, k + 2, \\ldots, j$. The parallel for loop need not update $y[i], \\ldots, y[k]$, since they have the correct values after the call to $\\text{P-SCAN-2-AUX}(x, y, i, k)$. For $l = k + 1, k + 2, \\ldots, j$, the parallel for loop sets $$ \\begin{aligned} y[l] & = y[k] \\otimes y[l] \\\\ & = x[i] \\otimes \\cdots \\otimes x[k] \\otimes x[k + 1] \\otimes \\cdots \\otimes x[l] \\\\ & = x[i] \\otimes \\cdots \\otimes x[l]. \\end{aligned} $$ as desired. We can run this loop in parallel because the $l$th iteration depends only on the values of $y[k]$, which is the same in all iterations, and $y[l]$. Therefore, when the call to $\\text{P-SCAN-2-AUX}$ from $\\text{P-SCAN-2}$ returns, array $y$ represents the $\\otimes$-prefix computation of array $x$. Because the work and span of $\\text{P-SCAN-2-AUX}$ dominate the work and span of $\\text{P-SCAN-2}$, we will concentrate on calculating these values for $\\text{P-SCAN-2-AUX}$ working on an array of size $n$. The work $PS2A_1(n)$ of $\\text{P-SCAN-2-AUX}$ is given by the recurrence $PS2A_1(n) = 2PS2A_1(n / 2) + \\Theta(\\lg n)$, which equals $\\Theta(n\\lg n)$ by case 2 of the master theorem. The span $PS2A_\\infty(n)$ of $\\text{P-SCAN-2-AUX}$ is given by the recurrence $PS2A_\\infty(n) = PS2A_\\infty(n / 2) + \\Theta(\\lg n)$, which equals $\\Theta(\\lg^2 n)$ per Exercise 4.6-2. That is, the work, span, and parallelism of $\\text{P-SCAN-2}$ are $\\Theta(n\\lg n)$, $\\Theta(\\lg^2 n)$, and $\\Theta(n / \\lg n)$, respectively. d. The missing expression in line 8 of $\\text{P-SCAN-UP}$ is $t[k] \\otimes right$ right. The missing expressions in lines 5 and 6 of $\\text{P-SCAN-DOWN}$ are $v$ and $v \\otimes t[k]$, respectively. As suggested in the hint, we will prove that the value $v$ passed to $\\text{P-SCAN-DOWN}(v, x, t, y, i, j)$ satisfies $v = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[i - 1]$, so that the value $v \\otimes x[i]$ stored into $y[i]$ in the base case of $\\text{P-SCAN-DOWN}$ is correct. In order to compute the arguments that are passed to $\\text{P-SCAN-DOWN}$, we must first understand what $t[k]$ holds as a result of the call to $\\text{P-SCAN-UP}$. A call to $\\text{P-SCAN-UP}(x, t, i, j)$ returns $x[i] \\otimes \\cdots \\otimes x[j]$; because $t[k]$ stores the return value of $\\text{P-SCAN-UP}(x, t, i, k)$, we can say that $t[k] = x[i] \\otimes \\cdots \\otimes x[k]$. The value $v = x[1]$ when $\\text{P-SCAN-DOWN}(x[1], x, t, y, 2, n)$ is called from $\\text{P-SCAN-3}$ clearly satisifies $v = x[1] \\otimes \\cdots \\otimes x[i - 1]$. Let us suppose that $v = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[i - 1]$ in a call of $\\text{P-SCAN-DOWN}(v, x, t, y, i, j)$. Therefore, $v$ meets the required condition in the first recursive call, with $i$ and $k$ as the last two arguments, in $\\text{P-SCAN-DOWN}$. If we can prove that the value $v \\otimes t[k]$ passed to the second recursive call in $\\text{P-SCAN-DOWN}$ equals $x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[k]$, we would have proved the required condition on $v$ for all calls to $\\text{P-SCAN-DOWN}$. Earlier, we proved that $t[k] = x[i] \\otimes \\cdots \\otimes x[k]$; therefore, $$ \\begin{aligned} v \\otimes t[k] & = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[i - 1] \\otimes x[i] \\otimes \\cdots x[k] \\\\ & = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[k]. \\end{aligned} $$ Thus, the value $v$ passed to $\\text{P-SCAN-DOWN}(v, x, t, y, i, j)$ satisfies $v = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[i - 1]$. e. Let $PSU_1(n)$ and $PSU_\\infty(n)$ denote the work and span of $\\text{P-SCAN-UP}$ and let $PSD_1(n)$ and $PSD_\\infty(n)$ denote the work and span of $\\text{P-SCAN-DOWN}$. Then the expressions $T_1(n) = PSU_1(n) + PSD_1(n) + \\Theta(1)$ and $T_\\infty(n) = PSU_\\infty(n) + PSD_\\infty(n) + \\Theta(1)$ characterize the work and span of $\\text{P-SCAN-3}$. The work $PSU_1(n)$ of $\\text{P-SCAN-UP}$ is given by the recurrence $$PSU_1(n) = 2PSU_1(n / 2) + \\Theta(1),$$ and its span is defined by the recurrence $$PSU_\\infty(n) = PSU_\\infty(n / 2) + \\Theta(1).$$ Using the master theorem to solve these recurrences, we get $PSU_1(n) = \\Theta(\\lg n)$ and $PSU_\\infty(n) = \\Theta(\\lg n)$. Similarly, the recurrences $$ \\begin{aligned} PSD_1 & = 2PSD_1(n / 2) + \\Theta(1), & \\text{(*)} \\\\ PSD_\\infty & = PSD_\\infty(n / 2) + \\Theta(1) & \\text{(**)} \\end{aligned} $$ define the work and span of $\\text{P-SCAN-DOWN}$, and they evaluate to $PSD_1(n) = \\Theta(\\lg n)$ and $PSD_\\infty(n) = \\Theta(\\lg n)$. Applying the results for the work and span of $\\text{P-SCAN-UP}$ and $\\text{P-SCAN-DOWN}$ obtained above in the expressions for the work and span of $\\text{P-SCAN-3}$, we get $T_1(n) = \\Theta(\\lg n)$ and $T_\\infty(n) = \\Theta(\\lg n)$. Hence, $\\text{P-SCAN-3}$ has $\\Theta(n / \\lg n)$ parallelism. $\\text{P-SCAN-3}$ performs less work than $\\text{P-SCAN-1}$, but with the same span, and it has the same parallelism as $\\text{P-SCAN-2}$ with less work and a lower span.","title":"27-4 Multithreading reductions and prefix computations"},{"location":"Chap27/Problems/27-5/","text":"Computational science is replete with algorithms that require the entries of an array to be filled in with values that depend on the values of certain already computed neighboring entries, along with other information that does not change over the course of the computation. The pattern of neighboring entries does not change during the computation and is called a stencil . For example, Section 15.4 presents a stencil algorithm to compute a longest common subsequence, where the value in entry $c[i, j]$ depends only on the values in $c[i - 1, j]$, $c[i, j - 1]$, and $c[i - 1, j - 1]$, as well as the elements $x_i$ and $y_j$ within the two sequences given as inputs. The input sequences are fixed, but the algorithm fills in the two-dimensional array $c$ so that it computes entry $c[i, j]$ after computing all three entries $c[i - 1, j]$, $c[i, j - 1]$, and $c[i - 1, j - 1]$. In this problem, we examine how to use nested parallelism to multithread a simple stencil calculation on an $n \\times n$ array $A$ in which, of the values in $A$, the value placed into entry $A[i, j]$ depends only on values in $A[i' , j']$, where $i' \\le i$ and $j' \\le j$ (and of course, $i' \\ne i$ or $j' \\ne j$). In other words, the value in an entry depends only on values in entries that are above it and/or to its left, along with static information outside of the array. Furthermore, we assume throughout this problem that once we have filled in the entries upon which $A[i, j]$ depends, we can fill in $A[i, j]$ in $\\Theta(1)$ time (as in the $\\text{LCS-LENGTH}$ procedure of Section 15.4). We can partition the $n \\times n$ array $A$ into four $n / 2 \\times n / 2$ subarrays as follows: $$ A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\tag{27.11} \\end{pmatrix} . $$ Observe now that we can fill in subarray $A_{11}$ recursively, since it does not depend on the entries of the other three subarrays. Once $A_{11}$ is complete, we can continue to fill in $A_{12}$ and $A_{21}$ recursively in parallel, because although they both depend on $A_{11}$ , they do not depend on each other. Finally, we can fill in $A_{22}$ recursively. a. Give multithreaded pseudocode that performs this simple stencil calculation using a divide-and-conquer algorithm $\\text{SIMPLE-STENCIL}$ based on the decomposition $\\text{(27.11)}$ and the discussion above. (Don't worry about the details of the base case, which depends on the specific stencil.) Give and solve recurrences for the work and span of this algorithm in terms of $n$. What is the parallelism? b. Modify your solution to part (a) to divide an $n \\times n$ array into nine $n / 3 \\times n / 3$ subarrays, again recursing with as much parallelism as possible. Analyze this algorithm. How much more or less parallelism does this algorithm have compared with the algorithm from part (a)? c. Generalize your solutions to parts (a) and (b) as follows. Choose an integer $b \\ge 2$. Divide an $n \\times n$ array into $b^2$ subarrays, each of size $n / b \\times n / b$, recursing with as much parallelism as possible. In terms of $n$ and $b$, what are the work, span, and parallelism of your algorithm? Argue that, using this approach, the parallelism must be $o(n)$ for any choice of $b \\ge 2$. ($\\textit{Hint:}$ For this last argument, show that the exponent of $n$ in the parallelism is strictly less than $1$ for any choice of $b \\ge 2$.) d. Give pseudocode for a multithreaded algorithm for this simple stencil calculation that achieves $\\Theta(n\\lg n)$ parallelism. Argue using notions of work and span that the problem, in fact, has $\\Theta(n)$ inherent parallelism. As it turns out, the divide-and-conquer nature of our multithreaded pseudocode does not let us achieve this maximal parallelism. a. In this part of the problem, we will assume that $n$ is an exact power of $2$, so that in a recursive step, when we divide the $n \\times n$ matrix $A$ into four $n / 2 \\times n / 2$ matrices, we will be guaranteed that $n / 2$ is an integer, for all $n \\ge 2$. We make this assumption simply to avoid introducing $\\lfloor n / 2 \\rfloor$ and $\\lceil n / 2 \\rceil$ terms in the pseudocode and the analysis that follow. In the pseudocode below, we assume that we have a procedure $\\text{BASE-CASE}$ available to us, which calculates the base case of the stencil. 1 2 3 4 5 6 7 8 9 10 11 SIMPLE - STENCIL ( A , i , j , n ) if n == 1 A [ i , j ] = BASE - CASE ( A , i , j ) else // Calculate submatrix A[1, 1]. SIMPLE - STENCIL ( A , i , j , n / 2 ) // Calculate submatrices A[1, 2] and A[2, 1] in parallel. spawn SIMPLE - STENCIL ( A , i , j + n / 2 , n / 2 ) SIMPLE - STENCIL ( A , i + n / 2 , j , n / 2 ) sync // Calculate submatrix A[2, 2]. SIMPLE - STENCIL ( A , i + n / 2 , j + n / 2 , n / 2 ) To perform a simple stencil calculation on an $n \\times n$ matrix $A$, we call $\\text{SIMPLE-STENCIL}(A, 1, 1, n)$. The recurrence for the work is $T_1(n) = 4T_1(n / 2) + \\Theta(1) = \\Theta(n^2)$. Of the four recursive calls in the algorithm above, only two run in parallel. Therefore, the recurrence for the span is $T_\\infty(n) = 3T_\\infty(n / 2) + \\Theta(1) = \\Theta(n^{\\lg 3})$, and the parallelism is $\\Theta(n^{2 - \\lg 3}) \\approx \\Theta(n^{0.415})$. b. Similar to $\\text{SIMPLE-STENCIL}$ of the previous part, we present $\\text{P-STENCIL-3}$, which divides $A$ into nine submatrices, each of size $n / 3 \\times n / 3$, and solves them recursively. To perform a stencil calculation on an $n \\times n$ matrix $A$, we call $\\text{P-STENCIL-3}(A, 1, 1, n)$. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 P - STENCIL - 3 ( A , i , j , n ) if n == 1 A [ i , j ] = BASE - CASE ( A , i , j ) else // Group 1: compute submatrix A[1, 1]. P - STENCIL - 3 ( A , i , j , n / 3 ) // Group 2: compute submatrices A[1, 2] and A[2, 1]. spawn P - STENCIL - 3 ( A , i , j + n / 3 , n / 3 ) P - STENCIL - 3 ( A , i + n / 3 , j , n / 3 ) sync // Group 3: compute submatrices A[1, 3], A[2, 2], and A[3, 1]. spawn P - STENCIL - 3 ( A , i , j + 2 n / 3 , n / 3 ) spawn P - STENCIL - 3 ( A , i + n / 3 , j + n / 3 , n / 3 ) P - STENCIL - 3 ( A , i + 2 n / 3 , j , n / 3 ) sync // Group 4: compute submatrices A[2, 3] and A[3, 2]. spawn P - STENCIL - 3 ( A , i + n / 3 , j + 2 n / 3 , n / 3 ) P - STENCIL - 3 ( A , i + 2 n / 3 , j + n / 3 , n / 3 ) sync // Group 5: compute submatrix A[3, 3]. P - STENCIL - 3 ( A , i + 2 n / 3 , j + 2 n / 3 , n / 3 ) From the pseudocode, we can informally say that we can solve the nine subproblems in five groups, as shown in the following matrix: $$ \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 3 & 4 \\\\ 3 & 4 & 5 \\end{pmatrix} . $$ Each entry in the above matrix specifies the group of the corresponding $n / 3 \\times n / 3$ submatrix of $A$; we can compute in parallel the entries of all submatrices that fall in the same group. In general, for $i = 2, 3, 4, 5$, we can calculate group $i$ after completing the computation of group $i - 1$. The recurrence for the work is $$T_1(n) = 9T_1(n / 3) + \\Theta(1) = \\Theta(n^2).$$ The recurrence for the span is $$T_\\infty(n) = 5T_\\infty(n / 3) + \\Theta(1) = \\Theta(n^{\\log_3 5}).$$ Therefore, the parallelism is $$\\Theta(n^{2 - \\log_3 5}) \\approx \\Theta(n^{0.535}).$$ c. Similar to the previous part, we can solve the $b^2$ subproblems in $2b - 1$ groups: $$ \\begin{pmatrix} 1 & 2 & 3 & \\cdots & b - 2 & b - 1 & b \\\\ 2 & 3 & 4 & \\cdots & b - 1 & b & b + 1 \\\\ 3 & 4 & 5 & \\cdots & b & b + 1 & b + 2 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\ b - 2 & b - 1 & b & \\cdots & 2b - 5 & 2b - 4 & 2b - 3 \\\\ b - 1 & b & b + 1 & \\cdots & 2b - 4 & 2b - 3 & 2b - 2 \\\\ b & b + 1 & b + 2 & \\cdots & 2b - 3 & 2b - 2 & 2b - 1 \\end{pmatrix} . $$ The recurrence for the work is $$T_1(n) = b^2 T_1(n / b) + \\Theta(1) = \\Theta(n^2).$$ The recurrence for the span is $$T_\\infty(n) = (2b - 1) T_\\infty(n / b) + \\Theta(1) = \\Theta(n^{\\log_b (2b - 1)}).$$ The parallelism is $$\\Theta(n^{2 - \\log_b (2b - 1)}).$$ As the hint suggests, in order to show that the parallelism must be $o(n)$ for any choice of $b \\ge 2$, we need to show that $2 - \\log_b (2b - 1)$, which is the exponent of $n$ in the parallelism, is strictly less than $1$ for any choice of $b \\ge 2$. Since $b \\ge 2$, we know that $2b - 1 > b$, which implies that $\\log_b (2b - 1) > \\log_b b = 1$. Hence, $2 - \\log_b (2b - 1) < 2 - 1 = 1$. d. The idea behind achieving $\\Theta(n / \\lg n)$ parallelism is similar to that presented in the previous part, except without recursive division. We will compute $A[1, 1]$ serially, which will enable us to compute entries $A[1, 2]$ and $A[2, 1]$ in parallel, after which we can compute entries $A[1, 3]$, $A[2, 2]$ and $A[3, 1]$ in parallel, and so on. Here is the pseudocode: 1 2 3 4 5 6 7 8 9 10 P - STENCIL ( A ) n = A . rows // Calculate all entries on the antidiagonal and above it. for i = 1 to n parallel for j = 1 to i A [ i - j + 1 , j ] = BASE - CASE ( A , i - j + 1 , j ) // Calculate all entries below the antidiagonal. for i = 2 to n parallel for j = i to n A [ n + i - j , j ] = BASE - CASE ( A , n + i - j , j ) For each value of index $i$ of the first serial for loop, the inner loop iterates $i$ times, doing constant work in each iteration. Because index $i$ ranges from $1$ to $n$ in the first for loop, we require $\\Theta(1 + 2 + \\cdots + n) = \\Theta(n^2)$ work to calculate all entries on the antidiagonal and above it. For each value of index $i$ of the second serial for loop, the inner loop iterates $n - i + 1$ times, doing constant work in each iteration. Because index $i$ ranges from $2$ to $n$ in the second for loop, we require $\\Theta((n - 1) + (n - 2) + \\cdots + 1) = \\Theta(n^2)$ work to calculate all entries on the antidiagonal and above it. Therefore, the work of $\\text{P-STENCIL}$ is $T_1(n) = \\Theta(n^2)$. Note that both for loops in $\\text{P-STENCIL}$, which execute parallel for loops within, are serial. Therefore, in order to calculate the span of $\\text{P-STENCIL}$, we must add the spans of all the parallel for loops. Given that any parallel for loop in $\\text{P-STENCIL}$ does constant work in each iteration, the span of a parallel for loop with $n'$ iterations is $\\Theta(\\lg n')$. Hence, $$ \\begin{aligned} T_\\infty(n) & = \\Theta((\\lg 1 + \\lg 2 + \\cdots + \\lg n) + (\\lg(n - 1) + \\cdots + 1)) \\\\ & = \\Theta(\\lg(n!) + \\lg(n - 1)!) \\\\ & = \\Theta(n\\lg n), \\end{aligned} $$ giving us $\\Theta(n / \\lg n)$ parallelism.","title":"27-5 Multithreading a simple stencil calculation"},{"location":"Chap27/Problems/27-6/","text":"Just as with ordinary serial algorithms, we sometimes want to implement randomized multithreaded algorithms. This problem explores how to adapt the various performance measures in order to handle the expected behavior of such algorithms. It also asks you to design and analyze a multithreaded algorithm for randomized quicksort. a. Explain how to modify the work law $\\text{(27.2)}$, span law $\\text{(27.3)}$, and greedy scheduler bound $\\text{(27.4)}$ to work with expectations when $T_P$, $T_1$, and $T_\\infty$ are all random variables. b. Consider a randomized multithreaded algorithm for which $1\\%$ of the time we have $T_1 = 10^4$ and $T_{10,000} = 1$, but for $99\\%$ of the time we have $T_1 = T_{10,000} = 10^9$. Argue that the speedup of a randomized multithreaded algorithm should be defined as $\\text E[T_1]/\\text E[T_P]$, rather than $\\text E[T_1 / T_P]$. c. Argue that the parallelism of a randomized multithreaded algorithm should be defined as the ratio $\\text E[T_1] / \\text E[T_\\infty]$. d. Multithread the $\\text{RANDOMIZED-QUICKSORT}$ algorithm on page 179 by using nested parallelism. (Do not parallelize $\\text{RANDOMIZED-PARTITION}$.) Give the pseudocode for your $\\text{P-RANDOMIZED-QUICKSORT}$ algorithm. e. Analyze your multithreaded algorithm for randomized quicksort. ($\\textit{Hint:}$ Review the analysis of $\\text{RANDOMIZED-SELECT}$ on page 216.) a. $$ \\begin{aligned} \\text E[T_P] & \\ge \\text E[T_1] / P \\\\ \\text E[T_P] & \\ge \\text E[T_\\infty] \\\\ \\text E[T_P] & \\le \\text E[T_1]/P + \\text E[T_\\infty]. \\end{aligned} $$ b. $$\\text E[T_1] \\approx \\text E[T_{10,000}] \\approx 9.9 \\times 10^8, \\text E[T_1]/\\text E[T_P] = 1.$$ $$\\text E[T_1 / T_{10,000}] = 10^4 * 0.01 + 0.99 = 100.99.$$ c. Same as the above. d. 1 2 3 4 5 6 RANDOMIZED - QUICKSORT ( A , p , r ) if p < r q = RANDOM - PARTITION ( A , p , r ) spawn RANDOMIZED - QUICKSORT ( A , p , q - 1 ) RANDOMIZED - QUICKSORT ( A , q + 1 , r ) sync e. $$ \\begin{aligned} \\text E[T_1] & = O(n\\lg n) \\\\ \\text E[T_\\infty] & = O(\\lg n) \\\\ \\text E[T_1] / \\text E[T_\\infty] & = O(n). \\end{aligned} $$","title":"27-6 Randomized multithreaded algorithms"},{"location":"Chap28/28.1/","text":"28.1-1 Solve the equation $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ -6 & 5 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 14 \\\\ -7 \\end{pmatrix} $$ by using forward substitution. $$ \\begin{pmatrix} 3 \\\\ 14 - 4 \\cdot 3 \\\\ -7 - 5 \\cdot (14 - 4 \\cdot 3) - (-6) \\cdot 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\end{pmatrix} . $$ 28.1-2 Find an $\\text{LU}$ decomposition of the matrix $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} . $$ $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 3 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & -5 & 6 \\\\ 0 & 4 & -5 \\\\ 0 & 0 & 4 \\end{pmatrix} . $$ 28.1-3 Solve the equation $$ \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} $$ by using forward substitution. We have $$ \\begin{aligned} A & = \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} , \\end{aligned} $$ and we with to solve for the unknown $x$. The $\\text{LUP}$ decomposition is $$ \\begin{aligned} L & = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} , \\\\ U & = \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} , \\\\ P & = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} . \\end{aligned} $$ Using forward substitution, we solve $Ly = Pb$ for $y$: $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 12 \\\\ 9 \\end{pmatrix} , $$ obtaining $$ y = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} $$ by computing first $y_1$, then $y_2$, and finally $y_3$. Using back substitution, we solve $Ux = y$ for $x$: $$ \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} , $$ thereby obtaining the desired answer $$ x = \\begin{pmatrix} -\\frac{3}{19} \\\\ -\\frac{1}{19} \\\\ \\frac{49}{19} \\end{pmatrix} $$ by computing first $x_3$, then $x_2$, and finally $x_1$. 28.1-4 Describe the $\\text{LUP}$ decomposition of a diagonal matrix. The $\\text{LUP}$ decomposition of a diagonal matrix $D$ is $D = IDI$ where $I$ is the identity matrix. 28.1-5 Describe the $\\text{LUP}$ decomposition of a permutation matrix $A$, and prove that it is unique. (Omit!) 28.1-6 Show that for all $n \\ge 1$, there exists a singular $n \\times n$ matrix that has an $\\text{LU}$ decomposition. The zero matrix always has an $\\text{LU}$ decomposition by taking $L$ to be any unit lower-triangular matrix and $U$ to be the zero matrix, which is upper triangular. 28.1-7 In $\\text{LU-DECOMPOSITION}$, is it necessary to perform the outermost for loop iteration when $k = n$? How about in $\\text{LUP-DECOMPOSITION}$? For $\\text{LU-DECOMPOSITION}$, it is indeed necessary. If we didn't run the line 6 of the outermost for loop, $u_{nn}$ would be left its initial value of $0$ instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the $\\text{LU-DECOMPOSITION}$ of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn} = 0$, the determinant of $U$ will be $0$ by problem D.2-2. For $\\text{LUP-DECOMPOSITION}$, the iteration of the outermost for loop that occurs with $k = n$ will not change the final answer. Since $\\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.","title":"28.1 Solving systems of linear equations"},{"location":"Chap28/28.1/#281-1","text":"Solve the equation $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ -6 & 5 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 14 \\\\ -7 \\end{pmatrix} $$ by using forward substitution. $$ \\begin{pmatrix} 3 \\\\ 14 - 4 \\cdot 3 \\\\ -7 - 5 \\cdot (14 - 4 \\cdot 3) - (-6) \\cdot 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\end{pmatrix} . $$","title":"28.1-1"},{"location":"Chap28/28.1/#281-2","text":"Find an $\\text{LU}$ decomposition of the matrix $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} . $$ $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 3 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & -5 & 6 \\\\ 0 & 4 & -5 \\\\ 0 & 0 & 4 \\end{pmatrix} . $$","title":"28.1-2"},{"location":"Chap28/28.1/#281-3","text":"Solve the equation $$ \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} $$ by using forward substitution. We have $$ \\begin{aligned} A & = \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} , \\end{aligned} $$ and we with to solve for the unknown $x$. The $\\text{LUP}$ decomposition is $$ \\begin{aligned} L & = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} , \\\\ U & = \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} , \\\\ P & = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} . \\end{aligned} $$ Using forward substitution, we solve $Ly = Pb$ for $y$: $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 12 \\\\ 9 \\end{pmatrix} , $$ obtaining $$ y = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} $$ by computing first $y_1$, then $y_2$, and finally $y_3$. Using back substitution, we solve $Ux = y$ for $x$: $$ \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} , $$ thereby obtaining the desired answer $$ x = \\begin{pmatrix} -\\frac{3}{19} \\\\ -\\frac{1}{19} \\\\ \\frac{49}{19} \\end{pmatrix} $$ by computing first $x_3$, then $x_2$, and finally $x_1$.","title":"28.1-3"},{"location":"Chap28/28.1/#281-4","text":"Describe the $\\text{LUP}$ decomposition of a diagonal matrix. The $\\text{LUP}$ decomposition of a diagonal matrix $D$ is $D = IDI$ where $I$ is the identity matrix.","title":"28.1-4"},{"location":"Chap28/28.1/#281-5","text":"Describe the $\\text{LUP}$ decomposition of a permutation matrix $A$, and prove that it is unique. (Omit!)","title":"28.1-5"},{"location":"Chap28/28.1/#281-6","text":"Show that for all $n \\ge 1$, there exists a singular $n \\times n$ matrix that has an $\\text{LU}$ decomposition. The zero matrix always has an $\\text{LU}$ decomposition by taking $L$ to be any unit lower-triangular matrix and $U$ to be the zero matrix, which is upper triangular.","title":"28.1-6"},{"location":"Chap28/28.1/#281-7","text":"In $\\text{LU-DECOMPOSITION}$, is it necessary to perform the outermost for loop iteration when $k = n$? How about in $\\text{LUP-DECOMPOSITION}$? For $\\text{LU-DECOMPOSITION}$, it is indeed necessary. If we didn't run the line 6 of the outermost for loop, $u_{nn}$ would be left its initial value of $0$ instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the $\\text{LU-DECOMPOSITION}$ of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn} = 0$, the determinant of $U$ will be $0$ by problem D.2-2. For $\\text{LUP-DECOMPOSITION}$, the iteration of the outermost for loop that occurs with $k = n$ will not change the final answer. Since $\\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.","title":"28.1-7"},{"location":"Chap28/28.2/","text":"28.2-1 Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $S(n)$ denote the time required to square an $n \\times n$ matrix. Show that multiplying and squaring matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time squaring algorithm, and an $S(n)$-time squaring algorithm implies an $O(S(n))$-time matrix-multiplication algorithm. Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself. The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. As we do this, we apply the same regularity condition that $S(2n) \\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix $$ C = \\begin{pmatrix} I & A \\\\ 0 & B \\end{pmatrix} $$ Then, we can find $C^2$ in time $S(2n) \\in O(S(n))$. Since $$ C^2 = \\begin{pmatrix} I & A + AB \\\\ 0 & B \\end{pmatrix} $$ Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$. 28.2-2 Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $L(n)$ be the time to compute the LUP decomposition of an $n \\times n$ matrix. Show that multiplying matrices and computing LUP decompositions of matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time LUP-decomposition algorithm, and an $L(n)$-time LUP-decomposition algorithm implies an $O(L(n))$-time matrix-multiplication algorithm. Let $A$ be an $n \\times n$ matrix. Without loss of generality we'll assume $n = 2^k$, and impose the regularity condition that $L(n / 2) \\le cL(n)$ where $c < 1 / 2$ and $L(n)$ is the time it takes to find an LUP decomposition of an $n \\times n$ matrix. First, decompose $A$ as $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} $$ where $A_1$ is $n / 2$ by $n$. Let $A_1 = L_1U_1P_1$ be an LUP decomposition of $A_1$, where $L_1$ is $ / 2$ by $n / 2$, $U_1$ is $n / 2$ by $n$, and $P_1$ is $n$ by $n$. Perform a block decomposition of $U_1$ and $A_2P_1^{-1}$ as $U_1 = [\\overline U_1|B]$ and $A_2P_1^{-1} = [C|D]$ where $\\overline U_1$ and $C$ are $n / 2$ by $n / 2$ matrices. Since we assume that $A$ is nonsingular, $\\overline U_1$ must also be nonsingular. Set $F = D - C\\overline U_1^{-1}B$. Then we have $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & I_{n / 2} \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & B \\\\ 0 & F \\end{pmatrix} P_1. $$ Now let $F = L_2U_2P_2$ be an LUP decomposition of $F$, and let $\\overline P = \\begin{pmatrix} I_{n / 2} & 0 \\\\ 0 & P_2 \\end{pmatrix}$. Then we may write $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & L_2 \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & BP_2^{-1} \\\\ 0 & U_2 \\end{pmatrix} \\overline P P_1. $$ This is an LUP decomposition of $A$. To achieve it, we computed two LUP decompositions of half size, a constant number of matrix multiplications, and a constant number of matrix inversions. Since matrix inversion and multiplication are computationally equivalent, we conclude that the runtime is $O(M(n))$. 28.2-3 Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $D(n)$ denote the time required to find the determinant of an $n \\times n$ matrix. Show that multiplying matrices and computing the determinant have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time determinant algorithm, and a $D(n)$-time determinant algorithm implies an $O(D(n))$-time matrix-multiplication algorithm. (Omit!) 28.2-4 Let $M(n)$ be the time to multiply two $n \\times n$ boolean matrices, and let $T(n)$ be the time to find the transitive closure of an $n \\times n$ boolean matrix. (See Section 25.2.) Show that an $M(n)$-time boolean matrix-multiplication algorithm implies an $O(M(n)\\lg n)$-time transitive-closure algorithm, and a $T(n)$-time transitive-closure algorithm implies an $O(T(n))$-time boolean matrix-multiplication algorithm. Suppose we can multiply boolean matrices in $M(n)$ time, where we assume this means that if we're multiplying boolean matrices $A$ and $B$, then $(AB)_{ij} = (a_{i1} \\wedge b_{1j}) \\vee \\dots \\vee (a_{in} \\wedge b_{nj})$. To find the transitive closure of a boolean matrix $A$ we just need to find the $n^{\\text{th}}$ power of $A$. We can do this by computing $A^2$, then $(A^2)^2$, then $((A^2)^2)^2$ and so on. This requires only $\\lg n$ multiplications, so the transitive closure can be computed in $O(M(n)\\lg n)$. For the other direction, first view $A$ and $B$ as adjacency matrices, and impose the regularity condition $T(3n) = O(T(n))$, where $T(n)$ is the time to compute the transitive closure of a graph on $n$ vertices. We will define a new graph whose transitive closure matrix contains the boolean product of $A$ and $B$. Start by placing $3n$ vertices down, labeling them $1, 2, \\dots, n, 1', 2', \\dots, n', 1'', 2'', \\dots, n''$. Connect vertex $i$ to vertex $j'$ if and only if $A_{ij} = 1$. Connect vertex $j'$ to vertex $k''$ if and only if $B_{jk} = 1$. In the resulting graph, the only way to get from the first set of $n$ vertices to the third set is to first take an edge which \"looks like\" an edge in $A$, then take an edge which \"looks like\" an edge in $B$. In particular, the transitive closure of this graph is: $$ \\begin{pmatrix} I & A & AB \\\\ 0 & I & B \\\\ 0 & 0 & I \\end{pmatrix} . $$ Since the graph is only of size $3n$, computing its transitive closure can be done in $O(T(3n)) = O(T(n))$ by the regularity condition. Therefore multiplying matrices and finding transitive closure are are equally hard. 28.2-5 Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix elements are drawn from the field of integers modulo $2$? Explain. It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^{\\text T}A$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \\ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^{\\text T}A$ is positive semi-definite instead of the positive definiteness that the algorithm requires. 28.2-6 $\\star$ Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices of complex numbers, and prove that your generalization works correctly. ($\\textit{Hint:}$ Instead of the transpose of $A$, use the conjugate transpose $A^*$, which you obtain from the transpose of $A$ by replacing every entry with its complex conjugate. Instead of symmetric matrices, consider Hermitian matrices, which are matrices $A$ such that $A = A^*$.) We may again assume that our matrix is a power of $2$, this time with complex entries. For the moment we assume our matrix $A$ is Hermitian and positivedefinite. The proof goes through exactly as before, with matrix transposes replaced by conjugate transposes, and using the fact that Hermitian positivedefinite matrices are invertible. Finally, we need to justify that we can obtain the same asymptotic running time for matrix multiplication as for matrix inversion when $A$ is invertible, but not Hermitian positive-definite. For any nonsingular matrix $A$, the matrix $A^*A$ is Hermitian and positive definite, since for any $x$ we have $x^*A^*Ax = \\langle Ax, Ax \\rangle > 0$ by the definition of inner product. To invert $A$, we first compute $(A^*A)^{-1} = A^{-1}(A^*)^{-1}$. Then we need only multiply this result on the right by $A^*$. Each of these steps takes $O(M(n))$ time, so we can invert any nonsingluar matrix with complex entries in $O(M(n))$ time.","title":"28.2 Inverting matrices"},{"location":"Chap28/28.2/#282-1","text":"Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $S(n)$ denote the time required to square an $n \\times n$ matrix. Show that multiplying and squaring matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time squaring algorithm, and an $S(n)$-time squaring algorithm implies an $O(S(n))$-time matrix-multiplication algorithm. Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself. The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. As we do this, we apply the same regularity condition that $S(2n) \\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix $$ C = \\begin{pmatrix} I & A \\\\ 0 & B \\end{pmatrix} $$ Then, we can find $C^2$ in time $S(2n) \\in O(S(n))$. Since $$ C^2 = \\begin{pmatrix} I & A + AB \\\\ 0 & B \\end{pmatrix} $$ Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$.","title":"28.2-1"},{"location":"Chap28/28.2/#282-2","text":"Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $L(n)$ be the time to compute the LUP decomposition of an $n \\times n$ matrix. Show that multiplying matrices and computing LUP decompositions of matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time LUP-decomposition algorithm, and an $L(n)$-time LUP-decomposition algorithm implies an $O(L(n))$-time matrix-multiplication algorithm. Let $A$ be an $n \\times n$ matrix. Without loss of generality we'll assume $n = 2^k$, and impose the regularity condition that $L(n / 2) \\le cL(n)$ where $c < 1 / 2$ and $L(n)$ is the time it takes to find an LUP decomposition of an $n \\times n$ matrix. First, decompose $A$ as $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} $$ where $A_1$ is $n / 2$ by $n$. Let $A_1 = L_1U_1P_1$ be an LUP decomposition of $A_1$, where $L_1$ is $ / 2$ by $n / 2$, $U_1$ is $n / 2$ by $n$, and $P_1$ is $n$ by $n$. Perform a block decomposition of $U_1$ and $A_2P_1^{-1}$ as $U_1 = [\\overline U_1|B]$ and $A_2P_1^{-1} = [C|D]$ where $\\overline U_1$ and $C$ are $n / 2$ by $n / 2$ matrices. Since we assume that $A$ is nonsingular, $\\overline U_1$ must also be nonsingular. Set $F = D - C\\overline U_1^{-1}B$. Then we have $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & I_{n / 2} \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & B \\\\ 0 & F \\end{pmatrix} P_1. $$ Now let $F = L_2U_2P_2$ be an LUP decomposition of $F$, and let $\\overline P = \\begin{pmatrix} I_{n / 2} & 0 \\\\ 0 & P_2 \\end{pmatrix}$. Then we may write $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & L_2 \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & BP_2^{-1} \\\\ 0 & U_2 \\end{pmatrix} \\overline P P_1. $$ This is an LUP decomposition of $A$. To achieve it, we computed two LUP decompositions of half size, a constant number of matrix multiplications, and a constant number of matrix inversions. Since matrix inversion and multiplication are computationally equivalent, we conclude that the runtime is $O(M(n))$.","title":"28.2-2"},{"location":"Chap28/28.2/#282-3","text":"Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $D(n)$ denote the time required to find the determinant of an $n \\times n$ matrix. Show that multiplying matrices and computing the determinant have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time determinant algorithm, and a $D(n)$-time determinant algorithm implies an $O(D(n))$-time matrix-multiplication algorithm. (Omit!)","title":"28.2-3"},{"location":"Chap28/28.2/#282-4","text":"Let $M(n)$ be the time to multiply two $n \\times n$ boolean matrices, and let $T(n)$ be the time to find the transitive closure of an $n \\times n$ boolean matrix. (See Section 25.2.) Show that an $M(n)$-time boolean matrix-multiplication algorithm implies an $O(M(n)\\lg n)$-time transitive-closure algorithm, and a $T(n)$-time transitive-closure algorithm implies an $O(T(n))$-time boolean matrix-multiplication algorithm. Suppose we can multiply boolean matrices in $M(n)$ time, where we assume this means that if we're multiplying boolean matrices $A$ and $B$, then $(AB)_{ij} = (a_{i1} \\wedge b_{1j}) \\vee \\dots \\vee (a_{in} \\wedge b_{nj})$. To find the transitive closure of a boolean matrix $A$ we just need to find the $n^{\\text{th}}$ power of $A$. We can do this by computing $A^2$, then $(A^2)^2$, then $((A^2)^2)^2$ and so on. This requires only $\\lg n$ multiplications, so the transitive closure can be computed in $O(M(n)\\lg n)$. For the other direction, first view $A$ and $B$ as adjacency matrices, and impose the regularity condition $T(3n) = O(T(n))$, where $T(n)$ is the time to compute the transitive closure of a graph on $n$ vertices. We will define a new graph whose transitive closure matrix contains the boolean product of $A$ and $B$. Start by placing $3n$ vertices down, labeling them $1, 2, \\dots, n, 1', 2', \\dots, n', 1'', 2'', \\dots, n''$. Connect vertex $i$ to vertex $j'$ if and only if $A_{ij} = 1$. Connect vertex $j'$ to vertex $k''$ if and only if $B_{jk} = 1$. In the resulting graph, the only way to get from the first set of $n$ vertices to the third set is to first take an edge which \"looks like\" an edge in $A$, then take an edge which \"looks like\" an edge in $B$. In particular, the transitive closure of this graph is: $$ \\begin{pmatrix} I & A & AB \\\\ 0 & I & B \\\\ 0 & 0 & I \\end{pmatrix} . $$ Since the graph is only of size $3n$, computing its transitive closure can be done in $O(T(3n)) = O(T(n))$ by the regularity condition. Therefore multiplying matrices and finding transitive closure are are equally hard.","title":"28.2-4"},{"location":"Chap28/28.2/#282-5","text":"Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix elements are drawn from the field of integers modulo $2$? Explain. It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^{\\text T}A$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \\ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^{\\text T}A$ is positive semi-definite instead of the positive definiteness that the algorithm requires.","title":"28.2-5"},{"location":"Chap28/28.2/#282-6-star","text":"Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices of complex numbers, and prove that your generalization works correctly. ($\\textit{Hint:}$ Instead of the transpose of $A$, use the conjugate transpose $A^*$, which you obtain from the transpose of $A$ by replacing every entry with its complex conjugate. Instead of symmetric matrices, consider Hermitian matrices, which are matrices $A$ such that $A = A^*$.) We may again assume that our matrix is a power of $2$, this time with complex entries. For the moment we assume our matrix $A$ is Hermitian and positivedefinite. The proof goes through exactly as before, with matrix transposes replaced by conjugate transposes, and using the fact that Hermitian positivedefinite matrices are invertible. Finally, we need to justify that we can obtain the same asymptotic running time for matrix multiplication as for matrix inversion when $A$ is invertible, but not Hermitian positive-definite. For any nonsingular matrix $A$, the matrix $A^*A$ is Hermitian and positive definite, since for any $x$ we have $x^*A^*Ax = \\langle Ax, Ax \\rangle > 0$ by the definition of inner product. To invert $A$, we first compute $(A^*A)^{-1} = A^{-1}(A^*)^{-1}$. Then we need only multiply this result on the right by $A^*$. Each of these steps takes $O(M(n))$ time, so we can invert any nonsingluar matrix with complex entries in $O(M(n))$ time.","title":"28.2-6 $\\star$"},{"location":"Chap28/28.3/","text":"28.3-1 Prove that every diagonal element of a symmetric positive-definite matrix is positive. To see this, let $e_i$ be the vector that is $0$s except for a $1$ in the $i$th position. Then, we consider the quantity $e_i^{\\text T}Ae_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the $i$th column of it, and puts those values into a column vector. Then, we multiply that on the left by $e_i^{\\text T}$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^{\\text T}Ae_i$ exactly the value of $A_{i, i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every $i$, we have that every entry along the diagonal must be positive. 28.3-2 Let $$ A = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} $$ be a $2 \\times 2$ symmetrix positive-definite matrix. Prove that its determinant $ac - b^2$ is positive by \"completing the square\" in a manner similar to that used in the proof of Lemma 28.5. Let $x = -by / a$. Since $A$ is positive-definite, we have $$ \\begin{aligned} 0 & < \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} A \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\ & = \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} \\begin{pmatrix} ax + by \\\\ bx + cy \\end{pmatrix} \\\\ & = ax^2 + 2bxy + cy^2 \\\\ & = cy^2 - \\frac{b^2y^2}{a} \\\\ & = (c - b^2 / a)y^2. \\end{aligned} $$ Thus, $c - b^2 / a > 0$, which implies $ac - b^2 > 0$, since $a > 0$. 28.3-3 Prove that the maximum element in a symmetric positive-definite matrix lies on the diagonal. Suppose to a contradiction that there were some element $a_{ij}$ with $i \\ne j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a $1$ at position $i$. Then, we consider the value $(e_i \u2212 e_j)^{\\text T} A(e_i \u2212 e_j)$. When we compute $A(e_i - e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, $$ \\begin{aligned} (e_i - e_j)^{\\text T} A(e_i - e_j) & = a_{ii} - a_{ij} - a_{ji} + a_{jj} \\\\ & = a_{ii} + a_{jj} - 2a_{ij} \\le 0 \\end{aligned} $$ Where we used symmetry to get that $a_{ij} = a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false. 28.3-4 Prove that the determinant of each leading submatrix of a symmetrix positive-definite matrix is positive. The claim clearly holds for matrices of size $1$ because the single entry in the matrix is positive the only leading submatrix is the matrix itself. Now suppose the claim holds for matrices of size $n$, and let $A$ be an $(n + 1) \\times (n + 1)$ symmetric positive-definite matrix. We can write $A$ as $$ A = \\left[ \\begin{array}{ccc|c} & & & \\\\ & A' & & w \\\\ & & & \\\\ \\hline & v & & c \\end{array} \\right] . $$ Then $A'$ is clearly symmetric, and for any $x$ we have $x^{\\text T} A'x = \\begin{pmatrix} x & 0 \\end{pmatrix} A \\begin{pmatrix} x \\\\ 0 \\end{pmatrix} > 0$, so $A'$ is positive-definite. By our induction hypothesis, every leading submatrix of $A'$ has positive determinant, so we are left only to show that $A$ has positive determinant. By Theorem D.4, the determinant of $A$ is equal to the determinant of the matrix $$ B = \\left[ \\begin{array}{c|ccc} c & & v & \\\\ \\hline & & & \\\\ w & & A' & \\\\ & & & \\end{array} \\right] . $$ Theorem D.4 also tells us that the determinant is unchanged if we add a multiple of one column of a matrix to another. Since $0 < e_{n + 1}^{\\text T} Ae_{n + 1} = c$, we can use multiples of the first column to zero out every entry in the first row other than $c$. Specifically, the determinant of $B$ is the same as the determinant of the matrix obtained in this way, which looks like $$ C = \\left[ \\begin{array}{c|ccc} c & & 0 & \\\\ \\hline & & & \\\\ w & & A'' & \\\\ & & & \\end{array} \\right] . $$ By definition, $\\det(A) = c\\det(A'')$. By our induction hypothesis, $\\det(A'') > 0$. Since $c > 0$ as well, we conclude that $\\det(A) > 0$, which completes the proof. 28.3-5 Let $A_k$ denote the $k$th leading submatrix of a symmetric positive-definite matrix $A$. Prove that $\\text{det}(A_k) / \\text{det}(A_{k - 1})$ is the $k$th pivot during $\\text{LU}$ decomposition, where, by convention, $\\text{det}(A_0) = 1$. When we do an LU decomposition of a positive definite symmetric matrix, we never need to permute the rows. This means that the pivot value being used from the first operation is the entry in the upper left corner. This gets us that for the case $k = 1$, it holds because we were told to define $\\det(A_0) = 1$, getting us, $a_{11} = \\det(A_1) / \\det(A_0)$. When Diagonalizing a matrix, the product of the pivot values used gives the determinant of the matrix. So, we have that the determinant of $A_k$ is a product of the $k$th pivot value with all the previous values. By induction, the product of all the previous values is $\\det(A_{k \u2212 1})$. So, we have that if $x$ is the $k$th pivot value, $\\det(A_k) = x\\det(A_{k \u2212 1})$, giving us the desired result that the $k$th pivot value is $\\det(A_k) / \\det(A_{k \u2212 1})$. 28.3-6 Find the function of the form $$F(x) = c_1 + c_2x\\lg x + c_3 e^x$$ that is the best least-squares fit to the data points $$(1, 1), (2, 1), (3, 3), (4, 8).$$ First we form the $A$ matrix $$ A = \\begin{pmatrix} 1 & 0 & e \\\\ 1 & 2 & e^2 \\\\ 1 & 3\\lg 3 & e^3 \\\\ 1 & 8 & e^4 \\end{pmatrix} . $$ We compute the pseudoinverse, then multiply it by $y$, to obtain the coefficient vector $$ c = \\begin{pmatrix} 0.411741 \\\\ -0.20487 \\\\ 0.16546 \\end{pmatrix} . $$ 28.3-7 Show that the pseudoinverse $A^+$ satisfies the following four equations: $$ \\begin{aligned} AA^+A & = A, \\\\ A^+AA^+ & = A^+, \\\\ (AA^+)^{\\text T} & = AA^+, \\\\ (A^+A)^{\\text T} & = A^+A. \\end{aligned} $$ $$ \\begin{aligned} AA^+A & = A((A^{\\text T}A)^{-1}A^{\\text T})A \\\\ & = A(A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A, \\end{aligned} $$ $$ \\begin{aligned} A^+AA^+ & = ((A^{\\text T}A)^{-1}A^{\\text T})A((A^{\\text T}A)^{-1}A^{\\text T}) \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A)(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = (A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = A^+, \\end{aligned} $$ $$ \\begin{aligned} (AA^+)^{\\text T} & = (A(A^{\\text T}A)^{-1}A^{\\text T})^{\\text T} \\\\ & = A((A^{\\text T}A)^{-1})^{\\text T}A^{\\text T} \\\\ & = A((A^{\\text T}A)^{\\text T})^{-1}A^{\\text T} \\\\ & = A(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = AA^+, \\end{aligned} $$ $$ \\begin{aligned} (A^+A)^{\\text T} & = ((A^{\\text T}A)^{-1}A^{\\text T}A)^{\\text T} \\\\ & = ((A^{\\text T}A)^{-1}(A^{\\text T}A))^{\\text T} \\\\ & = I^{\\text T} \\\\ & = I \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A^+A. \\end{aligned} $$","title":"28.3 Symmetric positive-definite matrices and least-squares approximation"},{"location":"Chap28/28.3/#283-1","text":"Prove that every diagonal element of a symmetric positive-definite matrix is positive. To see this, let $e_i$ be the vector that is $0$s except for a $1$ in the $i$th position. Then, we consider the quantity $e_i^{\\text T}Ae_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the $i$th column of it, and puts those values into a column vector. Then, we multiply that on the left by $e_i^{\\text T}$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^{\\text T}Ae_i$ exactly the value of $A_{i, i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every $i$, we have that every entry along the diagonal must be positive.","title":"28.3-1"},{"location":"Chap28/28.3/#283-2","text":"Let $$ A = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} $$ be a $2 \\times 2$ symmetrix positive-definite matrix. Prove that its determinant $ac - b^2$ is positive by \"completing the square\" in a manner similar to that used in the proof of Lemma 28.5. Let $x = -by / a$. Since $A$ is positive-definite, we have $$ \\begin{aligned} 0 & < \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} A \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\ & = \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} \\begin{pmatrix} ax + by \\\\ bx + cy \\end{pmatrix} \\\\ & = ax^2 + 2bxy + cy^2 \\\\ & = cy^2 - \\frac{b^2y^2}{a} \\\\ & = (c - b^2 / a)y^2. \\end{aligned} $$ Thus, $c - b^2 / a > 0$, which implies $ac - b^2 > 0$, since $a > 0$.","title":"28.3-2"},{"location":"Chap28/28.3/#283-3","text":"Prove that the maximum element in a symmetric positive-definite matrix lies on the diagonal. Suppose to a contradiction that there were some element $a_{ij}$ with $i \\ne j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a $1$ at position $i$. Then, we consider the value $(e_i \u2212 e_j)^{\\text T} A(e_i \u2212 e_j)$. When we compute $A(e_i - e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, $$ \\begin{aligned} (e_i - e_j)^{\\text T} A(e_i - e_j) & = a_{ii} - a_{ij} - a_{ji} + a_{jj} \\\\ & = a_{ii} + a_{jj} - 2a_{ij} \\le 0 \\end{aligned} $$ Where we used symmetry to get that $a_{ij} = a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false.","title":"28.3-3"},{"location":"Chap28/28.3/#283-4","text":"Prove that the determinant of each leading submatrix of a symmetrix positive-definite matrix is positive. The claim clearly holds for matrices of size $1$ because the single entry in the matrix is positive the only leading submatrix is the matrix itself. Now suppose the claim holds for matrices of size $n$, and let $A$ be an $(n + 1) \\times (n + 1)$ symmetric positive-definite matrix. We can write $A$ as $$ A = \\left[ \\begin{array}{ccc|c} & & & \\\\ & A' & & w \\\\ & & & \\\\ \\hline & v & & c \\end{array} \\right] . $$ Then $A'$ is clearly symmetric, and for any $x$ we have $x^{\\text T} A'x = \\begin{pmatrix} x & 0 \\end{pmatrix} A \\begin{pmatrix} x \\\\ 0 \\end{pmatrix} > 0$, so $A'$ is positive-definite. By our induction hypothesis, every leading submatrix of $A'$ has positive determinant, so we are left only to show that $A$ has positive determinant. By Theorem D.4, the determinant of $A$ is equal to the determinant of the matrix $$ B = \\left[ \\begin{array}{c|ccc} c & & v & \\\\ \\hline & & & \\\\ w & & A' & \\\\ & & & \\end{array} \\right] . $$ Theorem D.4 also tells us that the determinant is unchanged if we add a multiple of one column of a matrix to another. Since $0 < e_{n + 1}^{\\text T} Ae_{n + 1} = c$, we can use multiples of the first column to zero out every entry in the first row other than $c$. Specifically, the determinant of $B$ is the same as the determinant of the matrix obtained in this way, which looks like $$ C = \\left[ \\begin{array}{c|ccc} c & & 0 & \\\\ \\hline & & & \\\\ w & & A'' & \\\\ & & & \\end{array} \\right] . $$ By definition, $\\det(A) = c\\det(A'')$. By our induction hypothesis, $\\det(A'') > 0$. Since $c > 0$ as well, we conclude that $\\det(A) > 0$, which completes the proof.","title":"28.3-4"},{"location":"Chap28/28.3/#283-5","text":"Let $A_k$ denote the $k$th leading submatrix of a symmetric positive-definite matrix $A$. Prove that $\\text{det}(A_k) / \\text{det}(A_{k - 1})$ is the $k$th pivot during $\\text{LU}$ decomposition, where, by convention, $\\text{det}(A_0) = 1$. When we do an LU decomposition of a positive definite symmetric matrix, we never need to permute the rows. This means that the pivot value being used from the first operation is the entry in the upper left corner. This gets us that for the case $k = 1$, it holds because we were told to define $\\det(A_0) = 1$, getting us, $a_{11} = \\det(A_1) / \\det(A_0)$. When Diagonalizing a matrix, the product of the pivot values used gives the determinant of the matrix. So, we have that the determinant of $A_k$ is a product of the $k$th pivot value with all the previous values. By induction, the product of all the previous values is $\\det(A_{k \u2212 1})$. So, we have that if $x$ is the $k$th pivot value, $\\det(A_k) = x\\det(A_{k \u2212 1})$, giving us the desired result that the $k$th pivot value is $\\det(A_k) / \\det(A_{k \u2212 1})$.","title":"28.3-5"},{"location":"Chap28/28.3/#283-6","text":"Find the function of the form $$F(x) = c_1 + c_2x\\lg x + c_3 e^x$$ that is the best least-squares fit to the data points $$(1, 1), (2, 1), (3, 3), (4, 8).$$ First we form the $A$ matrix $$ A = \\begin{pmatrix} 1 & 0 & e \\\\ 1 & 2 & e^2 \\\\ 1 & 3\\lg 3 & e^3 \\\\ 1 & 8 & e^4 \\end{pmatrix} . $$ We compute the pseudoinverse, then multiply it by $y$, to obtain the coefficient vector $$ c = \\begin{pmatrix} 0.411741 \\\\ -0.20487 \\\\ 0.16546 \\end{pmatrix} . $$","title":"28.3-6"},{"location":"Chap28/28.3/#283-7","text":"Show that the pseudoinverse $A^+$ satisfies the following four equations: $$ \\begin{aligned} AA^+A & = A, \\\\ A^+AA^+ & = A^+, \\\\ (AA^+)^{\\text T} & = AA^+, \\\\ (A^+A)^{\\text T} & = A^+A. \\end{aligned} $$ $$ \\begin{aligned} AA^+A & = A((A^{\\text T}A)^{-1}A^{\\text T})A \\\\ & = A(A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A, \\end{aligned} $$ $$ \\begin{aligned} A^+AA^+ & = ((A^{\\text T}A)^{-1}A^{\\text T})A((A^{\\text T}A)^{-1}A^{\\text T}) \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A)(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = (A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = A^+, \\end{aligned} $$ $$ \\begin{aligned} (AA^+)^{\\text T} & = (A(A^{\\text T}A)^{-1}A^{\\text T})^{\\text T} \\\\ & = A((A^{\\text T}A)^{-1})^{\\text T}A^{\\text T} \\\\ & = A((A^{\\text T}A)^{\\text T})^{-1}A^{\\text T} \\\\ & = A(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = AA^+, \\end{aligned} $$ $$ \\begin{aligned} (A^+A)^{\\text T} & = ((A^{\\text T}A)^{-1}A^{\\text T}A)^{\\text T} \\\\ & = ((A^{\\text T}A)^{-1}(A^{\\text T}A))^{\\text T} \\\\ & = I^{\\text T} \\\\ & = I \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A^+A. \\end{aligned} $$","title":"28.3-7"},{"location":"Chap28/Problems/28-1/","text":"Consider the tridiagonal matrix $$ A = \\begin{pmatrix} 1 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} . $$ a. Find an $\\text{LU}$ decomposition of $A$. b. Solve the equation $Ax = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\end{pmatrix}^{\\text T}$ by using forward and back substitution. c. Find the inverse of $A$. d. Show how, for any $n \\times n$ symmetric positive-definite, tridiagonal matrix $A$ and any $n$-vector $b$, to solve the equation $Ax = b$ in $O(n)$ time by performing an $\\text{LU}$ decomposition. Argue that any method based on forming $A^{-1}$ is asymptotically more expensive in the worst case. e. Show how, for any $n \\times n$ nonsingular, tridiagonal matrix $A$ and any $n$-vector $b$, to solve the equation $Ax = b$ in $O(n)$ time by performing an $\\text{LUP}$ decomposition. a. $$ \\begin{aligned} L & = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ -1 & 1 & 0 & 0 & 0 \\\\ 0 & -1 & 1 & 0 & 0 \\\\ 0 & 0 & -1 & 1 & 0 \\\\ 0 & 0 & 0 & -1 & 1 \\end{pmatrix} , \\\\ U & = \\begin{pmatrix} 1 & -1 & 0 & 0 & 0 \\\\ 0 & 1 & -1 & 0 & 0 \\\\ 0 & 0 & 1 & -1 & 0 \\\\ 0 & 0 & 0 & 1 & -1 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} , \\\\ P & = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} . \\end{aligned} $$ b. We first do back substitution to obtain that $$ Ux = \\begin{pmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{pmatrix} . $$ By forward substitution, we have that $$ x = \\begin{pmatrix} 5 \\\\ 9 \\\\ 12 \\\\ 14 \\\\ 15 \\end{pmatrix} . $$ c. We will set $Ax = e_i$ for each $i$, where $e_i$ is the vector that is all zeroes except for a one in the $i$th position. Then, we will just concatenate all of these solutions together to get the desired inverse. $$ \\begin{array}{|c|c|} \\hline \\text{equation} & \\text{solution} \\\\ \\hline Ax_1 = e_1 & x_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\\\ \\hline Ax_2 = e_2 & x_2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 2 \\end{pmatrix} \\\\ \\hline Ax_3 = e_3 & x_3 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 3 \\\\ 3 \\end{pmatrix} \\\\ \\hline Ax_4 = e_4 & x_4 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 4 \\end{pmatrix} \\\\ \\hline Ax_5 = e_5 & x_5 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} \\\\ \\hline \\end{array} $$ Thus, $$ A^{-1} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 2 & 2 & 2 & 2 \\\\ 1 & 2 & 3 & 3 & 3 \\\\ 1 & 2 & 3 & 4 & 4 \\\\ 1 & 2 & 3 & 4 & 5 \\end{pmatrix} . $$ d. When performing the LU decomposition, we only need to take the max over at most two different rows, so the loop on line 7 of $\\text{LUP-DECOMPOSITION}$ drops to $O(1)$. There are only some constant number of nonzero entries in each row, so the loop on line 14 can also be reduced to being $O(1)$. Lastly, there are only some constant number of nonzero entries of the form $a_{ik}$ and $a_{kj}$. Since the square of a constant is also a constant, this means that the nested for loops on lines 16-19 also only take time $O(1)$ to run. Since the for loops on lines 3 and 5 both run $O(n)$ times and take $O(1)$ time each to run (provided we are smart to not consider a bunch of zero entries in the matrix), the total runtime can be brought down to $O(n)$. Since for a Tridiagonal matrix, it will only ever have finitely many nonzero entries in any row, we can do both the forward and back substitution each in time only $O(n)$. Since the asymptotics of performing the LU decomposition on a positive definite tridiagonal matrix is $O(n)$, and this decomposition can be used to solve the equation in time $O(n)$, the total time for solving it with this method is $O(n)$. However, to simply record the inverse of the tridiagonal matrix would take time $O(n^2)$ since there are that many entries, so, any method based on computing the inverse of the matrix would take time $\\Omega(n^2)$ which is clearly slower than the previous method. e. The runtime of our LUP decomposition algorithm drops to being $O(n)$ because we know there are only ever a constant number of nonzero entries in each row and column, as before. Once we have an LUP decomposition, we also know that that decomposition have both $L$ and $U$ having only a constant number of non-zero entries in each row and column. This means that when we perform the forward and backward substitution, we only spend a constant amount of time per entry in $x$, and so, only takes $O(n)$ time.","title":"28-1 Tridiagonal systems of linear equations"},{"location":"Chap28/Problems/28-2/","text":"A pratical method for interpolating a set of points with a curve is to use cubic splines . We are given a set $\\{(x_i, y_i): i = 0, 1, \\ldots, n\\}$ of $n + 1$ point-value pairs, where $x_0 < x_1 < \\cdots < x_n$. We wish to fit a piecewise-cubic curve (spline) $f(x)$ to the points. That is, the curve $f(x)$ is made up of $n$ cubic polynomials $f_i(x) = a_i + b_ix + c_ix^2 + d_ix^3$ for $i = 0, 1, \\ldots, n - 1$, where if $x$ falls in the range $x_i \\le x \\le x_{i + 1}$, then the value of the curve is given by $f(x) = f_i(x - x_i)$. The points $x_i$ at which the cubic polynomials are \"pasted\" together are called knots . For simplicity, we shall assume that $x_i = i$ for $i = 0, 1, \\ldots, n$. To ensure continuity of $f(x)$, we require that $$ \\begin{aligned} f(x_i) & = f_i(0) = y_i, \\\\ f(x_{i + 1}) & = f_i(1) = y_{i + 1} \\end{aligned} $$ for $i = 0, 1, \\ldots, n - 1$. To ensure that $f(x)$ is sufficiently smooth, we also insist that the first derivative be continuous at each knot: $$f'(x_{i + 1}) = f'_i(1) = f'_{i + 1}(0)$$ for $i = 0, 1, \\ldots, n - 2$. a. Suppose that for $i = 0, 1, \\ldots, n$, we are given not only the point-value pairs $\\{(x_i, y_i)\\}$ but also the first derivatives $D_i = f'(x_i)$ at each knot. Express each coefficient $a_i$, $b_i$, $c_i$ and $d_i$ in terms of the values $y_i$, $y_{i + 1}$, $D_i$, and $D_{i + 1}$. (Remember that $x_i = i$.) How quickly can we compute the $4n$ coefficients from the point-value pairs and first derivatives? The question remains of how to choose the first derivatives of $f(x)$ at the knots. One method is to require the second derivatives to be continuous at the knots: $$f''(x_{i + 1}) = f''_i(1) = f''_{i + 1}(0)$$ for $i = 0, 1, \\ldots, n - 2$. At the first and last knots, we assume that $f''(x_0) = f''_0(0) = 0$ and $f''(x_n) = f''_{n - 1}(1) = 0$; these assumptions make $f(x)$ a natural cubic spline. b. Use the continuity constraints on the second derivative to show that for $i = 1, 2, \\ldots, n - 1$, $$D_{i - 1} + 4D_i + D_{i + 1} = 3(y_{i + 1} - y_{i - 1}). \\tag{23.21}$$ c. Show that $$ \\begin{aligned} 2D_0 + D_1 & = 3(y_1 - y_0), & \\text{(28.22)} \\\\ D_{n - 1} + 2D_n & = 3(y_n - y_{n - 1}). & \\text{(28.23)} \\end{aligned} $$ d. Rewrite equations $\\text{(28.21)}$\u2013$\\text{(28.23)}$ as a matrix equation involving the vector $D = \\langle D_0, D_1, \\ldots, D_n \\rangle$ or unknowns. What attributes does the matrix in your equation have? e. Argue that a natural cubic spline can interpolate a set of $n + 1$ point-value pairs in $O(n)$ time (see Problem 28-1). f. Show how to determine a natural cubic spline that interpolates a set of $n + 1$ points $(x_i, y_i)$ satisfying $x_0 < x_1 < \\cdots < x_n$, even when $x_i$ is not necessarily equal to $i$. What matrix equation must your method solve, and how quickly does your algorithm run? a. We have $a_i = f_i(0) = y_i$ and $b_i = f_i'(0) = f'(x_i) = D_i$. Since $f_i(1) = a_i + b_i + c_i + d_i$ and $f_i'(1) = b_i + 2c_i + 3d_i$, we have $d_i = D_{i + 1} - 2y_{i + 1} + 2y_i + D_i$ which implies $c_i = 3y_{i + 1} - 3y_i - D_{i + 1} - 2D_i$. Since each coefficient can be computed in constant time from the known values, we can compute the $4n$ coefficients in linear time. b. By the continuity constraints, we have $f_i''(1) = f_{i + 1}''(0)$ which implies that $2c_i + 6d_i = 2c_{i + 1}$, or $c_i + 3d_i = c_{i + 1}$. Using our equations from above, this is equivalent to $$D_i + 2D_{i + 1} + 3y_i - 3y_{i + 1} = 3y_{i + 2} - 3y_{i + 1} - D_{i + 2} - 2D_{i + 1}.$$ Rearranging gives the desired equation $$D_i + 4D_{i + 1} + D_{i + 2} = 3(y_{i + 2} - y_i).$$ c. The condition on the left endpoint tells us that $f_0''(0) = 0$, which implies $2c_0 = 0$. By part (a), this means $3(y_1 \u2212 y_0) = 2D_0 + D_1$. The condition on the right endpoint tells us that $f_{n - 1}''(1) = 0$, which implies $c_{n - 1} + 3d_{n - 1} = 0$. By part (a), this means $3(y_n - y_{n - 1}) = D_{n - 1} + 2D_n$. d. The matrix equation has the form $AD = Y$, where $A$ is symmetric and tridiagonal. It looks like this: $$ \\begin{pmatrix} 2 & 1 & 0 & 0 & \\cdots & 0 \\\\ 1 & 4 & 1 & 0 & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\cdots & \\vdots \\\\ \\vdots & \\cdots & 1 & 4 & 1 & 0 \\\\ 0 & \\cdots & 0 & 1 & 4 & 1 \\\\ 0 & \\cdots & 0 & 0 & 1 & 2 \\\\ \\end{pmatrix} \\begin{pmatrix} D_0 \\\\ D_1 \\\\ D_2 \\\\ \\vdots \\\\ D_{n - 1} \\\\ D_n \\end{pmatrix} = \\begin{pmatrix} 3(y_1 - y_0) \\\\ 3(y_2 - y_0) \\\\ 3(y_3 - y_1) \\\\ \\vdots \\\\ 3(y_n - y_{n - 2}) \\\\ 3(y_n - y_{n - 1}) \\end{pmatrix} . $$ e. Since the matrix is symmetric and tridiagonal, Problem 28-1 (e) tells us that we can solve the equation in $O(n)$ time by performing an LUP decomposition. By part (a), once we know each $D_i$ we can compute each $f_i$ in $O(n)$ time. f. For the general case of solving the nonuniform natural cubic spline problem, we require that $f(x_{i + 1}) = f_i(x_{i + 1} \u2212 x_i) = y_{i + 1}$, $f'(x_{i + 1}) = f_i'(x_{i + 1} - x_i) = f_{i + 1}'(0)$ and $f''(x_{i + 1}) = f_i''(x_{i + 1} - x_i) = f_{i + 1}''(0)$. We can still solve for each of $a_i$, $b_i$, $c_i$ and $d_i$ in terms of $y_i$, $y_{i + 1}$, $D_i$ and $D_{i + 1}$, so we still get a tridiagonal matrix equation. The solution will be slightly messier, but ultimately it is solved just like the simpler case, in $O(n)$ time.","title":"28-2 Splines"},{"location":"Chap29/29.1/","text":"29.1-1 If we express the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$ in the compact notation of $\\text{(29.19)}$\u2013$\\text{(29.21)}$, what are $n$, $m$, $A$, $b$, and $c$? $$ \\begin{aligned} n = m & = 3, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} . \\end{aligned} $$ 29.1-2 Give three feasible solutions to the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$. What is the objective value of each one? $(x_1, x_2, x_3) = (6, 1, 0)$ with objective value $9$. $(x_1, x_2, x_3) = (5, 2, 0)$ with objective value $4$. $(x_1, x_2, x_3) = (4, 3, 0)$ with objective value $-1$. 29.1-3 For the slack form in $\\text{(29.38)}$\u2013$\\text{(29.41)}$, what are $N$, $B$, $A$, $b$, $c$, and $v$? $$ \\begin{aligned} N & = \\{1, 2, 3\\}, \\\\ B & = \\{4, 5, 6\\}, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} , \\\\ v & = 0. \\end{aligned} $$ 29.1-4 Convert the following linear program into standard form: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & 2x_1 & + & 7x_2 & + & x_3 & & \\\\ \\text{subject to} & \\\\ & x_1 & & & - & x_3 & = & 7 \\\\ & 3x_1 & + & x_2 & & & \\ge & 24 \\\\ & & & x_2 & & & \\ge & 0 \\\\ & & & & & x_3 & \\le & 0 & . \\end{array} $$ $$ \\begin{array}{lrcrcrcrcrl} \\text{maximize} & -2x_1 & - & 2x_2 & - & 7x_3 & + & x_4 & & \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & & & - & x_4 & \\le & -7 \\\\ & x_1 & - & x_2 & & & + & x_4 & \\le & 7 \\\\ & -3x_1 & + & 3x_2 & - & x_3 & & & \\le & -24 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\le & 0 & . \\end{array} $$ 29.1-5 Convert the following linear program into slack form: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & 2x_1 & & & - & 6x_3 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & - & x_3 & \\le & 7 \\\\ & 3x_1 & - & x_2 & & & \\ge & 8 \\\\ & -x_1 & + & 2x_2 & + & 2x_3 & \\ge & 0 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ What are the basic and nonbasic variables? First, we will multiply the second and third inequalities by minus one to make it so that they are all $\\le$ inequalities. We will introduce the three new variables $x_4$, $x_5$, $x_6$, and perform the usual procedure for rewriting in slack form $$ \\begin{array}{rcrcrcrcr} x_4 & = & 7 & - & x_1 & - & x_2 & + & x_3 \\\\ x_5 & = & -8 & + & 3x_1 & - & x_2 \\\\ x_6 & = & & - & x_1 & + & 2x_2 & + & 2x_3 \\\\ x_1, x_2, x_3, x_4, x_5, x_6 & \\ge & 0 & , \\end{array} $$ where we are sill trying to maximize $2x_1 - 6x_3$. The basic variables are $x_4$, $x_5$, $x_6$ and the nonbasic variables are $x_1$, $x_2$, $x_3$. 29.1-6 Show that the following linear program is infeasible: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 3x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 2 \\\\ & -2x_1 & - & 2x_2 & \\le & -10 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ By dividing the second constraint by $2$ and adding to the first, we have $0 \\le \u22123$, which is impossible. Therefore there linear program is unfeasible. 29.1-7 Show that the following linear program is unbounded: $$ \\begin{array}{lrcrcrl} \\text{minimize} & x_1 & - & x_2 \\\\ \\text{subject to} & \\\\ & -2x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & 2x_2 & \\le & -2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ For any number $r > 1$, we can set $x_1 = 2r$ and $x_2 = r$. Then, the restaints become $$ \\begin{array}{rcrcrl} -2(2r) & + & r = -3r & \\le & -1 \\\\ -2r & - & 2r = -4r & \\le & -2 \\\\ & 2r, r & & \\ge & 0 & . \\end{array} $$ All of these inequalities are clearly satisfied because of our initial restriction in selecting $r$. Now, we look to the objective function, it is $2r - r = r$. So, since we can select $r$ to be arbitrarily large, and still satisfy all of the constraints, we can achieve an arbitrarily large value of the objective function. 29.1-8 Suppose that we have a general linear program with $n$ variables and $m$ constraints, and suppose that we convert it into standard form. Give an upper bound on the number of variables and constraints in the resulting linear program. In the worst case, we have to introduce 2 variables for every variable to ensure that we have nonnegativity constraints, so the resulting program will have $2n$ variables. If each constraint is an equality, we would have to double the number of constraints to create inequalities, resulting in $2m$ constraints. Changing minimization to maximization and greater-than signs to less-than signs don't affect the number of variables or constraints, so the upper bound is $2n$ on variables and $2m$ on constraints. 29.1-9 Give an example of a linear program for which the feasible region is not bounded, but the optimal objective value is finite. Consider the linear program where we want to maximize $x_1 \u2212 x_2$ subject to the constraints $x_1 \u2212 x_2 \\le 1$ and $x_1$, $x_2 \\ge 0$. clearly the objective value can never be greater than one, and it is easy to achieve the optimal value of $1$, by setting $x_1 = 1$ and $x_2 = 0$. Then, this feasible region is unbounded because for any number $r$, we could set $x_1 = x_2 = r$, and that would be feasible because the difference of the two would be zero which is $\\le 1$. If we further wanted it so that there was a single solution that achieved the finite optimal value, we could add the requirements that $x_1 \\le 1$.","title":"29.1 Standard and slack forms"},{"location":"Chap29/29.1/#291-1","text":"If we express the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$ in the compact notation of $\\text{(29.19)}$\u2013$\\text{(29.21)}$, what are $n$, $m$, $A$, $b$, and $c$? $$ \\begin{aligned} n = m & = 3, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} . \\end{aligned} $$","title":"29.1-1"},{"location":"Chap29/29.1/#291-2","text":"Give three feasible solutions to the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$. What is the objective value of each one? $(x_1, x_2, x_3) = (6, 1, 0)$ with objective value $9$. $(x_1, x_2, x_3) = (5, 2, 0)$ with objective value $4$. $(x_1, x_2, x_3) = (4, 3, 0)$ with objective value $-1$.","title":"29.1-2"},{"location":"Chap29/29.1/#291-3","text":"For the slack form in $\\text{(29.38)}$\u2013$\\text{(29.41)}$, what are $N$, $B$, $A$, $b$, $c$, and $v$? $$ \\begin{aligned} N & = \\{1, 2, 3\\}, \\\\ B & = \\{4, 5, 6\\}, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} , \\\\ v & = 0. \\end{aligned} $$","title":"29.1-3"},{"location":"Chap29/29.1/#291-4","text":"Convert the following linear program into standard form: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & 2x_1 & + & 7x_2 & + & x_3 & & \\\\ \\text{subject to} & \\\\ & x_1 & & & - & x_3 & = & 7 \\\\ & 3x_1 & + & x_2 & & & \\ge & 24 \\\\ & & & x_2 & & & \\ge & 0 \\\\ & & & & & x_3 & \\le & 0 & . \\end{array} $$ $$ \\begin{array}{lrcrcrcrcrl} \\text{maximize} & -2x_1 & - & 2x_2 & - & 7x_3 & + & x_4 & & \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & & & - & x_4 & \\le & -7 \\\\ & x_1 & - & x_2 & & & + & x_4 & \\le & 7 \\\\ & -3x_1 & + & 3x_2 & - & x_3 & & & \\le & -24 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\le & 0 & . \\end{array} $$","title":"29.1-4"},{"location":"Chap29/29.1/#291-5","text":"Convert the following linear program into slack form: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & 2x_1 & & & - & 6x_3 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & - & x_3 & \\le & 7 \\\\ & 3x_1 & - & x_2 & & & \\ge & 8 \\\\ & -x_1 & + & 2x_2 & + & 2x_3 & \\ge & 0 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ What are the basic and nonbasic variables? First, we will multiply the second and third inequalities by minus one to make it so that they are all $\\le$ inequalities. We will introduce the three new variables $x_4$, $x_5$, $x_6$, and perform the usual procedure for rewriting in slack form $$ \\begin{array}{rcrcrcrcr} x_4 & = & 7 & - & x_1 & - & x_2 & + & x_3 \\\\ x_5 & = & -8 & + & 3x_1 & - & x_2 \\\\ x_6 & = & & - & x_1 & + & 2x_2 & + & 2x_3 \\\\ x_1, x_2, x_3, x_4, x_5, x_6 & \\ge & 0 & , \\end{array} $$ where we are sill trying to maximize $2x_1 - 6x_3$. The basic variables are $x_4$, $x_5$, $x_6$ and the nonbasic variables are $x_1$, $x_2$, $x_3$.","title":"29.1-5"},{"location":"Chap29/29.1/#291-6","text":"Show that the following linear program is infeasible: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 3x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 2 \\\\ & -2x_1 & - & 2x_2 & \\le & -10 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ By dividing the second constraint by $2$ and adding to the first, we have $0 \\le \u22123$, which is impossible. Therefore there linear program is unfeasible.","title":"29.1-6"},{"location":"Chap29/29.1/#291-7","text":"Show that the following linear program is unbounded: $$ \\begin{array}{lrcrcrl} \\text{minimize} & x_1 & - & x_2 \\\\ \\text{subject to} & \\\\ & -2x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & 2x_2 & \\le & -2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ For any number $r > 1$, we can set $x_1 = 2r$ and $x_2 = r$. Then, the restaints become $$ \\begin{array}{rcrcrl} -2(2r) & + & r = -3r & \\le & -1 \\\\ -2r & - & 2r = -4r & \\le & -2 \\\\ & 2r, r & & \\ge & 0 & . \\end{array} $$ All of these inequalities are clearly satisfied because of our initial restriction in selecting $r$. Now, we look to the objective function, it is $2r - r = r$. So, since we can select $r$ to be arbitrarily large, and still satisfy all of the constraints, we can achieve an arbitrarily large value of the objective function.","title":"29.1-7"},{"location":"Chap29/29.1/#291-8","text":"Suppose that we have a general linear program with $n$ variables and $m$ constraints, and suppose that we convert it into standard form. Give an upper bound on the number of variables and constraints in the resulting linear program. In the worst case, we have to introduce 2 variables for every variable to ensure that we have nonnegativity constraints, so the resulting program will have $2n$ variables. If each constraint is an equality, we would have to double the number of constraints to create inequalities, resulting in $2m$ constraints. Changing minimization to maximization and greater-than signs to less-than signs don't affect the number of variables or constraints, so the upper bound is $2n$ on variables and $2m$ on constraints.","title":"29.1-8"},{"location":"Chap29/29.1/#291-9","text":"Give an example of a linear program for which the feasible region is not bounded, but the optimal objective value is finite. Consider the linear program where we want to maximize $x_1 \u2212 x_2$ subject to the constraints $x_1 \u2212 x_2 \\le 1$ and $x_1$, $x_2 \\ge 0$. clearly the objective value can never be greater than one, and it is easy to achieve the optimal value of $1$, by setting $x_1 = 1$ and $x_2 = 0$. Then, this feasible region is unbounded because for any number $r$, we could set $x_1 = x_2 = r$, and that would be feasible because the difference of the two would be zero which is $\\le 1$. If we further wanted it so that there was a single solution that achieved the finite optimal value, we could add the requirements that $x_1 \\le 1$.","title":"29.1-9"},{"location":"Chap29/29.2/","text":"29.2-1 Put the single-pair shortest-path linear program from $\\text{(29.44)}$\u2013$\\text{(29.46)}$ into standard form. The objective is already in normal form. However, some of the constraints are equality constraints instead of $\\le$ constraints. This means that we need to rewrite them as a pair of inequality constraints, the overlap of whose solutions is just the case where we have equality. we also need to deal with the fact that most of the variables can be negative. To do that, we will introduce variables for the negative part and positive part, each of which need be positive, and we'll just be sure to subtract the negative part. $d_s$ need not be changed in this way since it can never be negative since we are not assuming the existence of negative weight cycles. $$ \\begin{aligned} d_v^+ - d_v^- - d_u^+ + d_u^- \\le w(u, v) \\text{ for every edge } (u, v) \\\\ d_s \\le 0 \\end{aligned} $$ 29.2-2 Write out explicitly the linear program corresponding to finding the shortest path from node $s$ to node $y$ in Figure 24.2(a). $$ \\begin{array}{ll} \\text{minimize} & d_y \\\\ \\text{subject to} & \\\\ & d_t \\le d_s + 3 \\\\ & d_x \\le d_t + 6 \\\\ & d_y \\le d_s + 5 \\\\ & d_y \\le d_t + 2 \\\\ & d_z \\le d_x + 2 \\\\ & d_t \\le d_y + 1 \\\\ & d_x \\le d_y + 4 \\\\ & d_z \\le d_y + 1 \\\\ & d_s \\le d_z + 1 \\\\ & d_x \\le d_z + 7 \\\\ & d_2 = 0. \\end{array} $$ 29.2-3 In the single-source shortest-paths problem, we want to find the shortest-path weights from a source vertex $s$ to all vertices $v \\in V$. Given a graph $G$, write a linear program for which the solution has the property that $d_v$ is the shortest-path weight from $s$ to $v$ for each vertex $v \\in V$. We will follow a similar idea to the way to when we were finding the shortest path between two particular vertices. $$ \\begin{array}{ll} \\text{maximize} & \\sum_{v \\in V} d_v \\\\ \\text{subject to} & \\\\ & d_v \\le d_u + w(u, v) \\text{ for each edge } (u, v) \\\\ & d_s = 0. \\end{array} $$ The first type of constraint makes sure that we never say that a vertex is further away than it would be if we just took the edge corresponding to that constraint. Also, since we are trying to maximize all of the variables, we will make it so that there is no slack anywhere, and so all the dv values will correspond to lengths of shortest paths to $v$. This is because the only thing holding back the variables is the information about relaxing along the edges, which is what determines shortest paths. 29.2-4 Write out explicitly the linear program corresponding to finding the maximum flow in Figure 26.1(a). $$ \\begin{array}{lll} \\text{maximize} & f_{sv_1} + f_{xv_2} \\\\ \\text{subject to} & \\\\ & f_{sv_1} & \\le 16 \\\\ & f_{sv_2} & \\le 14 \\\\ & f_{v_1v_3} & \\le 12 \\\\ & f_{v_2v_1} & \\le 4 \\\\ & f_{v_2v_4} & \\le 14 \\\\ & f_{v_3v_2} & \\le 9 \\\\ & f_{v_3t} & \\le 20 \\\\ & f_{v_4v_3} & \\le 7 \\\\ & f_{v_4t} & \\le 4 \\\\ & f_{sv_1} + f_{v_2v_1} & = f_{v_1v_3} \\\\ & f_{sv_2} + f_{v_3v_2} & = f_{v_2v_1} + f_{v_2v_4} \\\\ & f_{v_1v_3} + f_{v_4v_3} & = f_{v_3v_2} + f_{v_3t} \\\\ & f_{v_2v_4} & = f_{v_4v_3} + f_{v_4t} \\\\ & f_{uv} & \\ge 0 \\text{ for } u, v \\in \\{s, v_1, v_2, v_3, v_4, t\\}. \\end{array} $$ 29.2-5 Rewrite the linear program for maximum flow $\\text{(29.47)}$\u2013$\\text{(29.50)}$ so that it uses only $O(V + E)$ constraints. All we need to do to bring the number of constraints down from $O(V^2)$ to $O(V + E)$ is to replace the way we index the flows. Instead of indexing it by a pair of vertices, we will index it by an edge. This won't change anything about the analysis because between pairs of vertices that don't have an edge between them, there definitely won't be any flow. Also, it brings the number of constraints of the first and third time down to $O(E)$ and the number of constraints of the second kind stays at $O(V)$. $$ \\begin{array}{lll} \\text{maximize} & \\sum_{\\text{edges $e$ leaving $s$}} f_e - \\sum_{\\text{edges $e$ entering $s$}} f_s \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le c(u, v) \\text{ for each edge } (u, v) \\\\ & \\sum_{\\text{edges $e$ leaving $u$}} f_e - \\sum_{\\text{edges $e$ entering $u$}} f_e \\text{ for each edge } u \\in V - \\{s, t\\} \\\\ & f_e \\ge 0 \\text{ for each edge } e. \\end{array} $$ 29.2-6 Write a linear program that, given a bipartite graph $G = (V, E)$ solves the maximum-bipartite-matching problem. Recall from section 26.3 that we can solve the maximum-bipartite-matching problem by viewing it as a network flow problem, where we append a source $s$ and sink $t$, each connected to every vertex is $L$ and $R$ respectively by an edge with capacity $1$, and we give every edge already in the bipartite graph capacity $1$. The integral maximum flows are in correspondence with maximum bipartite matchings. In this setup, the linear programming problem to solve is as follows: $$ \\begin{aligned} \\text{maximize} & \\sum_{v \\in L} f_{sv} \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le 1 \\text{ for each } u, v \\in \\{s\\} \\cup L \\cup R \\cup \\{t\\} = V \\\\ & \\sum_{v \\in V} f_{vu} = \\sum_{v \\in V} f_{uv} \\text{ for each } u \\in L \\cup R \\\\ & f_{uv} \\ge 0 \\text{ for each } u, v \\in V \\end{aligned} $$ 29.2-7 In the minimum-cost multicommodity-flow problem , we are given directed graph $G = (V, E)$ in which each edge $(u, v) \\in E$ has a nonnegative capacity $c(u, v) \\ge 0$ and a cost $a(u, v)$. As in the multicommodity-flow problem, we are given $k$ different commodities, $K_1, K_2, \\ldots, K_k$, where we specify commodity $i$ by the triple $K_i = (s_i, t_i, d_i)$. We define the flow $f_i$ for commodity $i$ and the aggregate flow $f_{uv}$ on edge $(u, v)$ as in the multicommodity-flow problem. A feasible flow is one in which the aggregate flow on each edge $(u, v)$ is no more than the capacity of edge $(u, v)$. The cost of a flow is $\\sum_{u, v \\in V} a(u, v)f_{uv}$, and the goal is to find the feasible flow of minimum cost. Express this problem as a linear program. As in the minimum cost flow problem, we have constraints for the edge capacities, for the conservation of flow, and nonegativity. The difference is that the restraint that before we required exactly $d$ units to flow, now, we require that for each commodity, the right amount of that commodity flows. the conservation equalities will be applied to each different type of commodity independently. If we super script $f$ that will denote the type of commodity the flow is describing, if we do not superscript it, it will denote the aggregate flow We want to minimize $$\\sum_{u, v \\in V} a(u, v) f_{uv}.$$ The capacity constraints are that $$\\sum_{i \\in [k]} \\sum_{u, v \\in V} f_{uv}^i \\le c(u, v).$$ The conservation constraints are that for every $i \\in [k]$, for every $u \\in V \\backslash \\{s_i, t_i\\}$. $$\\sum_{v \\in V} f_{uv}^i = \\sum_{v \\in V} f_{vu}^i.$$ Now, the constraints that correspond to requiring a certain amount of flow are that for every $i \\in [k]$. $$\\sum_{v \\in V} f_{s_i, v}^i - \\sum_{v \\in V} f_{v, s_i}^i = d.$$ Now, we put in the constraint that makes sure what we called the aggregate flow is actually the aggregate flow, so, for every $u, v \\in V$, $$f_{uv} = \\sum_{i \\in [k]} f_{uv}^i.$$ Finally, we get to the fact that all flows are nonnegative, for every $u, v \\in V$, $$f_{uv} \\ge 0.$$","title":"29.2 Formulating problems as linear programs"},{"location":"Chap29/29.2/#292-1","text":"Put the single-pair shortest-path linear program from $\\text{(29.44)}$\u2013$\\text{(29.46)}$ into standard form. The objective is already in normal form. However, some of the constraints are equality constraints instead of $\\le$ constraints. This means that we need to rewrite them as a pair of inequality constraints, the overlap of whose solutions is just the case where we have equality. we also need to deal with the fact that most of the variables can be negative. To do that, we will introduce variables for the negative part and positive part, each of which need be positive, and we'll just be sure to subtract the negative part. $d_s$ need not be changed in this way since it can never be negative since we are not assuming the existence of negative weight cycles. $$ \\begin{aligned} d_v^+ - d_v^- - d_u^+ + d_u^- \\le w(u, v) \\text{ for every edge } (u, v) \\\\ d_s \\le 0 \\end{aligned} $$","title":"29.2-1"},{"location":"Chap29/29.2/#292-2","text":"Write out explicitly the linear program corresponding to finding the shortest path from node $s$ to node $y$ in Figure 24.2(a). $$ \\begin{array}{ll} \\text{minimize} & d_y \\\\ \\text{subject to} & \\\\ & d_t \\le d_s + 3 \\\\ & d_x \\le d_t + 6 \\\\ & d_y \\le d_s + 5 \\\\ & d_y \\le d_t + 2 \\\\ & d_z \\le d_x + 2 \\\\ & d_t \\le d_y + 1 \\\\ & d_x \\le d_y + 4 \\\\ & d_z \\le d_y + 1 \\\\ & d_s \\le d_z + 1 \\\\ & d_x \\le d_z + 7 \\\\ & d_2 = 0. \\end{array} $$","title":"29.2-2"},{"location":"Chap29/29.2/#292-3","text":"In the single-source shortest-paths problem, we want to find the shortest-path weights from a source vertex $s$ to all vertices $v \\in V$. Given a graph $G$, write a linear program for which the solution has the property that $d_v$ is the shortest-path weight from $s$ to $v$ for each vertex $v \\in V$. We will follow a similar idea to the way to when we were finding the shortest path between two particular vertices. $$ \\begin{array}{ll} \\text{maximize} & \\sum_{v \\in V} d_v \\\\ \\text{subject to} & \\\\ & d_v \\le d_u + w(u, v) \\text{ for each edge } (u, v) \\\\ & d_s = 0. \\end{array} $$ The first type of constraint makes sure that we never say that a vertex is further away than it would be if we just took the edge corresponding to that constraint. Also, since we are trying to maximize all of the variables, we will make it so that there is no slack anywhere, and so all the dv values will correspond to lengths of shortest paths to $v$. This is because the only thing holding back the variables is the information about relaxing along the edges, which is what determines shortest paths.","title":"29.2-3"},{"location":"Chap29/29.2/#292-4","text":"Write out explicitly the linear program corresponding to finding the maximum flow in Figure 26.1(a). $$ \\begin{array}{lll} \\text{maximize} & f_{sv_1} + f_{xv_2} \\\\ \\text{subject to} & \\\\ & f_{sv_1} & \\le 16 \\\\ & f_{sv_2} & \\le 14 \\\\ & f_{v_1v_3} & \\le 12 \\\\ & f_{v_2v_1} & \\le 4 \\\\ & f_{v_2v_4} & \\le 14 \\\\ & f_{v_3v_2} & \\le 9 \\\\ & f_{v_3t} & \\le 20 \\\\ & f_{v_4v_3} & \\le 7 \\\\ & f_{v_4t} & \\le 4 \\\\ & f_{sv_1} + f_{v_2v_1} & = f_{v_1v_3} \\\\ & f_{sv_2} + f_{v_3v_2} & = f_{v_2v_1} + f_{v_2v_4} \\\\ & f_{v_1v_3} + f_{v_4v_3} & = f_{v_3v_2} + f_{v_3t} \\\\ & f_{v_2v_4} & = f_{v_4v_3} + f_{v_4t} \\\\ & f_{uv} & \\ge 0 \\text{ for } u, v \\in \\{s, v_1, v_2, v_3, v_4, t\\}. \\end{array} $$","title":"29.2-4"},{"location":"Chap29/29.2/#292-5","text":"Rewrite the linear program for maximum flow $\\text{(29.47)}$\u2013$\\text{(29.50)}$ so that it uses only $O(V + E)$ constraints. All we need to do to bring the number of constraints down from $O(V^2)$ to $O(V + E)$ is to replace the way we index the flows. Instead of indexing it by a pair of vertices, we will index it by an edge. This won't change anything about the analysis because between pairs of vertices that don't have an edge between them, there definitely won't be any flow. Also, it brings the number of constraints of the first and third time down to $O(E)$ and the number of constraints of the second kind stays at $O(V)$. $$ \\begin{array}{lll} \\text{maximize} & \\sum_{\\text{edges $e$ leaving $s$}} f_e - \\sum_{\\text{edges $e$ entering $s$}} f_s \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le c(u, v) \\text{ for each edge } (u, v) \\\\ & \\sum_{\\text{edges $e$ leaving $u$}} f_e - \\sum_{\\text{edges $e$ entering $u$}} f_e \\text{ for each edge } u \\in V - \\{s, t\\} \\\\ & f_e \\ge 0 \\text{ for each edge } e. \\end{array} $$","title":"29.2-5"},{"location":"Chap29/29.2/#292-6","text":"Write a linear program that, given a bipartite graph $G = (V, E)$ solves the maximum-bipartite-matching problem. Recall from section 26.3 that we can solve the maximum-bipartite-matching problem by viewing it as a network flow problem, where we append a source $s$ and sink $t$, each connected to every vertex is $L$ and $R$ respectively by an edge with capacity $1$, and we give every edge already in the bipartite graph capacity $1$. The integral maximum flows are in correspondence with maximum bipartite matchings. In this setup, the linear programming problem to solve is as follows: $$ \\begin{aligned} \\text{maximize} & \\sum_{v \\in L} f_{sv} \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le 1 \\text{ for each } u, v \\in \\{s\\} \\cup L \\cup R \\cup \\{t\\} = V \\\\ & \\sum_{v \\in V} f_{vu} = \\sum_{v \\in V} f_{uv} \\text{ for each } u \\in L \\cup R \\\\ & f_{uv} \\ge 0 \\text{ for each } u, v \\in V \\end{aligned} $$","title":"29.2-6"},{"location":"Chap29/29.2/#292-7","text":"In the minimum-cost multicommodity-flow problem , we are given directed graph $G = (V, E)$ in which each edge $(u, v) \\in E$ has a nonnegative capacity $c(u, v) \\ge 0$ and a cost $a(u, v)$. As in the multicommodity-flow problem, we are given $k$ different commodities, $K_1, K_2, \\ldots, K_k$, where we specify commodity $i$ by the triple $K_i = (s_i, t_i, d_i)$. We define the flow $f_i$ for commodity $i$ and the aggregate flow $f_{uv}$ on edge $(u, v)$ as in the multicommodity-flow problem. A feasible flow is one in which the aggregate flow on each edge $(u, v)$ is no more than the capacity of edge $(u, v)$. The cost of a flow is $\\sum_{u, v \\in V} a(u, v)f_{uv}$, and the goal is to find the feasible flow of minimum cost. Express this problem as a linear program. As in the minimum cost flow problem, we have constraints for the edge capacities, for the conservation of flow, and nonegativity. The difference is that the restraint that before we required exactly $d$ units to flow, now, we require that for each commodity, the right amount of that commodity flows. the conservation equalities will be applied to each different type of commodity independently. If we super script $f$ that will denote the type of commodity the flow is describing, if we do not superscript it, it will denote the aggregate flow We want to minimize $$\\sum_{u, v \\in V} a(u, v) f_{uv}.$$ The capacity constraints are that $$\\sum_{i \\in [k]} \\sum_{u, v \\in V} f_{uv}^i \\le c(u, v).$$ The conservation constraints are that for every $i \\in [k]$, for every $u \\in V \\backslash \\{s_i, t_i\\}$. $$\\sum_{v \\in V} f_{uv}^i = \\sum_{v \\in V} f_{vu}^i.$$ Now, the constraints that correspond to requiring a certain amount of flow are that for every $i \\in [k]$. $$\\sum_{v \\in V} f_{s_i, v}^i - \\sum_{v \\in V} f_{v, s_i}^i = d.$$ Now, we put in the constraint that makes sure what we called the aggregate flow is actually the aggregate flow, so, for every $u, v \\in V$, $$f_{uv} = \\sum_{i \\in [k]} f_{uv}^i.$$ Finally, we get to the fact that all flows are nonnegative, for every $u, v \\in V$, $$f_{uv} \\ge 0.$$","title":"29.2-7"},{"location":"Chap29/29.3/","text":"29.3-1 Complete the proof of Lemma 29.4 by showing that it must be the case that $c = c'$ and $v = v'$. We subtract equation $\\text{(29.81)}$ from equation $\\text{(29.79)}$. $$z = v + \\sum_{j \\in N} c_j x_j, \\tag{29.79}$$ $$z = v' + \\sum_{j \\in N} c_j' x_j. \\tag{29.81}$$ Thus we have, $$ \\begin{aligned} 0 & = v - v' + \\sum_{j \\in N} (c_j - c_j') x_j. \\\\ \\sum_{j \\in N} c_j' x_j & = v - v' + \\sum_{j \\in N} c_j x_j. \\end{aligned} $$ By Lemma 29.3, we have $c_j = c_j'$ for every $j$ and $v = v'$ since $v - v' = 0$. 29.3-2 Show that the call to $\\text{PIVOT}$ in line 12 of $\\text{SIMPLEX}$ never decreases the value of $v$. The only time $v$ is updated in $\\text{PIVOT}$ is line 14, so it will suffice to show that $c_e \\hat b_e \\ge 0$. Prior to making the call to $\\text{PIVOT}$, we choose an index $e$ such that $c_e > 0$, and this is unchanged in $\\text{PIVOT}$. We set $\\hat b_e$ in line 3 to be $b_l / a_{le}$. The loop invariant proved in Lemma 29.2 tells us that $b_l \\ge 0$. The if-condition of line 6 of $\\text{SIMPLEX}$ tells us that only the noninfinite $\\delta_i$ must have $a_{ie} > 0$, and we choose $l$ to minimize $\\delta_l$, so we must have $a_{le} > 0$. Thus, $c_e \\hat b_e \\ge 0$, which implies $v$ can never decrease. 29.3-3 Prove that the slack form given to the $\\text{PIVOT}$ procedure and the slack form that the procedure returns are equivalent. To show that the two slack forms are equivalent, we will show both that they have equal objective functions, and their sets of feasible solutions are equal. First, we'll check that their sets of feasible solutions are equal. Basically all we do to the constraints when we pivot is take the non-basic variable, $e$, and solve the equation corresponding to the basic variable $l$ for $e$. We are then taking that expression and replacing $e$ in all the constraints with this expression we got by solving the equation corresponding to $l$. Since each of these algebraic operations are valid, the result of the sequence of them is also algebraically equivalent to the original. Next, we'll see that the objective functions are equal. We decrease each $c_j$ by $c_e \\hat a_{ej}$, which is to say that we replace the non-basic variable we are making basic with the expression we got it was equal to once we made it basic. Since the slack form returned by $\\text{PIVOT}$, has the same feasible region and an equal objective function, it is equivalent to the original slack form passed in. 29.3-4 Suppose we convert a linear program $(A, b, c)$ in standard form to slack form. Show that the basic solution is feasible if and only if $b_i \\ge 0$ for $i = 1, 2, \\ldots, m$. First suppose that the basic solution is feasible. We set each $x_i = 0$ for $1 \\le i \\le n$, so we have $x_{n + i} = b_i - \\sum_{j = 1}^n a_{ij}x_j = b_i$ as a satisfied constraint. Since we also require $x_{n + i} \\ge 0$ for all $1 \\le i \\le m$, this implies $b_i \\ge 0$. Now suppose $b_i \\ge 0$ for all $i$. In the basic solution we set $x_i = 0$ for $1 \\le i \\le n$ which satisfies the nonnegativity constraints. We set $x_{n + i} = b_i$ for $1 \\le i \\le m$ which satisfies the other constraint equations, and also the nonnegativity constraints on the basic variables since $b_i \\ge 0$. Thus, every constraint is satisfied, so the basic solution is feasible. 29.3-5 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 18x_1 & + & 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 20 \\\\ & x_1 & & & \\le & 12 \\\\ & & & x_2 & \\le & 16 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we rewrite the linear program into it's slack form $$ \\begin{array}{lrl} \\text{maximize} & 18x_1 + 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_3 & = 20 - x_1 - x_2 \\\\ & x_4 & = 12 - x_1 \\\\ & x_5 & = 16 - x_2 \\\\ & x_1, x_2, x_3, x_4, x_5 & \\ge 0. \\end{array} $$ We now stop since no more non-basic variables appear in the objective with a positive coefficient. Our solution is $(12, 8, 0, 0, 8)$ and has a value of $316$. Going back to the standard form we started with, we just disregard the values of $x_3$ through $x_5$ and have the solution that $x_1 = 12$ and $x_2 = 8$. We can check that this is both feasible and has the objective achieve $316$. 29.3-6 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 5x_1 & - & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 1 \\\\ & 2x_1 & + & x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we convert the linear program into it's slack form $$ \\begin{array}{rcrcrcrl} z & = & & & 5x_1 & - & 3x_2 \\\\ x_3 & = & 1 & - & x_1 & + & x_2 \\\\ x_4 & = & 2 & - & 2x_1 & - & x_2 & . \\end{array} $$ The nonbasic variables are $x_1$ and $x_2$. Of these, only $x_1$ has a positive coefficient in the objective function, so we must choose $x_e = x_1$. Both equations limit $x_1$ by $1$, so we'll choose the first one to rewrite $x_1$ with. Using $x_1 = 1 \u2212 x_3 + x_2$ we obtain the new system $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & 5x_3 & + & 2x_2 & \\\\ x_1 & = & 1 & - & x_3 & + & x_2 & \\\\ x_4 & = & & & 2x_3 & - & 2x_2 & . \\end{array} $$ Now $x_2$ is the only nonbasic variable with positive coefficient in the objective function, so we set $x_e = x_2$. The last equation limits $x_2$ by $0$ which is most restrictive, so we set $x_2 = x_3 \u2212 0.5x_4$. Rewriting, our new system becomes $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & 3x_3 & - & x_4 \\\\ x_1 & = & 1 & & & - & 0.5x_4 \\\\ x_2 & = & & & x_3 & - & 0.5x_4 & . \\end{array} $$ Every nonbasic variable now has negative coefficient in the objective function, so we take the basic solution $(x_1, x_2, x_3, x_4) = (1, 0, 0, 0)$. The objective value this achieves is $5$. 29.3-7 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & x_1 & + & x_2 & + & x_3 \\\\ \\text{subject to} & \\\\ & 2x_1 & + & 7.5x_2 & + & 3x_3 & \\ge & 10000 \\\\ & 20x_1 & & 5x_2 & + & 10x_3 & \\ge & 30000 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ First, we convert this equation to the slack form. Doing so doesn't change the objective, but the constraints become $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_1 & - & x_2 & - & x_3 \\\\ x_4 & = & -10000 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Also, since the objective is to minimize a given function, we'll change it over to maximizing the negative of that function. In particular maximize $\u2212x_1 \u2212 x_2 \u2212 x_3$. Now, we note that the initial basic solution is not feasible, because it would leave $x_4$ and $x_5$ being negative. This means that finding an initial solution requires using the method of section 29.5. The auxiliary linear program in slack form is $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_4 & = & -10000 & + & x_0 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & x_0 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We choose $x_0$ as the entering variable and $x_5$ as the leaving variable, since it is the basic variable whose value in the basic solution is most negative. After pivoting, we have the slack form $$ \\begin{array}{rcrcrcrcrcr} z & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 & - & x_5 \\\\ x_0 & = & 30000 & - & 20x_1 & - & 5x_2 & - & 10x_3 & + & x_5 \\\\ x_4 & = & 20000 & - & 18x_1 & + & 2.5x_2 & - & 7x_3 & + & x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The associated basic solution is feasible, so now we just need to repeatedly call $\\text{PIVOT}$ until we obtain an optimal solution to $L_{aux}$. We'll choose $x_2$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_2 & = & 6000 & - & 0.2x_0 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 0.5x_0 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form is the final solution to the auxiliary problem. Since this solution has $x_0 = 0$, we know that our initial problem was feasible. Furthermore, since $x_0 = 0$, we can just remove it from the set of constraints. We then restore the original objective function, with appropriate substitutions made to include only the nonbasic variables. This yields $$ \\begin{array}{rcrcrcrcr} z & = & -6000 & + & 3x_1 & + & x_3 & - & 0.2x_5 \\\\ x_2 & = & 6000 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form has a feasible basic solution, and we can return it to $\\text{SIMPLEX}$. We choose $x_1$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & -2250 & - & \\frac{2}{7} x_3 & - & \\frac{3}{28} x_4 & - & \\frac{11}{280} x_5 \\\\ x_1 & = & 1250 & - & \\frac{3}{7} x_3 & - & \\frac{1}{28} x_4 & + & \\frac{15}{280} x_5 \\\\ x_2 & = & 1000 & - & \\frac{2}{7} x_3 & + & \\frac{4}{28} x_4 & - & \\frac{4}{280} x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ At this point, all coefficients in the objective function are negative, so the basic solution is an optimal solution. This solution is $(x_1, x_2, x_3) = (1250, 1000, 0)$. 29.3-8 In the proof of Lemma 29.5, we argued that there are at most $\\binom{m + n}{n}$ ways to choose a set $B$ of basic variables. Give an example of a linear program in which there are strictly fewer than $\\binom{m + n}{n}$ ways to choose the set $B$. Consider the simple program $$ \\begin{array}{rcrcrl} z & = & & - & x_1 \\\\ x_2 & = & 1 & - & x_1 & . \\end{array} $$ In this case, we have $m = n = 1$, so $\\binom{m + n}{n} = \\binom{2}{1} = 2$, however, since the only coefficients of the objective function are negative, we can't make any other choices for basic variable. We must immediately terminate with the basic solution $(x_1, x_2) = (0, 1)$, which is optimal.","title":"29.3 The simplex algorithm"},{"location":"Chap29/29.3/#293-1","text":"Complete the proof of Lemma 29.4 by showing that it must be the case that $c = c'$ and $v = v'$. We subtract equation $\\text{(29.81)}$ from equation $\\text{(29.79)}$. $$z = v + \\sum_{j \\in N} c_j x_j, \\tag{29.79}$$ $$z = v' + \\sum_{j \\in N} c_j' x_j. \\tag{29.81}$$ Thus we have, $$ \\begin{aligned} 0 & = v - v' + \\sum_{j \\in N} (c_j - c_j') x_j. \\\\ \\sum_{j \\in N} c_j' x_j & = v - v' + \\sum_{j \\in N} c_j x_j. \\end{aligned} $$ By Lemma 29.3, we have $c_j = c_j'$ for every $j$ and $v = v'$ since $v - v' = 0$.","title":"29.3-1"},{"location":"Chap29/29.3/#293-2","text":"Show that the call to $\\text{PIVOT}$ in line 12 of $\\text{SIMPLEX}$ never decreases the value of $v$. The only time $v$ is updated in $\\text{PIVOT}$ is line 14, so it will suffice to show that $c_e \\hat b_e \\ge 0$. Prior to making the call to $\\text{PIVOT}$, we choose an index $e$ such that $c_e > 0$, and this is unchanged in $\\text{PIVOT}$. We set $\\hat b_e$ in line 3 to be $b_l / a_{le}$. The loop invariant proved in Lemma 29.2 tells us that $b_l \\ge 0$. The if-condition of line 6 of $\\text{SIMPLEX}$ tells us that only the noninfinite $\\delta_i$ must have $a_{ie} > 0$, and we choose $l$ to minimize $\\delta_l$, so we must have $a_{le} > 0$. Thus, $c_e \\hat b_e \\ge 0$, which implies $v$ can never decrease.","title":"29.3-2"},{"location":"Chap29/29.3/#293-3","text":"Prove that the slack form given to the $\\text{PIVOT}$ procedure and the slack form that the procedure returns are equivalent. To show that the two slack forms are equivalent, we will show both that they have equal objective functions, and their sets of feasible solutions are equal. First, we'll check that their sets of feasible solutions are equal. Basically all we do to the constraints when we pivot is take the non-basic variable, $e$, and solve the equation corresponding to the basic variable $l$ for $e$. We are then taking that expression and replacing $e$ in all the constraints with this expression we got by solving the equation corresponding to $l$. Since each of these algebraic operations are valid, the result of the sequence of them is also algebraically equivalent to the original. Next, we'll see that the objective functions are equal. We decrease each $c_j$ by $c_e \\hat a_{ej}$, which is to say that we replace the non-basic variable we are making basic with the expression we got it was equal to once we made it basic. Since the slack form returned by $\\text{PIVOT}$, has the same feasible region and an equal objective function, it is equivalent to the original slack form passed in.","title":"29.3-3"},{"location":"Chap29/29.3/#293-4","text":"Suppose we convert a linear program $(A, b, c)$ in standard form to slack form. Show that the basic solution is feasible if and only if $b_i \\ge 0$ for $i = 1, 2, \\ldots, m$. First suppose that the basic solution is feasible. We set each $x_i = 0$ for $1 \\le i \\le n$, so we have $x_{n + i} = b_i - \\sum_{j = 1}^n a_{ij}x_j = b_i$ as a satisfied constraint. Since we also require $x_{n + i} \\ge 0$ for all $1 \\le i \\le m$, this implies $b_i \\ge 0$. Now suppose $b_i \\ge 0$ for all $i$. In the basic solution we set $x_i = 0$ for $1 \\le i \\le n$ which satisfies the nonnegativity constraints. We set $x_{n + i} = b_i$ for $1 \\le i \\le m$ which satisfies the other constraint equations, and also the nonnegativity constraints on the basic variables since $b_i \\ge 0$. Thus, every constraint is satisfied, so the basic solution is feasible.","title":"29.3-4"},{"location":"Chap29/29.3/#293-5","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 18x_1 & + & 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 20 \\\\ & x_1 & & & \\le & 12 \\\\ & & & x_2 & \\le & 16 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we rewrite the linear program into it's slack form $$ \\begin{array}{lrl} \\text{maximize} & 18x_1 + 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_3 & = 20 - x_1 - x_2 \\\\ & x_4 & = 12 - x_1 \\\\ & x_5 & = 16 - x_2 \\\\ & x_1, x_2, x_3, x_4, x_5 & \\ge 0. \\end{array} $$ We now stop since no more non-basic variables appear in the objective with a positive coefficient. Our solution is $(12, 8, 0, 0, 8)$ and has a value of $316$. Going back to the standard form we started with, we just disregard the values of $x_3$ through $x_5$ and have the solution that $x_1 = 12$ and $x_2 = 8$. We can check that this is both feasible and has the objective achieve $316$.","title":"29.3-5"},{"location":"Chap29/29.3/#293-6","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 5x_1 & - & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 1 \\\\ & 2x_1 & + & x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we convert the linear program into it's slack form $$ \\begin{array}{rcrcrcrl} z & = & & & 5x_1 & - & 3x_2 \\\\ x_3 & = & 1 & - & x_1 & + & x_2 \\\\ x_4 & = & 2 & - & 2x_1 & - & x_2 & . \\end{array} $$ The nonbasic variables are $x_1$ and $x_2$. Of these, only $x_1$ has a positive coefficient in the objective function, so we must choose $x_e = x_1$. Both equations limit $x_1$ by $1$, so we'll choose the first one to rewrite $x_1$ with. Using $x_1 = 1 \u2212 x_3 + x_2$ we obtain the new system $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & 5x_3 & + & 2x_2 & \\\\ x_1 & = & 1 & - & x_3 & + & x_2 & \\\\ x_4 & = & & & 2x_3 & - & 2x_2 & . \\end{array} $$ Now $x_2$ is the only nonbasic variable with positive coefficient in the objective function, so we set $x_e = x_2$. The last equation limits $x_2$ by $0$ which is most restrictive, so we set $x_2 = x_3 \u2212 0.5x_4$. Rewriting, our new system becomes $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & 3x_3 & - & x_4 \\\\ x_1 & = & 1 & & & - & 0.5x_4 \\\\ x_2 & = & & & x_3 & - & 0.5x_4 & . \\end{array} $$ Every nonbasic variable now has negative coefficient in the objective function, so we take the basic solution $(x_1, x_2, x_3, x_4) = (1, 0, 0, 0)$. The objective value this achieves is $5$.","title":"29.3-6"},{"location":"Chap29/29.3/#293-7","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & x_1 & + & x_2 & + & x_3 \\\\ \\text{subject to} & \\\\ & 2x_1 & + & 7.5x_2 & + & 3x_3 & \\ge & 10000 \\\\ & 20x_1 & & 5x_2 & + & 10x_3 & \\ge & 30000 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ First, we convert this equation to the slack form. Doing so doesn't change the objective, but the constraints become $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_1 & - & x_2 & - & x_3 \\\\ x_4 & = & -10000 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Also, since the objective is to minimize a given function, we'll change it over to maximizing the negative of that function. In particular maximize $\u2212x_1 \u2212 x_2 \u2212 x_3$. Now, we note that the initial basic solution is not feasible, because it would leave $x_4$ and $x_5$ being negative. This means that finding an initial solution requires using the method of section 29.5. The auxiliary linear program in slack form is $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_4 & = & -10000 & + & x_0 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & x_0 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We choose $x_0$ as the entering variable and $x_5$ as the leaving variable, since it is the basic variable whose value in the basic solution is most negative. After pivoting, we have the slack form $$ \\begin{array}{rcrcrcrcrcr} z & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 & - & x_5 \\\\ x_0 & = & 30000 & - & 20x_1 & - & 5x_2 & - & 10x_3 & + & x_5 \\\\ x_4 & = & 20000 & - & 18x_1 & + & 2.5x_2 & - & 7x_3 & + & x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The associated basic solution is feasible, so now we just need to repeatedly call $\\text{PIVOT}$ until we obtain an optimal solution to $L_{aux}$. We'll choose $x_2$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_2 & = & 6000 & - & 0.2x_0 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 0.5x_0 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form is the final solution to the auxiliary problem. Since this solution has $x_0 = 0$, we know that our initial problem was feasible. Furthermore, since $x_0 = 0$, we can just remove it from the set of constraints. We then restore the original objective function, with appropriate substitutions made to include only the nonbasic variables. This yields $$ \\begin{array}{rcrcrcrcr} z & = & -6000 & + & 3x_1 & + & x_3 & - & 0.2x_5 \\\\ x_2 & = & 6000 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form has a feasible basic solution, and we can return it to $\\text{SIMPLEX}$. We choose $x_1$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & -2250 & - & \\frac{2}{7} x_3 & - & \\frac{3}{28} x_4 & - & \\frac{11}{280} x_5 \\\\ x_1 & = & 1250 & - & \\frac{3}{7} x_3 & - & \\frac{1}{28} x_4 & + & \\frac{15}{280} x_5 \\\\ x_2 & = & 1000 & - & \\frac{2}{7} x_3 & + & \\frac{4}{28} x_4 & - & \\frac{4}{280} x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ At this point, all coefficients in the objective function are negative, so the basic solution is an optimal solution. This solution is $(x_1, x_2, x_3) = (1250, 1000, 0)$.","title":"29.3-7"},{"location":"Chap29/29.3/#293-8","text":"In the proof of Lemma 29.5, we argued that there are at most $\\binom{m + n}{n}$ ways to choose a set $B$ of basic variables. Give an example of a linear program in which there are strictly fewer than $\\binom{m + n}{n}$ ways to choose the set $B$. Consider the simple program $$ \\begin{array}{rcrcrl} z & = & & - & x_1 \\\\ x_2 & = & 1 & - & x_1 & . \\end{array} $$ In this case, we have $m = n = 1$, so $\\binom{m + n}{n} = \\binom{2}{1} = 2$, however, since the only coefficients of the objective function are negative, we can't make any other choices for basic variable. We must immediately terminate with the basic solution $(x_1, x_2) = (0, 1)$, which is optimal.","title":"29.3-8"},{"location":"Chap29/29.4/","text":"29.4-1 Formulate the dual of the linear program given in Exercise 29.3-5. By just transposing $A$, swapping $b$ and $c$, and switching the maximization to a minimization, we want to minimize $20y_1 + 12y_2 + 16y_3$ subject to the constrain $$ \\begin{aligned} y_1 + y_2 & \\ge 18 \\\\ y_1 + y_3 & \\ge 12.5 \\\\ y_1, y_2, y_3 & \\ge 0 \\end{aligned} $$ 29.4-2 Suppose that we have a linear program that is not in standard form. We could produce the dual by first converting it to standard form, and then taking the dual. It would be more convenient, however, to be able to produce the dual directly. Explain how we can directly take the dual of an arbitrary linear program. By working through each aspect of putting a general linear program into standard form, as outlined on page 852, we can show how to deal with transforming each into the dual individually. If the problem is a minimization instead of a maximization, replace $c_j$ by $\u2212c_j$ in $\\text{(29.84)}$. If there is a lack of nonnegativity constraint on $x_j$ we duplicate the $j$th column of $A$, which corresponds to duplicating the $j$th row of $A^{\\text T}$. If there is an equality constraint for $b_i$, we convert it to two inequalities by duplicating then negating the $i$th column of $A^{\\text T}$, duplicating then negating the $i$th entry of $b$, and adding an extra $y_i$ variable. We handle the greater-than-or-equal-to sign $\\sum_{i = 1}^n a_{ij}x_j \\ge b_i$ by negating $i$th column of $A^{\\text T}$ and negating $b_i$. Then we solve the dual problem of minimizing $b^{\\text T}y$ subject to $A^{\\text T}y$ and $y \\ge 0$. 29.4-3 Write down the dual of the maximum-flow linear program, as given in lines $\\text{(29.47)}$\u2013$\\text{(29.50)}$ on page 860. Explain how to interpret this formulation as a minimum-cut problem. First, we'll convert the linear program for maximum flow described in equation $\\text{(29.47)}$-$\\text{(29.50)}$ into standard form. The objective function says that $c$ is a vector indexed by a pair of vertices, and it is positive one if $s$ is the first index and negative one if $s$ is the second index (zero if it is both). Next, we'll modify the constraints by switching the equalities over into inequalities to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} f_{vu} & \\ge & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Then, we'll convert all but the last set of the inequalities to be $\\le$ by multiplying the third line by $-1$. $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} & \\le & \\sum_{u \\in V} -f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Finally, we'll bring all the variables over to the left to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} - \\sum_{u \\in V} f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} - \\sum_{u \\in V} -f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Now, we can finally write down our $A$ and $b$. $A$ will be a $|V|^2 \\times |V|^2 + 2|V| \u2212 4$ matrix built from smaller matrices $A_1$ and $A_2$ which correspond to the three types of constraints that we have (of course, not counting the non-negativity constraints). We will let $g(u, v)$ be any bijective mapping from $V \\times V$ to $[|V|^2]$. We'll also let $h$ be any bijection from $V - \\{s, t\\}$ to $[|V| - 2]$. $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\\\ -A_2 \\end{pmatrix}, $$ where $A_1$ is defined as having its row $g(u, v)$ be all zeroes except for having the value $1$ at at the $g(u, v)$th entry. We define $A_2$ to have it's row $h(u)$ be equal to $1$ at all columns $j$ for which $j = g(v, u)$ for some $v$ and equal to $-1$ at all columns $j$ for which $j = g(u, v)$ for some $v$. Lastly, we mention that $b$ is defined as having it's $j$th entry be equal to $c(u, v)$ if $j = g(u, v)$ and zero if $j > |V|^2$. Now that we have placed the linear program in standard form, we can take its dual. We want to minimize $\\sum_{i = 1}^{|V|^2 + 2|V| - 2} b_iy_i$ given the constraints that all the $y$ values are non-negative, and $A^{\\text T} y \\ge c$. 29.4-4 Write down the dual of the minimum-cost-flow linear program, as given in lines $\\text{(29.51)}$\u2013$\\text{(29.52)}$ on page 862. Explain how to interpret this problem in terms of graphs and flows. First we need to put the linear programming problem into standard form, as follows: $$ \\begin{array}{lrcrl} \\text{maximize} & \\sum_{(u, v) \\in E} -a(u, v) f_{uv} \\\\ \\text{subject to} & \\\\ & f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ & \\sum_{v \\in V} f_{vu} - \\sum_{v \\in V} f_{uv} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{uv} - \\sum_{v \\in V} f_{vu} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{sv} - \\sum_{v \\in V} f_{vs} & \\le & d \\\\ & \\sum_{v \\in V} f_{vs} - \\sum_{v \\in V} f_{sv} & \\le & -d \\\\ & f_{uv} & \\ge & 0 & . \\end{array} $$ We now formulate the dual problem. Let the vertices be denoted $v_1, v_2, \\dots, v_n, s, t$ and the edges be $e_1, e_2, \\dots, e_k$. Then we have $b_i = c(e_i)$ for $1 \\le i \\le k$, $b_i = 0$ for $k + 1 \\le i \\le k + 2n$, $b_{k + 2n + 1} = d$, and $b_{k + 2n + 2} = \u2212d$. We also have $c_i = \u2212a(e_i)$ for $1 \\le i \\le k$. For notation, let $j.left$ denote the tail of edge $e_j$ and $j.right$ denote the head. Let $\\chi_s(e_j) = 1$ if $e_j$ enters $s$, set it equal to $-1$ if $e_j$ leaves $s$, and set it equal to $0$ if $e_j$ is not incident with $s$. The dual problem is: $$ \\begin{array}{ll} \\text{minimize} & \\sum_{i = 1}^k c(e_j)y_i + dy_{k + 2n + 1} - dy_{k + 2n + 2} \\\\ \\text{subject to} & \\\\ & y_j + y_{k + e_j.right} - y_{k + j.left} - y_{k + n + e_j.right} + y_{k + n + e_j.left} - \\chi_s(e_j) y_{k + 2n + 1} + \\chi_s(e_j) y_{k + 2n + 2} \\ge -a(e_j), \\end{array} $$ where $j$ runs between $1$ and $k$. There is one constraint equation for each edge $e_j$. 29.4-5 Show that the dual of the dual of a linear program is the primal linear program. Suppose that our original linear program is in standard form for some $A$, $b$, $c$. Then, the dual of this is to minimize $\\sum_{i = 1}^m b_iy_i$ subject to $A^{\\text T} y \\ge c$ This can be rewritten as wanting to maximize $\\sum_{i = 1}^m (\u2212b_i)y_i$ subject to $(\u2212A)^{\\text T} y \\le \u2212c$. Since this is a standard form, we can take its dual easily, it is minimize $\\sum_{j = 1}^n (\u2212c_j)x_j$ subject to $(\u2212A)x \\ge \u2212b$. This is the same as minimizing $\\sum_{j = 1}^n c_jx_j$ subject to $Ax \\le b$, which was the original linear program. 29.4-6 Which result from Chapter 26 can be interpreted as weak duality for the maximum-flow problem? Corollary 26.5 from Chapter 26 can be interpreted as weak duality.","title":"29.4 Duality"},{"location":"Chap29/29.4/#294-1","text":"Formulate the dual of the linear program given in Exercise 29.3-5. By just transposing $A$, swapping $b$ and $c$, and switching the maximization to a minimization, we want to minimize $20y_1 + 12y_2 + 16y_3$ subject to the constrain $$ \\begin{aligned} y_1 + y_2 & \\ge 18 \\\\ y_1 + y_3 & \\ge 12.5 \\\\ y_1, y_2, y_3 & \\ge 0 \\end{aligned} $$","title":"29.4-1"},{"location":"Chap29/29.4/#294-2","text":"Suppose that we have a linear program that is not in standard form. We could produce the dual by first converting it to standard form, and then taking the dual. It would be more convenient, however, to be able to produce the dual directly. Explain how we can directly take the dual of an arbitrary linear program. By working through each aspect of putting a general linear program into standard form, as outlined on page 852, we can show how to deal with transforming each into the dual individually. If the problem is a minimization instead of a maximization, replace $c_j$ by $\u2212c_j$ in $\\text{(29.84)}$. If there is a lack of nonnegativity constraint on $x_j$ we duplicate the $j$th column of $A$, which corresponds to duplicating the $j$th row of $A^{\\text T}$. If there is an equality constraint for $b_i$, we convert it to two inequalities by duplicating then negating the $i$th column of $A^{\\text T}$, duplicating then negating the $i$th entry of $b$, and adding an extra $y_i$ variable. We handle the greater-than-or-equal-to sign $\\sum_{i = 1}^n a_{ij}x_j \\ge b_i$ by negating $i$th column of $A^{\\text T}$ and negating $b_i$. Then we solve the dual problem of minimizing $b^{\\text T}y$ subject to $A^{\\text T}y$ and $y \\ge 0$.","title":"29.4-2"},{"location":"Chap29/29.4/#294-3","text":"Write down the dual of the maximum-flow linear program, as given in lines $\\text{(29.47)}$\u2013$\\text{(29.50)}$ on page 860. Explain how to interpret this formulation as a minimum-cut problem. First, we'll convert the linear program for maximum flow described in equation $\\text{(29.47)}$-$\\text{(29.50)}$ into standard form. The objective function says that $c$ is a vector indexed by a pair of vertices, and it is positive one if $s$ is the first index and negative one if $s$ is the second index (zero if it is both). Next, we'll modify the constraints by switching the equalities over into inequalities to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} f_{vu} & \\ge & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Then, we'll convert all but the last set of the inequalities to be $\\le$ by multiplying the third line by $-1$. $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} & \\le & \\sum_{u \\in V} -f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Finally, we'll bring all the variables over to the left to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} - \\sum_{u \\in V} f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} - \\sum_{u \\in V} -f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Now, we can finally write down our $A$ and $b$. $A$ will be a $|V|^2 \\times |V|^2 + 2|V| \u2212 4$ matrix built from smaller matrices $A_1$ and $A_2$ which correspond to the three types of constraints that we have (of course, not counting the non-negativity constraints). We will let $g(u, v)$ be any bijective mapping from $V \\times V$ to $[|V|^2]$. We'll also let $h$ be any bijection from $V - \\{s, t\\}$ to $[|V| - 2]$. $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\\\ -A_2 \\end{pmatrix}, $$ where $A_1$ is defined as having its row $g(u, v)$ be all zeroes except for having the value $1$ at at the $g(u, v)$th entry. We define $A_2$ to have it's row $h(u)$ be equal to $1$ at all columns $j$ for which $j = g(v, u)$ for some $v$ and equal to $-1$ at all columns $j$ for which $j = g(u, v)$ for some $v$. Lastly, we mention that $b$ is defined as having it's $j$th entry be equal to $c(u, v)$ if $j = g(u, v)$ and zero if $j > |V|^2$. Now that we have placed the linear program in standard form, we can take its dual. We want to minimize $\\sum_{i = 1}^{|V|^2 + 2|V| - 2} b_iy_i$ given the constraints that all the $y$ values are non-negative, and $A^{\\text T} y \\ge c$.","title":"29.4-3"},{"location":"Chap29/29.4/#294-4","text":"Write down the dual of the minimum-cost-flow linear program, as given in lines $\\text{(29.51)}$\u2013$\\text{(29.52)}$ on page 862. Explain how to interpret this problem in terms of graphs and flows. First we need to put the linear programming problem into standard form, as follows: $$ \\begin{array}{lrcrl} \\text{maximize} & \\sum_{(u, v) \\in E} -a(u, v) f_{uv} \\\\ \\text{subject to} & \\\\ & f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ & \\sum_{v \\in V} f_{vu} - \\sum_{v \\in V} f_{uv} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{uv} - \\sum_{v \\in V} f_{vu} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{sv} - \\sum_{v \\in V} f_{vs} & \\le & d \\\\ & \\sum_{v \\in V} f_{vs} - \\sum_{v \\in V} f_{sv} & \\le & -d \\\\ & f_{uv} & \\ge & 0 & . \\end{array} $$ We now formulate the dual problem. Let the vertices be denoted $v_1, v_2, \\dots, v_n, s, t$ and the edges be $e_1, e_2, \\dots, e_k$. Then we have $b_i = c(e_i)$ for $1 \\le i \\le k$, $b_i = 0$ for $k + 1 \\le i \\le k + 2n$, $b_{k + 2n + 1} = d$, and $b_{k + 2n + 2} = \u2212d$. We also have $c_i = \u2212a(e_i)$ for $1 \\le i \\le k$. For notation, let $j.left$ denote the tail of edge $e_j$ and $j.right$ denote the head. Let $\\chi_s(e_j) = 1$ if $e_j$ enters $s$, set it equal to $-1$ if $e_j$ leaves $s$, and set it equal to $0$ if $e_j$ is not incident with $s$. The dual problem is: $$ \\begin{array}{ll} \\text{minimize} & \\sum_{i = 1}^k c(e_j)y_i + dy_{k + 2n + 1} - dy_{k + 2n + 2} \\\\ \\text{subject to} & \\\\ & y_j + y_{k + e_j.right} - y_{k + j.left} - y_{k + n + e_j.right} + y_{k + n + e_j.left} - \\chi_s(e_j) y_{k + 2n + 1} + \\chi_s(e_j) y_{k + 2n + 2} \\ge -a(e_j), \\end{array} $$ where $j$ runs between $1$ and $k$. There is one constraint equation for each edge $e_j$.","title":"29.4-4"},{"location":"Chap29/29.4/#294-5","text":"Show that the dual of the dual of a linear program is the primal linear program. Suppose that our original linear program is in standard form for some $A$, $b$, $c$. Then, the dual of this is to minimize $\\sum_{i = 1}^m b_iy_i$ subject to $A^{\\text T} y \\ge c$ This can be rewritten as wanting to maximize $\\sum_{i = 1}^m (\u2212b_i)y_i$ subject to $(\u2212A)^{\\text T} y \\le \u2212c$. Since this is a standard form, we can take its dual easily, it is minimize $\\sum_{j = 1}^n (\u2212c_j)x_j$ subject to $(\u2212A)x \\ge \u2212b$. This is the same as minimizing $\\sum_{j = 1}^n c_jx_j$ subject to $Ax \\le b$, which was the original linear program.","title":"29.4-5"},{"location":"Chap29/29.4/#294-6","text":"Which result from Chapter 26 can be interpreted as weak duality for the maximum-flow problem? Corollary 26.5 from Chapter 26 can be interpreted as weak duality.","title":"29.4-6"},{"location":"Chap29/29.5/","text":"29.5-1 Give detailed pseudocode to implement lines 5 and 14 of $\\text{INITIALIZE-SIMPLEX}$. For line 5, first let $(N, B, A, b, c, v)$ be the result of calling $\\text{PIVOT}$ on $L_{aux}$ using $x_0$ as the entering variable. Then repeatedly call $\\text{PIVOT}$ until an optimal solution to $L_{aux}$ is obtained, and return this to $(N, B, A, b, c, v)$. To remove $x_0$ from the constraints, set $a_{i, 0} = 0$ for all $i \\in B$, and set $N = N \\backslash \\{0\\}$. To restore the original objective function of $L$, for each $j \\in N$ and each $i \\in B$, set $c_j = c_j \u2212 c_ia_{ij}$. 29.5-2 Show that when the main loop of $\\text{SIMPLEX}$ is run by $\\text{INITIALIZE-SIMPLEX}$, it can never return \"unbounded.\" In order to enter line 10 of $\\text{INITIALIZE-SIMPLEX}$ and begin iterating the main loop of $\\text{SIMPLEX}$, we must have recovered a basic solution which is feasible for $L_{aux}$. Since $x_0 \\ge 0$ and the objective function is $\u2212x_0$, the objective value associated to this solution (or any solution) must be negative. Since the goal is to aximize, we have an upper bound of $0$ on the objective value. By Lemma 29.2, $\\text{SIMPLEX}$ correctly determines whether or not the input linear program is unbounded. Since $L_{aux}$ is not unbounded, this can never be returned by $\\text{SIMPLEX}$. 29.5-3 Suppose that we are given a linear program $L$ in standard form, and suppose that for both $L$ and the dual of $L$, the basic solutions associated with the initial slack forms are feasible. Show that the optimal objective value of $L$ is $0$. Since it is in standard form, the objective function has no constant term, it is entirely given by $\\sum_{i = 1}^n c_ix_i$, which is going to be zero for any basic solution. The same thing goes for its dual. Since there is some solution which has the objective function achieve the same value both for the dual and the primal, by the corollary to the weak duality theorem, that common value must be the optimal value of the objective function. 29.5-4 Suppose that we allow strict inequalities in a linear program. Show that in this case, the fundamental theorem of linear programming does not hold. Consider the linear program in which we wish to maximize $x_1$ subject to the constraint $x_1 < 1$ and $x_1 \\ge 0$. This has no optimal solution, but it is clearly bounded and has feasible solutions. Thus, the Fundamental theorem of linear programming does not hold in the case of strict inequalities. 29.5-5 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 8 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & - & x_2 & \\le & 8 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 8 & + & x_0 & - & x_1 & + & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 11 & - & 2x_1 & & & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, so we repeatedly call $\\text{PIVOT}$ to get the optimal solution to $L_{aux}$. We'll choose $x_1$ to be our entering variable and $x_0$ to be the leaving variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & & & -x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_0 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is now optimal for $L_{aux}$, so we return this slack form to $\\text{SIMPLEX}$, set $x_0 = 0$, and update the objective function which yields $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_5$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1 & = & 2 & + & (1 / 5)x_4 & + & (1 / 5)x_5 \\\\ x_2 & = & 1 & + & (4 / 5)x_4 & - & (1 / 5)x_5 \\\\ x_3 & = & 7 & - & (3 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_4$ as our entering variable, which makes $x_3$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & (64 / 3) & - & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & (34 / 3) & - & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & (10 / 3) & - & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & (35 / 3) & - & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Now all coefficients in the objective function are negative, so the basic solution is the optimal solution. It is $(x_1, x_2) = (34 / 3, 10 / 3)$. 29.5-6 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & 2x_2 & \\le & 4 \\\\ & -2x_1 & - & 6x_2 & \\le & -12 \\\\ & & & x_2 & \\le & 1 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & + & 2x_2 & \\le & 4 \\\\ & -x_0 & - & 2x_1 & - & 6x_2 & \\le & -12 \\\\ & -x_0 & & & + & x_2 & \\le & 1 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 4 & + & x_0 & - & x_1 & - & 2x_2 \\\\ x_4 & = & -12 & + & x_0 & + & 2x_1 & + & 6x_2 \\\\ x_5 & = & 1 & + & x_0 & & & - & x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -12 & + & 2x_1 & + & 6x_2 & - & x_4 \\\\ x_0 & = & 12 & - & 2x_1 & - & 6x_2 & + & x_4 \\\\ x_3 & = & 16 & - & 3x_1 & - & 8x_2 & + & x_4 \\\\ x_5 & = & 13 & - & 2x_1 & - & 8x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is $(x_0, x_1, x_2, x_3, x_4, x_5) = (12, 0, 0, 16, 0, 13)$ which is feasible for the auxiliary program. Now we need to run $\\text{SIMPLEX}$ to find the optimal objective value to $L_{aux}$. Let $x_1$ be our next entering variable. It is most constrained by $x_3$, which will be our leaving variable. After $\\text{PIVOT}$, the new linear program is $$ \\begin{array}{rcrcrcrcr} z & = & -(4 / 3) & + & (2 / 3)x_2 & - & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0 & = & (4 / 3) & - & (2 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_1 & = & (16 / 3) & - & (8 / 3)x_2 & - & (1 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_5 & = & (7 / 3) & - & (8 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Every coefficient in the objective function is negative, so we take the basic solution $(x_0, x_1, x_2, x_3, x_4, x_5) = (4 / 3, 16 / 3, 0, 0, 0, 7 / 3)$ which is also optimal. Since $x_0 \\ne 0$, the original linear program must be unfeasible. 29.5-7 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & - & x_1 & + & x_2 & \\le & -1 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & -1 & + & x_0 & + & x_1 & - & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Let $x_1$ be our entering variable. Then $x_0$ is our leaving variable, and we have $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, and optimal for $L_{aux}$, so we return this and run $\\text{SIMPLEX}$. Updating the objective function and setting $x_0 = 0$ gives $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_3$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & - & x_3 & + & 2x_4 \\\\ x_1 & = & 2 & + & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_2 & = & 1 & - & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_5 & = & & & (5 / 2)x_3 & - & (3 / 2)x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we use $x_4$ as our entering variable, which makes $x_5$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & 2 & + & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & 1 & + & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & & & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Finally, we would like to choose $x_3$ as our entering variable, but every coefficient on $x_3$ is positive, so $\\text{SIMPLEX}$ returns that the linear program is unbounded. 29.5-8 Solve the linear program given in $\\text{(29.6)}$\u2013$\\text{(29.10)}$. We first put the linear program in standard form, $$ \\begin{array}{lrcrcrcrcrl} \\text{maxmize} & x_1 & + & x_2 & + & x_3 & + & x_4 \\\\ \\text{subject to} & \\\\ & 2x_1 & - & 8x_2 & & & - & 10x_4 & \\le & -50 \\\\ & -5x_1 & - & 2x_2 & & & & & \\le & -100 \\\\ & -3x_1 & + & 5x_2 & - & 10x_3 & + & 2x_4 & \\le & -25 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program. $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & & - & x_0 \\\\ x_5 & = & -50 & + & x_0 & - & 2x_1 & + & 8x_2 & & & + & 10x_4 \\\\ x_6 & = & -100 & + & x_0 & + & 5x_1 & + & 2x_2 \\\\ x_7 & = & -25 & + & x_0 & + & 3x_1 & - & 5x_2 & + & 10x_3 & - & 2x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The index of the minimum $b_i$ is $2$, so we take $x_0$ to be our entering variable and $x_6$ to be our leaving variable. The call to $\\text{PIVOT}$ on line 8 yields $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & -100 & + & 5x_1 & + & 2x_2 & & & & & - & x_6 \\\\ x_0 & = & 100 & - & 5x_1 & - & 2x_2 & & & & & + & x_6 \\\\ x_5 & = & 50 & - & 7x_1 & + & 8x_2 & & & + & 10x_4 & + & x_6 \\\\ x_7 & = & 75 & - & 2x_1 & - & 7x_2 & + & 10x_3 & - & 2x_4 & + & x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ Next we'll take $x_2$ to be our entering variable and $x_5$ to be our leaving variable. The call to $\\text{PIVOT}$ yields $$ \\begin{array}{rcrcrcrcrcrcr} z & = & -225 / 2 & + & (27 / 4)x_1 & & & - & (10 / 4)x_4 & + & (1 / 4)x_5 & - & (5 / 4)x_6 \\\\ x_0 & = & 225 / 2 & - & (27 / 4)x_1 & & & + & (10 / 4)x_4 & - & (1 / 4)x_5 & + & (5 / 4)x_6 \\\\ x_2 & = & -50 / 8 & + & (7 / 8)x_1 & & & - & (10 / 8)x_4 & + & (1 / 8)x_5 & - & (1 / 8)x_6 \\\\ x_7 & = & 475 / 4 & - & (65 / 8)x_1 & + & 10x_3 & + & (54 / 8)x_4 & - & (7 / 8)x_5 & + & (15 / 8)x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The work gets rather messy, but $\\text{INITIALIZE-SIMPLEX}$ does eventually give a feasible solution to the linear program, and after running the simplex method we find that $(x_1, x_2, x_3, x_4) = (175 / 11, 225 / 22, 125 / 44, 0)$ is an optimal solution to the original linear programming problem. 29.5-9 Consider the following $1$-variable linear program, which we call $P$: $$ \\begin{array}{lrcrl} \\text{maximize} & tx \\\\ \\text{subject to} & rx & \\le & s \\\\ & x & \\ge & 0 & , \\end{array} $$ where $r$, $s$, and $t$ are arbitrary real numbers. Let $D$ be the dual of $P$. State for which values of $r$, $s$, and $t$ you can assert that Both $P$ and $D$ have optimal solutions with finite objective values. $P$ is feasible, but $D$ is infeasible. $D$ is feasible, but $P$ is infeasible. Neither $P$ nor $D$ is feasible. One option is that $r = 0$, $s \\ge 0$ and $t \\le 0$. Suppose that $r > 0$, then, if we have that $s$ is non-negative and $t$ is non-positive, it will be as we want. We will split into two cases based on $r$. If $r = 0$, then this is exactly when $t$ is non-positive and $s$ is non-negative. The other possible case is that $r$ is negative, and $t$ is positive. In which case, because $r$ is negative, we can always get $rx$ as small as we want so s doesn't matter, however, we can never make $rx$ positive so it can never be $\\ge t$. Again, we split into two possible cases for $r$. If $r = 0$, then it is when $t$ is nonnegative and $s$ is non-positive. The other possible case is that $r$ is positive, and $s$ is negative. Since $r$ is positive, $rx$ will always be non-negative, so it cannot be $\\le s$. But since $r$ is positive, we have that we can always make $rx$ as big as we want, in particular, greater than $t$. If we have that $r = 0$ and $t$ is positive and $s$ is negative. If $r$ is nonzero, then we can always either make $rx$ really big or really small depending on the sign of $r$, meaning that either the primal or the dual would be feasable.","title":"29.5 The initial basic feasible solution"},{"location":"Chap29/29.5/#295-1","text":"Give detailed pseudocode to implement lines 5 and 14 of $\\text{INITIALIZE-SIMPLEX}$. For line 5, first let $(N, B, A, b, c, v)$ be the result of calling $\\text{PIVOT}$ on $L_{aux}$ using $x_0$ as the entering variable. Then repeatedly call $\\text{PIVOT}$ until an optimal solution to $L_{aux}$ is obtained, and return this to $(N, B, A, b, c, v)$. To remove $x_0$ from the constraints, set $a_{i, 0} = 0$ for all $i \\in B$, and set $N = N \\backslash \\{0\\}$. To restore the original objective function of $L$, for each $j \\in N$ and each $i \\in B$, set $c_j = c_j \u2212 c_ia_{ij}$.","title":"29.5-1"},{"location":"Chap29/29.5/#295-2","text":"Show that when the main loop of $\\text{SIMPLEX}$ is run by $\\text{INITIALIZE-SIMPLEX}$, it can never return \"unbounded.\" In order to enter line 10 of $\\text{INITIALIZE-SIMPLEX}$ and begin iterating the main loop of $\\text{SIMPLEX}$, we must have recovered a basic solution which is feasible for $L_{aux}$. Since $x_0 \\ge 0$ and the objective function is $\u2212x_0$, the objective value associated to this solution (or any solution) must be negative. Since the goal is to aximize, we have an upper bound of $0$ on the objective value. By Lemma 29.2, $\\text{SIMPLEX}$ correctly determines whether or not the input linear program is unbounded. Since $L_{aux}$ is not unbounded, this can never be returned by $\\text{SIMPLEX}$.","title":"29.5-2"},{"location":"Chap29/29.5/#295-3","text":"Suppose that we are given a linear program $L$ in standard form, and suppose that for both $L$ and the dual of $L$, the basic solutions associated with the initial slack forms are feasible. Show that the optimal objective value of $L$ is $0$. Since it is in standard form, the objective function has no constant term, it is entirely given by $\\sum_{i = 1}^n c_ix_i$, which is going to be zero for any basic solution. The same thing goes for its dual. Since there is some solution which has the objective function achieve the same value both for the dual and the primal, by the corollary to the weak duality theorem, that common value must be the optimal value of the objective function.","title":"29.5-3"},{"location":"Chap29/29.5/#295-4","text":"Suppose that we allow strict inequalities in a linear program. Show that in this case, the fundamental theorem of linear programming does not hold. Consider the linear program in which we wish to maximize $x_1$ subject to the constraint $x_1 < 1$ and $x_1 \\ge 0$. This has no optimal solution, but it is clearly bounded and has feasible solutions. Thus, the Fundamental theorem of linear programming does not hold in the case of strict inequalities.","title":"29.5-4"},{"location":"Chap29/29.5/#295-5","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 8 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & - & x_2 & \\le & 8 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 8 & + & x_0 & - & x_1 & + & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 11 & - & 2x_1 & & & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, so we repeatedly call $\\text{PIVOT}$ to get the optimal solution to $L_{aux}$. We'll choose $x_1$ to be our entering variable and $x_0$ to be the leaving variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & & & -x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_0 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is now optimal for $L_{aux}$, so we return this slack form to $\\text{SIMPLEX}$, set $x_0 = 0$, and update the objective function which yields $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_5$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1 & = & 2 & + & (1 / 5)x_4 & + & (1 / 5)x_5 \\\\ x_2 & = & 1 & + & (4 / 5)x_4 & - & (1 / 5)x_5 \\\\ x_3 & = & 7 & - & (3 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_4$ as our entering variable, which makes $x_3$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & (64 / 3) & - & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & (34 / 3) & - & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & (10 / 3) & - & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & (35 / 3) & - & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Now all coefficients in the objective function are negative, so the basic solution is the optimal solution. It is $(x_1, x_2) = (34 / 3, 10 / 3)$.","title":"29.5-5"},{"location":"Chap29/29.5/#295-6","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & 2x_2 & \\le & 4 \\\\ & -2x_1 & - & 6x_2 & \\le & -12 \\\\ & & & x_2 & \\le & 1 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & + & 2x_2 & \\le & 4 \\\\ & -x_0 & - & 2x_1 & - & 6x_2 & \\le & -12 \\\\ & -x_0 & & & + & x_2 & \\le & 1 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 4 & + & x_0 & - & x_1 & - & 2x_2 \\\\ x_4 & = & -12 & + & x_0 & + & 2x_1 & + & 6x_2 \\\\ x_5 & = & 1 & + & x_0 & & & - & x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -12 & + & 2x_1 & + & 6x_2 & - & x_4 \\\\ x_0 & = & 12 & - & 2x_1 & - & 6x_2 & + & x_4 \\\\ x_3 & = & 16 & - & 3x_1 & - & 8x_2 & + & x_4 \\\\ x_5 & = & 13 & - & 2x_1 & - & 8x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is $(x_0, x_1, x_2, x_3, x_4, x_5) = (12, 0, 0, 16, 0, 13)$ which is feasible for the auxiliary program. Now we need to run $\\text{SIMPLEX}$ to find the optimal objective value to $L_{aux}$. Let $x_1$ be our next entering variable. It is most constrained by $x_3$, which will be our leaving variable. After $\\text{PIVOT}$, the new linear program is $$ \\begin{array}{rcrcrcrcr} z & = & -(4 / 3) & + & (2 / 3)x_2 & - & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0 & = & (4 / 3) & - & (2 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_1 & = & (16 / 3) & - & (8 / 3)x_2 & - & (1 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_5 & = & (7 / 3) & - & (8 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Every coefficient in the objective function is negative, so we take the basic solution $(x_0, x_1, x_2, x_3, x_4, x_5) = (4 / 3, 16 / 3, 0, 0, 0, 7 / 3)$ which is also optimal. Since $x_0 \\ne 0$, the original linear program must be unfeasible.","title":"29.5-6"},{"location":"Chap29/29.5/#295-7","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & - & x_1 & + & x_2 & \\le & -1 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & -1 & + & x_0 & + & x_1 & - & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Let $x_1$ be our entering variable. Then $x_0$ is our leaving variable, and we have $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, and optimal for $L_{aux}$, so we return this and run $\\text{SIMPLEX}$. Updating the objective function and setting $x_0 = 0$ gives $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_3$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & - & x_3 & + & 2x_4 \\\\ x_1 & = & 2 & + & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_2 & = & 1 & - & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_5 & = & & & (5 / 2)x_3 & - & (3 / 2)x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we use $x_4$ as our entering variable, which makes $x_5$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & 2 & + & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & 1 & + & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & & & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Finally, we would like to choose $x_3$ as our entering variable, but every coefficient on $x_3$ is positive, so $\\text{SIMPLEX}$ returns that the linear program is unbounded.","title":"29.5-7"},{"location":"Chap29/29.5/#295-8","text":"Solve the linear program given in $\\text{(29.6)}$\u2013$\\text{(29.10)}$. We first put the linear program in standard form, $$ \\begin{array}{lrcrcrcrcrl} \\text{maxmize} & x_1 & + & x_2 & + & x_3 & + & x_4 \\\\ \\text{subject to} & \\\\ & 2x_1 & - & 8x_2 & & & - & 10x_4 & \\le & -50 \\\\ & -5x_1 & - & 2x_2 & & & & & \\le & -100 \\\\ & -3x_1 & + & 5x_2 & - & 10x_3 & + & 2x_4 & \\le & -25 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program. $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & & - & x_0 \\\\ x_5 & = & -50 & + & x_0 & - & 2x_1 & + & 8x_2 & & & + & 10x_4 \\\\ x_6 & = & -100 & + & x_0 & + & 5x_1 & + & 2x_2 \\\\ x_7 & = & -25 & + & x_0 & + & 3x_1 & - & 5x_2 & + & 10x_3 & - & 2x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The index of the minimum $b_i$ is $2$, so we take $x_0$ to be our entering variable and $x_6$ to be our leaving variable. The call to $\\text{PIVOT}$ on line 8 yields $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & -100 & + & 5x_1 & + & 2x_2 & & & & & - & x_6 \\\\ x_0 & = & 100 & - & 5x_1 & - & 2x_2 & & & & & + & x_6 \\\\ x_5 & = & 50 & - & 7x_1 & + & 8x_2 & & & + & 10x_4 & + & x_6 \\\\ x_7 & = & 75 & - & 2x_1 & - & 7x_2 & + & 10x_3 & - & 2x_4 & + & x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ Next we'll take $x_2$ to be our entering variable and $x_5$ to be our leaving variable. The call to $\\text{PIVOT}$ yields $$ \\begin{array}{rcrcrcrcrcrcr} z & = & -225 / 2 & + & (27 / 4)x_1 & & & - & (10 / 4)x_4 & + & (1 / 4)x_5 & - & (5 / 4)x_6 \\\\ x_0 & = & 225 / 2 & - & (27 / 4)x_1 & & & + & (10 / 4)x_4 & - & (1 / 4)x_5 & + & (5 / 4)x_6 \\\\ x_2 & = & -50 / 8 & + & (7 / 8)x_1 & & & - & (10 / 8)x_4 & + & (1 / 8)x_5 & - & (1 / 8)x_6 \\\\ x_7 & = & 475 / 4 & - & (65 / 8)x_1 & + & 10x_3 & + & (54 / 8)x_4 & - & (7 / 8)x_5 & + & (15 / 8)x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The work gets rather messy, but $\\text{INITIALIZE-SIMPLEX}$ does eventually give a feasible solution to the linear program, and after running the simplex method we find that $(x_1, x_2, x_3, x_4) = (175 / 11, 225 / 22, 125 / 44, 0)$ is an optimal solution to the original linear programming problem.","title":"29.5-8"},{"location":"Chap29/29.5/#295-9","text":"Consider the following $1$-variable linear program, which we call $P$: $$ \\begin{array}{lrcrl} \\text{maximize} & tx \\\\ \\text{subject to} & rx & \\le & s \\\\ & x & \\ge & 0 & , \\end{array} $$ where $r$, $s$, and $t$ are arbitrary real numbers. Let $D$ be the dual of $P$. State for which values of $r$, $s$, and $t$ you can assert that Both $P$ and $D$ have optimal solutions with finite objective values. $P$ is feasible, but $D$ is infeasible. $D$ is feasible, but $P$ is infeasible. Neither $P$ nor $D$ is feasible. One option is that $r = 0$, $s \\ge 0$ and $t \\le 0$. Suppose that $r > 0$, then, if we have that $s$ is non-negative and $t$ is non-positive, it will be as we want. We will split into two cases based on $r$. If $r = 0$, then this is exactly when $t$ is non-positive and $s$ is non-negative. The other possible case is that $r$ is negative, and $t$ is positive. In which case, because $r$ is negative, we can always get $rx$ as small as we want so s doesn't matter, however, we can never make $rx$ positive so it can never be $\\ge t$. Again, we split into two possible cases for $r$. If $r = 0$, then it is when $t$ is nonnegative and $s$ is non-positive. The other possible case is that $r$ is positive, and $s$ is negative. Since $r$ is positive, $rx$ will always be non-negative, so it cannot be $\\le s$. But since $r$ is positive, we have that we can always make $rx$ as big as we want, in particular, greater than $t$. If we have that $r = 0$ and $t$ is positive and $s$ is negative. If $r$ is nonzero, then we can always either make $rx$ really big or really small depending on the sign of $r$, meaning that either the primal or the dual would be feasable.","title":"29.5-9"},{"location":"Chap29/Problems/29-1/","text":"Given a set of $m$ linear inequalities on $n$ variables $x_1, x_2, \\dots, x_n$, the linear-inequality feasibility problem asks whether there is a setting of the variables that simultaneously satisfies each of the inequalities. a. Show that if we have an algorithm for linear programming, we can use it to solve a linear-inequality feasibility problem. The number of variables and constraints that you use in the linear-programming problem should be polynomial in $n$ and $m$. b. Show that if we have an algorithm for the linear-inequality feasibility problem, we can use it to solve a linear-programming problem. The number of variables and linear inequalities that you use in the linear-inequality feasibility problem should be polynomial in $n$ and $m$, the number of variables and constraints in the linear program. a. We just let the linear inequalities that we need to satisfy be our set of constraints in the linear program. We let our function to maximize just be a constant. The solver for linear programs would fail to detect any feasible solution if the linear constraints were not feasible. If the linear programming solver returns any solution at all, we know that the linear constraints are feasible. b. Suppose that we are trying to solve the linear program in standard form with some particular $A$, $b$, $c$. That is, we want to maximize $\\sum_{j = 1}^n c_jx_j$ subject to $Ax \\le b$ and all entries of the $x$ vector are non-negative. Now, consider the dual program, that is, we want to minimize $\\sum_{i = 1}^m b_iy_i$ subject to $A^{\\text T} y \\ge c$ and all the entries in the $y$ vector are nonzero. We know by Corollary 29.9, if $x$ and $y$ are feasible solutions to their respective problems, then, if we have that their objective functions are equal, then, they are both optimal solutions. We can force their objective functions to be equal. To do this, let $c_k$ be some nonzero entry in the $c$ vector. If there are no nonzero entries, then the function we are trying to optimize is just the zero function, and it is exactly a feasibility question, so we we would be done. Then, we add two linear inequalities to require $x_k = \\frac{1}{c_k} \\Big(\\sum_{i = 1}^m b_iy_i - \\sum_{j = 1}^n c_jx_j \\Big)$. This will require that whatever values the variables take, their objective functions will be equal. Lastly, we just throw these in with the inequalities we already had. So, the constraints will be: $$ \\begin{aligned} Ax & \\le b \\\\ A^{\\text T} y & \\ge c \\\\ x_k & \\le \\frac{1}{c_k} \\Bigg(\\sum_{i = 1}^m b_iy_i - \\sum_{j = 1}^n c_jx_j \\Bigg) \\\\ x_k & \\ge \\frac{1}{c_k} \\Bigg(\\sum_{i = 1}^m b_iy_i - \\sum_{j = 1}^n c_jx_j \\Bigg) \\\\ x_1, x_2, \\dots, x_n, y_1, y_2, \\dots, y_m & \\ge 0. \\end{aligned} $$ We have a number of variables equal to $n + m$ and a number of constraints equal to $2 + 2n + 2m$, so both are polynomial in $n$ and $m$. Also, any assignment of variables which satisfy all of these constraints will be a feasible solution to both the problem and its dual that cause the respective objective functions to take the same value, and so, must be an optimal solution to both the original problem and its dual. This of course assumes that the linear inequality feasibility solver doesn't merely say that the inequalities are satisfiable, but actually returns a satisfying assignment. Lastly, it is necessary to note that if there is some optimal solution $x$, then, we can obtain an optimal solution for the dual that makes the objective functions equal by theorem 29.10. This ensures that the two constraints we added to force the objectives of the primal and the dual to be equal don't cause us to change the optimal solution to the linear program.","title":"29-1 Linear-inequality feasibility"},{"location":"Chap29/Problems/29-2/","text":"Complementary slackness describes a relationship between the values of primal variables and dual constraints and between the values of dual variables and primal constraints. Let $\\bar x$ be a feasible solution to the primal linear program given in $\\text{(29.16)\u2013(29.18)}$, and let $\\bar y$ be a feasible solution to the dual linear program given in $\\text{(29.83)\u2013(29.85)}$. Complementary slackness states that the following conditions are necessary and sufficient for $\\bar x$ and $\\bar y$ to be optimal: $$\\sum_{i = 1}^m a_{ij}\\bar y_i = c_j \\text{ or } \\bar x_j = 0 \\text{ for } j = 1, 2, \\dots, n$$ and $$\\sum_{j = 1}^m a_{ij}\\bar x_j = b_i \\text{ or } \\bar y_i = 0 \\text{ for } j = 1, 2, \\dots, m.$$ a. Verify that complementary slackness holds for the linear program in lines $\\text{(29.53)\u2013(29.57)}$. b. Prove that complementary slackness holds for any primal linear program and its corresponding dual. c. Prove that a feasible solution $\\bar x$ to a primal linear program given in lines $\\text{(29.16)\u2013(29.18)}$ is optimal if and only if there exist values $\\bar y = (\\bar y_1, \\bar y_2, \\dots, \\bar y_m)$ such that $\\bar y$ is a feasible solution to the dual linear program given in $\\text{(29.83)\u2013(29.85)}$, $\\sum_{i = 1}^m a_{ij}\\bar y_i = c_j$ for all $j$ such that $\\bar x_j > 0$, and $\\bar y_i = 0$ for all $i$ such that $\\sum_{j = 1}^n a_{ij}\\bar x_j < b_i$. a. An optimal solution to the LP program given in $\\text{(29.53)}$-$\\text{(29.57)}$ is $(x_1, x_2, x_3) = (8, 4, 0)$. An optimal solution to the dual is $(y_1, y_2, y_3) = (0, 1 / 6, 2 / 3)$. It is then straightforward to verify that the equations hold. b. First suppose that complementary slackness holds. Then the optimal objective value of the primal problem is, if it exists, $$ \\begin{aligned} \\sum_{k = 1} c_kx_k & = \\sum_{k = 1}^n \\sum_{i = 1}^m a_{ik}y_ix_k \\\\ & = \\sum_{i = 1}^m \\sum_{k = 1}^n a_{ik}x_ky_i \\\\ & = \\sum_{i = 1}^m b_iy_i, \\end{aligned} $$ which is precisely the optimal objective value of the dual problem. If any $x_j$ is $0$, then those terms drop out of them sum, so we can safely replace $c_k$ by whatever we like in those terms. Since the objective values are equal, they must be optimal. An identical argument shows that if an optimal solution exists for the dual problem then any feasible solution for the primal problem which satisfies the second equality of complementary slackness must also be optimal. Now suppose that $x$ and $y$ are optimal solutions, but that complementary slackness fails. In other words, there exists some $j$ such that $x_j \\ne 0$ but $\\sum_{i = 1}^m a_{ij}y_i > c_j$, or there exists some $i$ such that $y_i \\ne 0$ but $\\sum_{j = 1}^n a_{ij}x_j < b_i$. In the first case we have $$ \\begin{aligned} \\sum_{k = 1} c_kx_k & < \\sum_{k = 1}^n \\sum_{i = 1}^m a_{ik}y_ix_k \\\\ & = \\sum_{i = 1}^m \\sum_{k = 1}^n a_{ik}x_ky_i \\\\ & = \\sum_{i = 1}^m b_iy_i. \\end{aligned} $$ This implies that the optimal objective value of the primal solution is strictly less than the optimal value of the dual solution, a contradiction. The argument for the second case is identical. Thus, $x$ and $y$ are optimal solutions if and only if complementary slackness holds. c. This follows immediately from part (b). If $x$ is feasible and $y$ satisfies conditions 1, 2, and 3, then complementary slackness holds, so $x$ and $y$ are optimal. On the other hand, if $x$ is optimal, then the dual linear program must have an optimal solution $y$ as well, according to Theorem 29.10. Optimal solutions are feasible, and by part (b), $x$ and $y$ satisfy complementary slackness. Thus, conditions 1, 2, and 3 hold.","title":"29-2 Complementary slackness"},{"location":"Chap29/Problems/29-3/","text":"An integer linear-programming problem is a linear-programming problem with the additional constraint that the variables $x$ must take on integral values. Exercise 34.5-3 shows that just determining whether an integer linear program has a feasible solution is NP-hard, which means that there is no known polynomial-time algorithm for this problem. a. Show that weak duality (Lemma 29.8) holds for an integer linear program. b. Show that duality (Theorem 29.10) does not always hold for an integer linear program. c. Given a primal linear program in standard form, let us define $P$ to be the optimal objective value for the primal linear program, $D$ to be the optimal objective value for its dual, $IP$ to be the optimal objective value for the integer version of the primal (that is, the primal with the added constraint that the variables take on integer values), and $ID$ to be the optimal objective value for the integer version of the dual. Assuming that both the primal integer program and the dual integer program are feasible and bounded, show that $$IP \\le P = D \\le ID.$$ a. The proof for weak duality goes through identically. Nowhere in it does it use the integrality of the solutions. b. Consider the linear program given in standard form by $A = (1)$, $b = (\\frac{1}{2})$ and $c = (2)$. The highest we can get this is $0$ since that's that only value that $x$ can be. Now, consider the dual to this, that is, we are trying to minimize $\\frac{x}{2}$ subject to the constraint that $x \\ge 2$. This will be minimized when $x = 2$, so, the smallest solution we can get is $1$. Since we have just exhibited an example of a linear program that has a different optimal solution as it's dual, the duality theorem does not hold for integer linear program. c. The first inequality comes from looking at the fact that by adding the restriction that the solution must be integer valued, we obtain a set of feasible solutions that is a subset of the feasible solutions of the original primal linear program. Since, to get $IP$, we are taking the max over a subset of the things we are taking a max over to get $P$, we must get a number that is no larger. The third inequality is similar, except since we are taking min over a subset, the inequality goes the other way. The middle equality is given by Theorem 29.10.","title":"29-3 Integer linear programming"},{"location":"Chap29/Problems/29-4/","text":"Let $A$ be an $m \\times n$ matrix and $c$ be an $n$-vector. Then Farkas's lemma states that exactly one of the systems $$ \\begin{aligned} Ax & \\le 0, \\\\ c^Tx & > 0 \\end{aligned} $$ and $$ \\begin{aligned} A^Ty & = c, \\\\ y & \\le 0 \\end{aligned} $$ is solvable, where $x$ is an $n$-vector and $y$ is an $m$-vector. Prove Farkas's lemma. Suppose that both systems are solvable, let $x$ denote a solution to the first system, and $y$ denote a solution to the second. Taking transposes we have $x^{\\text T}A^{\\text T} \\le 0^{\\text T}$. Right multiplying by $y$ gives $x^{\\text T}c = x^{\\text T}A^{\\text T}y \\le 0^{\\text T}$, which is a contradiction to the fact that $c^{\\text T}x > 0$. Thus, both systems cannot be simultaneously solved. Now suppose that the second system fails. Consider the following linear program: $$\\text{maximize } 0x \\text{ subject to } A^{\\text T}y = c \\text{ and } y \\ge 0,$$ and its corresponding dual program $$\\text{minimize } -c^{\\text T}x \\text{ subject to } Ax \\le 0.$$ Since the second system fails, the primal is infeasible. However, the dual is always feasible by taking $x = 0$. If there were a finite solution to the dual, then duality says there would also be a finite solution to the primal. Thus, the dual must be unbounded. Thus, there must exist a vector $x$ which makes $\u2212c^{\\text T}x$ arbitrarily small, implying that there exist vectors $x$ for which $c^{\\text T}x$ is strictly greater than $0$. Thus, there is always at least one solution.","title":"29-4 Farkas'ss lemma"},{"location":"Chap29/Problems/29-5/","text":"In this problem, we consider a variant of the minimum-cost-flow problem from Section 29.2 in which we are not given a demand, a source, or a sink. Instead, we are given, as before, a flow network and edge costs $a(u, v)$ flow is feasible if it satisfies the capacity constraint on every edge and flow conservation at every vertex. The goal is to find, among all feasible flows, the one of minimum cost. We call this problem the minimum-cost-circulation problem . a. Formulate the minimum-cost-circulation problem as a linear program. b. Suppose that for all edges $(u, v) \\in E$, we have $a(u, v) > 0$. Characterize an optimal solution to the minimum-cost-circulation problem. c. Formulate the maximum-flow problem as a minimum-cost-circulation problem linear program. That is given a maximum-flow problem instance $G = (V, E)$ with source $s$, sink $t$ and edge capacities $c$, create a minimum-cost-circulation problem by giving a (possibly different) network $G' = (V', E')$ with edge capacities $c'$ and edge costs $a'$ such that you can discern a solution to the maximum-flow problem from a solution to the minimum-cost-circulation problem. d. Formulate the single-source shortest-path problem as a minimum-cost-circulation problem linear program. a. This is exactly the linear program given in equations $\\text{(29.51)}$-$\\text{(29.52)}$ except that the equation on the third line of the constraints should be removed, and for the equation on the second line of the constraints, $u$ should be selected from all of $V$ instead of from $V \\backslash \\{s, t\\}$. b. If $a(u, v) > 0$ for every pair of vertices, then, there is no point in sending any flow at all. So, an optimal solution is just to have no flow. This obviously satisfies the capacity constraints, it also satisfies the conservation constraints because the flow into and out of each vertex is zero. c. We assume that the edge $(t, s)$ is not in $E$ because that would be a silly edge to have for a maximum flow from $s$ to $t$. If it is there, remove it and it won't decrease the maximum flow. Let $V' = V$ and $E' = E \\cup \\{(t, s)\\}$. For the edges of $E'$ that are in $E$, let the capacity be as it is in $E$ and let the cost be $0$. For the other edge, we set $c(t, s) = \\infty$ and $a(t, s) = -1$. Then, if we have any circulation in $G'$, it will be trying to get as much flow to go across the edge $(t, s)$ in order to drive the objective function lower, the other flows will have no affect on the objective function. Then, by Kirchhoff's current law (a.k.a. common sense), the amount going across the edge $(t, s)$ is the same as the total flow in the rest of the network from $s$ to $t$. This means that maximizing the flow across the edge $(t, s)$ is also maximizing the flow from $s$ to $t$. So, all we need to do to recover the maximum flow for the original network is to keep the same flow values, but throw away the edge $(t, s)$. d. Suppose that $s$ is the vertex that we are computing shortest distance from. Then, we make the circulation network by first starting with the original graph, giving each edge a cost of whatever it was before and infinite capacity. Then, we place an edge going from every vertex that isn't $s$ to $s$ that has a capacity of $1$ and a cost of $-|E|$ times the largest cost appearing among all the edge costs already in the graph. Giving it such a negative cost ensures that placing other flow through the network in order to get a unit of flow across it will cause the total cost to decrease. Then, to recover the shortest path for any vertex, start at that vertex and then go to any vertex that is sending a unit of flow to it. Repeat this until you've reached $s$.","title":"29-5 Minimum-cost circulation"},{"location":"Chap30/30.1/","text":"30.1-1 Multiply the polynomials $A(x) = 7x^3 - x^2 + x - 10$ and $B(x) = 8x^3 - 6x + 3$ using equations $\\text{(30.1)}$ and $\\text{(30.2)}$. $$ \\begin{array}{rl} & 56x^6 - 8x^5 + (8 - 42)x^4 + (-80 + 6 + 21)x^3 + (-3 - 6)x^2 + (60 + 3)x - 30 \\\\ = & 56x^6 - 8x^5 - 34x^4 - 53x^3 - 9x^2 + 63x - 30. \\end{array} $$ 30.1-2 Another way to evaluate a polynomial $A(x)$ of degree-bound $n$ at a given point $x_0$ is to divide $A(x)$ by the polynomial $(x - x_0)$, obtaining a quotient polynomial $q(x)$ of degree-bound $n - 1$ and a remainder $r$, such that $$A(x) = q(x)(x - x_0) + r.$$ Clearly, $A(x_0) = r$. Show how to compute the remainder $r$ and the coefficients of $q(x)$ in time $\\Theta(n)$ from $x_0$ and the coefficients of $A$. Let $A$ be the matrix with $1$'s on the diagonal, $\u2212x_0$'s on the super diagonal, and $0$'s everywhere else. Let $q$ be the vector $(r, q_0, q_1, \\dots, x_{n \u2212 2})$. If $a = (a_0, a_1, \\dots, a_{n \u2212 1})$ then we need to solve the matrix equation $Aq = a$ to compute the remainder and coefficients. Since $A$ is tridiagonal, Problem 28-1 (e) tells us how to solve this equation in linear time. 30.1-3 Derive a point-value representation for $A^\\text{rev}(x) = \\sum_{j = 0}^{n - 1} a_{n - 1 - j}x^j$ from a point-value representation for $A(x) = \\sum_{j = 0}^{n - 1} a_jx^j$, assuming that none of the points is $0$. For each pair of points, $(p, A(p))$, we can compute the pair $(\\frac{1}{p}, A^{rev}(\\frac{1}{p}))$. To do this, we note that $$ \\begin{aligned} A^{rev}(\\frac{1}{p}) & = \\sum_{j = 0}^{n - 1} a_{n - 1 - j} (\\frac{1}{p})^j \\\\ & = \\sum_{j = 0}^{n - 1} a_j(\\frac{1}{p})^{n - 1 - j} \\\\ & = p^{1 - n} \\sum_{j = 0}^{n - 1} a_jp^j \\\\ & = p^{1 - n}A(p), \\end{aligned} $$ since we know what $A(p)$ is, we can compute $A^{rev}(\\frac{1}{p})$ of course, we are using the fact that $p \\ne 0$ because we are dividing by it. Also, we know that each of these points are distinct, because $\\frac{1}{p} = \\frac{1}{p'}$ implies that $p = p'$ by cross multiplication. So, since all the $x$ values were distinct in the point value representation of $A$, they will be distinct in this point value representation of $A^{rev}$ that we have made. 30.1-4 Prove that $n$ distinct point-value pairs are necessary to uniquely specify a polynomial of degree-bound $n$, that is, if fewer than $n$ distinct point-value pairs are given, they fail to specify a unique polynomial of degree-bound $n$. ($\\textit{Hint:}$ Using Theorem 30.1, what can you say about a set of $n - 1$ point-value pairs to which you add one more arbitrarily chosen point-value pair?) Suppose that just $n \u2212 1$ point-value pairs uniquely determine a polynomial $P$ which satisfies them. Append the point value pair $(x_{n - 1}, y_{n \u2212 1})$ to them, and let $P'$ be the unique polynomial which agrees with the $n$ pairs, given by Theorem 30.1. Now append instead $(x_{n - 1}, y'_{n \u2212 1})$ where $y_{n \u2212 1} \\ne y'_{n - 1}$, and let $P''$ be the polynomial obtained from these points via Theorem 30.1. Since polynomials coming from $n$ pairs are unique, $P' \\ne P''$. However, $P'$ and $P''$ agree on the original $n \u2212 1$ point-value pairs, contradicting the fact that $P$ was determined uniquely. 30.1-5 Show how to use equation $\\text{(30.5)}$ to interpolate in time $\\Theta(n^2)$. ($\\textit{Hint:}$ First compute the coefficient representation of the polynomial $\\prod_j (x - x_j)$ and then divide by $(x - x_k)$ as necessary for the numerator of each term; see Exercise 30.1-2. You can compute each of the $n$ denominators in time $O(n)$.) First, we show that we can compute the coefficient representation of $\\prod_j (x - x_j)$ in time $\\Theta(n^2)$. We will do it by recursion, showing that multiplying $\\prod_{j < k} (x \u2212 x_j)$ by $(x \u2212 x_k)$ only takes time $O(n)$, since this only needs to be done $n$ times, this gets is total runtime of $O(n)$. Suppose that $\\sum_{i = 0}^{k - 1} k_ix^i$ is a coefficient representation of $\\prod_{j < k} (x \u2212 x_j)$. To multiply this by $(x \u2212 x_k)$, we just set $(k + 1)_i = k_{i \u2212 1} - x_kk_i$ for $i = 1, \\dots, k$ and $(k + 1)_0 = \u2212x_k \\cdot k_0$. Each of these coefficients can be computed in constant time, since there are only linearly many coefficients, then, the time to compute the next partial product is just $O(n)$. Now that we have a coefficient representation of $\\prod_j (x \u2212 x_j)$, we need to compute, for each $k \\prod_{j \u2212 k} (x \u2212 x_j)$, each of which can be computed in time $\\Theta(n)$ by problem 30.1-2. Since the polynomial is defined as a product of things containing the thing we are dividing by, we have that the remainder in each case is equal to $0$. Lets call these polynomials $f_k$. Then, we need only compute the sum $\\sum_k y_k \\frac{f_k(x)}{f_k(x_k)}$. That is, we compute $f(x_k)$ each in time $\\Theta(n)$, so all told, only $\\Theta(n^2)$ time is spent computing all the $f(x_k)$ values. For each of the terms in the sum, dividing the polynomial $f_k(x)$ by the number $f_k(x_k)$ and multiplying by $y_k$ only takes time $\\Theta(n)$, so total it takes time $\\Theta(n^2)$. Lastly, we are adding up $n$ polynomials, each of degree bound $n \u2212 1$, so the total time taken there is $\\Theta(n^2)$. 30.1-6 Explain what is wrong with the \"obvious\" approach to polynomial division using a point-value representation, i.e., dividing the corresponding $y$ values. Discuss separately the case in which the division comes out exactly and the case in which it doesn't. If we wish to compute $P / Q$ but $Q$ takes on the value zero at some of these points, then we can't carry out the \"obvious\" method. However, as long as all point value pairs $(x_i, y_i)$ we choose for $Q$ are such that $y_i \\ne 0$, then the approach comes out exactly as we would like. 30.1-7 Consider two sets $A$ and $B$, each having $n$ integers in the range from $0$ to $10n$. We wish to compute the Cartesian sum of $A$ and $B$, defined by $$C = \\{x + y: x \\in A \\text{ and } y \\in B\\}.$$ Note that the integers in $C$ are in the range from $0$ to $20n$. We want to find the elements of $C$ and the number of times each element of $C$ is realized as a sum of elements in $A$ and $B$. Show how to solve the problem in $O(n\\lg n)$ time. ($\\textit{Hint:}$ Represent $A$ and $B$ as polynomials of degree at most $10n$.) For the set $A$, we define the polynomial $f_A$ to have a coefficient representation that has $a_i$ equal zero if $i \\notin A$ and equal to $1$ if $i \\in A$. Similarly define $f_B$. Then, we claim that looking at $f_C := f_A \\cdot f_B$ in coefficient form, we have that the $i$th coefficient, $c_i$ is exactly equal to the number of times that $i$ is realized as a sum of elements from $A$ and $B$. Since we can perform the polynomial multiplication in time $O(n \\lg n)$ by the methods of this chapter, we can get the final answer in time $O(n \\lg n)$. To see that $f_C$ has the nice property described, we'll look at the ways that we could end up having a term of $x^i$ appear. Each contribution to that coefficient must come from there being some $k$ so that $a_k \\ne 0$ and $b_{i \u2212 k} \\ne 0$, because the powers of $x$ attached to each are additive when we multiply. Since each of these contributions is only ever $1$, the final coefficient is counting the total number of such contributions, therefore counting the number of $k \\in A$ such that $\u2212k \\in B$, which is exactly what we claimed $f_C$ was counting.","title":"30.1 Representing polynomials"},{"location":"Chap30/30.1/#301-1","text":"Multiply the polynomials $A(x) = 7x^3 - x^2 + x - 10$ and $B(x) = 8x^3 - 6x + 3$ using equations $\\text{(30.1)}$ and $\\text{(30.2)}$. $$ \\begin{array}{rl} & 56x^6 - 8x^5 + (8 - 42)x^4 + (-80 + 6 + 21)x^3 + (-3 - 6)x^2 + (60 + 3)x - 30 \\\\ = & 56x^6 - 8x^5 - 34x^4 - 53x^3 - 9x^2 + 63x - 30. \\end{array} $$","title":"30.1-1"},{"location":"Chap30/30.1/#301-2","text":"Another way to evaluate a polynomial $A(x)$ of degree-bound $n$ at a given point $x_0$ is to divide $A(x)$ by the polynomial $(x - x_0)$, obtaining a quotient polynomial $q(x)$ of degree-bound $n - 1$ and a remainder $r$, such that $$A(x) = q(x)(x - x_0) + r.$$ Clearly, $A(x_0) = r$. Show how to compute the remainder $r$ and the coefficients of $q(x)$ in time $\\Theta(n)$ from $x_0$ and the coefficients of $A$. Let $A$ be the matrix with $1$'s on the diagonal, $\u2212x_0$'s on the super diagonal, and $0$'s everywhere else. Let $q$ be the vector $(r, q_0, q_1, \\dots, x_{n \u2212 2})$. If $a = (a_0, a_1, \\dots, a_{n \u2212 1})$ then we need to solve the matrix equation $Aq = a$ to compute the remainder and coefficients. Since $A$ is tridiagonal, Problem 28-1 (e) tells us how to solve this equation in linear time.","title":"30.1-2"},{"location":"Chap30/30.1/#301-3","text":"Derive a point-value representation for $A^\\text{rev}(x) = \\sum_{j = 0}^{n - 1} a_{n - 1 - j}x^j$ from a point-value representation for $A(x) = \\sum_{j = 0}^{n - 1} a_jx^j$, assuming that none of the points is $0$. For each pair of points, $(p, A(p))$, we can compute the pair $(\\frac{1}{p}, A^{rev}(\\frac{1}{p}))$. To do this, we note that $$ \\begin{aligned} A^{rev}(\\frac{1}{p}) & = \\sum_{j = 0}^{n - 1} a_{n - 1 - j} (\\frac{1}{p})^j \\\\ & = \\sum_{j = 0}^{n - 1} a_j(\\frac{1}{p})^{n - 1 - j} \\\\ & = p^{1 - n} \\sum_{j = 0}^{n - 1} a_jp^j \\\\ & = p^{1 - n}A(p), \\end{aligned} $$ since we know what $A(p)$ is, we can compute $A^{rev}(\\frac{1}{p})$ of course, we are using the fact that $p \\ne 0$ because we are dividing by it. Also, we know that each of these points are distinct, because $\\frac{1}{p} = \\frac{1}{p'}$ implies that $p = p'$ by cross multiplication. So, since all the $x$ values were distinct in the point value representation of $A$, they will be distinct in this point value representation of $A^{rev}$ that we have made.","title":"30.1-3"},{"location":"Chap30/30.1/#301-4","text":"Prove that $n$ distinct point-value pairs are necessary to uniquely specify a polynomial of degree-bound $n$, that is, if fewer than $n$ distinct point-value pairs are given, they fail to specify a unique polynomial of degree-bound $n$. ($\\textit{Hint:}$ Using Theorem 30.1, what can you say about a set of $n - 1$ point-value pairs to which you add one more arbitrarily chosen point-value pair?) Suppose that just $n \u2212 1$ point-value pairs uniquely determine a polynomial $P$ which satisfies them. Append the point value pair $(x_{n - 1}, y_{n \u2212 1})$ to them, and let $P'$ be the unique polynomial which agrees with the $n$ pairs, given by Theorem 30.1. Now append instead $(x_{n - 1}, y'_{n \u2212 1})$ where $y_{n \u2212 1} \\ne y'_{n - 1}$, and let $P''$ be the polynomial obtained from these points via Theorem 30.1. Since polynomials coming from $n$ pairs are unique, $P' \\ne P''$. However, $P'$ and $P''$ agree on the original $n \u2212 1$ point-value pairs, contradicting the fact that $P$ was determined uniquely.","title":"30.1-4"},{"location":"Chap30/30.1/#301-5","text":"Show how to use equation $\\text{(30.5)}$ to interpolate in time $\\Theta(n^2)$. ($\\textit{Hint:}$ First compute the coefficient representation of the polynomial $\\prod_j (x - x_j)$ and then divide by $(x - x_k)$ as necessary for the numerator of each term; see Exercise 30.1-2. You can compute each of the $n$ denominators in time $O(n)$.) First, we show that we can compute the coefficient representation of $\\prod_j (x - x_j)$ in time $\\Theta(n^2)$. We will do it by recursion, showing that multiplying $\\prod_{j < k} (x \u2212 x_j)$ by $(x \u2212 x_k)$ only takes time $O(n)$, since this only needs to be done $n$ times, this gets is total runtime of $O(n)$. Suppose that $\\sum_{i = 0}^{k - 1} k_ix^i$ is a coefficient representation of $\\prod_{j < k} (x \u2212 x_j)$. To multiply this by $(x \u2212 x_k)$, we just set $(k + 1)_i = k_{i \u2212 1} - x_kk_i$ for $i = 1, \\dots, k$ and $(k + 1)_0 = \u2212x_k \\cdot k_0$. Each of these coefficients can be computed in constant time, since there are only linearly many coefficients, then, the time to compute the next partial product is just $O(n)$. Now that we have a coefficient representation of $\\prod_j (x \u2212 x_j)$, we need to compute, for each $k \\prod_{j \u2212 k} (x \u2212 x_j)$, each of which can be computed in time $\\Theta(n)$ by problem 30.1-2. Since the polynomial is defined as a product of things containing the thing we are dividing by, we have that the remainder in each case is equal to $0$. Lets call these polynomials $f_k$. Then, we need only compute the sum $\\sum_k y_k \\frac{f_k(x)}{f_k(x_k)}$. That is, we compute $f(x_k)$ each in time $\\Theta(n)$, so all told, only $\\Theta(n^2)$ time is spent computing all the $f(x_k)$ values. For each of the terms in the sum, dividing the polynomial $f_k(x)$ by the number $f_k(x_k)$ and multiplying by $y_k$ only takes time $\\Theta(n)$, so total it takes time $\\Theta(n^2)$. Lastly, we are adding up $n$ polynomials, each of degree bound $n \u2212 1$, so the total time taken there is $\\Theta(n^2)$.","title":"30.1-5"},{"location":"Chap30/30.1/#301-6","text":"Explain what is wrong with the \"obvious\" approach to polynomial division using a point-value representation, i.e., dividing the corresponding $y$ values. Discuss separately the case in which the division comes out exactly and the case in which it doesn't. If we wish to compute $P / Q$ but $Q$ takes on the value zero at some of these points, then we can't carry out the \"obvious\" method. However, as long as all point value pairs $(x_i, y_i)$ we choose for $Q$ are such that $y_i \\ne 0$, then the approach comes out exactly as we would like.","title":"30.1-6"},{"location":"Chap30/30.1/#301-7","text":"Consider two sets $A$ and $B$, each having $n$ integers in the range from $0$ to $10n$. We wish to compute the Cartesian sum of $A$ and $B$, defined by $$C = \\{x + y: x \\in A \\text{ and } y \\in B\\}.$$ Note that the integers in $C$ are in the range from $0$ to $20n$. We want to find the elements of $C$ and the number of times each element of $C$ is realized as a sum of elements in $A$ and $B$. Show how to solve the problem in $O(n\\lg n)$ time. ($\\textit{Hint:}$ Represent $A$ and $B$ as polynomials of degree at most $10n$.) For the set $A$, we define the polynomial $f_A$ to have a coefficient representation that has $a_i$ equal zero if $i \\notin A$ and equal to $1$ if $i \\in A$. Similarly define $f_B$. Then, we claim that looking at $f_C := f_A \\cdot f_B$ in coefficient form, we have that the $i$th coefficient, $c_i$ is exactly equal to the number of times that $i$ is realized as a sum of elements from $A$ and $B$. Since we can perform the polynomial multiplication in time $O(n \\lg n)$ by the methods of this chapter, we can get the final answer in time $O(n \\lg n)$. To see that $f_C$ has the nice property described, we'll look at the ways that we could end up having a term of $x^i$ appear. Each contribution to that coefficient must come from there being some $k$ so that $a_k \\ne 0$ and $b_{i \u2212 k} \\ne 0$, because the powers of $x$ attached to each are additive when we multiply. Since each of these contributions is only ever $1$, the final coefficient is counting the total number of such contributions, therefore counting the number of $k \\in A$ such that $\u2212k \\in B$, which is exactly what we claimed $f_C$ was counting.","title":"30.1-7"},{"location":"Chap30/30.2/","text":"30.2-1 Prove Corollary 30.4. (Omit!) 30.2-2 Compute the $\\text{DFT}$ of the vector $(0, 1, 2, 3)$. (Omit!) 30.2-3 Do Exercise 30.1-1 by using the $\\Theta(n\\lg n)$-time scheme. (Omit!) 30.2-4 Write pseudocode to compute $\\text{DFT}_n^{-1}$ in $\\Theta(n\\lg n)$ time. (Omit!) 30.2-5 Describe the generalization of the $\\text{FFT}$ procedure to the case in which $n$ is a power of $3$. Give a recurrence for the running time, and solve the recurrence. (Omit!) 30.2-6 $\\star$ Suppose that instead of performing an $n$-element $\\text{FFT}$ over the field of complex numbers (where $n$ is even), we use the ring $\\mathbb Z_m$ of integers modulo $m$, where $m = 2^{tn / 2} + 1$ and $t$ is an arbitrary positive integer. Use $\\omega = 2^t$ instead of $\\omega_n$ as a principal nth root of unity, modulo $m$. Prove that the $\\text{DFT}$ and the inverse $\\text{DFT}$ are well defined in this system. (Omit!) 30.2-7 Given a list of values $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions), show how to find the coefficients of a polynomial $P(x)$ of degree-bound $n + 1$ that has zeros only at $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions). Your procedure should run in time $O(n\\lg^2 n)$. ($\\textit{Hint:}$ The polynomial $P(x)$ has a zero at $z_j$ if and only if $P(x)$ is a multiple of $(x - z_j)$.) (Omit!) 30.2-8 $\\star$ The chirp transform of a vector $a = (a_0, a_1, \\dots, a_{n - 1})$ is the vector $y = (y_0, y_1, \\dots, y_{n - 1})$, where $y_k = \\sum_{j = 0}^{n - 1} a_jz^{kj}$ and $z$ is any complex number. The $\\text{DFT}$ is therefore a special case of the chirp transform, obtained by taking $z = \\omega_n$. Show how to evaluate the chirp transform in time $O(n\\lg n)$ for any complex number $z$. ($\\textit{Hint:}$ Use the equation $$y_k = z^{k^2 / 2} \\sum_{j = 0}^{n - 1} \\Big(a_jz^{j^2 / 2}\\Big) \\Big(z^{-(k - j)^2 / 2}\\Big)$$ to view the chirp transform as a convolution.) (Omit!)","title":"30.2 The DFT and FFT"},{"location":"Chap30/30.2/#302-1","text":"Prove Corollary 30.4. (Omit!)","title":"30.2-1"},{"location":"Chap30/30.2/#302-2","text":"Compute the $\\text{DFT}$ of the vector $(0, 1, 2, 3)$. (Omit!)","title":"30.2-2"},{"location":"Chap30/30.2/#302-3","text":"Do Exercise 30.1-1 by using the $\\Theta(n\\lg n)$-time scheme. (Omit!)","title":"30.2-3"},{"location":"Chap30/30.2/#302-4","text":"Write pseudocode to compute $\\text{DFT}_n^{-1}$ in $\\Theta(n\\lg n)$ time. (Omit!)","title":"30.2-4"},{"location":"Chap30/30.2/#302-5","text":"Describe the generalization of the $\\text{FFT}$ procedure to the case in which $n$ is a power of $3$. Give a recurrence for the running time, and solve the recurrence. (Omit!)","title":"30.2-5"},{"location":"Chap30/30.2/#302-6-star","text":"Suppose that instead of performing an $n$-element $\\text{FFT}$ over the field of complex numbers (where $n$ is even), we use the ring $\\mathbb Z_m$ of integers modulo $m$, where $m = 2^{tn / 2} + 1$ and $t$ is an arbitrary positive integer. Use $\\omega = 2^t$ instead of $\\omega_n$ as a principal nth root of unity, modulo $m$. Prove that the $\\text{DFT}$ and the inverse $\\text{DFT}$ are well defined in this system. (Omit!)","title":"30.2-6 $\\star$"},{"location":"Chap30/30.2/#302-7","text":"Given a list of values $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions), show how to find the coefficients of a polynomial $P(x)$ of degree-bound $n + 1$ that has zeros only at $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions). Your procedure should run in time $O(n\\lg^2 n)$. ($\\textit{Hint:}$ The polynomial $P(x)$ has a zero at $z_j$ if and only if $P(x)$ is a multiple of $(x - z_j)$.) (Omit!)","title":"30.2-7"},{"location":"Chap30/30.2/#302-8-star","text":"The chirp transform of a vector $a = (a_0, a_1, \\dots, a_{n - 1})$ is the vector $y = (y_0, y_1, \\dots, y_{n - 1})$, where $y_k = \\sum_{j = 0}^{n - 1} a_jz^{kj}$ and $z$ is any complex number. The $\\text{DFT}$ is therefore a special case of the chirp transform, obtained by taking $z = \\omega_n$. Show how to evaluate the chirp transform in time $O(n\\lg n)$ for any complex number $z$. ($\\textit{Hint:}$ Use the equation $$y_k = z^{k^2 / 2} \\sum_{j = 0}^{n - 1} \\Big(a_jz^{j^2 / 2}\\Big) \\Big(z^{-(k - j)^2 / 2}\\Big)$$ to view the chirp transform as a convolution.) (Omit!)","title":"30.2-8 $\\star$"},{"location":"Chap30/30.3/","text":"30.3-1 Show how $\\text{ITERATIVE-FFT}$ computes the $\\text{DFT}$ of the input vector $(0, 2, 3, -1, 4, 5, 7, 9)$. (Omit!) 30.3-2 Show how to implement an $\\text{FFT}$ algorithm with the bit-reversal permutation occurring at the end, rather than at the beginning, of the computation. ($\\textit{Hint:}$ Consider the inverse $\\text{DFT}$.) (Omit!) 30.3-3 How many times does $\\text{ITERATIVE-FFT}$ compute twiddle factors in each stage? Rewrite $\\text{ITERATIVE-FFT}$ to compute twiddle factors only $2^{s - 1}$ times in stage $s$. (Omit!) 30.3-4 $\\star$ Suppose that the adders within the butterfly operations of the $\\text{FFT}$ circuit sometimes fail in such a manner that they always produce a zero output, independent of their inputs. Suppose that exactly one adder has failed, but that you don't know which one. Describe how you can identify the failed adder by supplying inputs to the overall $\\text{FFT}$ circuit and observing the outputs. How efficient is your method? (Omit!)","title":"30.3 Efficient FFT implementations"},{"location":"Chap30/30.3/#303-1","text":"Show how $\\text{ITERATIVE-FFT}$ computes the $\\text{DFT}$ of the input vector $(0, 2, 3, -1, 4, 5, 7, 9)$. (Omit!)","title":"30.3-1"},{"location":"Chap30/30.3/#303-2","text":"Show how to implement an $\\text{FFT}$ algorithm with the bit-reversal permutation occurring at the end, rather than at the beginning, of the computation. ($\\textit{Hint:}$ Consider the inverse $\\text{DFT}$.) (Omit!)","title":"30.3-2"},{"location":"Chap30/30.3/#303-3","text":"How many times does $\\text{ITERATIVE-FFT}$ compute twiddle factors in each stage? Rewrite $\\text{ITERATIVE-FFT}$ to compute twiddle factors only $2^{s - 1}$ times in stage $s$. (Omit!)","title":"30.3-3"},{"location":"Chap30/30.3/#303-4-star","text":"Suppose that the adders within the butterfly operations of the $\\text{FFT}$ circuit sometimes fail in such a manner that they always produce a zero output, independent of their inputs. Suppose that exactly one adder has failed, but that you don't know which one. Describe how you can identify the failed adder by supplying inputs to the overall $\\text{FFT}$ circuit and observing the outputs. How efficient is your method? (Omit!)","title":"30.3-4 $\\star$"},{"location":"Chap30/Problems/30-1/","text":"a. Show how to multiply two linear polynomials $ax + b$ and $cx + d$ using only three multiplications. ($\\textit{Hint:}$ One of the multiplications is $(a + b) \\cdot (c + d)$.) b. Give two divide-and-conquer algorithms for multiplying two polynomials of degree-bound $n$ in $\\Theta(n^{\\lg 3})$ time. The first algorithm should divide the input polynomial coefficients into a high half and a low half, and the second algorithm should divide them according to whether their index is odd or even. c. Show how to multiply two $n$-bit integers in $O(n^{\\lg 3})$ steps, where each step operates on at most a constant number of $1$-bit values. (Omit!)","title":"30-1 Divide-and-conquer multiplication"},{"location":"Chap30/Problems/30-2/","text":"A Toeplitz matrix is an $n \\times n$ matrix $A = (a_{ij})$ such that $a_{ij} = a_{i - 1, j - 1}$ for $i = 2, 3, \\dots, n$ and $j = 2, 3, \\dots, n$. a. Is the sum of two Toeplitz matrices necessarily Toeplitz? What about the product? b. Describe how to represent a Toeplitz matrix so that you can add two $n \\times n$ Toeplitz matrices in $O(n)$ time. c. Give an $O(n\\lg n)$-time algorithm for multiplying an $n \\times n$ Toeplitz matrix by a vector of length $n$. Use your representation from part (b). d. Give an efficient algorithm for multiplying two $n \\times n$ Toeplitz matrices. Analyze its running time. (Omit!)","title":"30-2 Toeplitz matrices"},{"location":"Chap30/Problems/30-3/","text":"We can generalize the $1$-dimensional discrete Fourier transform defined by equation $\\text{(30.8)}$ to $d$ dimensions. The input is a $d$-dimensional array $A = (a_{j_1, j_2, \\dots, j_d})$ whose dimensions are $n_1, n_2, \\dots, n_d$, where $n_1n_2 \\cdots n_d = n$. We define the $d$-dimensional discrete Fourier transform by the equation $$y_{k_1, k_2, \\dots, k_d} = \\sum_{j_1 = 0}^{n_1 - 1} \\sum_{j_2 = 0}^{n_2 - 1} \\cdots \\sum_{j_d = 0}^{n_d - 1} a_{j_1, j_2, \\cdots, j_d} \\omega_{n_1}^{j_1k_1}\\omega_{n_2}^{j_2k_2} \\cdots \\omega_{n_d}^{j_dk_d}$$ for $0 \\le k_1 < n_1, 0 \\le k_2 < n_2, \\dots, 0 \\le k_d < n_d$. a. Show that we can compute a $d$-dimensional $\\text{DFT}$ by computing $1$-dimensional $\\text{DFT}$s on each dimension in turn. That is, we first compute $n / n_1$ separate $1$-dimensional $\\text{DFT}$s along dimension $1$. Then, using the result of the $\\text{DFT}$s along dimension $1$ as the input, we compute $n / n_2$ separate $1$-dimensional $\\text{DFT}$s along dimension $2$. Using this result as the input, we compute $n / n_3$ separate $1$-dimensional $\\text{DFT}$s along dimension $3$, and so on, through dimension $d$. b. Show that the ordering of dimensions does not matter, so that we can compute a $d$-dimensional $\\text{DFT}$ by computing the $1$-dimensional $\\text{DFT}$s in any order of the $d$ dimensions. c. Show that if we compute each $1$-dimensional $\\text{DFT}$ by computing the fast Fourier transform, the total time to compute a $d$-dimensional $\\text{DFT}$ is $O(n\\lg n)$, independent of $d$. (Omit!)","title":"30-3 Multidimensional fast Fourier transform"},{"location":"Chap30/Problems/30-4/","text":"Given a polynomial $A(x)$ of degree-bound $n$, we define its $t$th derivative by $$ A^{(t)}(x) = \\begin{cases} A(x) & \\text{ if } t = 0, \\\\ \\frac{d}{dx} A^{(t - 1)}(x) & \\text{ if } 1 \\le t \\le n - 1, \\\\ 0 & \\text{ if } t \\ge n. \\end{cases} $$ From the coefficient representation $(a_0, a_1, \\dots, a_{n - 1})$ of $A(x)$ and a given point $x_0$, we wish to determine $A^{(t)}(x_0)$ for $t = 0, 1, \\dots, n- 1$. a. Given coefficients $b_0, b_1, \\dots, b_{n - 1}$ such that $$A(x) = \\sum_{j = 0}^{n - 1} b_j(x - x_0)^j,$$ show how to compute $A^{(t)}(x_0)$ for $t = 0, 1, \\dots, n - 1$, in $O(n)$ time. b. Explain how to find $b_0, b_1, \\dots, b_{n - 1}$ in $O(n\\lg n)$ time, given $A(x_0 + \\omega_n^k)$ for $k = 0, 1, \\dots, n - 1$. c. Prove that $$A(x_0 + \\omega_n^k) = \\sum_{r = 0}^{n - 1} \\Bigg(\\frac{\\omega_n^{kr}}{r!} \\sum_{j = 0}^{n - 1} f(j)g(r - j)\\Bigg),$$ where $f(j) = a_j \\cdot j!$ and $$ g(l) = \\begin{cases} x_0^{-l} / (-l)! & \\text{ if } -(n - 1) \\le l \\le 0, \\\\ 0 & \\text{ if } 1 \\le l \\le n - 1. \\end{cases} $$ d. Explain how to evaluate $A(x_0 + \\omega_n^k)$ for $k = 0, 1, \\dots, n - 1$ in $O(n\\lg n)$ time. Conclude that we can evaluate all nontrivial derivatives of $A(x)$ at $x_0$ in $O(n\\lg n)$ time. (Omit!)","title":"30-4 Evaluating all derivatives of a polynomial at a point"},{"location":"Chap30/Problems/30-5/","text":"We have seen how to evaluate a polynomial of degree-bound $n$ at a single point in $O(n)$ time using Horner's rule. We have also discovered how to evaluate such a polynomial at all $n$ complex roots of unity in $O(n\\lg n)$ time using the $\\text{FFT}$. We shall now show how to evaluate a polynomial of degree-bound $n$ at $n$ arbitrary points in $O(n\\lg^2 n)$ time. To do so, we shall assume that we can compute the polynomial remainder when one such polynomial is divided by another in $O(n\\lg n)$ time, a result that we state without proof. For example, the remainder of $3x^3 + x^2 - 3x + 1$ when divided by $x^2 + x + 2$ is $$(3x^3 + x^2 - 3x + 1) \\mod (x^2 + x + 2) = -7x + 5.$$ Given the coefficient representation of a polynomial $A(x) = \\sum_{k = 0}^{n - 1} a_kx^k$ and $n$ points $x_0, x_1, \\dots, x_{n - 1}$, we wish to compute the $n$ values $A(x_0), A(x_1), \\dots, A(x_{n - 1})$. For $0 \\le i \\le j \\le n - 1$, define the polynomials $P_{ij}(x) = \\prod_{k = i}^j (x - x_k)$ and $Q_{ij}(x) = A(x) \\mod P_{ij}(x)$. Note that $Q_{ij}(x)$ has degree at most $j - i$. a. Prove that $A(x) \\mod (x - z) = A(z)$ for any point $z$. b. Prove that $Q_{kk}(x) = A(x_k)$ and that $Q_{0, n - 1}(x) = A(x)$. c. Prove that for $i \\le k \\le j$, we have $Q_{ik}(x) = Q_{ij}(x) \\mod P_{ik}(x)$ and $Q_{kj}(x) = Q_{ij}(x) \\mod P_{kj}(x)$. d. Give an $O(n\\lg^2 n)$-time algorithm to evaluate $A(x_0), A(x_1), \\dots, A(x_{n - 1})$. (Omit!)","title":"30-5 Polynomial evaluation at multiple points"},{"location":"Chap30/Problems/30-6/","text":"As defined, the discrete Fourier transform requires us to compute with complex numbers, which can result in a loss of precision due to round-off errors. For some problems, the answer is known to contain only integers, and by using a variant of the $\\text{FFT}$ based on modular arithmetic, we can guarantee that the answer is calculated exactly. An example of such a problem is that of multiplying two polynomials with integer coefficients. Exercise 30.2-6 gives one approach, using a modulus of length $\\Omega(n)$ bits to handle a $\\text{DFT}$ on $n$ points. This problem gives another approach, which uses a modulus of the more reasonable length $O(\\lg n)$; it requires that you understand the material of Chapter 31. Let $n$ be a power of $2$. a. Suppose that we search for the smallest $k$ such that $p = kn + 1$ is prime. Give a simple heuristic argument why we might expect $k$ to be approximately $\\ln n$. (The value of $k$ might be much larger or smaller, but we can reasonably expect to examine $O(\\lg n)$ candidate values of $k$ on average.) How does the expected length of $p$ compare to the length of $n$? Let $g$ be a generator of $\\mathbb Z_p^*$, and let $w = g^k \\mod p$. b. Argue that the $\\text{DFT}$ and the inverse $\\text{DFT}$ are well-defined inverse operations modulo $p$, where $w$ is used as a principal $n$th root of unity. c. Show how to make the $\\text{FFT}$ and its inverse work modulo $p$ in time $O(n\\lg n)$, where operations on words of $O(\\lg n)$ bits take unit time. Assume that the algorithm is given $p$ and $w$. d. Compute the $\\text{DFT}$ modulo $p = 17$ of the vector $(0, 5, 3, 7, 7, 2, 1, 6)$. Note that $g = 3$ is a generator of $\\mathbb Z_{17}^*$. (Omit!)","title":"30-6 FFT using modular arithmetic"},{"location":"Chap31/31.1/","text":"31.1-1 Prove that if $a > b > 0$ and $c = a + b$, then $c \\mod a = b$. $$ \\begin{aligned} c \\mod a & = (a + b) \\mod a \\\\ & = (a \\mod a) + (b \\mod a) \\\\ & = 0 + b \\\\ & = b. \\end{aligned} $$ 31.1-2 Prove that there are infinitely many primes. $$ \\begin{aligned} ((p_1 p_2 \\cdots p_k) + 1) \\mod p_i & = (p_1 p_2 \\cdots p_k) \\mod p_i + (1 \\mod p_i) \\\\ & = 0 + 1 \\\\ & = 1. \\end{aligned} $$ 31.1-3 Prove that if $a \\mid b$ and $b \\mid c$, then $a \\mid c$. If $a \\mid b$, then $b = a \\cdot k_1$. If $b \\mid c$, then $c = b \\cdot k_2 = a \\cdot (k_1 \\cdot k_2) = a \\cdot k_3$, then $a \\mid c$. 31.1-4 Prove that if $p$ is prime and $0 < k < p$, then $\\gcd(k, p) = 1$. If $k \\ne 1$, then $k \\nmid p$. If $k = 1$, then the divisor is $1$. 31.1-5 Prove Corollary 31.5. For all positive integers $n$, $a$, and $b$, if $n \\mid ab$ and $\\gcd(a, n) = 1$, then $n \\mid b$. Since $n \\mid ab$, $$ \\begin{aligned} ab & = nk \\\\ b & = nk / a. \\end{aligned} $$ Since $\\gcd(a, n) = 1$, $n / a$ could not be an integer. Since $b$ is an integer, then $k / a$ must be an integer, we have $$b = nk / a = n (k / a) = n k',$$ therefore $n \\mid b$. 31.1-6 Prove that if $p$ is prime and $0 < k < p$, then $p \\mid \\binom{p}{k}$. Conclude that for all integers $a$ and $b$ and all primes $p$, $(a + b)^p \\equiv a^p + b^p (\\mod p)$. $$ \\begin{array}{rlll} (a + b) ^ p & \\equiv & a^p + \\binom{p}{1} a^{p - 1}b^{1} + \\cdots + \\binom{p}{p - 1} a^{1}b^{p - 1} + b^p & (\\mod p) \\\\ & \\equiv & a^p + 0 + \\cdots + 0 + b^p & (\\mod p) \\\\ & \\equiv & a^p + b^p & (\\mod p) \\end{array} $$ 31.1-7 Prove that if $a$ and $b$ are any positive integers such that $a \\mid b$, then $$(x \\mod b) \\mod a = x \\mod a$$ for any $x$. Prove, under the same assumptions, that $x \\equiv y (\\mod b)$ implies $x \\equiv y (\\mod a)$ for any integers $x$ and $y$. Suppose $x = kb + c$, we have $$(x \\mod b) \\mod a = c \\mod a,$$ and $$x \\mod a = (kb + c) \\mod a = (kb \\mod a) + (c \\mod a) = c \\mod a.$$ 31.1-8 For any integer $k > 0$, an integer $n$ is a $k$th power if there exists an integer $a$ such that $a^k = n$. Furthermore, $n > 1$ is a nontrivial power if it is a $k$th power for some integer $k > 1$. Show how to determine whether a given $\\beta$-bit integer $n$ is a nontrivial power in time polynomial in $\\beta$. Because $2^\\beta > n$, we only need to test values of $k$ that satisfy $2 \\le k < \\beta$, therefore the testing procedure remains $O(\\beta)$. For any nontrivial power $k$, where $2 \\le k < \\beta$, do a binary search on $a$ that costs $$O(\\log \\sqrt n) = O(\\log \\sqrt{2^\\beta}) = O(\\frac 1 2\\log 2^\\beta) = O(\\beta).$$ Thus, the total time complexity is $$O(\\beta) \\times O(\\beta) = O(\\beta^2).$$ 31.1-9 Prove equations $\\text{(31.6)}$\u2013$\\text{(31.10)}$. (Omit!) 31.1-10 Show that the gcd operator is associative. That is, prove that for all integers $a$, $b$, and $c$, $\\gcd(a, \\gcd(b, c)) = \\gcd(\\gcd(a, b), c)$. [The following proof is provided by my friend, Tony Xiao.] Let $d = \\gcd(a, b, c)$, $a = dp$, $b = dq$ and $c = dr$. Claim $\\gcd(a, \\gcd(b, c)) = d.$ Let $e = \\gcd(b, c)$, thus $$ \\begin{aligned} b = es, \\\\ c = et. \\end{aligned} $$ Since $d \\mid b$ and $d \\mid c$, thus $d \\mid e$. Let $e = dm$, thus $$ \\begin{aligned} b = (dm)s & = dq, \\\\ c = (dm)t & = dr. \\end{aligned} $$ Suppose $k = \\gcd(p, m)$, $$ \\begin{aligned} & k \\mid p, k \\mid m, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid dm, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid (dm)s, dk \\mid (dm)t, \\\\ \\Rightarrow & dk \\mid a, dk \\mid b, dk \\mid c. \\end{aligned} $$ Since $d = \\gcd(a, b, c)$, thus $k = 1$. $$ \\begin{aligned} \\gcd(a, \\gcd(b, c)) & = \\gcd(a, e) \\\\ & = \\gcd(dp, dm) \\\\ & = d \\cdot \\gcd(p, m) \\\\ & = d \\cdot k \\\\ & = d. \\end{aligned} $$ By the claim, $$\\gcd(a, \\gcd(b, c)) = d = \\gcd(\\gcd(a, b), c).$$ 31.1-11 $\\star$ Prove Theorem 31.8. (Omit!) 31.1-12 Give efficient algorithms for the operations of dividing a $\\beta$-bit integer by a shorter integer and of taking the remainder of a $\\beta$-bit integer when divided by a shorter integer. Your algorithms should run in time $\\Theta(\\beta^2)$. Shift left until the two numbers have the same length, then repeatedly subtract with proper multiplier and shift right. 31.1-13 Give an efficient algorithm to convert a given $\\beta$-bit (binary) integer to a decimal representation. Argue that if multiplication or division of integers whose length is at most $\\beta$ takes time $M(\\beta)$, then we can convert binary to decimal in time $\\Theta(M(\\beta) \\lg\\beta)$. (Omit!)","title":"31.1 Elementary number-theoretic notions"},{"location":"Chap31/31.1/#311-1","text":"Prove that if $a > b > 0$ and $c = a + b$, then $c \\mod a = b$. $$ \\begin{aligned} c \\mod a & = (a + b) \\mod a \\\\ & = (a \\mod a) + (b \\mod a) \\\\ & = 0 + b \\\\ & = b. \\end{aligned} $$","title":"31.1-1"},{"location":"Chap31/31.1/#311-2","text":"Prove that there are infinitely many primes. $$ \\begin{aligned} ((p_1 p_2 \\cdots p_k) + 1) \\mod p_i & = (p_1 p_2 \\cdots p_k) \\mod p_i + (1 \\mod p_i) \\\\ & = 0 + 1 \\\\ & = 1. \\end{aligned} $$","title":"31.1-2"},{"location":"Chap31/31.1/#311-3","text":"Prove that if $a \\mid b$ and $b \\mid c$, then $a \\mid c$. If $a \\mid b$, then $b = a \\cdot k_1$. If $b \\mid c$, then $c = b \\cdot k_2 = a \\cdot (k_1 \\cdot k_2) = a \\cdot k_3$, then $a \\mid c$.","title":"31.1-3"},{"location":"Chap31/31.1/#311-4","text":"Prove that if $p$ is prime and $0 < k < p$, then $\\gcd(k, p) = 1$. If $k \\ne 1$, then $k \\nmid p$. If $k = 1$, then the divisor is $1$.","title":"31.1-4"},{"location":"Chap31/31.1/#311-5","text":"Prove Corollary 31.5. For all positive integers $n$, $a$, and $b$, if $n \\mid ab$ and $\\gcd(a, n) = 1$, then $n \\mid b$. Since $n \\mid ab$, $$ \\begin{aligned} ab & = nk \\\\ b & = nk / a. \\end{aligned} $$ Since $\\gcd(a, n) = 1$, $n / a$ could not be an integer. Since $b$ is an integer, then $k / a$ must be an integer, we have $$b = nk / a = n (k / a) = n k',$$ therefore $n \\mid b$.","title":"31.1-5"},{"location":"Chap31/31.1/#311-6","text":"Prove that if $p$ is prime and $0 < k < p$, then $p \\mid \\binom{p}{k}$. Conclude that for all integers $a$ and $b$ and all primes $p$, $(a + b)^p \\equiv a^p + b^p (\\mod p)$. $$ \\begin{array}{rlll} (a + b) ^ p & \\equiv & a^p + \\binom{p}{1} a^{p - 1}b^{1} + \\cdots + \\binom{p}{p - 1} a^{1}b^{p - 1} + b^p & (\\mod p) \\\\ & \\equiv & a^p + 0 + \\cdots + 0 + b^p & (\\mod p) \\\\ & \\equiv & a^p + b^p & (\\mod p) \\end{array} $$","title":"31.1-6"},{"location":"Chap31/31.1/#311-7","text":"Prove that if $a$ and $b$ are any positive integers such that $a \\mid b$, then $$(x \\mod b) \\mod a = x \\mod a$$ for any $x$. Prove, under the same assumptions, that $x \\equiv y (\\mod b)$ implies $x \\equiv y (\\mod a)$ for any integers $x$ and $y$. Suppose $x = kb + c$, we have $$(x \\mod b) \\mod a = c \\mod a,$$ and $$x \\mod a = (kb + c) \\mod a = (kb \\mod a) + (c \\mod a) = c \\mod a.$$","title":"31.1-7"},{"location":"Chap31/31.1/#311-8","text":"For any integer $k > 0$, an integer $n$ is a $k$th power if there exists an integer $a$ such that $a^k = n$. Furthermore, $n > 1$ is a nontrivial power if it is a $k$th power for some integer $k > 1$. Show how to determine whether a given $\\beta$-bit integer $n$ is a nontrivial power in time polynomial in $\\beta$. Because $2^\\beta > n$, we only need to test values of $k$ that satisfy $2 \\le k < \\beta$, therefore the testing procedure remains $O(\\beta)$. For any nontrivial power $k$, where $2 \\le k < \\beta$, do a binary search on $a$ that costs $$O(\\log \\sqrt n) = O(\\log \\sqrt{2^\\beta}) = O(\\frac 1 2\\log 2^\\beta) = O(\\beta).$$ Thus, the total time complexity is $$O(\\beta) \\times O(\\beta) = O(\\beta^2).$$","title":"31.1-8"},{"location":"Chap31/31.1/#311-9","text":"Prove equations $\\text{(31.6)}$\u2013$\\text{(31.10)}$. (Omit!)","title":"31.1-9"},{"location":"Chap31/31.1/#311-10","text":"Show that the gcd operator is associative. That is, prove that for all integers $a$, $b$, and $c$, $\\gcd(a, \\gcd(b, c)) = \\gcd(\\gcd(a, b), c)$. [The following proof is provided by my friend, Tony Xiao.] Let $d = \\gcd(a, b, c)$, $a = dp$, $b = dq$ and $c = dr$. Claim $\\gcd(a, \\gcd(b, c)) = d.$ Let $e = \\gcd(b, c)$, thus $$ \\begin{aligned} b = es, \\\\ c = et. \\end{aligned} $$ Since $d \\mid b$ and $d \\mid c$, thus $d \\mid e$. Let $e = dm$, thus $$ \\begin{aligned} b = (dm)s & = dq, \\\\ c = (dm)t & = dr. \\end{aligned} $$ Suppose $k = \\gcd(p, m)$, $$ \\begin{aligned} & k \\mid p, k \\mid m, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid dm, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid (dm)s, dk \\mid (dm)t, \\\\ \\Rightarrow & dk \\mid a, dk \\mid b, dk \\mid c. \\end{aligned} $$ Since $d = \\gcd(a, b, c)$, thus $k = 1$. $$ \\begin{aligned} \\gcd(a, \\gcd(b, c)) & = \\gcd(a, e) \\\\ & = \\gcd(dp, dm) \\\\ & = d \\cdot \\gcd(p, m) \\\\ & = d \\cdot k \\\\ & = d. \\end{aligned} $$ By the claim, $$\\gcd(a, \\gcd(b, c)) = d = \\gcd(\\gcd(a, b), c).$$","title":"31.1-10"},{"location":"Chap31/31.1/#311-11-star","text":"Prove Theorem 31.8. (Omit!)","title":"31.1-11 $\\star$"},{"location":"Chap31/31.1/#311-12","text":"Give efficient algorithms for the operations of dividing a $\\beta$-bit integer by a shorter integer and of taking the remainder of a $\\beta$-bit integer when divided by a shorter integer. Your algorithms should run in time $\\Theta(\\beta^2)$. Shift left until the two numbers have the same length, then repeatedly subtract with proper multiplier and shift right.","title":"31.1-12"},{"location":"Chap31/31.1/#311-13","text":"Give an efficient algorithm to convert a given $\\beta$-bit (binary) integer to a decimal representation. Argue that if multiplication or division of integers whose length is at most $\\beta$ takes time $M(\\beta)$, then we can convert binary to decimal in time $\\Theta(M(\\beta) \\lg\\beta)$. (Omit!)","title":"31.1-13"},{"location":"Chap31/31.2/","text":"31.2-1 Prove that equations $\\text{(31.11)}$ and $\\text{(31.12)}$ imply equation $\\text{(31.13)}$. (Omit!) 31.2-2 Compute the values $(d, x, y)$ that the call $\\text{EXTENDED-EUCLID}(899, 493)$ returns. $(29, -6, 11)$. 31.2-3 Prove that for all integers $a$, $k$, and $n$, $\\gcd(a, n) = \\gcd(a + kn, n)$. $\\gcd(a, n) \\mid \\gcd(a + kn, n)$. Let $d = \\gcd(a, n)$, then $d \\mid a$ and $d \\mid n$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = 0$$ and $d \\mid n$, we have $$d \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a, n) \\mid \\gcd(a + kn, n).$$ $\\gcd(a + kn, n) \\mid \\gcd(a, n)$. Suppose $d = \\gcd(a + kn, n)$, we have $d \\mid n$ and $d \\mid (a + kn)$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = a \\mod d = 0,$$ we have $d \\mid a$. Since $d \\mid a$ and $d \\mid n$, we have $$d \\mid \\gcd(a, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n).$$ Since $$\\gcd(a, n) \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n),$$ we have $$\\gcd(a, n) = \\gcd(a + kn, n).$$ 31.2-4 Rewrite $\\text{EUCLID}$ in an iterative form that uses only a constant amount of memory (that is, stores only a constant number of integer values). 1 2 3 4 5 6 EUCLID ( a , b ) while b != 0 t = a a = b b = t % b return a 31.2-5 If $a > b \\ge 0$, show that the call EUCLID$(a, b)$ makes at most $1 + \\log_\\phi b$ recursive calls. Improve this bound to $1 + \\log_\\phi(b / \\gcd(a, b))$. $$b \\ge F_{k + 1} \\approx \\phi^{k + 1} / \\sqrt{5}$$ $$k + 1 < \\log_\\phi \\sqrt{5} + \\log_\\phi b \\approx 1.67 + \\log_\\phi b$$ $$k < 0.67 + \\log_\\phi b < 1 + \\log_\\phi b.$$ Since $d \\cdot a \\mod d \\cdot b = d \\cdot (a \\mod b)$, $\\gcd(d \\cdot a, d \\cdot b)$ has the same number of recursive calls with $\\gcd(a, b)$, therefore we could let $b' = b / \\gcd(a, b)$, the inequality $k < 1 + \\log_\\phi(b') = 1 + \\log_\\phi(b / \\gcd(a, b))$. will holds. 31.2-6 What does $\\text{EXTENDED-EUCLID}(F_{k + 1}, F_k)$ return? Prove your answer correct. If $k$ is odd, then $(1, -F_{k-2}, F_{k - 1})$. If $k$ is even, then $(1, F_{k-2}, -F_{k - 1})$. 31.2-7 Define the $\\gcd$ function for more than two arguments by the recursive equation $\\gcd(a_0, a_1, \\cdots, a_n) = \\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n))$. Show that the $\\gcd$ function returns the same answer independent of the order in which its arguments are specified. Also show how to find integers $x_0, x_1, \\cdots, x_n$ such that $\\gcd(a_0, a_1, \\ldots, a_n) = a_0 x_0 + a_1 x_1 + \\cdots + a_n x_n$. Show that the number of divisions performed by your algorithm is $O(n + \\lg (max \\{a_0, a_1, \\cdots, a_n \\}))$. Suppose $$\\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n)) = a_0 \\cdot x + \\gcd(a_1, a_2, \\cdots, a_n) \\cdot y$$ and $$\\gcd(a_1, \\gcd(a_2, a_3, \\cdots, a_n)) = a_1 \\cdot x' + \\gcd(a_2, a_3, \\cdots, a_n) \\cdot y',$$ then the coefficient of $a_1$ is $y \\cdot x'$. 1 2 3 4 5 EXTENDED - EUCLID ( a , b ) if b == 0 return ( a , 1 , 0 ) ( d , x , y ) = EXTENDED - EUCLID ( b , a % b ) return ( d , y , x - ( a / b ) * y ) 1 2 3 4 5 6 7 8 9 10 11 12 13 EXTENDED - EUCLID - MULTIPLE ( a ) if a . length == 1 return ( a [ 0 ], 1 ) g = a [ a . length - 1 ] xs = [ 1 ] * a . length ys = [ 0 ] * a . length for i = a . length - 2 downto 0 ( g , xs [ i ], ys [ i + 1 ]) = EXTENDED - EUCLID ( a [ i ], g ) m = 1 for i = 1 to a . length m *= ys [ i ] xs [ i ] *= m return ( g , xs ) 31.2-8 Define $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ to be the least common multiple of the $n$ integers $a_1, a_2, \\ldots, a_n$, that is, the smallest nonnegative integer that is a multiple of each $a_i$. Show how to compute $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ efficiently using the (two-argument) $\\gcd$ operation as a subroutine. 1 2 3 4 GCD ( a , b ) if b == 0 return a return GCD ( b , a % b ) 1 2 LCM ( a , b ) return a / GCD ( a , b ) * b 1 2 3 4 5 LCM - MULTIPLE ( a ) l = a [ 0 ] for i = 1 to a . length l = LCM ( l , a [ i ]) return l 31.2-9 Prove that $n_1$, $n_2$, $n_3$, and $n_4$ are pairwise relatively prime if and only if $\\gcd(n_1n_2,n_3n_4) = \\gcd(n_1n_3, n_2n_4) = 1.$ More generally, show that $n_1, n_2, \\ldots, n_k$ are pairwise relatively prime if and only if a set of $\\lceil \\lg k \\rceil$ pairs of numbers derived from the $n_i$ are relatively prime. Suppose $n_1n_2 x + n_3n_4 y = 1$, then $n_1(n_2 x) + n_3(n_4 y) = 1$, thus $n_1$ and $n_3$ are relatively prime, $n_1$ and $n_4$, $n_2$ and $n_3$, $n_2$ and $n_4$ are the all relatively prime. And since $\\gcd(n_1n_3, n_2n_4) = 1$, all the pairs are relatively prime. General: in the first round, divide the elements into two sets with equal number of elements, calculate the products of the two set separately, if the two products are relatively prime, then the element in one set is pairwise relatively prime with the element in the other set. In the next iterations, for each set, we divide the elements into two subsets, suppose we have subsets $\\{ (s_1, s_2), (s_3, s_4), \\ldots \\}$, then we calculate the products of $\\{ s_1, s_3, \\ldots \\}$ and $\\{ s_2, s_4, \\ldots \\}$, if the two products are relatively prime, then all the pairs of subset are pairwise relatively prime similar to the first round. In each iteration, the number of elements in a subset is half of the original set, thus there are $\\lceil \\lg k \\rceil$ pairs of products. To choose the subsets efficiently, in the $k$th iteration, we could divide the numbers based on the value of the index's $k$th bit.","title":"31.2 Greatest common divisor"},{"location":"Chap31/31.2/#312-1","text":"Prove that equations $\\text{(31.11)}$ and $\\text{(31.12)}$ imply equation $\\text{(31.13)}$. (Omit!)","title":"31.2-1"},{"location":"Chap31/31.2/#312-2","text":"Compute the values $(d, x, y)$ that the call $\\text{EXTENDED-EUCLID}(899, 493)$ returns. $(29, -6, 11)$.","title":"31.2-2"},{"location":"Chap31/31.2/#312-3","text":"Prove that for all integers $a$, $k$, and $n$, $\\gcd(a, n) = \\gcd(a + kn, n)$. $\\gcd(a, n) \\mid \\gcd(a + kn, n)$. Let $d = \\gcd(a, n)$, then $d \\mid a$ and $d \\mid n$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = 0$$ and $d \\mid n$, we have $$d \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a, n) \\mid \\gcd(a + kn, n).$$ $\\gcd(a + kn, n) \\mid \\gcd(a, n)$. Suppose $d = \\gcd(a + kn, n)$, we have $d \\mid n$ and $d \\mid (a + kn)$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = a \\mod d = 0,$$ we have $d \\mid a$. Since $d \\mid a$ and $d \\mid n$, we have $$d \\mid \\gcd(a, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n).$$ Since $$\\gcd(a, n) \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n),$$ we have $$\\gcd(a, n) = \\gcd(a + kn, n).$$","title":"31.2-3"},{"location":"Chap31/31.2/#312-4","text":"Rewrite $\\text{EUCLID}$ in an iterative form that uses only a constant amount of memory (that is, stores only a constant number of integer values). 1 2 3 4 5 6 EUCLID ( a , b ) while b != 0 t = a a = b b = t % b return a","title":"31.2-4"},{"location":"Chap31/31.2/#312-5","text":"If $a > b \\ge 0$, show that the call EUCLID$(a, b)$ makes at most $1 + \\log_\\phi b$ recursive calls. Improve this bound to $1 + \\log_\\phi(b / \\gcd(a, b))$. $$b \\ge F_{k + 1} \\approx \\phi^{k + 1} / \\sqrt{5}$$ $$k + 1 < \\log_\\phi \\sqrt{5} + \\log_\\phi b \\approx 1.67 + \\log_\\phi b$$ $$k < 0.67 + \\log_\\phi b < 1 + \\log_\\phi b.$$ Since $d \\cdot a \\mod d \\cdot b = d \\cdot (a \\mod b)$, $\\gcd(d \\cdot a, d \\cdot b)$ has the same number of recursive calls with $\\gcd(a, b)$, therefore we could let $b' = b / \\gcd(a, b)$, the inequality $k < 1 + \\log_\\phi(b') = 1 + \\log_\\phi(b / \\gcd(a, b))$. will holds.","title":"31.2-5"},{"location":"Chap31/31.2/#312-6","text":"What does $\\text{EXTENDED-EUCLID}(F_{k + 1}, F_k)$ return? Prove your answer correct. If $k$ is odd, then $(1, -F_{k-2}, F_{k - 1})$. If $k$ is even, then $(1, F_{k-2}, -F_{k - 1})$.","title":"31.2-6"},{"location":"Chap31/31.2/#312-7","text":"Define the $\\gcd$ function for more than two arguments by the recursive equation $\\gcd(a_0, a_1, \\cdots, a_n) = \\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n))$. Show that the $\\gcd$ function returns the same answer independent of the order in which its arguments are specified. Also show how to find integers $x_0, x_1, \\cdots, x_n$ such that $\\gcd(a_0, a_1, \\ldots, a_n) = a_0 x_0 + a_1 x_1 + \\cdots + a_n x_n$. Show that the number of divisions performed by your algorithm is $O(n + \\lg (max \\{a_0, a_1, \\cdots, a_n \\}))$. Suppose $$\\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n)) = a_0 \\cdot x + \\gcd(a_1, a_2, \\cdots, a_n) \\cdot y$$ and $$\\gcd(a_1, \\gcd(a_2, a_3, \\cdots, a_n)) = a_1 \\cdot x' + \\gcd(a_2, a_3, \\cdots, a_n) \\cdot y',$$ then the coefficient of $a_1$ is $y \\cdot x'$. 1 2 3 4 5 EXTENDED - EUCLID ( a , b ) if b == 0 return ( a , 1 , 0 ) ( d , x , y ) = EXTENDED - EUCLID ( b , a % b ) return ( d , y , x - ( a / b ) * y ) 1 2 3 4 5 6 7 8 9 10 11 12 13 EXTENDED - EUCLID - MULTIPLE ( a ) if a . length == 1 return ( a [ 0 ], 1 ) g = a [ a . length - 1 ] xs = [ 1 ] * a . length ys = [ 0 ] * a . length for i = a . length - 2 downto 0 ( g , xs [ i ], ys [ i + 1 ]) = EXTENDED - EUCLID ( a [ i ], g ) m = 1 for i = 1 to a . length m *= ys [ i ] xs [ i ] *= m return ( g , xs )","title":"31.2-7"},{"location":"Chap31/31.2/#312-8","text":"Define $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ to be the least common multiple of the $n$ integers $a_1, a_2, \\ldots, a_n$, that is, the smallest nonnegative integer that is a multiple of each $a_i$. Show how to compute $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ efficiently using the (two-argument) $\\gcd$ operation as a subroutine. 1 2 3 4 GCD ( a , b ) if b == 0 return a return GCD ( b , a % b ) 1 2 LCM ( a , b ) return a / GCD ( a , b ) * b 1 2 3 4 5 LCM - MULTIPLE ( a ) l = a [ 0 ] for i = 1 to a . length l = LCM ( l , a [ i ]) return l","title":"31.2-8"},{"location":"Chap31/31.2/#312-9","text":"Prove that $n_1$, $n_2$, $n_3$, and $n_4$ are pairwise relatively prime if and only if $\\gcd(n_1n_2,n_3n_4) = \\gcd(n_1n_3, n_2n_4) = 1.$ More generally, show that $n_1, n_2, \\ldots, n_k$ are pairwise relatively prime if and only if a set of $\\lceil \\lg k \\rceil$ pairs of numbers derived from the $n_i$ are relatively prime. Suppose $n_1n_2 x + n_3n_4 y = 1$, then $n_1(n_2 x) + n_3(n_4 y) = 1$, thus $n_1$ and $n_3$ are relatively prime, $n_1$ and $n_4$, $n_2$ and $n_3$, $n_2$ and $n_4$ are the all relatively prime. And since $\\gcd(n_1n_3, n_2n_4) = 1$, all the pairs are relatively prime. General: in the first round, divide the elements into two sets with equal number of elements, calculate the products of the two set separately, if the two products are relatively prime, then the element in one set is pairwise relatively prime with the element in the other set. In the next iterations, for each set, we divide the elements into two subsets, suppose we have subsets $\\{ (s_1, s_2), (s_3, s_4), \\ldots \\}$, then we calculate the products of $\\{ s_1, s_3, \\ldots \\}$ and $\\{ s_2, s_4, \\ldots \\}$, if the two products are relatively prime, then all the pairs of subset are pairwise relatively prime similar to the first round. In each iteration, the number of elements in a subset is half of the original set, thus there are $\\lceil \\lg k \\rceil$ pairs of products. To choose the subsets efficiently, in the $k$th iteration, we could divide the numbers based on the value of the index's $k$th bit.","title":"31.2-9"},{"location":"Chap31/31.3/","text":"31.3-1 Draw the group operation tables for the groups $(\\mathbb Z_4, +_4)$ and $(\\mathbb Z_5^*, \\cdot_5)$. Show that these groups are isomorphic by exhibiting a one-to-one correspondence $\\alpha$ between their elements such that $a + b \\equiv c (\\mod 4)$ if and only if $\\alpha(a) \\cdot \\alpha(b) \\equiv \\alpha(c) (\\mod 5)$. $(\\mathbb Z_4, +_4)$: $\\{ 0, 1, 2, 3 \\}$. $(\\mathbb Z_5^*, \\cdot_5)$: $\\{ 1, 2, 3, 4 \\}$. $\\alpha(x) = 2^{x-1}$. 31.3-2 List all subgroups of $\\mathbb Z_9$ and of $\\mathbb Z_{13}^*$. $\\mathbb Z_9$: $\\langle 0 \\rangle = \\{ 0 \\}$, $\\langle 1 \\rangle = \\{ 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}$, $\\langle 2 \\rangle = \\{ 0, 2, 4, 6, 8 \\}$. $\\mathbb Z_{13}^*$: $\\langle 1 \\rangle = \\{ 1 \\}$, $\\langle 2 \\rangle = \\{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 \\}$. 31.3-3 Prove Theorem 31.14. A nonempty closed subset of a finite group is a subgroup. Closure: the subset is closed. Identity: suppose $a \\in S'$, then $a^{(k)} \\in S'$. Since the subset is finite, there must be a period such that $a^{(m)} = a^{(n)}$, hence $a^{(m)}a^{(-n)} = a^{(m - n)} = 1$, therefore the subset must contain the identity. Associativity: inherit from the origin group. Inverses: suppose $a^{(k)} = 1$, the inverse of element $a$ is $a^{(k - 1)}$ since $aa^{(k - 1)} = a^{(k)} = 1$. 31.3-4 Show that if $p$ is prime and $e$ is a positive integer, then $\\phi(p^e) = p^{e - 1}(p - 1)$. $\\phi(p^e) = p^e \\cdot \\left ( 1 - \\frac{1}{p} \\right ) = p^{e - 1}(p - 1)$. 31.3-5 Show that for any integer $n > 1$ and for any $a \\in \\mathbb Z_n^*$, the function $f_a : \\mathbb Z_n^* \\rightarrow \\mathbb Z_n^*$ defined by $f_a(x) = ax \\mod n$ is a permutation of $\\mathbb Z_n^*$. To prove it is a permutation, we need to prove that for each element $x \\in \\mathbb Z_n^*$, $f_a(x) \\in \\mathbb Z_n^*$, the numbers generated by $f_a$ are distinct. Since $a \\in \\mathbb Z_{n}^*$ and $x \\in \\mathbb Z_n^*$, then $f_a(x) = ax \\mod n \\in \\mathbb Z_n^*$ by the closure property. Suppose there are two distinct numbers $x \\in \\mathbb Z_n^*$ and $y \\in \\mathbb Z_n^*$ that $f_a(x) = f_a(y)$, $$ \\begin{aligned} f_a(x) & = f_a(y) \\\\ ax \\mod n & = ay \\mod n \\\\ (a \\mod n)(x \\mod n) & = (a \\mod n)(y \\mod n) \\\\ (x \\mod n) & = y \\mod n \\\\ x & \\equiv y \\mod n, \\end{aligned} $$ which contradicts the assumption, therefore the numbers generated by $f_a$ are distinct.","title":"31.3 Modular arithmetic"},{"location":"Chap31/31.3/#313-1","text":"Draw the group operation tables for the groups $(\\mathbb Z_4, +_4)$ and $(\\mathbb Z_5^*, \\cdot_5)$. Show that these groups are isomorphic by exhibiting a one-to-one correspondence $\\alpha$ between their elements such that $a + b \\equiv c (\\mod 4)$ if and only if $\\alpha(a) \\cdot \\alpha(b) \\equiv \\alpha(c) (\\mod 5)$. $(\\mathbb Z_4, +_4)$: $\\{ 0, 1, 2, 3 \\}$. $(\\mathbb Z_5^*, \\cdot_5)$: $\\{ 1, 2, 3, 4 \\}$. $\\alpha(x) = 2^{x-1}$.","title":"31.3-1"},{"location":"Chap31/31.3/#313-2","text":"List all subgroups of $\\mathbb Z_9$ and of $\\mathbb Z_{13}^*$. $\\mathbb Z_9$: $\\langle 0 \\rangle = \\{ 0 \\}$, $\\langle 1 \\rangle = \\{ 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}$, $\\langle 2 \\rangle = \\{ 0, 2, 4, 6, 8 \\}$. $\\mathbb Z_{13}^*$: $\\langle 1 \\rangle = \\{ 1 \\}$, $\\langle 2 \\rangle = \\{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 \\}$.","title":"31.3-2"},{"location":"Chap31/31.3/#313-3","text":"Prove Theorem 31.14. A nonempty closed subset of a finite group is a subgroup. Closure: the subset is closed. Identity: suppose $a \\in S'$, then $a^{(k)} \\in S'$. Since the subset is finite, there must be a period such that $a^{(m)} = a^{(n)}$, hence $a^{(m)}a^{(-n)} = a^{(m - n)} = 1$, therefore the subset must contain the identity. Associativity: inherit from the origin group. Inverses: suppose $a^{(k)} = 1$, the inverse of element $a$ is $a^{(k - 1)}$ since $aa^{(k - 1)} = a^{(k)} = 1$.","title":"31.3-3"},{"location":"Chap31/31.3/#313-4","text":"Show that if $p$ is prime and $e$ is a positive integer, then $\\phi(p^e) = p^{e - 1}(p - 1)$. $\\phi(p^e) = p^e \\cdot \\left ( 1 - \\frac{1}{p} \\right ) = p^{e - 1}(p - 1)$.","title":"31.3-4"},{"location":"Chap31/31.3/#313-5","text":"Show that for any integer $n > 1$ and for any $a \\in \\mathbb Z_n^*$, the function $f_a : \\mathbb Z_n^* \\rightarrow \\mathbb Z_n^*$ defined by $f_a(x) = ax \\mod n$ is a permutation of $\\mathbb Z_n^*$. To prove it is a permutation, we need to prove that for each element $x \\in \\mathbb Z_n^*$, $f_a(x) \\in \\mathbb Z_n^*$, the numbers generated by $f_a$ are distinct. Since $a \\in \\mathbb Z_{n}^*$ and $x \\in \\mathbb Z_n^*$, then $f_a(x) = ax \\mod n \\in \\mathbb Z_n^*$ by the closure property. Suppose there are two distinct numbers $x \\in \\mathbb Z_n^*$ and $y \\in \\mathbb Z_n^*$ that $f_a(x) = f_a(y)$, $$ \\begin{aligned} f_a(x) & = f_a(y) \\\\ ax \\mod n & = ay \\mod n \\\\ (a \\mod n)(x \\mod n) & = (a \\mod n)(y \\mod n) \\\\ (x \\mod n) & = y \\mod n \\\\ x & \\equiv y \\mod n, \\end{aligned} $$ which contradicts the assumption, therefore the numbers generated by $f_a$ are distinct.","title":"31.3-5"},{"location":"Chap31/31.4/","text":"31.4-1 Find all solutions to the equation $35x \\equiv 10 (\\mod 50)$. $\\{6, 16, 26, 36, 46\\}$. 31.4-2 Prove that the equation $ax \\equiv ay (\\mod n)$ implies $x \\equiv y (\\mod n)$ whenever $\\gcd(a, n) = 1$. Show that the condition $\\gcd(a, n) = 1$ is necessary by supplying a counterexample with $\\gcd(a, n) > 1$. $$d = \\gcd(ax, n) = \\gcd(x, n)$$ Since $ax \\cdot x' + n \\cdot y' = d$, we have $$x \\cdot (ax') + n \\cdot y' = d.$$ $$ \\begin{aligned} x_0 & = x'(ay / d), \\\\ x_0' & = (ax')(y / d) = x'(ay / d) = x_0. \\end{aligned} $$ 31.4-3 Consider the following change to line 3 of the procedure $\\text{MODULAR-LINEAR-EQUATION-SOLVER}$: 1 3 x0 = x ' ( b / d ) mod ( n / d ) Will this work? Explain why or why not. Assume that $x_0 \\ge n / d$, then the largest solution is $x_0 + (d - 1) \\cdot (n / d) \\ge d \\cdot n / d \\ge n$, which is impossible, therefore $x_0 < n / d$. 31.4-4 $\\star$ Let $p$ be prime and $f(x) \\equiv f_0 + f_1 x + \\cdots + f_tx^t (\\mod p)$ be a polynomial of degree $t$, with coefficients $f_i$ drawn from $\\mathbb Z_p$. We say that $a \\in \\mathbb Z_p$ is a zero of $f$ if $f(a) \\equiv 0 (\\mod p)$. Prove that if $a$ is a zero of $f$, then $f(x) \\equiv (x - a) g(x) (\\mod p)$ for some polynomial $g(x)$ of degree $t - 1$. Prove by induction on $t$ that if $p$ is prime, then a polynomial $f(x)$ of degree $t$ can have at most $t$ distinct zeros modulo $p$. (Omit!)","title":"31.4 Solving modular linear equations"},{"location":"Chap31/31.4/#314-1","text":"Find all solutions to the equation $35x \\equiv 10 (\\mod 50)$. $\\{6, 16, 26, 36, 46\\}$.","title":"31.4-1"},{"location":"Chap31/31.4/#314-2","text":"Prove that the equation $ax \\equiv ay (\\mod n)$ implies $x \\equiv y (\\mod n)$ whenever $\\gcd(a, n) = 1$. Show that the condition $\\gcd(a, n) = 1$ is necessary by supplying a counterexample with $\\gcd(a, n) > 1$. $$d = \\gcd(ax, n) = \\gcd(x, n)$$ Since $ax \\cdot x' + n \\cdot y' = d$, we have $$x \\cdot (ax') + n \\cdot y' = d.$$ $$ \\begin{aligned} x_0 & = x'(ay / d), \\\\ x_0' & = (ax')(y / d) = x'(ay / d) = x_0. \\end{aligned} $$","title":"31.4-2"},{"location":"Chap31/31.4/#314-3","text":"Consider the following change to line 3 of the procedure $\\text{MODULAR-LINEAR-EQUATION-SOLVER}$: 1 3 x0 = x ' ( b / d ) mod ( n / d ) Will this work? Explain why or why not. Assume that $x_0 \\ge n / d$, then the largest solution is $x_0 + (d - 1) \\cdot (n / d) \\ge d \\cdot n / d \\ge n$, which is impossible, therefore $x_0 < n / d$.","title":"31.4-3"},{"location":"Chap31/31.4/#314-4-star","text":"Let $p$ be prime and $f(x) \\equiv f_0 + f_1 x + \\cdots + f_tx^t (\\mod p)$ be a polynomial of degree $t$, with coefficients $f_i$ drawn from $\\mathbb Z_p$. We say that $a \\in \\mathbb Z_p$ is a zero of $f$ if $f(a) \\equiv 0 (\\mod p)$. Prove that if $a$ is a zero of $f$, then $f(x) \\equiv (x - a) g(x) (\\mod p)$ for some polynomial $g(x)$ of degree $t - 1$. Prove by induction on $t$ that if $p$ is prime, then a polynomial $f(x)$ of degree $t$ can have at most $t$ distinct zeros modulo $p$. (Omit!)","title":"31.4-4 $\\star$"},{"location":"Chap31/31.5/","text":"31.5-1 Find all solutions to the equations $x \\equiv 4 (\\mod 5)$ and $x \\equiv 5 (\\mod 11)$. $$ \\begin{aligned} m_1 & = 11, m_2 = 5. \\\\ m_1^{-1} & = 1, m_2^{-1} = 9. \\\\ c_1 & = 11, c_2 = 45. \\\\ a & = (c_1 \\cdot a_1 + c_2 \\cdot a_2) \\mod (n_1 \\cdot n_2) \\\\ & = (11 \\cdot 4 + 45 \\cdot 5) \\mod 55 = 49. \\end{aligned} $$ 31.5-2 Find all integers $x$ that leave remainders $1$, $2$, $3$ when divided by $9$, $8$, $7$ respectively. $10 + 504i$, $i \\in \\mathbb Z$. 31.5-3 Argue that, under the definitions of Theorem 31.27, if $\\gcd(a, n) = 1$, then $$(a^{-1} \\mod n) \\leftrightarrow ((a_1^{-1} \\mod n_1), (a_2^{-1} \\mod n_2), \\ldots, (a_k^{-1} \\mod n_k)).$$ $$\\gcd(a, n) = 1 \\rightarrow \\gcd(a, n_i) = 1.$$ 31.5-4 Under the definitions of Theorem 31.27, prove that for any polynomial $f$, the number of roots of the equation $f(x) \\equiv 0 (\\mod n)$ equals the product of the number of roots of each of the equations $$f(x) \\equiv 0 (\\mod n_1), f(x) \\equiv 0 (\\mod n_2), \\ldots, f(x) \\equiv 0 (\\mod n_k).$$ Based on $\\text{31.28}$\u2013$\\text{31.30}$.","title":"31.5 The Chinese remainder theorem"},{"location":"Chap31/31.5/#315-1","text":"Find all solutions to the equations $x \\equiv 4 (\\mod 5)$ and $x \\equiv 5 (\\mod 11)$. $$ \\begin{aligned} m_1 & = 11, m_2 = 5. \\\\ m_1^{-1} & = 1, m_2^{-1} = 9. \\\\ c_1 & = 11, c_2 = 45. \\\\ a & = (c_1 \\cdot a_1 + c_2 \\cdot a_2) \\mod (n_1 \\cdot n_2) \\\\ & = (11 \\cdot 4 + 45 \\cdot 5) \\mod 55 = 49. \\end{aligned} $$","title":"31.5-1"},{"location":"Chap31/31.5/#315-2","text":"Find all integers $x$ that leave remainders $1$, $2$, $3$ when divided by $9$, $8$, $7$ respectively. $10 + 504i$, $i \\in \\mathbb Z$.","title":"31.5-2"},{"location":"Chap31/31.5/#315-3","text":"Argue that, under the definitions of Theorem 31.27, if $\\gcd(a, n) = 1$, then $$(a^{-1} \\mod n) \\leftrightarrow ((a_1^{-1} \\mod n_1), (a_2^{-1} \\mod n_2), \\ldots, (a_k^{-1} \\mod n_k)).$$ $$\\gcd(a, n) = 1 \\rightarrow \\gcd(a, n_i) = 1.$$","title":"31.5-3"},{"location":"Chap31/31.5/#315-4","text":"Under the definitions of Theorem 31.27, prove that for any polynomial $f$, the number of roots of the equation $f(x) \\equiv 0 (\\mod n)$ equals the product of the number of roots of each of the equations $$f(x) \\equiv 0 (\\mod n_1), f(x) \\equiv 0 (\\mod n_2), \\ldots, f(x) \\equiv 0 (\\mod n_k).$$ Based on $\\text{31.28}$\u2013$\\text{31.30}$.","title":"31.5-4"},{"location":"Chap31/31.6/","text":"31.6-1 Draw a table showing the order of every element in $\\mathbb Z_{11}^*$. Pick the smallest primitive root $g$ and compute a table giving $\\text{ind}_{11, g}(x)$ for all $x \\in \\mathbb Z_{11}^*$. $g = 2$, $\\{1, 2, 4, 8, 5, 10, 9, 7, 3, 6\\}$. 31.6-2 Give a modular exponentiation algorithm that examines the bits of $b$ from right to left instead of left to right. 1 2 3 4 5 6 7 8 9 MODULAR - EXPONENTIATION ( a , b , n ) i = 0 d = 1 while ( 1 << i ) \u2264 b if ( b & ( 1 << i )) > 0 d = ( d * a ) % n a = ( a * a ) % n i = i + 1 return d 31.6-3 Assuming that you know $\\phi(n)$, explain how to compute $a^{-1} \\mod n$ for any $a \\in \\mathbb Z_n^*$ using the procedure $\\text{MODULAR-EXPONENTIATION}$. $$ \\begin{array}{rlll} a^{\\phi(n)} & \\equiv & 1 & (\\mod n), \\\\ a\\cdot a^{\\phi(n) - 1} & \\equiv & 1 & (\\mod n), \\\\ a^{-1} & \\equiv & a^{\\phi(n)-1} & (\\mod n). \\end{array} $$","title":"31.6 Powers of an element"},{"location":"Chap31/31.6/#316-1","text":"Draw a table showing the order of every element in $\\mathbb Z_{11}^*$. Pick the smallest primitive root $g$ and compute a table giving $\\text{ind}_{11, g}(x)$ for all $x \\in \\mathbb Z_{11}^*$. $g = 2$, $\\{1, 2, 4, 8, 5, 10, 9, 7, 3, 6\\}$.","title":"31.6-1"},{"location":"Chap31/31.6/#316-2","text":"Give a modular exponentiation algorithm that examines the bits of $b$ from right to left instead of left to right. 1 2 3 4 5 6 7 8 9 MODULAR - EXPONENTIATION ( a , b , n ) i = 0 d = 1 while ( 1 << i ) \u2264 b if ( b & ( 1 << i )) > 0 d = ( d * a ) % n a = ( a * a ) % n i = i + 1 return d","title":"31.6-2"},{"location":"Chap31/31.6/#316-3","text":"Assuming that you know $\\phi(n)$, explain how to compute $a^{-1} \\mod n$ for any $a \\in \\mathbb Z_n^*$ using the procedure $\\text{MODULAR-EXPONENTIATION}$. $$ \\begin{array}{rlll} a^{\\phi(n)} & \\equiv & 1 & (\\mod n), \\\\ a\\cdot a^{\\phi(n) - 1} & \\equiv & 1 & (\\mod n), \\\\ a^{-1} & \\equiv & a^{\\phi(n)-1} & (\\mod n). \\end{array} $$","title":"31.6-3"},{"location":"Chap31/31.7/","text":"31.7-1 Consider an RSA key set with $p = 11$, $q = 29$, $n = 319$, and $e = 3$. What value of $d$ should be used in the secret key? What is the encryption of the message $M = 100$? $\\phi(n) = (p - 1) \\cdot (q - 1) = 280$. $d = e^{-1} \\mod \\phi(n) = 187$. $P(M) = M^e \\mod n = 254$. $S(C) = C^d \\mod n = 254^{187} \\mod n = 100$. 31.7-2 Prove that if Alice's public exponent $e$ is $3$ and an adversary obtains Alice's secret exponent $d$, where $0 < d < \\phi(n)$, then the adversary can factor Alice's modulus $n$ in time polynomial in the number of bits in $n$. (Although you are not asked to prove it, you may be interested to know that this result remains true even if the condition $e = 3$ is removed. See Miller [255].) $$ed \\equiv 1 \\mod \\phi(n)$$ $$ed - 1 = 3d - 1 = k \\phi(n)$$ If $p, q < n / 4$, then $$\\phi(n) = n - (p + q) + 1 > n - n / 2 + 1 = n / 2 + 1 > n / 2.$$ $kn / 2 < 3d - 1 < 3d < 3n$, then $k < 6$, then we can solve $3d - 1 = n - p - n / p + 1$. 31.7-3 $\\star$ Prove that RSA is multiplicative in the sense that $P_A(M_1) P_A(M_2) \\equiv P_A(M_1, M_2) (\\mod n)$. Use this fact to prove that if an adversary had a procedure that could efficiently decrypt $1$ percent of messages from $\\mathbb Z_n$ encrypted with $P_A$, then he could employ a probabilistic algorithm to decrypt every message encrypted with $P_A$ with high probability. Multiplicative: $e$ is relatively prime to $n$. Decrypt: In each iteration randomly choose a prime number $m$ that $m$ is relatively prime to $n$, if we can decrypt $m \\cdot M$, then we can return $m^{-1}M$ since $m^{-1} = m^{n - 2}$.","title":"31.7 The RSA public-key cryptosystem"},{"location":"Chap31/31.7/#317-1","text":"Consider an RSA key set with $p = 11$, $q = 29$, $n = 319$, and $e = 3$. What value of $d$ should be used in the secret key? What is the encryption of the message $M = 100$? $\\phi(n) = (p - 1) \\cdot (q - 1) = 280$. $d = e^{-1} \\mod \\phi(n) = 187$. $P(M) = M^e \\mod n = 254$. $S(C) = C^d \\mod n = 254^{187} \\mod n = 100$.","title":"31.7-1"},{"location":"Chap31/31.7/#317-2","text":"Prove that if Alice's public exponent $e$ is $3$ and an adversary obtains Alice's secret exponent $d$, where $0 < d < \\phi(n)$, then the adversary can factor Alice's modulus $n$ in time polynomial in the number of bits in $n$. (Although you are not asked to prove it, you may be interested to know that this result remains true even if the condition $e = 3$ is removed. See Miller [255].) $$ed \\equiv 1 \\mod \\phi(n)$$ $$ed - 1 = 3d - 1 = k \\phi(n)$$ If $p, q < n / 4$, then $$\\phi(n) = n - (p + q) + 1 > n - n / 2 + 1 = n / 2 + 1 > n / 2.$$ $kn / 2 < 3d - 1 < 3d < 3n$, then $k < 6$, then we can solve $3d - 1 = n - p - n / p + 1$.","title":"31.7-2"},{"location":"Chap31/31.7/#317-3-star","text":"Prove that RSA is multiplicative in the sense that $P_A(M_1) P_A(M_2) \\equiv P_A(M_1, M_2) (\\mod n)$. Use this fact to prove that if an adversary had a procedure that could efficiently decrypt $1$ percent of messages from $\\mathbb Z_n$ encrypted with $P_A$, then he could employ a probabilistic algorithm to decrypt every message encrypted with $P_A$ with high probability. Multiplicative: $e$ is relatively prime to $n$. Decrypt: In each iteration randomly choose a prime number $m$ that $m$ is relatively prime to $n$, if we can decrypt $m \\cdot M$, then we can return $m^{-1}M$ since $m^{-1} = m^{n - 2}$.","title":"31.7-3 $\\star$"},{"location":"Chap31/31.8/","text":"31.8-1 Prove that if an odd integer $n > 1$ is not a prime or a prime power, then there exists a nontrivial square root of $1$ modulo $n$. (Omit!) 31.8-2 $\\star$ It is possible to strengthen Euler's theorem slightly to the form $a^{\\lambda(n)} \\equiv 1 (\\mod n)$ for all $a \\in \\mathbb Z_n^*$, where $n = p_1^{e_1} \\cdots p_r^{e_r}$ and $\\lambda(n)$ is defined by $$\\lambda(n) = \\text{lcm}(\\phi(p_1^{e_1}), \\ldots, \\phi\\phi(p_r^{e_r})). \\tag{31.42}$$ Prove that $\\lambda(n) \\mid \\phi(n)$. A composite number $n$ is a Carmichael number if \u0005$\\lambda(n) \\mid n - 1$. The smallest Carmichael number is $561 = 3 \\cdot 11 \\cdot 17$; here, $\\lambda(n) = \\text{lcm}(2, 10, 16) = 80$, which divides $560$. Prove that Carmichael numbers must be both \"square-free\" (not divisible by the square of any prime) and the product of at least three primes. (For this reason, they are not very common.) (Omit!) 31.8-3 Prove that if $x$ is a nontrivial square root of $1$, modulo $n$, then $\\gcd(x - 1, n)$ and $\\gcd(x + 1, n)$ are both nontrivial divisors of $n$. $$ \\begin{array}{rlll} x^2 & \\equiv & 1 & (\\mod n), \\\\ x^2 - 1 & \\equiv & 0 & (\\mod n), \\\\ (x + 1)(x - 1) & \\equiv & 0 & (\\mod n). \\end{array} $$ $n \\mid (x + 1)(x - 1)$, suppose $\\gcd(x - 1, n) = 1$, then $n \\mid (x + 1)$, then $x \\equiv -1 (\\mod n)$ which is trivial, it contradicts the fact that $x$ is nontrivial, therefore $\\gcd(x - 1, n) \\ne 1$, $\\gcd(x + 1, n) \\ne 1$.","title":"31.8 Primality testing"},{"location":"Chap31/31.8/#318-1","text":"Prove that if an odd integer $n > 1$ is not a prime or a prime power, then there exists a nontrivial square root of $1$ modulo $n$. (Omit!)","title":"31.8-1"},{"location":"Chap31/31.8/#318-2-star","text":"It is possible to strengthen Euler's theorem slightly to the form $a^{\\lambda(n)} \\equiv 1 (\\mod n)$ for all $a \\in \\mathbb Z_n^*$, where $n = p_1^{e_1} \\cdots p_r^{e_r}$ and $\\lambda(n)$ is defined by $$\\lambda(n) = \\text{lcm}(\\phi(p_1^{e_1}), \\ldots, \\phi\\phi(p_r^{e_r})). \\tag{31.42}$$ Prove that $\\lambda(n) \\mid \\phi(n)$. A composite number $n$ is a Carmichael number if \u0005$\\lambda(n) \\mid n - 1$. The smallest Carmichael number is $561 = 3 \\cdot 11 \\cdot 17$; here, $\\lambda(n) = \\text{lcm}(2, 10, 16) = 80$, which divides $560$. Prove that Carmichael numbers must be both \"square-free\" (not divisible by the square of any prime) and the product of at least three primes. (For this reason, they are not very common.) (Omit!)","title":"31.8-2 $\\star$"},{"location":"Chap31/31.8/#318-3","text":"Prove that if $x$ is a nontrivial square root of $1$, modulo $n$, then $\\gcd(x - 1, n)$ and $\\gcd(x + 1, n)$ are both nontrivial divisors of $n$. $$ \\begin{array}{rlll} x^2 & \\equiv & 1 & (\\mod n), \\\\ x^2 - 1 & \\equiv & 0 & (\\mod n), \\\\ (x + 1)(x - 1) & \\equiv & 0 & (\\mod n). \\end{array} $$ $n \\mid (x + 1)(x - 1)$, suppose $\\gcd(x - 1, n) = 1$, then $n \\mid (x + 1)$, then $x \\equiv -1 (\\mod n)$ which is trivial, it contradicts the fact that $x$ is nontrivial, therefore $\\gcd(x - 1, n) \\ne 1$, $\\gcd(x + 1, n) \\ne 1$.","title":"31.8-3"},{"location":"Chap31/31.9/","text":"31.9-1 Referring to the execution history shown in Figure 31.7(a), when does \\text{POLLARDRHO} print the factor $73$ of $1387$? $x = 84$, $y = 814$. 31.9-2 Suppose that we are given a function $f : \\mathbb Z_n \\rightarrow \\mathbb Z_n$ and an initial value $x_0 \\in \\mathbb Z_n$. Define $x_i = f(x_{i - 1})$ for $i = 1, 2, \\ldots$. Let $t$ and $u > 0$ be the smallest values such that $x_{t + i} = x_{t + u + i}$ for $i = 0, 1, \\ldots$. In the terminology of Pollard's rho algorithm, $t$ is the length of the tail and $u$ is the length of the cycle of the rho. Give an efficient algorithm to determine $t$ and $u$ exactly, and analyze its running time. (Omit!) 31.9-3 How many steps would you expect $\\text{POLLARD-RHO}$ to require to discover a factor of the form $p^e$, where $p$ is prime and $e > 1$? $\\Theta(\\sqrt p)$. 31.9-4 $\\star$ One disadvantage of $\\text{POLLARD-RHO}$ as written is that it requires one gcd computation for each step of the recurrence. Instead, we could batch the gcd computations by accumulating the product of several $x_i$ values in a row and then using this product instead of $x_i$ in the gcd computation. Describe carefully how you would implement this idea, why it works, and what batch size you would pick as the most effective when working on a $\\beta$-bit number $n$. (Omit!)","title":"31.9 Integer factorization"},{"location":"Chap31/31.9/#319-1","text":"Referring to the execution history shown in Figure 31.7(a), when does \\text{POLLARDRHO} print the factor $73$ of $1387$? $x = 84$, $y = 814$.","title":"31.9-1"},{"location":"Chap31/31.9/#319-2","text":"Suppose that we are given a function $f : \\mathbb Z_n \\rightarrow \\mathbb Z_n$ and an initial value $x_0 \\in \\mathbb Z_n$. Define $x_i = f(x_{i - 1})$ for $i = 1, 2, \\ldots$. Let $t$ and $u > 0$ be the smallest values such that $x_{t + i} = x_{t + u + i}$ for $i = 0, 1, \\ldots$. In the terminology of Pollard's rho algorithm, $t$ is the length of the tail and $u$ is the length of the cycle of the rho. Give an efficient algorithm to determine $t$ and $u$ exactly, and analyze its running time. (Omit!)","title":"31.9-2"},{"location":"Chap31/31.9/#319-3","text":"How many steps would you expect $\\text{POLLARD-RHO}$ to require to discover a factor of the form $p^e$, where $p$ is prime and $e > 1$? $\\Theta(\\sqrt p)$.","title":"31.9-3"},{"location":"Chap31/31.9/#319-4-star","text":"One disadvantage of $\\text{POLLARD-RHO}$ as written is that it requires one gcd computation for each step of the recurrence. Instead, we could batch the gcd computations by accumulating the product of several $x_i$ values in a row and then using this product instead of $x_i$ in the gcd computation. Describe carefully how you would implement this idea, why it works, and what batch size you would pick as the most effective when working on a $\\beta$-bit number $n$. (Omit!)","title":"31.9-4 $\\star$"},{"location":"Chap31/Problems/31-1/","text":"Most computers can perform the operations of subtraction, testing the parity (odd or even) of a binary integer, and halving more quickly than computing remainders. This problem investigates the binary gcd algorithm , which avoids the remainder computations used in Euclid's algorithm. a. Prove that if $a$ and $b$ are both even, then $\\gcd(a, b) = 2 \\cdot \\gcd(a / 2, b / 2)$. b. Prove that if $a$ is odd and $b$ is even, then $\\gcd(a, b) = \\gcd(a, b / 2)$. c. Prove that if $a$ and $b$ are both odd, then $\\gcd(a, b) = \\gcd((a - b) / 2, b)$. d. Design an efficient binary gcd algorithm for input integers $a$ and $b$, where $a \\ge b$, that runs in $O(\\lg a)$ time. Assume that each subtraction, parity test, and halving takes unit time. (Omit!) d. 1 2 3 4 5 6 7 8 9 10 11 12 BINARY - GCD ( a , b ) if a < b return BINARY - GCD ( b , a ) if b == 0 return a if ( a & 1 == 1 ) and ( b & 1 == 1 ) return BINARY - GCD (( a - b ) >> 1 , b ) if ( a & 1 == 0 ) and ( b & 1 == 0 ) return BINARY - GCD ( a >> 1 , b >> 1 ) << 1 if a & 1 == 1 return BINARY - GCD ( a , b >> 1 ) return BINARY - GCD ( a >> 1 , b )","title":"31-1 Binary gcd algorithm"},{"location":"Chap31/Problems/31-2/","text":"a. Consider the ordinary \"paper and pencil\" algorithm for long division: dividing $a$ by $b$, which yields a quotient $q$ and remainder $r$. Show that this method requires $O((1 + \\lg q) \\lg b)$ bit operations. b. Define $\\mu(a, b) = (1 + \\lg a)(1 + \\lg b)$. Show that the number of bit operations performed by $\\text{EUCLID}$ in reducing the problem of computing $\\gcd(a, b)$ to that of computing $\\gcd(b, a \\mod b)$ is at most $c(\\mu(a, b) - \\mu(b, a \\mod b))$ for some sufficiently large constant $c > 0$. c. Show that $\\text{EUCLID}(a, b)$ requires $O(\\mu(a, b))$ bit operations in general and $O(\\beta^2)$ bit operations when applied to two $\\beta$-bit inputs. a. Number of comparisons and subtractions: $\\lceil \\lg a \\rceil - \\lceil \\lg b \\rceil + 1 = \\lceil \\lg q \\rceil$. Length of subtraction: $\\lceil \\lg b \\rceil$. Total: $O((1 + \\lg q) \\lg b)$. b. $$ \\begin{array}{rlll} & \\mu(a, b) - \\mu(b, a \\mod b) \\\\ = & \\mu(a, b) - \\mu(b, r) \\\\ = & (1 + \\lg a)(1 + \\lg b) - (1 + \\lg b)(1 + \\lg r) \\\\ = & (1 + \\lg b)(\\lg a - \\lg r) & (\\lg r \\le \\lg b) \\\\ \\ge & (1 + \\lg b)(\\lg a - \\lg b) \\\\ = & (1 + \\lg b)(\\lg q + 1) \\\\ \\ge & (1 + \\lg q) \\lg b \\end{array} $$ c. $\\mu(a, b) = (1 + \\lg a)(1 + \\lg b) \\approx \\beta^2$","title":"31-2 Analysis of bit operations in Euclid's algorithm"},{"location":"Chap31/Problems/31-3/","text":"This problem compares the efficiency of three methods for computing the $n$th Fibonacci number $F_n$, given $n$. Assume that the cost of adding, subtracting, or multiplying two numbers is $O(1)$, independent of the size of the numbers. a. Show that the running time of the straightforward recursive method for computing $F_n$ based on recurrence $\\text{(3.22)}$ is exponential in $n$. (See, for example, the FIB procedure on page 775.) b. Show how to compute $F_n$ in $O(n)$ time using memoization. c. Show how to compute $F_n$ in $O(\\lg n)$ time using only integer addition and multiplication. ($\\textit{Hint:}$ Consider the matrix $$ \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} $$ and its powers.) d. Assume now that adding two $\\beta$-bit numbers takes $\\Theta(\\beta)$ time and that multiplying two $\\beta$-bit numbers takes $\\Theta(\\beta^2)$ time. What is the running time of these three methods under this more reasonable cost measure for the elementary arithmetic operations? a. In order to solve $\\text{FIB}(n)$, we need to compute $\\text{FIB}(n - 1)$ and $\\text{FIB}(n - 1)$. Therefore we have the recurrence $$T(n) = T(n - 1) + T(n - 2) + \\Theta(1).$$ We can get the upper bound of Fibonacci as $O(2^n)$, but this is not the tight upper bound. The Fibonacci recurrence is defined as $$F(n) = F(n - 1) + F(n - 2).$$ The characteristic equation for this function will be $$ \\begin{aligned} x^2 & = x + 1 \\\\ x^2 - x - 1 & = 0. \\end{aligned} $$ We can get the roots by quadratic formula: $x = \\frac{1 \\pm \\sqrt 5}{2}$. We know the solution of a linear recursive function is given as $$ \\begin{aligned} F(n) & = \\alpha_1^n + \\alpha_2^n \\\\ & = \\bigg(\\frac{1 + \\sqrt 5}{2}\\bigg)^n + \\bigg(\\frac{1 - \\sqrt 5}{2}\\bigg)^n, \\end{aligned} $$ where $\\alpha_1$ and $\\alpha_2$ are the roots of the characteristic equation. Since both $T(n)$ and $F(n)$ are representing the same thing, they are asymptotically the same. Hence, $$ \\begin{aligned} T(n) & = \\bigg(\\frac{1 + \\sqrt 5}{2}\\bigg)^n + \\bigg(\\frac{1 - \\sqrt 5}{2}\\bigg)^n \\\\ & = \\bigg(\\frac{1 + \\sqrt 5}{2}\\bigg)^n \\\\ & \\approx O(1.618)^n. \\end{aligned} $$ b. This is same as 15.1-5 . c. Assume that all integer multiplications and additions can be done in $O(1)$. First, we want to show that $$ \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}^k = \\begin{pmatrix} F_{k - 1} & F_k \\\\ F_k & F_{k + 1} \\end{pmatrix} . $$ By induction, $$ \\begin{aligned} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}^{k + 1} & = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}^k \\\\ & = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} F_{k - 1} & F_k \\\\ F_k & F_{k + 1} \\end{pmatrix}^k \\\\ & = \\begin{pmatrix} F_k & F_{k + 1} \\\\ F_{k - 1} + F_k & F_k + F_{k + 1} \\end{pmatrix} \\\\ & = \\begin{pmatrix} F_k & F_{k + 1} \\\\ F_{k + 1} & F_{k + 2} \\end{pmatrix} . \\end{aligned} $$ We show that we can compute the given matrix to the power $n - 1$ in time $O(\\lg n)$, and the bottom right entry is $F_n$. We should note that by 8 multiplications and 4 additions, we can multiply any two $2$ by $2$ matrices, that means matrix multiplications can be done in constant time. Thus we only need to bound the number of those in our algorithm. It takes $O(\\lg n)$ to run the algorithm $\\text{MATRIX-POW}(A, n - 1)$ becasue we half the value of $n$ in each step, and within each step, we perform a constant amount of calculation. The recurrence is $$T(n) = T(n / 2) + \\Theta(1).$$ 1 2 3 4 MATRIX - POW ( A , n ) if n % 2 == 1 return A * MATRIX - POW ( A ^ 2 , ( n - 1 ) / 2 ) return MATRIX - POW ( A ^ 2 , n / 2 ) d. First, we should note that $\\beta = O(\\log n)$. For part (a), We naively add a $\\beta$-bit number which is growing exponentially each time, so the recurrence becomes $$ \\begin{aligned} T(n) & = T(n - 1) + T(n - 2) + \\Theta(\\beta) \\\\ & = T(n - 1) + T(n - 2) + \\Theta(\\log n), \\end{aligned} $$ which has the same solution $T(n) = O\\Big(\\frac{1 + \\sqrt 5}{2}\\Big)^n$ since $\\Theta(\\log n)$ can be absorbed by exponential time. For part (b), The recurrence of the memoized verstion becomes $$M(n) = M(n - 1) + \\Theta(\\beta).$$ This has a solution of $\\sum_{i = 2}^n \\beta = \\Theta(n\\beta) = \\Theta(2^\\beta \\cdot \\beta)$ or $\\Theta(n \\log n)$. For part (c), We perform a constant number of both additions and multiplications. The recurrence becomes $$P(n) = P(n / 2) + \\Theta(\\beta^2),$$ which has a solution of $\\Theta(\\log n \\cdot \\beta^2) = \\Theta(\\beta^3)$ or $\\Theta(\\log^3 n)$.","title":"31-3 Three algorithms for Fibonacci numbers"},{"location":"Chap31/Problems/31-4/","text":"Let $p$ be an odd prime. A number $a \\in Z_p^*$ is a quadratic residue if the equation $x^2 = a ~(\\text{mod}~p)$ has a solution for the unknown $x$. a. Show that there are exactly $(p - 1) / 2$ quadratic residues, modulo $p$. b. If $p$ is prime, we define the Legendre symbol $(\\frac{a}{p})$, for $a \\in \\mathbb Z_p^*$, to be $1$ if $a$ is a quadratic residue modulo $p$ and $-1$ otherwise. Prove that if $a \\in \\mathbb Z_p^*$, then $$(\\frac{a}{p}) \\equiv a^{(p - 1) / 2} (\\mod p).$$ Give an efficient algorithm that determines whether a given number $a$ is a quadratic residue modulo $p$. Analyze the efficiency of your algorithm. c. Prove that if $p$ is a prime of the form $4k + 3$ and $a$ is a quadratic residue in $\\mathbb Z_b^*$, then $a^{k + 1} \\mod p$ is a square root of $a$, modulo $p$. How much time is required to find the square root of a quadratic residue $a$ modulo $p$? d. Describe an efficient randomized algorithm for finding a nonquadratic residue, modulo an arbitrary prime $p$, that is, a member of $\\mathbb Z_p^*$ that is not a quadratic residue. How many arithmetic operations does your algorithm require on average? (Omit!)","title":"31-4 Quadratic residues"},{"location":"Chap32/32.1/","text":"32.1-1 Show the comparisons the naive string matcher makes for the pattern $P = 0001$ in the text $T = 000010001010001$. 1 2 3 4 5 STRING - MATCHER ( P , T , i ) for j = i to i + P . length if P [ j - i + 1 ] != T [ j ] return false return true 32.1-2 Suppose that all characters in the pattern $P$ are different. Show how to accelerate $\\text{NAIVE-STRING-MATCHER}$ to run in time $O(n)$ on an $n$-character text $T$. Suppose $T[i] \\ne P[j]$, then for $k \\in [1, j)$, $T[i - k] = P[j - k] \\ne P[0]$, the $[i - k, i)$ are all invalid shifts which could be skipped, therefore we can compare $T[i]$ with $P[0]$ in the next iteration. 32.1-3 Suppose that pattern $P$ and text $T$ are randomly chosen strings of length $m$ and $n$, respectively, from the $d$-ary alphabet $\\Sigma_d = \\{ 0, 1, \\ldots, d - 1 \\}$, where $d \\ge 2$. Show that the expected number of character-to-character comparisons made by the implicit loop in line 4 of the naive algorithm is $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2(n - m + 1)$$ over all executions of this loop. (Assume that the naive algorithm stops comparing characters for a given shift once it finds a mismatch or matches the entire pattern.) Thus, for randomly chosen strings, the naive algorithm is quite efficient. Suppose for each shift, the number of compared characters is $L$, then: $$ \\begin{aligned} \\text E[L] & = 1 \\cdot \\frac{d - 1}{d} + 2 \\cdot (\\frac{1}{d})^1 \\frac{d - 1}{d} + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} S & = 1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\\\ \\frac{1}{d}S & = 1 \\cdot (\\frac{1}{d})^1 + \\cdots + (m - 1) \\cdot (\\frac{1}{d})^{m - 1} + m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = 1 + (\\frac{1}{d})^1 + \\cdots + \\cdot (\\frac{1}{d})^{m - 1} - m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} \\text E[L] & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}}. \\end{aligned} $$ There are $n - m + 1$ shifts, therefore the expected number of comparisons is: $$(n - m + 1) \\cdot \\text E[L] = (n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}}$$ Since $d \\ge 2$, $1 - d^{-1} \\ge 0.5$, $1 - d^{-m} < 1$, and $ \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2$, therefore $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2 (n - m + 1).$$ 32.1-4 Suppose we allow the pattern $P$ to contain occurrences of a gap character $\\diamond$ that can match an arbitrary string of characters (even one of zero length). For example, the pattern $ab\\diamond ba\\diamond c$ occurs in the text $cabccbacbacab$ as $$c \\underbrace{ab}_{ab} \\underbrace{cc}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{cba}_{\\diamond} \\underbrace{c}_{c} ab$$ and as $$c \\underbrace{ab}_{ab} \\underbrace{ccbac}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{\\text{ }}_{\\diamond} \\underbrace{c}_{c} ab$$ Note that the gap character may occur an arbitrary number of times in the pattern but not at all in the text. Give a polynomial-time algorithm to determine whether such a pattern $P$ occurs in a given text $T$, and analyze the running time of your algorithm. By using dynamic programming, the time complexity is $O(mn)$ where $m$ is the length of the text $T$ and $n$ is the length of the pattern $P$; the space complexity is $O(mn)$, too. This problem is similar to LeetCode 44. WildCard Matching , except that it has no question mark ( ? ) requirement. You can see my naive DP implementation here .","title":"32.1 The naive string-matching algorithm"},{"location":"Chap32/32.1/#321-1","text":"Show the comparisons the naive string matcher makes for the pattern $P = 0001$ in the text $T = 000010001010001$. 1 2 3 4 5 STRING - MATCHER ( P , T , i ) for j = i to i + P . length if P [ j - i + 1 ] != T [ j ] return false return true","title":"32.1-1"},{"location":"Chap32/32.1/#321-2","text":"Suppose that all characters in the pattern $P$ are different. Show how to accelerate $\\text{NAIVE-STRING-MATCHER}$ to run in time $O(n)$ on an $n$-character text $T$. Suppose $T[i] \\ne P[j]$, then for $k \\in [1, j)$, $T[i - k] = P[j - k] \\ne P[0]$, the $[i - k, i)$ are all invalid shifts which could be skipped, therefore we can compare $T[i]$ with $P[0]$ in the next iteration.","title":"32.1-2"},{"location":"Chap32/32.1/#321-3","text":"Suppose that pattern $P$ and text $T$ are randomly chosen strings of length $m$ and $n$, respectively, from the $d$-ary alphabet $\\Sigma_d = \\{ 0, 1, \\ldots, d - 1 \\}$, where $d \\ge 2$. Show that the expected number of character-to-character comparisons made by the implicit loop in line 4 of the naive algorithm is $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2(n - m + 1)$$ over all executions of this loop. (Assume that the naive algorithm stops comparing characters for a given shift once it finds a mismatch or matches the entire pattern.) Thus, for randomly chosen strings, the naive algorithm is quite efficient. Suppose for each shift, the number of compared characters is $L$, then: $$ \\begin{aligned} \\text E[L] & = 1 \\cdot \\frac{d - 1}{d} + 2 \\cdot (\\frac{1}{d})^1 \\frac{d - 1}{d} + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} S & = 1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\\\ \\frac{1}{d}S & = 1 \\cdot (\\frac{1}{d})^1 + \\cdots + (m - 1) \\cdot (\\frac{1}{d})^{m - 1} + m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = 1 + (\\frac{1}{d})^1 + \\cdots + \\cdot (\\frac{1}{d})^{m - 1} - m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} \\text E[L] & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}}. \\end{aligned} $$ There are $n - m + 1$ shifts, therefore the expected number of comparisons is: $$(n - m + 1) \\cdot \\text E[L] = (n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}}$$ Since $d \\ge 2$, $1 - d^{-1} \\ge 0.5$, $1 - d^{-m} < 1$, and $ \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2$, therefore $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2 (n - m + 1).$$","title":"32.1-3"},{"location":"Chap32/32.1/#321-4","text":"Suppose we allow the pattern $P$ to contain occurrences of a gap character $\\diamond$ that can match an arbitrary string of characters (even one of zero length). For example, the pattern $ab\\diamond ba\\diamond c$ occurs in the text $cabccbacbacab$ as $$c \\underbrace{ab}_{ab} \\underbrace{cc}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{cba}_{\\diamond} \\underbrace{c}_{c} ab$$ and as $$c \\underbrace{ab}_{ab} \\underbrace{ccbac}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{\\text{ }}_{\\diamond} \\underbrace{c}_{c} ab$$ Note that the gap character may occur an arbitrary number of times in the pattern but not at all in the text. Give a polynomial-time algorithm to determine whether such a pattern $P$ occurs in a given text $T$, and analyze the running time of your algorithm. By using dynamic programming, the time complexity is $O(mn)$ where $m$ is the length of the text $T$ and $n$ is the length of the pattern $P$; the space complexity is $O(mn)$, too. This problem is similar to LeetCode 44. WildCard Matching , except that it has no question mark ( ? ) requirement. You can see my naive DP implementation here .","title":"32.1-4"},{"location":"Chap32/32.2/","text":"32.2-1 Working modulo $q = 11$, how many spurious hits does the Rabin-Karp matcher encounter in the text $T = 3141592653589793$ when looking for the pattern $P = 26$? $|\\{15, 59, 92, 26\\}| = 4$. 32.2-2 How would you extend the Rabin-Karp method to the problem of searching a text string for an occurrence of any one of a given set of $k$ patterns? Start by assuming that all $k$ patterns have the same length. Then generalize your solution to allow the patterns to have different lengths. Truncation. 32.2-3 Show how to extend the Rabin-Karp method to handle the problem of looking for a given $m \\times m$ pattern in an $n \\times n$ array of characters. (The pattern may be shifted vertically and horizontally, but it may not be rotated.) Calculate the hashes in each column just like the Rabin-Karp in one-dimension, then treat the hashes in each row as the characters and hashing again. 32.2-4 Alice has a copy of a long $n$-bit file $A = \\langle a_{n - 1}, a_{n - 2}, \\ldots, a_0 \\rangle$, and Bob similarly has an $n$-bit file $B = \\langle b_{n - 1}, b_{n - 2}, \\ldots, b_0 \\rangle$. Alice and Bob wish to know if their files are identical. To avoid transmitting all of $A$ or $B$, they use the following fast probabilistic check. Together, they select a prime $q > 1000n$ and randomly select an integer $x$ from $\\{ 0, 1, \\ldots, q - 1 \\}$. Then, Alice evaluates $$A(x) = (\\sum_{i = 0}^{n - 1} a_i x^i) \\mod q$$ and Bob similarly evaluates $B(x)$. Prove that if $A \\ne B$, there is at most one chance in $1000$ that $A(x) = B(x)$, whereas if the two files are the same, $A(x)$ is necessarily the same as $B(x)$. ($\\textit{Hint:}$ See Exercise 31.4-4.) (Omit!)","title":"32.2 The Rabin-Karp algorithm"},{"location":"Chap32/32.2/#322-1","text":"Working modulo $q = 11$, how many spurious hits does the Rabin-Karp matcher encounter in the text $T = 3141592653589793$ when looking for the pattern $P = 26$? $|\\{15, 59, 92, 26\\}| = 4$.","title":"32.2-1"},{"location":"Chap32/32.2/#322-2","text":"How would you extend the Rabin-Karp method to the problem of searching a text string for an occurrence of any one of a given set of $k$ patterns? Start by assuming that all $k$ patterns have the same length. Then generalize your solution to allow the patterns to have different lengths. Truncation.","title":"32.2-2"},{"location":"Chap32/32.2/#322-3","text":"Show how to extend the Rabin-Karp method to handle the problem of looking for a given $m \\times m$ pattern in an $n \\times n$ array of characters. (The pattern may be shifted vertically and horizontally, but it may not be rotated.) Calculate the hashes in each column just like the Rabin-Karp in one-dimension, then treat the hashes in each row as the characters and hashing again.","title":"32.2-3"},{"location":"Chap32/32.2/#322-4","text":"Alice has a copy of a long $n$-bit file $A = \\langle a_{n - 1}, a_{n - 2}, \\ldots, a_0 \\rangle$, and Bob similarly has an $n$-bit file $B = \\langle b_{n - 1}, b_{n - 2}, \\ldots, b_0 \\rangle$. Alice and Bob wish to know if their files are identical. To avoid transmitting all of $A$ or $B$, they use the following fast probabilistic check. Together, they select a prime $q > 1000n$ and randomly select an integer $x$ from $\\{ 0, 1, \\ldots, q - 1 \\}$. Then, Alice evaluates $$A(x) = (\\sum_{i = 0}^{n - 1} a_i x^i) \\mod q$$ and Bob similarly evaluates $B(x)$. Prove that if $A \\ne B$, there is at most one chance in $1000$ that $A(x) = B(x)$, whereas if the two files are the same, $A(x)$ is necessarily the same as $B(x)$. ($\\textit{Hint:}$ See Exercise 31.4-4.) (Omit!)","title":"32.2-4"},{"location":"Chap32/32.3/","text":"32.3-1 Construct the string-matching automaton for the pattern $P = aabab$ and illustrate its operation on the text string $T = \\text{aaababaabaababaab}$. $$0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3.$$ 32.3-2 Draw a state-transition diagram for a string-matching automaton for the pattern $ababbabbababbababbabb$ over the alphabet $\\sigma = \\{a, b\\}$. $$ \\begin{array}{c|c|c} 0 & 1 & 0 \\\\ 1 & 1 & 2 \\\\ 2 & 3 & 0 \\\\ 3 & 1 & 4 \\\\ 4 & 3 & 5 \\\\ 5 & 6 & 0 \\\\ 6 & 1 & 7 \\\\ 7 & 3 & 8 \\\\ 8 & 9 & 0 \\\\ 9 & 1 & 10 \\\\ 10 & 11 & 0 \\\\ 11 & 1 & 12 \\\\ 12 & 3 & 13 \\\\ 13 & 14 & 0 \\\\ 14 & 1 & 15 \\\\ 15 & 16 & 8 \\\\ 16 & 1 & 17 \\\\ 17 & 3 & 18 \\\\ 18 & 19 & 0 \\\\ 19 & 1 & 20 \\\\ 20 & 3 & 21 \\\\ 21 & 9 & 0 \\end{array} $$ 32.3-3 We call a pattern $P$ nonoverlappable if $P_k \\sqsupset P_q$ implies $k = 0$ or $k = q$. Describe the state-transition diagram of the string-matching automaton for a nonoverlappable pattern. $\\delta(q, a) \\in \\{0, 1, q + 1\\}$. 32.3-4 $\\star$ Given two patterns $P$ and $P'$, describe how to construct a finite automaton that determines all occurrences of either pattern. Try to minimize the number of states in your automaton. Combine the common prefix and suffix. 32.3-5 Given a pattern $P$ containing gap characters (see Exercise 32.1-4), show how to build a finite automaton that can find an occurrence of $P$ in a text $T$ in $O(n)$ matching time, where $n = |T|$. Split the string with the gap characters, build finite automatons for each substring. When a substring is matched, moved to the next finite automaton.","title":"32.3 String matching with finite automata"},{"location":"Chap32/32.3/#323-1","text":"Construct the string-matching automaton for the pattern $P = aabab$ and illustrate its operation on the text string $T = \\text{aaababaabaababaab}$. $$0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3.$$","title":"32.3-1"},{"location":"Chap32/32.3/#323-2","text":"Draw a state-transition diagram for a string-matching automaton for the pattern $ababbabbababbababbabb$ over the alphabet $\\sigma = \\{a, b\\}$. $$ \\begin{array}{c|c|c} 0 & 1 & 0 \\\\ 1 & 1 & 2 \\\\ 2 & 3 & 0 \\\\ 3 & 1 & 4 \\\\ 4 & 3 & 5 \\\\ 5 & 6 & 0 \\\\ 6 & 1 & 7 \\\\ 7 & 3 & 8 \\\\ 8 & 9 & 0 \\\\ 9 & 1 & 10 \\\\ 10 & 11 & 0 \\\\ 11 & 1 & 12 \\\\ 12 & 3 & 13 \\\\ 13 & 14 & 0 \\\\ 14 & 1 & 15 \\\\ 15 & 16 & 8 \\\\ 16 & 1 & 17 \\\\ 17 & 3 & 18 \\\\ 18 & 19 & 0 \\\\ 19 & 1 & 20 \\\\ 20 & 3 & 21 \\\\ 21 & 9 & 0 \\end{array} $$","title":"32.3-2"},{"location":"Chap32/32.3/#323-3","text":"We call a pattern $P$ nonoverlappable if $P_k \\sqsupset P_q$ implies $k = 0$ or $k = q$. Describe the state-transition diagram of the string-matching automaton for a nonoverlappable pattern. $\\delta(q, a) \\in \\{0, 1, q + 1\\}$.","title":"32.3-3"},{"location":"Chap32/32.3/#323-4-star","text":"Given two patterns $P$ and $P'$, describe how to construct a finite automaton that determines all occurrences of either pattern. Try to minimize the number of states in your automaton. Combine the common prefix and suffix.","title":"32.3-4 $\\star$"},{"location":"Chap32/32.3/#323-5","text":"Given a pattern $P$ containing gap characters (see Exercise 32.1-4), show how to build a finite automaton that can find an occurrence of $P$ in a text $T$ in $O(n)$ matching time, where $n = |T|$. Split the string with the gap characters, build finite automatons for each substring. When a substring is matched, moved to the next finite automaton.","title":"32.3-5"},{"location":"Chap32/32.4/","text":"32.4-1 Compute the prefix function $\\pi$ for the pattern $\\text{ababbabbabbababbabb}$. $$\\pi = \\{ 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}.$$ 32.4-2 Give an upper bound on the size of $\\pi^*[q]$ as a function of $q$. Give an example to show that your bound is tight. $|\\pi^*[q]| < q$. 32.4-3 Explain how to determine the occurrences of pattern $P$ in the text $T$ by examining the $\\pi$ function for the string $PT$ (the string of length $m + n$ that is the concatenation of $P$ and $T$). $\\{ q \\mid \\pi[q] = m \\text{ and } q \\ge 2m \\}$. 32.4-4 Use an aggregate analysis to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta$. The number of $q = q + 1$ is at most $n$. 32.4-5 Use a potential function to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta(n)$. $\\Phi = p.$ 32.4-6 Show how to improve $\\text{KMP-MATCHER}$ by replacing the occurrence of $\\pi$ in line 7 (but not line 12) by $\\pi'$, where $\\pi'$ is defined recursively for $q = 1, 2, \\ldots, m - 1$ by the equation $$ \\pi'[q] = \\begin{cases} 0 & \\text{ if } \\pi[q] = 0, \\\\ \\pi'[\\pi[q]] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] = P[q + 1] \\\\ \\pi[q] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] \\ne P[q + 1]. \\end{cases} $$ Explain why the modified algorithm is correct, and explain in what sense this change constitutes an improvement. If $P[q + 1] \\ne T[i]$, then if $P[\\pi[q] + q] = P[q + 1] \\ne T[i]$, there is no need to compare $P[\\pi[q] + q]$ with $T[i]$. 32.4-7 Give a linear-time algorithm to determine whether a text $T$ is a cyclic rotation of another string $T'$. For example, $\\text{arc}$ and $\\text{car}$ are cyclic rotations of each other. Find $T'$ in $TT$. 32.4-8 $\\star$ Give an $O(m|\\Sigma|)$-time algorithm for computing the transition function $\\delta$ for the string-matching automaton corresponding to a given pattern $P$. (Hint: Prove that $\\delta(q, a) = \\delta(\\pi[q], a)$ if $q = m$ or $P[q + 1] \\ne a$.) Compute the prefix function $m$ times.","title":"32.4 The Knuth-Morris-Pratt algorithm"},{"location":"Chap32/32.4/#324-1","text":"Compute the prefix function $\\pi$ for the pattern $\\text{ababbabbabbababbabb}$. $$\\pi = \\{ 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}.$$","title":"32.4-1"},{"location":"Chap32/32.4/#324-2","text":"Give an upper bound on the size of $\\pi^*[q]$ as a function of $q$. Give an example to show that your bound is tight. $|\\pi^*[q]| < q$.","title":"32.4-2"},{"location":"Chap32/32.4/#324-3","text":"Explain how to determine the occurrences of pattern $P$ in the text $T$ by examining the $\\pi$ function for the string $PT$ (the string of length $m + n$ that is the concatenation of $P$ and $T$). $\\{ q \\mid \\pi[q] = m \\text{ and } q \\ge 2m \\}$.","title":"32.4-3"},{"location":"Chap32/32.4/#324-4","text":"Use an aggregate analysis to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta$. The number of $q = q + 1$ is at most $n$.","title":"32.4-4"},{"location":"Chap32/32.4/#324-5","text":"Use a potential function to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta(n)$. $\\Phi = p.$","title":"32.4-5"},{"location":"Chap32/32.4/#324-6","text":"Show how to improve $\\text{KMP-MATCHER}$ by replacing the occurrence of $\\pi$ in line 7 (but not line 12) by $\\pi'$, where $\\pi'$ is defined recursively for $q = 1, 2, \\ldots, m - 1$ by the equation $$ \\pi'[q] = \\begin{cases} 0 & \\text{ if } \\pi[q] = 0, \\\\ \\pi'[\\pi[q]] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] = P[q + 1] \\\\ \\pi[q] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] \\ne P[q + 1]. \\end{cases} $$ Explain why the modified algorithm is correct, and explain in what sense this change constitutes an improvement. If $P[q + 1] \\ne T[i]$, then if $P[\\pi[q] + q] = P[q + 1] \\ne T[i]$, there is no need to compare $P[\\pi[q] + q]$ with $T[i]$.","title":"32.4-6"},{"location":"Chap32/32.4/#324-7","text":"Give a linear-time algorithm to determine whether a text $T$ is a cyclic rotation of another string $T'$. For example, $\\text{arc}$ and $\\text{car}$ are cyclic rotations of each other. Find $T'$ in $TT$.","title":"32.4-7"},{"location":"Chap32/32.4/#324-8-star","text":"Give an $O(m|\\Sigma|)$-time algorithm for computing the transition function $\\delta$ for the string-matching automaton corresponding to a given pattern $P$. (Hint: Prove that $\\delta(q, a) = \\delta(\\pi[q], a)$ if $q = m$ or $P[q + 1] \\ne a$.) Compute the prefix function $m$ times.","title":"32.4-8 $\\star$"},{"location":"Chap32/Problems/32-1/","text":"Let $y^i$ denote the concatenation of string $y$ with itself $i$ times. For example, $(\\text{ab})^3 = \\text{ababab}$. We say that a string $x \\in \\Sigma^*$ has repetition factor $r$ if $x = y ^ r$ for some string $y \\in \\Sigma^*$ and some $r > 0$. Let $\\rho(x)$ denote the largest $r$ such that $x$ has repetition factor $r$. a. Give an efficient algorithm that takes as input a pattern $P[1 \\ldots m]$ and computes the value $\\rho(P_i)$ for $i = 1, 2, \\ldots, m$. What is the running time of your algorithm? b. For any pattern $P[1 \\ldots m]$, let $\\rho^*(P)$ be defined as $\\max_{1 \\le i \\le m} \\rho(P_i)$. Prove that if the pattern $P$ is chosen randomly from the set of all binary strings of length $m$, then the expected value of $\\rho^*(P)$ is $O(1)$. c. Argue that the following string-matching algorithm correctly finds all occurrences of pattern $P$ in a text $T[1 \\ldots n]$ in time $O(\\rho^*(P)n + m)$: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 REPETITION_MATCHER ( P , T ) m = P . length n = T . length k = 1 + \u03c1 * ( P ) q = 0 s = 0 while s \u2264 n - m if T [ s + q + 1 ] == P [ q + 1 ] q = q + 1 if q == m print \"Pattern occurs with shift\" s if q == m or T [ s + q + 1 ] != P [ q + 1 ] s = s + max ( 1 , ceil ( q / k )) q = 0 This algorithm is due to Galil and Seiferas. By extending these ideas greatly, they obtained a linear-time string-matching algorithm that uses only $O(1)$ storage beyond what is required for $P$ and $T$. a. Compute $\\pi$, let $l = m - \\pi[m]$, if $m ~\\text{mod}~ l = 0$ and for all $p = m - i \\cdot l > 0$, $p - \\pi[p] = l$, then $\\rho(P_i) = m / l$, otherwise $\\rho(P_i) = 1$. The running time is $\\Theta(n)$. b. $$ \\begin{aligned} P(\\rho^*(P) \\ge 2) & = \\frac{1}{2} + \\frac{1}{8} + \\frac{1}{32} + \\cdots \\approx \\frac{2}{3} \\\\ P(\\rho^*(P) \\ge 3) & = \\frac{1}{4} + \\frac{1}{32} + \\frac{1}{256} + \\cdots \\approx \\frac{2}{7} \\\\ P(\\rho^*(P) \\ge 4) & = \\frac{1}{8} + \\frac{1}{128} + \\frac{1}{2048} + \\cdots \\approx \\frac{2}{15} \\\\ P(\\rho^*(P) = 1) & = \\frac{1}{3} \\\\ P(\\rho^*(P) = 2) & = \\frac{8}{21} \\\\ P(\\rho^*(P) = 3) & = \\frac{16}{105} \\\\ \\text E[\\rho^*(P)] & = 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{8}{21} + 3 \\cdot \\frac{16}{105} + \\ldots \\approx 2.21 \\end{aligned} $$ c. (Omit!)","title":"32-1 String matching based on repetition factors"},{"location":"Chap33/33.1/","text":"33.1-1 Prove that if $p_1 \\times p_2$ is positive, then vector $p_1$ is clockwise from vector $p_2$ with respect to the origin $(0, 0)$ and that if this cross product is negative, then $p_1$ is counterclockwise from $p_2$. (Omit!) 33.1-2 Professor van Pelt proposes that only the $x$-dimension needs to be tested in line 1 of ON-SEGMENT. Show why the professor is wrong. $(0, 0), (5, 5), (10, 0)$. 33.1-3 The polar angle of a point $p_1$ with respect to an origin point $p_0$ is the angle of the vector $p_1 - p_0$ in the usual polar coordinate system. For example, the polar angle of $(3, 5)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $45$ degrees or $\\pi / 4$ radians. The polar angle of $(3, 3)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $315$ degrees or $7\\pi / 4$ radians. Write pseudocode to sort a sequence $\\langle p_1, p_2, \\ldots, p_n \\rangle$ of $n$ points according to their polar angles with respect to a given origin point $p_0$. Your procedure should take $O(n\\lg n)$ time and use cross products to compare angles. (Omit!) 33.1-4 Show how to determine in $O(n^2 \\lg n)$ time whether any three points in a set of $n$ points are colinear. Based on exercise 33.1-3, for each point $p_i$, let $p_i$ be $p_0$ and sort other points according to their polar angles mod $\\pi$. Then scan linearly to see whether two points have the same polar angle. $O(n \\cdot n\\lg n) = O(n^2 \\lg n)$. 33.1-5 A polygon is a piecewise-linear, closed curve in the plane. That is, it is a curve ending on itself that is formed by a sequence of straight-line segments, called the sides of the polygon. A point joining two consecutive sides is a vertex of the polygon. If the polygon is simple , as we shall generally assume, it does not cross itself. The set of points in the plane enclosed by a simple polygon forms the interior of the polygon, the set of points on the polygon itself forms its boundary , and the set of points surrounding the polygon forms its exterior . A simple polygon is convex if, given any two points on its boundary or in its interior, all points on the line segment drawn between them are contained in the polygon's boundary or interior. A vertex of a convex polygon cannot be expressed as a convex combination of any two distinct points on the boundary or in the interior of the polygon. Professor Amundsen proposes the following method to determine whether a sequence $\\langle p_1, p_2, \\ldots, p_{n - 1} \\rangle$ of $n$ points forms the consecutive vertices of a convex polygon. Output \"yes\" if the set ${ \\angle p_i p_{i + 1} p_{i + 2}: i = 0, 1, \\ldots, n - 1 }$, where subscript addition is performed modulo $n$, does not contain both left turns and right turns; otherwise, output \"no.\" Show that although this method runs in linear time, it does not always produce the correct answer. Modify the professor's method so that it always produces the correct answer in linear time. A line. 33.1-6 Given a point $p_0 = (x_0, y_0)$, the right horizontal ray from $p_0$ is the set of points ${ p_i = (x_i, y_i) : x_i \\ge x_0 ~\\text{and}~ y_i = y_0 }$, that is, it is the set of points due right of $p_0$ along with $p_0$ itself. Show how to determine whether a given right horizontal ray from $p_0$ intersects a line segment $\\overline{p_1 p_2}$ in $O(1)$ time by reducing the problem to that of determining whether two line segments intersect. $p_1.y = p_2.y = 0$ and $\\max(p_1.x, p_2.x) \\ge 0$. or $\\text{sign}(p_1.y) \\ne \\text{sign}(p_2.y)$ and \\$\\displaystyle p_1.y \\cdot \\frac{p_1.x - p_2.x}{p_1.y - p_2.y} \\ge 0{equation} 33.1-7 One way to determine whether a point $p_0$ is in the interior of a simple, but not necessarily convex, polygon $P$ is to look at any ray from $p_0$ and check that the ray intersects the boundary of $P$ an odd number of times but that $p_0$ itself is not on the boundary of $P$. Show how to compute in $\\Theta(n)$ time whether a point $p_0$ is in the interior of an $n$-vertex polygon $P$. (Hint: Use Exercise 33.1-6. Make sure your algorithm is correct when the ray intersects the polygon boundary at a vertex and when the ray overlaps a side of the polygon.) Based on exercise 33.1-6, use $p_i - p_0$ as $p_i$. 33.1-8 Show how to compute the area of an $n$-vertex simple, but not necessarily convex, polygon in $\\Theta(n)$ time. (See Exercise 33.1-5 for definitions pertaining to polygons.) Half of the sum of the cross products of ${\\overline{p_1 p_i}, \\overline{p_1 p_{i + 1}} ~|~ i \\in [2, n - 1] }$.","title":"33.1 Line-segment properties"},{"location":"Chap33/33.1/#331-1","text":"Prove that if $p_1 \\times p_2$ is positive, then vector $p_1$ is clockwise from vector $p_2$ with respect to the origin $(0, 0)$ and that if this cross product is negative, then $p_1$ is counterclockwise from $p_2$. (Omit!)","title":"33.1-1"},{"location":"Chap33/33.1/#331-2","text":"Professor van Pelt proposes that only the $x$-dimension needs to be tested in line 1 of ON-SEGMENT. Show why the professor is wrong. $(0, 0), (5, 5), (10, 0)$.","title":"33.1-2"},{"location":"Chap33/33.1/#331-3","text":"The polar angle of a point $p_1$ with respect to an origin point $p_0$ is the angle of the vector $p_1 - p_0$ in the usual polar coordinate system. For example, the polar angle of $(3, 5)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $45$ degrees or $\\pi / 4$ radians. The polar angle of $(3, 3)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $315$ degrees or $7\\pi / 4$ radians. Write pseudocode to sort a sequence $\\langle p_1, p_2, \\ldots, p_n \\rangle$ of $n$ points according to their polar angles with respect to a given origin point $p_0$. Your procedure should take $O(n\\lg n)$ time and use cross products to compare angles. (Omit!)","title":"33.1-3"},{"location":"Chap33/33.1/#331-4","text":"Show how to determine in $O(n^2 \\lg n)$ time whether any three points in a set of $n$ points are colinear. Based on exercise 33.1-3, for each point $p_i$, let $p_i$ be $p_0$ and sort other points according to their polar angles mod $\\pi$. Then scan linearly to see whether two points have the same polar angle. $O(n \\cdot n\\lg n) = O(n^2 \\lg n)$.","title":"33.1-4"},{"location":"Chap33/33.1/#331-5","text":"A polygon is a piecewise-linear, closed curve in the plane. That is, it is a curve ending on itself that is formed by a sequence of straight-line segments, called the sides of the polygon. A point joining two consecutive sides is a vertex of the polygon. If the polygon is simple , as we shall generally assume, it does not cross itself. The set of points in the plane enclosed by a simple polygon forms the interior of the polygon, the set of points on the polygon itself forms its boundary , and the set of points surrounding the polygon forms its exterior . A simple polygon is convex if, given any two points on its boundary or in its interior, all points on the line segment drawn between them are contained in the polygon's boundary or interior. A vertex of a convex polygon cannot be expressed as a convex combination of any two distinct points on the boundary or in the interior of the polygon. Professor Amundsen proposes the following method to determine whether a sequence $\\langle p_1, p_2, \\ldots, p_{n - 1} \\rangle$ of $n$ points forms the consecutive vertices of a convex polygon. Output \"yes\" if the set ${ \\angle p_i p_{i + 1} p_{i + 2}: i = 0, 1, \\ldots, n - 1 }$, where subscript addition is performed modulo $n$, does not contain both left turns and right turns; otherwise, output \"no.\" Show that although this method runs in linear time, it does not always produce the correct answer. Modify the professor's method so that it always produces the correct answer in linear time. A line.","title":"33.1-5"},{"location":"Chap33/33.1/#331-6","text":"Given a point $p_0 = (x_0, y_0)$, the right horizontal ray from $p_0$ is the set of points ${ p_i = (x_i, y_i) : x_i \\ge x_0 ~\\text{and}~ y_i = y_0 }$, that is, it is the set of points due right of $p_0$ along with $p_0$ itself. Show how to determine whether a given right horizontal ray from $p_0$ intersects a line segment $\\overline{p_1 p_2}$ in $O(1)$ time by reducing the problem to that of determining whether two line segments intersect. $p_1.y = p_2.y = 0$ and $\\max(p_1.x, p_2.x) \\ge 0$. or $\\text{sign}(p_1.y) \\ne \\text{sign}(p_2.y)$ and \\$\\displaystyle p_1.y \\cdot \\frac{p_1.x - p_2.x}{p_1.y - p_2.y} \\ge 0{equation}","title":"33.1-6"},{"location":"Chap33/33.1/#331-7","text":"One way to determine whether a point $p_0$ is in the interior of a simple, but not necessarily convex, polygon $P$ is to look at any ray from $p_0$ and check that the ray intersects the boundary of $P$ an odd number of times but that $p_0$ itself is not on the boundary of $P$. Show how to compute in $\\Theta(n)$ time whether a point $p_0$ is in the interior of an $n$-vertex polygon $P$. (Hint: Use Exercise 33.1-6. Make sure your algorithm is correct when the ray intersects the polygon boundary at a vertex and when the ray overlaps a side of the polygon.) Based on exercise 33.1-6, use $p_i - p_0$ as $p_i$.","title":"33.1-7"},{"location":"Chap33/33.1/#331-8","text":"Show how to compute the area of an $n$-vertex simple, but not necessarily convex, polygon in $\\Theta(n)$ time. (See Exercise 33.1-5 for definitions pertaining to polygons.) Half of the sum of the cross products of ${\\overline{p_1 p_i}, \\overline{p_1 p_{i + 1}} ~|~ i \\in [2, n - 1] }$.","title":"33.1-8"},{"location":"Chap33/33.2/","text":"33.2-1 Show that a set of $n$ line segments may contain $\\Theta(n ^ 2)$ intersections. Star. 33.2-2 Given two segments $a$ and $b$ that are comparable at $x$, show how to determine in $O(1)$ time which of $a \\succeq_x b$ or $b \\succeq_x a$ holds. Assume that neither segment is vertical. (Omit!) 33.2-3 Professor Mason suggests that we modify $\\text{ANY-SEGMENTS-INTERSECT}$ so that instead of returning upon finding an intersection, it prints the segments that intersect and continues on to the next iteration of the for loop. The professor calls the resulting procedure $\\text{PRINT-INTERSECTING-SEGMENTS}$ and claims that it prints all intersections, from left to right, as they occur in the set of line segments. Professor Dixon disagrees, claiming that Professor Mason's idea is incorrect. Which professor is right? Will $\\text{PRINT-INTERSECTING-SEGMENTS}$ always find the leftmost intersection first? Will it always find all the intersections? No. 33.2-4 Give an $O(n\\lg n)$-time algorithm to determine whether an n-vertex polygon is simple. Same as $\\text{ANY-SEGMENTS-INTERSECT}$. 33.2-5 Give an $O(n\\lg n)$-time algorithm to determine whether two simple polygons with a total of $n$ vertices intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$. 33.2-6 A disk consists of a circle plus its interior and is represented by its center point and radius. Two disks intersect if they have any point in common. Give an $O(n\\lg n)$- time algorithm to determine whether any two disks in a set of $n$ intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$. 33.2-7 Given a set of $n$ line segments containing a total of $k$ intersections, show how to output all $k$ intersections in $O((n + k) \\lg)$ time. Treat the intersection points as event points. 33.2-8 Argue that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly even if three or more segments intersect at the same point. (Omit!) 33.2-9 Show that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly in the presence of vertical segments if we treat the bottom endpoint of a vertical segment as if it were a left endpoint and the top endpoint as if it were a right endpoint. How does your answer to Exercise 33.2-2 change if we allow vertical segments? (Omit!)","title":"33.2 Determining whether any pair of segments intersects"},{"location":"Chap33/33.2/#332-1","text":"Show that a set of $n$ line segments may contain $\\Theta(n ^ 2)$ intersections. Star.","title":"33.2-1"},{"location":"Chap33/33.2/#332-2","text":"Given two segments $a$ and $b$ that are comparable at $x$, show how to determine in $O(1)$ time which of $a \\succeq_x b$ or $b \\succeq_x a$ holds. Assume that neither segment is vertical. (Omit!)","title":"33.2-2"},{"location":"Chap33/33.2/#332-3","text":"Professor Mason suggests that we modify $\\text{ANY-SEGMENTS-INTERSECT}$ so that instead of returning upon finding an intersection, it prints the segments that intersect and continues on to the next iteration of the for loop. The professor calls the resulting procedure $\\text{PRINT-INTERSECTING-SEGMENTS}$ and claims that it prints all intersections, from left to right, as they occur in the set of line segments. Professor Dixon disagrees, claiming that Professor Mason's idea is incorrect. Which professor is right? Will $\\text{PRINT-INTERSECTING-SEGMENTS}$ always find the leftmost intersection first? Will it always find all the intersections? No.","title":"33.2-3"},{"location":"Chap33/33.2/#332-4","text":"Give an $O(n\\lg n)$-time algorithm to determine whether an n-vertex polygon is simple. Same as $\\text{ANY-SEGMENTS-INTERSECT}$.","title":"33.2-4"},{"location":"Chap33/33.2/#332-5","text":"Give an $O(n\\lg n)$-time algorithm to determine whether two simple polygons with a total of $n$ vertices intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$.","title":"33.2-5"},{"location":"Chap33/33.2/#332-6","text":"A disk consists of a circle plus its interior and is represented by its center point and radius. Two disks intersect if they have any point in common. Give an $O(n\\lg n)$- time algorithm to determine whether any two disks in a set of $n$ intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$.","title":"33.2-6"},{"location":"Chap33/33.2/#332-7","text":"Given a set of $n$ line segments containing a total of $k$ intersections, show how to output all $k$ intersections in $O((n + k) \\lg)$ time. Treat the intersection points as event points.","title":"33.2-7"},{"location":"Chap33/33.2/#332-8","text":"Argue that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly even if three or more segments intersect at the same point. (Omit!)","title":"33.2-8"},{"location":"Chap33/33.2/#332-9","text":"Show that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly in the presence of vertical segments if we treat the bottom endpoint of a vertical segment as if it were a left endpoint and the top endpoint as if it were a right endpoint. How does your answer to Exercise 33.2-2 change if we allow vertical segments? (Omit!)","title":"33.2-9"},{"location":"Chap33/33.3/","text":"33.3-1 Prove that in the procedure $\\text{GRAHAM-SCAN}$, points $p_1$ and $p_m$ must be vertices of $\\text{CH}(Q)$. To see this, note that $p_1$ and $p_m$ are the points with the lowest and highest polar angle with respect to $p_0$. By symmetry, we may just show it for $p_1$ and we would also have it for $p_m$ just by reflecting the set of points across a vertical line. To see a contradiction, suppose that we have the convex hull doesn't contain $p_1$. Then, let $p$ be the point in the convex hull that has the lowest polar angle with respect to $p_0$. If $p$ is on the line from $p_0$ to $p_1$, we could replace it with $p_1$ and have a convex hull, meaning we didn't start with a convex hull. If we have that it is not on that line, then there is no way that the convex hull given contains $p_1$, also contradicting the fact that we had selected a convex hull 33.3-2 Consider a model of computation that supports addition, comparison, and multiplication and for which there is a lower bound of $\\Omega(n\\lg n)$ to sort $n$ numbers. Prove that $\\Omega(n\\lg n)$ is a lower bound for computing, in order, the vertices of the convex hull of a set of $n$ points in such a model. Let our $n$ numbers be $a_1, a_2, \\dots, a_n$ and $f$ be a strictly convex function, such as $e^x$. Let $p_i = (a_i, f(a_i))$. Compute the convex hull of $p_1, p_2, \\dots, p_n$. Then every point is in the convex hull. We can recover the numbers themselves by looking at the $x$-coordinates of the points in the order returned by the convex-hull algorithm, which will necessarily be a cyclic shift of the numbers in increasing order, so we can recover the proper order in linear time. In an algorithm such as $\\text{GRAHAM-SCAN}$ which starts with the point with minimum $y$-coordinate, the order returned actually gives the numbers in increasing order. 33.3-3 Given a set of points $Q$, prove that the pair of points farthest from each other must be vertices of $\\text{CH}(Q)$. Suppose that $p$ and $q$ are the two furthest apart points. Also, to a contradiction, suppose, without loss of generality that $p$ is on the interior of the convex hull. Then, construct the circle whose center is $q$ and which has $p$ on the circle. Then, if we have that there are any vertices of the convex hull that are outside this circle, we could pick that vertex and $q$, they would have a higher distance than between $p$ and $q$. So, we know that all of the vertices of the convex hull lie inside the circle. This means that the sides of the convex hull consist of line segments that are contained within the circle. So, the only way that they could contain $p$, a point on the circle is if it was a vertex, but we suppsed that $p$ wasn't a vertex of the convex hull, giving us our contradiction. 33.3-4 For a given polygon $P$ and a point $q$ on its boundary, the shadow of $q$ is the set of points $r$ such that the segment $\\overline{qr}$ is entirely on the boundary or in the interior of $P$. As Figure 33.10 illustrates, a polygon $P$ is star-shaped if there exists a point $p$ in the interior of $P$ that is in the shadow of every point on the boundary of $P$. The set of all such points $p$ is called the kernel of $P$. Given an $n$-vertex, star-shaped polygon $P$ specified by its vertices in counterclockwise order, show how to compute $\\text{CH}(P)$ in $O(n)$ time. We simply run $\\text{GRAHAM-SCAN}$ but without sorting the points, so the runtime becomes $O(n)$. To prove this, we'll prove the following loop invariant: At the start of each iteration of the for loop of lines 7-10, stack $S$ consists of, from bottom to top, exactly the vertices of $\\text{CH}(Q_{i - 1})$. The proof is quite similar to the proof of correctness. The invariant holds the first time we execute line 7 for the same reasons outline in the section. At the start of the $i$th iteration, $S$ contains $\\text{CH}(Q_{i - 1})$. Let $p_j$ be the top point on $S$ after executing the while loop of lines 8-9, but before $p_i$ is pushed, and let pk be the point just below $p_j$ on $S$. At this point, $S$ contains $\\text{CH}(Q_j)$ in counterclockwise order from bottom to top. Thus, when we push $p_i$, $S$ contains exactly the vertices of $\\text{CH}(Q_j \\cup \\{p_i\\})$. We now show that this is the same set of points as $\\text{CH}(Q_i)$. Let $p_t$ be any point that was popped from $S$ during iteration $i$ and $p_r$ be the point just below $p_t$ on stack $S$ at the time $p_t$ was popped. Let $p$ be a point in the kernel of $P$. Since the angle $\\angle p_rp_tp_i$ makes a nonelft turn and $P$ is star shaped, $p_t$ must be in the interior or on the boundary of the triangle formed by $p_r$, $p_i$, and $p$. Thus, $p_t$ is not in the convex hull of $Q_i$, so we have $\\text{CH}(Q_i - \\{p_t\\}) = \\text{CH}(Q_i)$. Applying this equality repeatedly for each point removed from $S$ in the while loop of lines 8-9, we have $\\text{CH}(Q_j \\cup \\{p_i\\}) = \\text{CH}(Q_i)$. When the loop terminates, the loop invariant implies that $S$ consists of exactly the vertices of $\\text{CH}(Q_m)$ in counterclockwise order, proving correctness. 33.3-5 In the on-line convex-hull problem , we are given the set $Q$ of $n$ points one point at a time. After receiving each point, we compute the convex hull of the points seen so far. Obviously, we could run Graham's scan once for each point, with a total running time of $O(n^2\\lg n)$. Show how to solve the on-line convex-hull problem in a total of $O(n^2)$ time. Suppose that we have a convex hull computed from the previous stage $\\{q_0, q_1, \\dots, q_m\\}$, and we want to add a new vertex, $p$ in and keep track of how we should change the convex hull. First, process the vertices in a clockwise manner, and look for the first time that we would have to make a non-left to get to $p$. This tells us where to start cutting vertices out of the convex hull. To find out the upper bound on the vertices that we need to cut out, turn around, start processing vertices in a clockwise manner and see the first time that we would need to make a non-right. Then, we just remove the vertices that are in this set of vertices and replace the with $p$. There is one last case to consider, which is when we end up passing ourselves when we do our clockwise sweep. Then we just remove no vertices and add $p$ in in between the two vertices that we had found in the two sweeps. Since for each vertex we add we are only considering each point in the previous step's convex hull twice, the runtime is $O(nh) = O(n^2)$ where h is the number of points in the convex hull. 33.3-6 $\\star$ Show how to implement the incremental method for computing the convex hull of $n$ points so that it runs in $O(n\\lg n)$ time. (Omit!)","title":"33.3 Finding the convex hull"},{"location":"Chap33/33.3/#333-1","text":"Prove that in the procedure $\\text{GRAHAM-SCAN}$, points $p_1$ and $p_m$ must be vertices of $\\text{CH}(Q)$. To see this, note that $p_1$ and $p_m$ are the points with the lowest and highest polar angle with respect to $p_0$. By symmetry, we may just show it for $p_1$ and we would also have it for $p_m$ just by reflecting the set of points across a vertical line. To see a contradiction, suppose that we have the convex hull doesn't contain $p_1$. Then, let $p$ be the point in the convex hull that has the lowest polar angle with respect to $p_0$. If $p$ is on the line from $p_0$ to $p_1$, we could replace it with $p_1$ and have a convex hull, meaning we didn't start with a convex hull. If we have that it is not on that line, then there is no way that the convex hull given contains $p_1$, also contradicting the fact that we had selected a convex hull","title":"33.3-1"},{"location":"Chap33/33.3/#333-2","text":"Consider a model of computation that supports addition, comparison, and multiplication and for which there is a lower bound of $\\Omega(n\\lg n)$ to sort $n$ numbers. Prove that $\\Omega(n\\lg n)$ is a lower bound for computing, in order, the vertices of the convex hull of a set of $n$ points in such a model. Let our $n$ numbers be $a_1, a_2, \\dots, a_n$ and $f$ be a strictly convex function, such as $e^x$. Let $p_i = (a_i, f(a_i))$. Compute the convex hull of $p_1, p_2, \\dots, p_n$. Then every point is in the convex hull. We can recover the numbers themselves by looking at the $x$-coordinates of the points in the order returned by the convex-hull algorithm, which will necessarily be a cyclic shift of the numbers in increasing order, so we can recover the proper order in linear time. In an algorithm such as $\\text{GRAHAM-SCAN}$ which starts with the point with minimum $y$-coordinate, the order returned actually gives the numbers in increasing order.","title":"33.3-2"},{"location":"Chap33/33.3/#333-3","text":"Given a set of points $Q$, prove that the pair of points farthest from each other must be vertices of $\\text{CH}(Q)$. Suppose that $p$ and $q$ are the two furthest apart points. Also, to a contradiction, suppose, without loss of generality that $p$ is on the interior of the convex hull. Then, construct the circle whose center is $q$ and which has $p$ on the circle. Then, if we have that there are any vertices of the convex hull that are outside this circle, we could pick that vertex and $q$, they would have a higher distance than between $p$ and $q$. So, we know that all of the vertices of the convex hull lie inside the circle. This means that the sides of the convex hull consist of line segments that are contained within the circle. So, the only way that they could contain $p$, a point on the circle is if it was a vertex, but we suppsed that $p$ wasn't a vertex of the convex hull, giving us our contradiction.","title":"33.3-3"},{"location":"Chap33/33.3/#333-4","text":"For a given polygon $P$ and a point $q$ on its boundary, the shadow of $q$ is the set of points $r$ such that the segment $\\overline{qr}$ is entirely on the boundary or in the interior of $P$. As Figure 33.10 illustrates, a polygon $P$ is star-shaped if there exists a point $p$ in the interior of $P$ that is in the shadow of every point on the boundary of $P$. The set of all such points $p$ is called the kernel of $P$. Given an $n$-vertex, star-shaped polygon $P$ specified by its vertices in counterclockwise order, show how to compute $\\text{CH}(P)$ in $O(n)$ time. We simply run $\\text{GRAHAM-SCAN}$ but without sorting the points, so the runtime becomes $O(n)$. To prove this, we'll prove the following loop invariant: At the start of each iteration of the for loop of lines 7-10, stack $S$ consists of, from bottom to top, exactly the vertices of $\\text{CH}(Q_{i - 1})$. The proof is quite similar to the proof of correctness. The invariant holds the first time we execute line 7 for the same reasons outline in the section. At the start of the $i$th iteration, $S$ contains $\\text{CH}(Q_{i - 1})$. Let $p_j$ be the top point on $S$ after executing the while loop of lines 8-9, but before $p_i$ is pushed, and let pk be the point just below $p_j$ on $S$. At this point, $S$ contains $\\text{CH}(Q_j)$ in counterclockwise order from bottom to top. Thus, when we push $p_i$, $S$ contains exactly the vertices of $\\text{CH}(Q_j \\cup \\{p_i\\})$. We now show that this is the same set of points as $\\text{CH}(Q_i)$. Let $p_t$ be any point that was popped from $S$ during iteration $i$ and $p_r$ be the point just below $p_t$ on stack $S$ at the time $p_t$ was popped. Let $p$ be a point in the kernel of $P$. Since the angle $\\angle p_rp_tp_i$ makes a nonelft turn and $P$ is star shaped, $p_t$ must be in the interior or on the boundary of the triangle formed by $p_r$, $p_i$, and $p$. Thus, $p_t$ is not in the convex hull of $Q_i$, so we have $\\text{CH}(Q_i - \\{p_t\\}) = \\text{CH}(Q_i)$. Applying this equality repeatedly for each point removed from $S$ in the while loop of lines 8-9, we have $\\text{CH}(Q_j \\cup \\{p_i\\}) = \\text{CH}(Q_i)$. When the loop terminates, the loop invariant implies that $S$ consists of exactly the vertices of $\\text{CH}(Q_m)$ in counterclockwise order, proving correctness.","title":"33.3-4"},{"location":"Chap33/33.3/#333-5","text":"In the on-line convex-hull problem , we are given the set $Q$ of $n$ points one point at a time. After receiving each point, we compute the convex hull of the points seen so far. Obviously, we could run Graham's scan once for each point, with a total running time of $O(n^2\\lg n)$. Show how to solve the on-line convex-hull problem in a total of $O(n^2)$ time. Suppose that we have a convex hull computed from the previous stage $\\{q_0, q_1, \\dots, q_m\\}$, and we want to add a new vertex, $p$ in and keep track of how we should change the convex hull. First, process the vertices in a clockwise manner, and look for the first time that we would have to make a non-left to get to $p$. This tells us where to start cutting vertices out of the convex hull. To find out the upper bound on the vertices that we need to cut out, turn around, start processing vertices in a clockwise manner and see the first time that we would need to make a non-right. Then, we just remove the vertices that are in this set of vertices and replace the with $p$. There is one last case to consider, which is when we end up passing ourselves when we do our clockwise sweep. Then we just remove no vertices and add $p$ in in between the two vertices that we had found in the two sweeps. Since for each vertex we add we are only considering each point in the previous step's convex hull twice, the runtime is $O(nh) = O(n^2)$ where h is the number of points in the convex hull.","title":"33.3-5"},{"location":"Chap33/33.3/#333-6-star","text":"Show how to implement the incremental method for computing the convex hull of $n$ points so that it runs in $O(n\\lg n)$ time. (Omit!)","title":"33.3-6 $\\star$"},{"location":"Chap33/33.4/","text":"33.4-1 Professor Williams comes up with a scheme that allows the closest-pair algorithm to check only $5$ points following each point in array $Y'$. The idea is always to place points on line $l$ into set $P_L$. Then, there cannot be pairs of coincident points on line $l$ with one point in $P_L$ and one in $P_R$. Thus, at most $6$ points can reside in the $\\delta \\times 2\\delta$ rectangle. What is the flaw in the professor's scheme? In particular, when we select line $l$, we may be unable perform an even split of the vertices. So, we don't neccesarily have that both the left set of points and right set of points have fallen to roughly half. For example, suppose that the points are all arranged on a vertical line, then, when we recurse on the the left set of points, we haven't reduced the problem size at all, let alone by a factor of two. There is also the issue in this setup that you may end up asking about a set of size less than two when looking at the right set of points. 33.4-2 Show that it actually suffices to check only the points in the $5$ array positions following each point in the array $Y'$. Since we only care about the shortest distance, the distance $\\delta'$ must be strictly less than $\\delta$. The picture in Figure 33.11(b) only illustrates the case of a nonstrict inequality. If we exclude the possibility of points whose $x$ coordinate differs by exactly $\\delta$ from $l$, then it is only possible to place at most $6$ points in the $\\delta \\times 2\\delta$ rectangle, so it suffices to check on the points in the $5$ array positions following each point in the array $Y'$. 33.4-3 We can define the distance between two points in ways other than euclidean. In the plane, the $L_m$-distance between points $p_1$ and $p_2$ is given by the expression $(|x_1 - x_2|^m + |y_1 - y_2|^m)^{1 / m}$. Euclidean distance, therefore, is $L_2$-distance. Modify the closest-pair algorithm to use the $L_1$-distance, which is also known as the Manhattan distance . In the analysis of the algorithm, most of it goes through just based on the triangle inequality. The only main point of difference is in looking at the number of points that can be fit into a $\\delta \\times 2\\delta$ rectangle. In particular, we can cram in two more points than the eight shown into the rectangle by placing points at the centers of the two squares that the rectangle breaks into. This means that we need to consider points up to $9$ away in $Y'$ instead of $7$ away. This has no impact on the asymptotics of the algorithm and it is the only correction to the algorithm that is needed if we switch from $L_2$ to $L_1$. 33.4-4 Given two points $p_1$ and $p_2$ in the plane, the $L_\\infty$-distance between them is given by $\\max(|x_1 - x_2|, |y_1 - y_2|)$. Modify the closest-pair algorithm to use the $L_\\infty$-distance. We can simply run the divide and conquer algorithm described in the section, modifying the brute force search for $|P| \\le 3$ and the check against the next $7$ points in $Y'$ to use the $L_\\infty$ distance. Since the $L_\\infty$ distance between two points is always less than the euclidean distance, there can be at most $8$ points in the $\\delta \\times 2\\delta$ rectangle which we need to examine in order to determine whether the closest pair is in that box. Thus, the modified algorithm is still correct and has the same runtime. 33.4-5 Suppose that $\\Omega(n)$ of the points given to the closest-pair algorithm are covertical. Show how to determine the sets $P_L$ and $P_R$ and how to determine whether each point of $Y$ is in $P_L$ or $P_R$ so that the running time for the closest-pair algorithm remains $O(n\\lg n)$. We select the line $l$ so that it is roughly equal, and then, we won't run into any issue if we just pick an arbitrary subset of the vertices that are on the line to go to one side or the other. Since the analysis of the algorithm allowed for both elements from $P_L$ and $P_R$ to be on the line, we still have correctness if we do this. To determine what values of $Y$ belong to which of the set can be made easier if we select our set going to $P_L$ to be the lowest however many points are needed, and the $P_R$ to be the higher points. Then, just knowing the index of $Y$ that we are looking at, we know whether that point belonged to $P_L$ or to $P_R$. 33.4-6 Suggest a change to the closest-pair algorithm that avoids presorting the $Y$ array but leaves the running time as $O(n\\lg n)$. ($\\textit{Hint:}$ Merge sorted arrays $Y_L$ and $Y_R$ to form the sorted array $Y$.) In addition to returning the distance of the closest pair, the modify the algorithm to also return the points passed to it, sorted by $y$-coordinate, as $Y$. To do this, merge $Y_L$ and $Y_R$ returned by each of its recursive calls. If we are at the base case, when $n \\le 3$, simply use insertion sort to sort the elements by y-coordinate directly. Since each merge takes linear time, this doesn't affect the recursive equation for the runtime.","title":"33.4 Finding the closest pair of points"},{"location":"Chap33/33.4/#334-1","text":"Professor Williams comes up with a scheme that allows the closest-pair algorithm to check only $5$ points following each point in array $Y'$. The idea is always to place points on line $l$ into set $P_L$. Then, there cannot be pairs of coincident points on line $l$ with one point in $P_L$ and one in $P_R$. Thus, at most $6$ points can reside in the $\\delta \\times 2\\delta$ rectangle. What is the flaw in the professor's scheme? In particular, when we select line $l$, we may be unable perform an even split of the vertices. So, we don't neccesarily have that both the left set of points and right set of points have fallen to roughly half. For example, suppose that the points are all arranged on a vertical line, then, when we recurse on the the left set of points, we haven't reduced the problem size at all, let alone by a factor of two. There is also the issue in this setup that you may end up asking about a set of size less than two when looking at the right set of points.","title":"33.4-1"},{"location":"Chap33/33.4/#334-2","text":"Show that it actually suffices to check only the points in the $5$ array positions following each point in the array $Y'$. Since we only care about the shortest distance, the distance $\\delta'$ must be strictly less than $\\delta$. The picture in Figure 33.11(b) only illustrates the case of a nonstrict inequality. If we exclude the possibility of points whose $x$ coordinate differs by exactly $\\delta$ from $l$, then it is only possible to place at most $6$ points in the $\\delta \\times 2\\delta$ rectangle, so it suffices to check on the points in the $5$ array positions following each point in the array $Y'$.","title":"33.4-2"},{"location":"Chap33/33.4/#334-3","text":"We can define the distance between two points in ways other than euclidean. In the plane, the $L_m$-distance between points $p_1$ and $p_2$ is given by the expression $(|x_1 - x_2|^m + |y_1 - y_2|^m)^{1 / m}$. Euclidean distance, therefore, is $L_2$-distance. Modify the closest-pair algorithm to use the $L_1$-distance, which is also known as the Manhattan distance . In the analysis of the algorithm, most of it goes through just based on the triangle inequality. The only main point of difference is in looking at the number of points that can be fit into a $\\delta \\times 2\\delta$ rectangle. In particular, we can cram in two more points than the eight shown into the rectangle by placing points at the centers of the two squares that the rectangle breaks into. This means that we need to consider points up to $9$ away in $Y'$ instead of $7$ away. This has no impact on the asymptotics of the algorithm and it is the only correction to the algorithm that is needed if we switch from $L_2$ to $L_1$.","title":"33.4-3"},{"location":"Chap33/33.4/#334-4","text":"Given two points $p_1$ and $p_2$ in the plane, the $L_\\infty$-distance between them is given by $\\max(|x_1 - x_2|, |y_1 - y_2|)$. Modify the closest-pair algorithm to use the $L_\\infty$-distance. We can simply run the divide and conquer algorithm described in the section, modifying the brute force search for $|P| \\le 3$ and the check against the next $7$ points in $Y'$ to use the $L_\\infty$ distance. Since the $L_\\infty$ distance between two points is always less than the euclidean distance, there can be at most $8$ points in the $\\delta \\times 2\\delta$ rectangle which we need to examine in order to determine whether the closest pair is in that box. Thus, the modified algorithm is still correct and has the same runtime.","title":"33.4-4"},{"location":"Chap33/33.4/#334-5","text":"Suppose that $\\Omega(n)$ of the points given to the closest-pair algorithm are covertical. Show how to determine the sets $P_L$ and $P_R$ and how to determine whether each point of $Y$ is in $P_L$ or $P_R$ so that the running time for the closest-pair algorithm remains $O(n\\lg n)$. We select the line $l$ so that it is roughly equal, and then, we won't run into any issue if we just pick an arbitrary subset of the vertices that are on the line to go to one side or the other. Since the analysis of the algorithm allowed for both elements from $P_L$ and $P_R$ to be on the line, we still have correctness if we do this. To determine what values of $Y$ belong to which of the set can be made easier if we select our set going to $P_L$ to be the lowest however many points are needed, and the $P_R$ to be the higher points. Then, just knowing the index of $Y$ that we are looking at, we know whether that point belonged to $P_L$ or to $P_R$.","title":"33.4-5"},{"location":"Chap33/33.4/#334-6","text":"Suggest a change to the closest-pair algorithm that avoids presorting the $Y$ array but leaves the running time as $O(n\\lg n)$. ($\\textit{Hint:}$ Merge sorted arrays $Y_L$ and $Y_R$ to form the sorted array $Y$.) In addition to returning the distance of the closest pair, the modify the algorithm to also return the points passed to it, sorted by $y$-coordinate, as $Y$. To do this, merge $Y_L$ and $Y_R$ returned by each of its recursive calls. If we are at the base case, when $n \\le 3$, simply use insertion sort to sort the elements by y-coordinate directly. Since each merge takes linear time, this doesn't affect the recursive equation for the runtime.","title":"33.4-6"},{"location":"Chap33/Problems/33-1/","text":"Given a set $Q$ of points in the plane, we define the convex layers of $Q$ inductively. The first convex layer of $Q$ consists of those points in $Q$ that are vertices of $\\text{CH}(Q)$. For $i > 1$, define $Q_i$ to consist of the points of $Q$ with all points in convex layers $i, 2, \\dots, i - 1$ removed. Then, the $i$th convex layer of $Q$ is $\\text{CH}(Q_i)$ if $Q_i \\ne \\emptyset$ and is undefined otherwise. a. Give an $O(n^2)$- time algorithm to find the convex layers of a set of $n$ points. b. Prove that $\\Omega(n\\lg n)$ time is required to compute the convex layers of a set of $n$ points with any model of computation that requires $\\Omega(n\\lg n)$ time to sort $n$ real numbers. (Omit!)","title":"33-1 Convex layers"},{"location":"Chap33/Problems/33-2/","text":"Let $Q$ be a set of $n$ points in the plane. We say that point $(x, y)$ dominates point $(x', y')$ if $x \\ge x'$ and $y \\ge y'$. A point in $Q$ that is dominated by no other points in $Q$ is said to be maximal . Note that $Q$ may contain many maximal points, which can be organized into maximal layers as follows. The first maximal layer $L_1$ is the set of maximal points of $Q$. For $i > 1$, the $i$th maximal layer $L_i$ is the set of maximal points in $Q - \\bigcup_{j = 1}^{i - 1} L_j$. Suppose that $Q$ has $k$ nonempty maximal layers, and let $y_i$ be the $y$-coordinate of the leftmost point in $L_i$ for $i = 1, 2, \\dots, k$. For now, assume that no two points in $Q$ have the same $x$- or $y$-coordinate. a. Show that $y_1 > y_2 > \\cdots > y_k$. Consider a point $(x, y)$ that is to the left of any point in $Q$ and for which $y$ is distinct from the $y$-coordinate of any point in $Q$. Let $Q' = Q \\cup \\{(w, y)\\}$. b. Let $j$ be the minimum index such that $y_j < y$, unless $y < y_k$, in which case we let $j = k + 1$. Show that the maximal layers of $Q'$ are as follows: If $j \\le k$, then the maximal layers of $Q'$ are the same as the maximal layers of $Q$, except that $L_j$ also includes $(x, y)$ as its new leftmost point. If $j = k + 1$, then the first $k$ maximal layers of $Q'$ are the same as for $Q$, but in addition, $Q'$ has a nonempty $(k + 1)$st maximal layer: $L_{k + 1} = \\{(x, y)\\}$. c. Describe an $O(n\\lg n)$-time algorithm to compute the maximal layers of a set $Q$ of $n$ points. ($\\textit{Hint:}$ Move a sweep line from right to left.) d. Do any difficulties arise if we now allow input points to have the same $x$- or $y$-coordinate? Suggest a way to resolve such problems. (Omit!)","title":"33-2 Maximal layers"},{"location":"Chap33/Problems/33-3/","text":"A group of $n$ Ghostbusters is battling n ghosts. Each Ghostbuster carries a proton pack, which shoots a stream at a ghost, eradicating it. A stream goes in a straight line and terminates when it hits the ghost. The Ghostbusters decide upon the following strategy. They will pair off with the ghosts, forming $n$ Ghostbuster-ghost pairs, and then simultaneously each Ghostbuster will shoot a stream at his chosen ghost. As we all know, it is very dangerous to let streams cross, and so the Ghostbusters must choose pairings for which no streams will cross. Assume that the position of each Ghostbuster and each ghost is a fixed point in the plane and that no three positions are colinear. a. Argue that there exists a line passing through one Ghostbuster and one ghost such that the number of Ghostbusters on one side of the line equals the number of ghosts on the same side. Describe how to find such a line in $O(n\\lg n)$ time. b. Give an $O(n^2\\lg n)$-time algorithm to pair Ghostbusters with ghosts in such a way that no streams cross. (Omit!)","title":"33-3 Ghostbusters and ghosts"},{"location":"Chap33/Problems/33-4/","text":"Professor Charon has a set of $n$ sticks, which are piled up in some configuration. Each stick is specified by its endpoints, and each endpoint is an ordered triple giving its $(x, y, z)$ coordinates. No stick is vertical. He wishes to pick up all the sticks, one at a time, subject to the condition that he may pick up a stick only if there is no other stick on top of it. a. Give a procedure that takes two sticks $a$ and $b$ and reports whether $a$ is above, below, or unrelated to $b$. b. Describe an efficient algorithm that determines whether it is possible to pick up all the sticks, and if so, provides a legal order in which to pick them up. (Omit!)","title":"33-4 Picking up sticks"},{"location":"Chap33/Problems/33-5/","text":"Consider the problem of computing the convex hull of a set of points in the plane that have been drawn according to some known random distribution. Sometimes, the number of points, or size, of the convex hull of $n$ points drawn from such a distribution has expectation $O(n^{1 - \\epsilon})$ for some constant $\\epsilon > 0$. We call such a distribution sparse-hulled . Sparse-hulled distributions include the following: Points drawn uniformly from a unit-radius disk. The convex hull has expected size $\\Theta(n^{1 / 3})$. Points drawn uniformly from the interior of a convex polygon with $k$ sides, for any constant $k$. The convex hull has expected size $\\Theta(\\lg n)$. Points drawn according to a two-dimensional normal distribution. The convex $p$ hull has expected size $\\Theta(\\sqrt{\\lg n})$. a. Given two convex polygons with $n_1$ and $n_2$ vertices respectively, show how to compute the convex hull of all $n_1 + n_2$ points in $O(n_1 + n_2)$ time. (The polygons may overlap.) b. Show how to compute the convex hull of a set of $n$ points drawn independently according to a sparse-hulled distribution in $O(n)$ average-case time. ($\\textit{Hint:}$ Recursively find the convex hulls of the first $n / 2$ points and the second $n / 2$ points, and then combine the results.) (Omit!)","title":"33-5 Sparse-hulled distributions"},{"location":"Chap34/34.1/","text":"34.1-1 Define the optimization problem $\\text{LONGEST-PATH-LENGTH}$ as the relation that associates each instance of an undirected graph and two vertices with the number of edges in a longest simple path between the two vertices. Define the decision problem $\\text{LONGEST-PATH}$ $= \\{\\langle G, u, v, k\\rangle: G = (V, E)$ is an undirected graph, $u, v \\in V, k \\ge 0$ is an integer, and there exists a simple path from $u$ to $v$ in $G$ consisting of at least $k$ edges $\\}$. Show that the optimization problem $\\text{LONGEST-PATH-LENGTH}$ can be solved in polynomial time if and only if $\\text{LONGEST-PATH} \\in P$. Showing that $\\text{LONGEST-PATH-LENGTH}$ being polynomial implies that $\\text{LONGEST-PATH}$ is polynomial is trivial, because we can just compute the length of the longest path and reject the instance of $\\text{LONGEST-PATH}$ if and only if $k$ is larger than the number we computed as the length of the longest path. Since we know that the number of edges in the longest path length is between $0$ and $|E|$, we can perform a binary search for it's length. That is, we construct an instance of $\\text{LONGEST-PATH}$ with the given parameters along with $k = \\frac{|E|}{2}$. If we hear yes, we know that the length of the longest path is somewhere above the halfway point. If we hear no, we know it is somewhere below. Since each time we are halving the possible range, we have that the procedure can require $O(\\lg |E|)$ many steps. However, running a polynomial time subroutine $\\lg n$ many times still gets us a polynomial time procedure, since we know that with this procedure we will never be feeding output of one call of $\\text{LONGEST-PATH}$ into the next. 34.1-2 Give a formal definition for the problem of finding the longest simple cycle in an undirected graph. Give a related decision problem. Give the language corresponding to the decision problem. The problem $\\text{LONGST-SIMPLE-CYCLE}$ is the relation that associates each instance of a graph with the longest simple cycle contained in that graph. The decision problem is, given $k$, to determine whether or not the instance graph has a simple cycle of length at least $k$. If yes, output $1$. Otherwise output $0$. The language corresponding to the decision problem is the set of all $\\langle G, k\\rangle$ such that $G = (V, E)$ is an undirected graph, $k \\ge 0$ is an integer, and there exists a simple cycle in $G$ consisting of at least $k$ edges. 34.1-3 Give a formal encoding of directed graphs as binary strings using an adjacencymatrix representation. Do the same using an adjacency-list representation. Argue that the two representations are polynomially related. (Omit!) 34.1-4 Is the dynamic-programming algorithm for the 0-1 knapsack problem that is asked for in Exercise 16.2-2 a polynomial-time algorithm? Explain your answer. This isn't a polynomial-time algorithm. Recall that the algorithm from Exercise 16.2-2 had running time $\\Theta(nW)$ where $W$ was the maximum weight supported by the knapsack. Consider an encoding of the problem. There is a polynomial encoding of each item by giving the binary representation of its index, worth, and weight, represented as some binary string of length $a = \\Omega(n)$. We then encode $W$, in polynomial time. This will have length $\\Theta(\\lg W) = b$. The solution to this problem of length $a + b$ is found in time $\\Theta(nW) = \\Theta(a \\cdot 2^b)$. Thus, the algorithm is actually exponential. 34.1-5 Show that if an algorithm makes at most a constant number of calls to polynomial-time subroutines and performs an additional amount of work that also takes polynomial time, then it runs in polynomial time. Also show that a polynomial number of calls to polynomial-time subroutines may result in an exponential-time algorithm. (Omit!) 34.1-6 Show that the class $P$, viewed as a set of languages, is closed under union, intersection, concatenation, complement, and Kleene star. That is, if $L_1, L_2 \\in P$, then $L_1 \\cup L_2 \\in P$, $L_1 \\cap L_2 \\in P$, $L_1L_2 \\in P$, $\\bar L_1 \\in P$, and $L_1^* \\in P$. (Omit!)","title":"34.1 Polynomial time"},{"location":"Chap34/34.1/#341-1","text":"Define the optimization problem $\\text{LONGEST-PATH-LENGTH}$ as the relation that associates each instance of an undirected graph and two vertices with the number of edges in a longest simple path between the two vertices. Define the decision problem $\\text{LONGEST-PATH}$ $= \\{\\langle G, u, v, k\\rangle: G = (V, E)$ is an undirected graph, $u, v \\in V, k \\ge 0$ is an integer, and there exists a simple path from $u$ to $v$ in $G$ consisting of at least $k$ edges $\\}$. Show that the optimization problem $\\text{LONGEST-PATH-LENGTH}$ can be solved in polynomial time if and only if $\\text{LONGEST-PATH} \\in P$. Showing that $\\text{LONGEST-PATH-LENGTH}$ being polynomial implies that $\\text{LONGEST-PATH}$ is polynomial is trivial, because we can just compute the length of the longest path and reject the instance of $\\text{LONGEST-PATH}$ if and only if $k$ is larger than the number we computed as the length of the longest path. Since we know that the number of edges in the longest path length is between $0$ and $|E|$, we can perform a binary search for it's length. That is, we construct an instance of $\\text{LONGEST-PATH}$ with the given parameters along with $k = \\frac{|E|}{2}$. If we hear yes, we know that the length of the longest path is somewhere above the halfway point. If we hear no, we know it is somewhere below. Since each time we are halving the possible range, we have that the procedure can require $O(\\lg |E|)$ many steps. However, running a polynomial time subroutine $\\lg n$ many times still gets us a polynomial time procedure, since we know that with this procedure we will never be feeding output of one call of $\\text{LONGEST-PATH}$ into the next.","title":"34.1-1"},{"location":"Chap34/34.1/#341-2","text":"Give a formal definition for the problem of finding the longest simple cycle in an undirected graph. Give a related decision problem. Give the language corresponding to the decision problem. The problem $\\text{LONGST-SIMPLE-CYCLE}$ is the relation that associates each instance of a graph with the longest simple cycle contained in that graph. The decision problem is, given $k$, to determine whether or not the instance graph has a simple cycle of length at least $k$. If yes, output $1$. Otherwise output $0$. The language corresponding to the decision problem is the set of all $\\langle G, k\\rangle$ such that $G = (V, E)$ is an undirected graph, $k \\ge 0$ is an integer, and there exists a simple cycle in $G$ consisting of at least $k$ edges.","title":"34.1-2"},{"location":"Chap34/34.1/#341-3","text":"Give a formal encoding of directed graphs as binary strings using an adjacencymatrix representation. Do the same using an adjacency-list representation. Argue that the two representations are polynomially related. (Omit!)","title":"34.1-3"},{"location":"Chap34/34.1/#341-4","text":"Is the dynamic-programming algorithm for the 0-1 knapsack problem that is asked for in Exercise 16.2-2 a polynomial-time algorithm? Explain your answer. This isn't a polynomial-time algorithm. Recall that the algorithm from Exercise 16.2-2 had running time $\\Theta(nW)$ where $W$ was the maximum weight supported by the knapsack. Consider an encoding of the problem. There is a polynomial encoding of each item by giving the binary representation of its index, worth, and weight, represented as some binary string of length $a = \\Omega(n)$. We then encode $W$, in polynomial time. This will have length $\\Theta(\\lg W) = b$. The solution to this problem of length $a + b$ is found in time $\\Theta(nW) = \\Theta(a \\cdot 2^b)$. Thus, the algorithm is actually exponential.","title":"34.1-4"},{"location":"Chap34/34.1/#341-5","text":"Show that if an algorithm makes at most a constant number of calls to polynomial-time subroutines and performs an additional amount of work that also takes polynomial time, then it runs in polynomial time. Also show that a polynomial number of calls to polynomial-time subroutines may result in an exponential-time algorithm. (Omit!)","title":"34.1-5"},{"location":"Chap34/34.1/#341-6","text":"Show that the class $P$, viewed as a set of languages, is closed under union, intersection, concatenation, complement, and Kleene star. That is, if $L_1, L_2 \\in P$, then $L_1 \\cup L_2 \\in P$, $L_1 \\cap L_2 \\in P$, $L_1L_2 \\in P$, $\\bar L_1 \\in P$, and $L_1^* \\in P$. (Omit!)","title":"34.1-6"},{"location":"Chap34/34.2/","text":"34.2-1 Consider the language $\\text{GRAPH-ISOMORPHISM}$ $= \\{\\langle G_1, G_2 \\rangle: G_1$ and $G_2$ are isomorphic graphs$\\}$. Prove that $\\text{GRAPH-ISOMORPHISM} \\in \\text{NP}$ by describing a polynomial-time algorithm to verify the language. (Omit!) 34.2-2 Prove that if $G$ is an undirected bipartite graph with an odd number of vertices, then $G$ is nonhamiltonian. (Omit!) 34.2-3 Show that if $\\text{HAM-CYCLE} \\in P$, then the problem of listing the vertices of a hamiltonian cycle, in order, is polynomial-time solvable. (Omit!) 34.2-4 Prove that the class $\\text{NP}$ of languages is closed under union, intersection, concatenation, and Kleene star. Discuss the closure of $\\text{NP}$ under complement. (Omit!) 34.2-5 Show that any language in $\\text{NP}$ can be decided by an algorithm running in time $2^{O(n^k)}$ for some constant $k$. (Omit!) 34.2-6 A hamiltonian path in a graph is a simple path that visits every vertex exactly once. Show that the language $\\text{HAM-PATH}$ $= \\{\\langle G, u, v \\rangle:$ there is a hamiltonian path from $u$ to $v$ in graph $G\\}$ belongs to $\\text{NP}$. (Omit!) 34.2-7 Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved in polynomial time on directed acyclic graphs. Give an efficient algorithm for the problem. (Omit!) 34.2-8 Let $\\phi$ be a boolean formula constructed from the boolean input variables $x_1, x_2, \\dots, x_k$, negations ($\\neg$), ANDs ($\\vee$), ORs ($\\wedge$), and parentheses. The formula $\\phi$ is a tautology if it evaluates to $1$ for every assignment of $1$ and $0$ to the input variables. Define $\\text{TAUTOLOGY}$ as the language of boolean formulas that are tautologies. Show that $\\text{TAUTOLOGY} \\in \\text{co-NP}$. (Omit!) 34.2-9 Prove that $\\text P \\subseteq \\text{co-NP}$. (Omit!) 34.2-10 Prove that if $\\text{NP} \\ne \\text{co-NP}$, then $\\text P \\ne \\text{NP}$. (Omit!) 34.2-11 Let $G$ be a connected, undirected graph with at least $3$ vertices, and let $G^3$ be the graph obtained by connecting all pairs of vertices that are connected by a path in $G$ of length at most $3$. Prove that $G^3$ is hamiltonian. ($\\textit{Hint:}$ Construct a spanning tree for $G$, and use an inductive argument.) (Omit!)","title":"34.2 Polynomial-time verification"},{"location":"Chap34/34.2/#342-1","text":"Consider the language $\\text{GRAPH-ISOMORPHISM}$ $= \\{\\langle G_1, G_2 \\rangle: G_1$ and $G_2$ are isomorphic graphs$\\}$. Prove that $\\text{GRAPH-ISOMORPHISM} \\in \\text{NP}$ by describing a polynomial-time algorithm to verify the language. (Omit!)","title":"34.2-1"},{"location":"Chap34/34.2/#342-2","text":"Prove that if $G$ is an undirected bipartite graph with an odd number of vertices, then $G$ is nonhamiltonian. (Omit!)","title":"34.2-2"},{"location":"Chap34/34.2/#342-3","text":"Show that if $\\text{HAM-CYCLE} \\in P$, then the problem of listing the vertices of a hamiltonian cycle, in order, is polynomial-time solvable. (Omit!)","title":"34.2-3"},{"location":"Chap34/34.2/#342-4","text":"Prove that the class $\\text{NP}$ of languages is closed under union, intersection, concatenation, and Kleene star. Discuss the closure of $\\text{NP}$ under complement. (Omit!)","title":"34.2-4"},{"location":"Chap34/34.2/#342-5","text":"Show that any language in $\\text{NP}$ can be decided by an algorithm running in time $2^{O(n^k)}$ for some constant $k$. (Omit!)","title":"34.2-5"},{"location":"Chap34/34.2/#342-6","text":"A hamiltonian path in a graph is a simple path that visits every vertex exactly once. Show that the language $\\text{HAM-PATH}$ $= \\{\\langle G, u, v \\rangle:$ there is a hamiltonian path from $u$ to $v$ in graph $G\\}$ belongs to $\\text{NP}$. (Omit!)","title":"34.2-6"},{"location":"Chap34/34.2/#342-7","text":"Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved in polynomial time on directed acyclic graphs. Give an efficient algorithm for the problem. (Omit!)","title":"34.2-7"},{"location":"Chap34/34.2/#342-8","text":"Let $\\phi$ be a boolean formula constructed from the boolean input variables $x_1, x_2, \\dots, x_k$, negations ($\\neg$), ANDs ($\\vee$), ORs ($\\wedge$), and parentheses. The formula $\\phi$ is a tautology if it evaluates to $1$ for every assignment of $1$ and $0$ to the input variables. Define $\\text{TAUTOLOGY}$ as the language of boolean formulas that are tautologies. Show that $\\text{TAUTOLOGY} \\in \\text{co-NP}$. (Omit!)","title":"34.2-8"},{"location":"Chap34/34.2/#342-9","text":"Prove that $\\text P \\subseteq \\text{co-NP}$. (Omit!)","title":"34.2-9"},{"location":"Chap34/34.2/#342-10","text":"Prove that if $\\text{NP} \\ne \\text{co-NP}$, then $\\text P \\ne \\text{NP}$. (Omit!)","title":"34.2-10"},{"location":"Chap34/34.2/#342-11","text":"Let $G$ be a connected, undirected graph with at least $3$ vertices, and let $G^3$ be the graph obtained by connecting all pairs of vertices that are connected by a path in $G$ of length at most $3$. Prove that $G^3$ is hamiltonian. ($\\textit{Hint:}$ Construct a spanning tree for $G$, and use an inductive argument.) (Omit!)","title":"34.2-11"},{"location":"Chap34/34.3/","text":"34.3-1 Verify that the circuit in Figure 34.8(b) is unsatisfiable. (Omit!) 34.3-2 Show that the $\\le_\\text P$ relation is a transitive relation on languages. That is, show that if $L_1 \\le_\\text P L_2$ and $L_2 \\le_\\text P L_3$, then $L_1 \\le_\\text P L_3$. (Omit!) 34.3-3 Prove that $L \\le_\\text P \\bar L$ if and only if $\\bar L \\le_\\text P L$. (Omit!) 34.3-4 Show that we could have used a satisfying assignment as a certificate in an alternative proof of Lemma 34.5. Which certificate makes for an easier proof? (Omit!) 34.3-5 The proof of Lemma 34.6 assumes that the working storage for algorithm A occupies a contiguous region of polynomial size. Where in the proof do we exploit this assumption? Argue that this assumption does not involve any loss of generality. (Omit!) 34.3-6 A language $L$ is complete for a language class $C$ with respect to polynomial-time reductions if $L \\in C$ and $L' \\le_\\text P L$ for all $L' \\in C$. Show that $\\emptyset$ and $\\{0, 1\\}^*$ are the only languages in $\\text P$ that are not complete for $\\text P$ with respect to polynomial-time reductions. (Omit!) 34.3-7 Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), $L$ is complete for $\\text{NP}$ if and only if $\\bar L$ is complete for \\text{co-NP}\\$. (Omit!) 34.3-8 The reduction algorithm $F$ in the proof of Lemma 34.6 constructs the circuit $C = f(x)$ based on knowledge of $x$, $A$, and $k$. Professor Sartre observes that the string $x$ is input to $F$, but only the existence of $A$, $k$, and the constant factor implicit in the $O(n^k)$ running time is known to $F$ (since the language $L$ belongs to $\\text{NP}$), not their actual values. Thus, the professor concludes that $F$ can't possibly construct the circuit $C$ and that the language $\\text{CIRCUIT-SAT}$ is not necessarily $\\text{NP-hard}$. Explain the flaw in the professor's reasoning. (Omit!)","title":"34.3 NP-completeness and reducibility"},{"location":"Chap34/34.3/#343-1","text":"Verify that the circuit in Figure 34.8(b) is unsatisfiable. (Omit!)","title":"34.3-1"},{"location":"Chap34/34.3/#343-2","text":"Show that the $\\le_\\text P$ relation is a transitive relation on languages. That is, show that if $L_1 \\le_\\text P L_2$ and $L_2 \\le_\\text P L_3$, then $L_1 \\le_\\text P L_3$. (Omit!)","title":"34.3-2"},{"location":"Chap34/34.3/#343-3","text":"Prove that $L \\le_\\text P \\bar L$ if and only if $\\bar L \\le_\\text P L$. (Omit!)","title":"34.3-3"},{"location":"Chap34/34.3/#343-4","text":"Show that we could have used a satisfying assignment as a certificate in an alternative proof of Lemma 34.5. Which certificate makes for an easier proof? (Omit!)","title":"34.3-4"},{"location":"Chap34/34.3/#343-5","text":"The proof of Lemma 34.6 assumes that the working storage for algorithm A occupies a contiguous region of polynomial size. Where in the proof do we exploit this assumption? Argue that this assumption does not involve any loss of generality. (Omit!)","title":"34.3-5"},{"location":"Chap34/34.3/#343-6","text":"A language $L$ is complete for a language class $C$ with respect to polynomial-time reductions if $L \\in C$ and $L' \\le_\\text P L$ for all $L' \\in C$. Show that $\\emptyset$ and $\\{0, 1\\}^*$ are the only languages in $\\text P$ that are not complete for $\\text P$ with respect to polynomial-time reductions. (Omit!)","title":"34.3-6"},{"location":"Chap34/34.3/#343-7","text":"Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), $L$ is complete for $\\text{NP}$ if and only if $\\bar L$ is complete for \\text{co-NP}\\$. (Omit!)","title":"34.3-7"},{"location":"Chap34/34.3/#343-8","text":"The reduction algorithm $F$ in the proof of Lemma 34.6 constructs the circuit $C = f(x)$ based on knowledge of $x$, $A$, and $k$. Professor Sartre observes that the string $x$ is input to $F$, but only the existence of $A$, $k$, and the constant factor implicit in the $O(n^k)$ running time is known to $F$ (since the language $L$ belongs to $\\text{NP}$), not their actual values. Thus, the professor concludes that $F$ can't possibly construct the circuit $C$ and that the language $\\text{CIRCUIT-SAT}$ is not necessarily $\\text{NP-hard}$. Explain the flaw in the professor's reasoning. (Omit!)","title":"34.3-8"},{"location":"Chap34/34.4/","text":"34.4-1 Consider the straightforward (nonpolynomial-time) reduction in the proof of Theorem 34.9. Describe a circuit of size $n$ that, when converted to a formula by this method, yields a formula whose size is exponential in $n$. (Omit!) 34.4-2 Show the $\\text{3-CNF}$ formula that results when we use the method of Theorem 34.10 on the formula $\\text{(34.3)}$. (Omit!) 34.4-3 Professor Jagger proposes to show that $\\text{SAT} \\le_\\text P \\text{3-CNF-SAT}$ by using only the truth-table technique in the proof of Theorem 34.10, and not the other steps. That is, the professor proposes to take the boolean formula $\\phi$, form a truth table for its variables, derive from the truth table a formula in $\\text{3-DNF}$ that is equivalent to $\\neg\\phi$, and then negate and apply DeMorgan's laws to produce a $\\text{3-CNF}$ formula equivalent to $\\phi$. Show that this strategy does not yield a polynomial-time reduction. (Omit!) 34.4-4 Show that the problem of determining whether a boolean formula is a tautology is complete for $\\text{co-NP}$. ($\\textit{Hint:}$ See Exercise 34.3-7.) (Omit!) 34.4-5 Show that the problem of determining the satisfiability of boolean formulas in disjunctive normal form is polynomial-time solvable. (Omit!) 34.4-6 Suppose that someone gives you a polynomial-time algorithm to decide formula satisfiability. Describe how to use this algorithm to find satisfying assignments in polynomial time. (Omit!) 34.4-7 Let $\\text{2-CNF-SAT}$ be the set of satisfiable boolean formulas in $\\text{CNF}$ with exactly $2$ literals per clause. Show that $\\text{2-CNF-SAT} \\in P$. Make your algorithm as efficient as possible. ($\\textit{Hint:}$ Observe that $x \\vee y$ is equivalent to $\\neg x \\to y$. Reduce $\\text{2-CNF-SAT}$ to an efficiently solvable problem on a directed graph.) (Omit!)","title":"34.4 NP-completeness proofs"},{"location":"Chap34/34.4/#344-1","text":"Consider the straightforward (nonpolynomial-time) reduction in the proof of Theorem 34.9. Describe a circuit of size $n$ that, when converted to a formula by this method, yields a formula whose size is exponential in $n$. (Omit!)","title":"34.4-1"},{"location":"Chap34/34.4/#344-2","text":"Show the $\\text{3-CNF}$ formula that results when we use the method of Theorem 34.10 on the formula $\\text{(34.3)}$. (Omit!)","title":"34.4-2"},{"location":"Chap34/34.4/#344-3","text":"Professor Jagger proposes to show that $\\text{SAT} \\le_\\text P \\text{3-CNF-SAT}$ by using only the truth-table technique in the proof of Theorem 34.10, and not the other steps. That is, the professor proposes to take the boolean formula $\\phi$, form a truth table for its variables, derive from the truth table a formula in $\\text{3-DNF}$ that is equivalent to $\\neg\\phi$, and then negate and apply DeMorgan's laws to produce a $\\text{3-CNF}$ formula equivalent to $\\phi$. Show that this strategy does not yield a polynomial-time reduction. (Omit!)","title":"34.4-3"},{"location":"Chap34/34.4/#344-4","text":"Show that the problem of determining whether a boolean formula is a tautology is complete for $\\text{co-NP}$. ($\\textit{Hint:}$ See Exercise 34.3-7.) (Omit!)","title":"34.4-4"},{"location":"Chap34/34.4/#344-5","text":"Show that the problem of determining the satisfiability of boolean formulas in disjunctive normal form is polynomial-time solvable. (Omit!)","title":"34.4-5"},{"location":"Chap34/34.4/#344-6","text":"Suppose that someone gives you a polynomial-time algorithm to decide formula satisfiability. Describe how to use this algorithm to find satisfying assignments in polynomial time. (Omit!)","title":"34.4-6"},{"location":"Chap34/34.4/#344-7","text":"Let $\\text{2-CNF-SAT}$ be the set of satisfiable boolean formulas in $\\text{CNF}$ with exactly $2$ literals per clause. Show that $\\text{2-CNF-SAT} \\in P$. Make your algorithm as efficient as possible. ($\\textit{Hint:}$ Observe that $x \\vee y$ is equivalent to $\\neg x \\to y$. Reduce $\\text{2-CNF-SAT}$ to an efficiently solvable problem on a directed graph.) (Omit!)","title":"34.4-7"},{"location":"Chap34/34.5/","text":"34.5-1 The subgraph-isomorphism problem takes two undirected graphs $G_1$ and $G_2$, and it asks whether $G_1$ is isomorphic to a subgraph of $G_2$. Show that the subgraphisomorphism problem is $\\text{NP-complete}$. (Omit!) 34.5-2 Given an integer $m \\times n$ matrix $A$ and an integer $m$-vector $b$, the 0-1 integerprogramming problem asks whether there exists an integer $n$-vector $x$ with elements in the set $\\{0, 1\\}$ such that $Ax \\le b$. Prove that 0-1 integer programming is $\\text{NP-complete}$. ($\\textit{Hint:}$ Reduce from $\\text{3-CNF-SAT}$.) (Omit!) 34.5-3 The integer linear-programming problem is like the 0-1 integer-programming problem given in Exercise 34.5-2, except that the values of the vector $x$ may be any integers rather than just $0$ or $1$. Assuming that the 0-1 integer-programming problem is $\\text{NP-hard}$, show that the integer linear-programming problem is $\\text{NP-complete}$. (Omit!) 34.5-4 Show how to solve the subset-sum problem in polynomial time if the target value $t$ is expressed in unary. (Omit!) 34.5-5 The set-partition problem takes as input a set $S$ of numbers. The question is whether the numbers can be partitioned into two sets $A$ and $\\bar A = S - A$ such that $\\sum_{x \\in A} x = \\sum_{x \\in \\bar A} x$. Show that the set-partition problem is $\\text{NP-complete}$. (Omit!) 34.5-6 Show that the hamiltonian-path problem is $\\text{NP-complete}$. (Omit!) 34.5-7 The longest-simple-cycle problem is the problem of determining a simple cycle (no repeated vertices) of maximum length in a graph. Formulate a related decision problem, and show that the decision problem is $\\text{NP-complete}$. (Omit!) 34.5-8 In the half 3-CNF satisfiability problem, we are given a $\\text{3-CNF}$ formula $\\phi$ with $n$ variables and $m$ clauses, where $m$ is even. We wish to determine whether there exists a truth assignment to the variables of $\\phi$ such that exactly half the clauses evaluate to $0$ and exactly half the clauses evaluate to $1$. Prove that the half $\\text{3-CNF}$ satisfiability problem is $\\text{NP-complete}$. (Omit!)","title":"34.5 NP-complete problems"},{"location":"Chap34/34.5/#345-1","text":"The subgraph-isomorphism problem takes two undirected graphs $G_1$ and $G_2$, and it asks whether $G_1$ is isomorphic to a subgraph of $G_2$. Show that the subgraphisomorphism problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-1"},{"location":"Chap34/34.5/#345-2","text":"Given an integer $m \\times n$ matrix $A$ and an integer $m$-vector $b$, the 0-1 integerprogramming problem asks whether there exists an integer $n$-vector $x$ with elements in the set $\\{0, 1\\}$ such that $Ax \\le b$. Prove that 0-1 integer programming is $\\text{NP-complete}$. ($\\textit{Hint:}$ Reduce from $\\text{3-CNF-SAT}$.) (Omit!)","title":"34.5-2"},{"location":"Chap34/34.5/#345-3","text":"The integer linear-programming problem is like the 0-1 integer-programming problem given in Exercise 34.5-2, except that the values of the vector $x$ may be any integers rather than just $0$ or $1$. Assuming that the 0-1 integer-programming problem is $\\text{NP-hard}$, show that the integer linear-programming problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-3"},{"location":"Chap34/34.5/#345-4","text":"Show how to solve the subset-sum problem in polynomial time if the target value $t$ is expressed in unary. (Omit!)","title":"34.5-4"},{"location":"Chap34/34.5/#345-5","text":"The set-partition problem takes as input a set $S$ of numbers. The question is whether the numbers can be partitioned into two sets $A$ and $\\bar A = S - A$ such that $\\sum_{x \\in A} x = \\sum_{x \\in \\bar A} x$. Show that the set-partition problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-5"},{"location":"Chap34/34.5/#345-6","text":"Show that the hamiltonian-path problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-6"},{"location":"Chap34/34.5/#345-7","text":"The longest-simple-cycle problem is the problem of determining a simple cycle (no repeated vertices) of maximum length in a graph. Formulate a related decision problem, and show that the decision problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-7"},{"location":"Chap34/34.5/#345-8","text":"In the half 3-CNF satisfiability problem, we are given a $\\text{3-CNF}$ formula $\\phi$ with $n$ variables and $m$ clauses, where $m$ is even. We wish to determine whether there exists a truth assignment to the variables of $\\phi$ such that exactly half the clauses evaluate to $0$ and exactly half the clauses evaluate to $1$. Prove that the half $\\text{3-CNF}$ satisfiability problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-8"},{"location":"Chap34/Problems/34-1/","text":"An independent set of a graph $G = (V, E)$ is a subset $V' \\subseteq V$ of vertices such that each edge in $E$ is incident on at most one vertex in $V'$. The independent-set problem is to find a maximum-size independent set in $G$. a. Formulate a related decision problem for the independent-set problem, and prove that it is $\\text{NP-complete}$. ($\\textit{Hint:}$ Reduce from the clique problem.) b. Suppose that you are given a \"black-box\" subroutine to solve the decision problem you defined in part (a). Give an algorithm to find an independent set of maximum size. The running time of your algorithm should be polynomial in $|V|$ and $|E|$, counting queries to the black box as a single step. Although the independent-set decision problem is $\\text{NP-complete}$, certain special cases are polynomial-time solvable. c. Give an efficient algorithm to solve the independent-set problem when each vertex in $G$ has degree $2$. Analyze the running time, and prove that your algorithm works correctly. d. Give an efficient algorithm to solve the independent-set problem when $G$ is bipartite. Analyze the running time, and prove that your algorithm works correctly. ($\\text{Hint:}$ Use the results of Section 26.3.) (Omit!)","title":"34-1 Independent set"},{"location":"Chap34/Problems/34-2/","text":"Bonnie and Clyde have just robbed a bank. They have a bag of money and want to divide it up. For each of the following scenarios, either give a polynomial-time algorithm, or prove that the problem is $\\text{NP-complete}$. The input in each case is a list of the $n$ items in the bag, along with the value of each. a. The bag contains $n$ coins, but only $2$ different denominations: some coins are worth $x$ dollars, and some are worth $y$ dollars. Bonnie and Clyde wish to divide the money exactly evenly. b. The bag contains $n$ coins, with an arbitrary number of different denominations, but each denomination is a nonnegative integer power of $2$, i.e., the possible denominations are $1$ dollar, $2$ dollars, $4$ dollars, etc. Bonnie and Clyde wish to divide the money exactly evenly. c. The bag contains $n$ checks, which are, in an amazing coincidence, made out to \"Bonnie or Clyde.\" They wish to divide the checks so that they each get the exact same amount of money. d. The bag contains $n$ checks as in part (c), but this time Bonnie and Clyde are willing to accept a split in which the difference is no larger than $100$ dollars. (Omit!)","title":"34-2 Bonnie and Clyde"},{"location":"Chap34/Problems/34-3/","text":"Mapmakers try to use as few colors as possible when coloring countries on a map, as long as no two countries that share a border have the same color. We can model this problem with an undirected graph $G = (V, E)$ in which each vertex represents a country and vertices whose respective countries share a border are adjacent. Then, a $k$-coloring is a function $c: V \\to \\{1, 2, \\dots, k \\}$ such that $c(u) \\ne c(v)$ for every edge $(u, v) \\in E$. In other words, the numbers $1, 2, \\dots, k$ represent the $k$ colors, and adjacent vertices must have different colors. The graph-coloring problem is to determine the minimum number of colors needed to color a given graph. a. Give an efficient algorithm to determine a $2$-coloring of a graph, if one exists. b. Cast the graph-coloring problem as a decision problem. Show that your decision problem is solvable in polynomial time if and only if the graph-coloring problem is solvable in polynomial time. c. Let the language $\\text{3-COLOR}$ be the set of graphs that can be $3$-colored. Show that if $\\text{3-COLOR}$ is $\\text{NP-complete}$, then your decision problem from part (b) is $\\text{NP-complete}$. To prove that $\\text{3-COLOR}$ is $\\text{NP-complete}$, we use a reduction from $\\text{3-CNF-SAT}$. Given a formula $\\phi$ of $m$ clauses on $n$ variables $x_1, x_2, \\dots, x_n$, we construct a graph $G = (V, E)$ as follows. The set $V$ consists of a vertex for each variable, a vertex for the negation of each variable, $5$ vertices for each clause, and $3$ special vertices: $\\text{TRUE}$, $\\text{FALSE}$, and $\\text{RED}$. The edges of the graph are of two types: \"literal\" edges that are independent of the clauses and \"clause\" edges that depend on the clauses. The literal edges form a triangle on the special vertices and also form a triangle on $x_i, \\neg x_i$, and $\\text{RED}$ for $i = 1, 2, \\dots, n$. d. Argue that in any $3$-coloring $c$ of a graph containing the literal edges, exactly one of a variable and its negation is colored $c(\\text{TRUE})$ and the other is colored $c(\\text{FALSE})$. Argue that for any truth assignment for $\\phi$, there exists a $3$-coloring of the graph containing just the literal edges. The widget shown in Figure 34.20 helps to enforce the condition corresponding to a clause $(x \\vee y \\vee z)$. Each clause requires a unique copy of the $5$ vertices that are heavily shaded in the figure; they connect as shown to the literals of the clause and the special vertex $\\text{TRUE}$. e. Argue that if each of $x$, $y$, and $z$ is colored $c(\\text{TRUE})$ or $c(\\text{FALSE})$, then the widget is $3$-colorable if and only if at least one of $x$, $y$, or $z$ is colored $c(\\text{TRUE})$. f. Complete the proof that $\\text{3-COLOR}$ is $\\text{NP-complete}$. (Omit!)","title":"34-3 Graph coloring"},{"location":"Chap34/Problems/34-4/","text":"Suppose that we have one machine and a set of $n$ tasks $a_1, a_2, \\dots, a_n$, each of which requires time on the machine. Each task $a_j$ requires $t_j$ time units on the machine (its processing time), yields a profit of $p_j$, and has a deadline $d_j$. The machine can process only one task at a time, and task $a_j$ must run without interruption for $t_j$ consecutive time units. If we complete task $a_j$ by its deadline $d_j$, we receive a profit $p_j$, but if we complete it after its deadline, we receive no profit. As an optimization problem, we are given the processing times, profits, and deadlines for a set of $n$ tasks, and we wish to find a schedule that completes all the tasks and returns the greatest amount of profit. The processing times, profits, and deadlines are all nonnegative numbers. a. State this problem as a decision problem. b. Show that the decision problem is $\\text{NP-complete}$. c. Give a polynomial-time algorithm for the decision problem, assuming that all processing times are integers from $1$ to $n$. ($\\textit{Hint:}$ Use dynamic programming.) d. Give a polynomial-time algorithm for the optimization problem, assuming that all processing times are integers from $1$ to $n$. (Omit!)","title":"34-4 Scheduling with profits and deadlines"},{"location":"Chap35/35.1/","text":"35.1-1 Give an example of a graph for which $\\text{APPROX-VERTEX-COVER}$ always yields a suboptimal solution. (Omit!) 35.1-2 Prove that the set of edges picked in line 4 of $\\text{APPROX-VERTEX-COVER}$ forms a maximal matching in the graph $G$. (Omit!) 35.1-3 $\\star$ Professor B\u00fcndchen proposes the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its incident edges. Give an example to show that the professor's heuristic does not have an approximation ratio of $2$. ($\\textit{Hint:}$ Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.) Consider a bibartite graph with left part $L$ and right part $R$ such that $L$ has $5$ vertices of degrees $(5, 5, 5, 5, 5)$ and $R$ has $11$ vertices of degrees $(5, 4, 4, 3, 2, 2, 1, 1, 1, 1, 1)$ (the graph is easy to draw and the figure is omitted here). Clearly there exists a vertex-cover of size $5$ (the left vertices). The idea is to show that the proposed algorithm chooses all the vertices on the right part, resulting in the approximation ratio of $11 / 5 > 2$. After choosing the first vertex in $R$, the degrees on $L$ decrease to $(4, 4, 4, 4, 4)$. After choosing the second vertex in $R$, the degrees on $L$ decrease to $(4, 3, 3, 3, 3)$. After choosing the third vertex in $R$, the degrees on $L$ decrease to $(3, 3, 2, 2, 2)$. After choosing the fourth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 2, 2, 1)$. After choosing the fifth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 1, 1, 1)$. After choosing the sixth vertex in $R$, the degrees on $L$ decrease to $(1, 1, 1, 1, 1)$. Now the algorithm still has to choose $5$ more vertices. 35.1-4 Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time. (Omit!) 35.1-5 From the proof of Theorem 34.12, we know that the vertex-cover problem and the $\\text{NP-complete}$ clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer. (Omit!)","title":"35.1 The vertex-cover problem"},{"location":"Chap35/35.1/#351-1","text":"Give an example of a graph for which $\\text{APPROX-VERTEX-COVER}$ always yields a suboptimal solution. (Omit!)","title":"35.1-1"},{"location":"Chap35/35.1/#351-2","text":"Prove that the set of edges picked in line 4 of $\\text{APPROX-VERTEX-COVER}$ forms a maximal matching in the graph $G$. (Omit!)","title":"35.1-2"},{"location":"Chap35/35.1/#351-3-star","text":"Professor B\u00fcndchen proposes the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its incident edges. Give an example to show that the professor's heuristic does not have an approximation ratio of $2$. ($\\textit{Hint:}$ Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.) Consider a bibartite graph with left part $L$ and right part $R$ such that $L$ has $5$ vertices of degrees $(5, 5, 5, 5, 5)$ and $R$ has $11$ vertices of degrees $(5, 4, 4, 3, 2, 2, 1, 1, 1, 1, 1)$ (the graph is easy to draw and the figure is omitted here). Clearly there exists a vertex-cover of size $5$ (the left vertices). The idea is to show that the proposed algorithm chooses all the vertices on the right part, resulting in the approximation ratio of $11 / 5 > 2$. After choosing the first vertex in $R$, the degrees on $L$ decrease to $(4, 4, 4, 4, 4)$. After choosing the second vertex in $R$, the degrees on $L$ decrease to $(4, 3, 3, 3, 3)$. After choosing the third vertex in $R$, the degrees on $L$ decrease to $(3, 3, 2, 2, 2)$. After choosing the fourth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 2, 2, 1)$. After choosing the fifth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 1, 1, 1)$. After choosing the sixth vertex in $R$, the degrees on $L$ decrease to $(1, 1, 1, 1, 1)$. Now the algorithm still has to choose $5$ more vertices.","title":"35.1-3 $\\star$"},{"location":"Chap35/35.1/#351-4","text":"Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time. (Omit!)","title":"35.1-4"},{"location":"Chap35/35.1/#351-5","text":"From the proof of Theorem 34.12, we know that the vertex-cover problem and the $\\text{NP-complete}$ clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer. (Omit!)","title":"35.1-5"},{"location":"Chap35/35.2/","text":"35.2-1 Suppose that a complete undirected graph $G = (V, E)$ with at least $3$ vertices has a cost function $c$ that satisfies the triangle inequality. Prove that $c(u, v) \\ge 0$ for all $u, v \\in V$. (Omit!) 35.2-2 Show how in polynomial time we can transform one instance of the traveling-salesman problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why such a polynomial-time transformation does not contradict Theorem 35.3, assuming that $\\text P \\ne \\text{NP}$. (Omit!) 35.2-3 Consider the following closest-point heuristic for building an approximate traveling-salesman tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex $u$ that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest $u$ is vertex $v$. Extend the cycle to include $u$ by inserting $u$ just after $v$. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour. (Omit!) 35.2-4 In the bottleneck traveling-salesman problem , we wish to find the hamiltonian cycle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial-time approximation algorithm with approximation ratio $3$ for this problem. ($\\textit{Hint:}$ Show recursively that we can visit all the nodes in a bottleneck spanning tree, as discussed in Problem 23-3, exactly once by taking a full walk of the tree and skipping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost that is at most the cost of the costliest edge in a bottleneck hamiltonian cycle.) (Omit!) 35.2-5 Suppose that the vertices for an instance of the traveling-salesman problem are points in the plane and that the cost $c(u, v)$ is the euclidean distance between points $u$ and $v$. Show that an optimal tour never crosses itself. (Omit!)","title":"35.2 The traveling-salesman problem"},{"location":"Chap35/35.2/#352-1","text":"Suppose that a complete undirected graph $G = (V, E)$ with at least $3$ vertices has a cost function $c$ that satisfies the triangle inequality. Prove that $c(u, v) \\ge 0$ for all $u, v \\in V$. (Omit!)","title":"35.2-1"},{"location":"Chap35/35.2/#352-2","text":"Show how in polynomial time we can transform one instance of the traveling-salesman problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why such a polynomial-time transformation does not contradict Theorem 35.3, assuming that $\\text P \\ne \\text{NP}$. (Omit!)","title":"35.2-2"},{"location":"Chap35/35.2/#352-3","text":"Consider the following closest-point heuristic for building an approximate traveling-salesman tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex $u$ that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest $u$ is vertex $v$. Extend the cycle to include $u$ by inserting $u$ just after $v$. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour. (Omit!)","title":"35.2-3"},{"location":"Chap35/35.2/#352-4","text":"In the bottleneck traveling-salesman problem , we wish to find the hamiltonian cycle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial-time approximation algorithm with approximation ratio $3$ for this problem. ($\\textit{Hint:}$ Show recursively that we can visit all the nodes in a bottleneck spanning tree, as discussed in Problem 23-3, exactly once by taking a full walk of the tree and skipping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost that is at most the cost of the costliest edge in a bottleneck hamiltonian cycle.) (Omit!)","title":"35.2-4"},{"location":"Chap35/35.2/#352-5","text":"Suppose that the vertices for an instance of the traveling-salesman problem are points in the plane and that the cost $c(u, v)$ is the euclidean distance between points $u$ and $v$. Show that an optimal tour never crosses itself. (Omit!)","title":"35.2-5"},{"location":"Chap35/35.3/","text":"35.3-1 Consider each of the following words as a set of letters: $\\{\\text{arid}$, $\\text{dash}$, $\\text{drain}$, $\\text{heard}$, $\\text{lost}$, $\\text{nose}$, $\\text{shun}$, $\\text{slate}$, $\\text{snare}$, $\\text{thread}\\}$. Show which set cover $\\text{GREEDY-SET-COVER}$ produces when we break ties in favor of the word that appears \ufb01rst in the dictionary. (Omit!) 35.3-2 Show that the decision version of the set-covering problem is $\\text{NP-complete}$ by reducing it from the vertex-cover problem. (Omit!) 35.3-3 Show how to implement $\\text{GREEDY-SET-COVER}$ in such a way that it runs in time $O\\Big(\\sum_{S \\in \\mathcal F} |S|\\Big)$. (Omit!) 35.3-4 Show that the following weaker form of Theorem 35.4 is trivially true: $$|\\mathcal C| \\le |\\mathcal C^*| \\max\\{|S|: S \\in \\mathcal F\\}.$$ (Omit!) 35.3-5 $\\text{GREEDY-SET-COVER}$ can return a number of different solutions, depending on how we break ties in line 4. Give a procedure $\\text{BAD-SET-COVER-INSTANCE}(n)$ that returns an $n$-element instance of the set-covering problem for which, depending on how we break ties in line 4, $\\text{GREEDY-SET-COVER}$ can return a number of different solutions that is exponential in $n$. (Omit!)","title":"35.3 The set-covering problem"},{"location":"Chap35/35.3/#353-1","text":"Consider each of the following words as a set of letters: $\\{\\text{arid}$, $\\text{dash}$, $\\text{drain}$, $\\text{heard}$, $\\text{lost}$, $\\text{nose}$, $\\text{shun}$, $\\text{slate}$, $\\text{snare}$, $\\text{thread}\\}$. Show which set cover $\\text{GREEDY-SET-COVER}$ produces when we break ties in favor of the word that appears \ufb01rst in the dictionary. (Omit!)","title":"35.3-1"},{"location":"Chap35/35.3/#353-2","text":"Show that the decision version of the set-covering problem is $\\text{NP-complete}$ by reducing it from the vertex-cover problem. (Omit!)","title":"35.3-2"},{"location":"Chap35/35.3/#353-3","text":"Show how to implement $\\text{GREEDY-SET-COVER}$ in such a way that it runs in time $O\\Big(\\sum_{S \\in \\mathcal F} |S|\\Big)$. (Omit!)","title":"35.3-3"},{"location":"Chap35/35.3/#353-4","text":"Show that the following weaker form of Theorem 35.4 is trivially true: $$|\\mathcal C| \\le |\\mathcal C^*| \\max\\{|S|: S \\in \\mathcal F\\}.$$ (Omit!)","title":"35.3-4"},{"location":"Chap35/35.3/#353-5","text":"$\\text{GREEDY-SET-COVER}$ can return a number of different solutions, depending on how we break ties in line 4. Give a procedure $\\text{BAD-SET-COVER-INSTANCE}(n)$ that returns an $n$-element instance of the set-covering problem for which, depending on how we break ties in line 4, $\\text{GREEDY-SET-COVER}$ can return a number of different solutions that is exponential in $n$. (Omit!)","title":"35.3-5"},{"location":"Chap35/35.4/","text":"35.4-1 Show that even if we allow a clause to contain both a variable and its negation, randomly setting each variable to 1 with probability $1 / 2$ and to $0$ with probability $1 / 2$ still yields a randomized $8 / 7$-approximation algorithm. (Omit!) 35.4-2 The MAX-CNF satisfiability problem is like the $\\text{MAX-3-CNF}$ satisfiability problem, except that it does not restrict each clause to have exactly $3$ literals. Give a randomized $2$-approximation algorithm for the $\\text{MAX-CNF}$ satisfiability problem. (Omit!) 35.4-3 In the $\\text{MAX-CUT}$ problem, we are given an unweighted undirected graph $G = (V, E)$. We define a cut $(S, V - S)$ as in Chapter 23 and the weight of a cut as the number of edges crossing the cut. The goal is to find a cut of maximum weight. Suppose that for each vertex $v$, we randomly and independently place $v$ in $S$ with probability $1 / 2$ and in $V - S$ with probability $1 / 2$. Show that this algorithm is a randomized $2$-approximation algorithm. (Omit!) 35.4-4 Show that the constraints in line $\\text{(35.19)}$ are redundant in the sense that if we remove them from the linear program in lines $\\text{(35.17)}\u2013\\text{(35.20)}$, any optimal solution to the resulting linear program must satisfy $x(v) \\le 1$ for each $v \\in V$. (Omit!)","title":"35.4 Randomization and linear programming"},{"location":"Chap35/35.4/#354-1","text":"Show that even if we allow a clause to contain both a variable and its negation, randomly setting each variable to 1 with probability $1 / 2$ and to $0$ with probability $1 / 2$ still yields a randomized $8 / 7$-approximation algorithm. (Omit!)","title":"35.4-1"},{"location":"Chap35/35.4/#354-2","text":"The MAX-CNF satisfiability problem is like the $\\text{MAX-3-CNF}$ satisfiability problem, except that it does not restrict each clause to have exactly $3$ literals. Give a randomized $2$-approximation algorithm for the $\\text{MAX-CNF}$ satisfiability problem. (Omit!)","title":"35.4-2"},{"location":"Chap35/35.4/#354-3","text":"In the $\\text{MAX-CUT}$ problem, we are given an unweighted undirected graph $G = (V, E)$. We define a cut $(S, V - S)$ as in Chapter 23 and the weight of a cut as the number of edges crossing the cut. The goal is to find a cut of maximum weight. Suppose that for each vertex $v$, we randomly and independently place $v$ in $S$ with probability $1 / 2$ and in $V - S$ with probability $1 / 2$. Show that this algorithm is a randomized $2$-approximation algorithm. (Omit!)","title":"35.4-3"},{"location":"Chap35/35.4/#354-4","text":"Show that the constraints in line $\\text{(35.19)}$ are redundant in the sense that if we remove them from the linear program in lines $\\text{(35.17)}\u2013\\text{(35.20)}$, any optimal solution to the resulting linear program must satisfy $x(v) \\le 1$ for each $v \\in V$. (Omit!)","title":"35.4-4"},{"location":"Chap35/35.5/","text":"35.5-1 Prove equation $\\text{(35.23)}$. Then show that after executing line 5 of $\\text{EXACT-SUBSET-SUM}$, $L_i$ is a sorted list containing every element of $P_i$ whose value is not more than $t$. (Omit!) 35.5-2 Using induction on $i$, prove inequality $\\text{(35.26)}$. (Omit!) 35.5-3 Prove inequality $\\text{(35.29)}$. (Omit!) 35.5-4 How would you modify the approximation scheme presented in this section to \ufb01nd a good approximation to the smallest value not less than $t$ that is a sum of some subset of the given input list? (Omit!) 35.5-5 Modify the $\\text{APPROX-SUBSET-SUM}$ procedure to also return the subset of $S$ that sums to the value $z^*$. (Omit!)","title":"35.5 The subset-sum problem"},{"location":"Chap35/35.5/#355-1","text":"Prove equation $\\text{(35.23)}$. Then show that after executing line 5 of $\\text{EXACT-SUBSET-SUM}$, $L_i$ is a sorted list containing every element of $P_i$ whose value is not more than $t$. (Omit!)","title":"35.5-1"},{"location":"Chap35/35.5/#355-2","text":"Using induction on $i$, prove inequality $\\text{(35.26)}$. (Omit!)","title":"35.5-2"},{"location":"Chap35/35.5/#355-3","text":"Prove inequality $\\text{(35.29)}$. (Omit!)","title":"35.5-3"},{"location":"Chap35/35.5/#355-4","text":"How would you modify the approximation scheme presented in this section to \ufb01nd a good approximation to the smallest value not less than $t$ that is a sum of some subset of the given input list? (Omit!)","title":"35.5-4"},{"location":"Chap35/35.5/#355-5","text":"Modify the $\\text{APPROX-SUBSET-SUM}$ procedure to also return the subset of $S$ that sums to the value $z^*$. (Omit!)","title":"35.5-5"},{"location":"Chap35/Problems/35-1/","text":"Suppose that we are given a set of $n$ objects, where the size $s_i$ of the $i$th object satisfies $0 < s_i < 1$. We wish to pack all the objects into the minimum number of unit-size bins. Each bin can hold any subset of the objects whose total size does not exceed $1$. a. Prove that the problem of determining the minimum number of bins required is $\\text{NP-hard}$. ($\\textit{Hint:}$ Reduce from the subset-sum problem.) The first-fit heuristic takes each object in turn and places it into the first bin that can accommodate it. Let $S = \\sum_{i = 1}^n s_i$. b. Argue that the optimal number of bins required is at least $\\lceil S \\rceil$. c. Argue that the first-fit heuristic leaves at most one bin less than half full. d. Prove that the number of bins used by the first-fit heuristic is never more than $\\lceil 2S \\rceil$. e. Prove an approximation ratio of $2$ for the first-fit heuristic. f. Give an efficient implementation of the first-fit heuristic, and analyze its running time. (Omit!)","title":"35-1 Bin packing"},{"location":"Chap35/Problems/35-2/","text":"Let $G = (V, E)$ be an undirected graph. For any $k \\ge 1$, define $G^{(k)}$ to be the undirected graph $(V^{(k)}, E^{(k)})$, where $V^{(k)}$ is the set of all ordered $k$-tuples of vertices from $V$ and $E^{(k)}$ is defined so that $(v_1, v_2, \\dots, v_k)$ is adjacent to $(w_1, w_2, \\dots, w_k)$ if and only if for $i = 1, 2, \\dots, k$, either vertex $v_i$ is adjacent to $w_i$ in $G$, or else $v_i = w_i$. a. Prove that the size of the maximum clique in $G^{(k)}$ is equal to the $k$th power of the size of the maximum clique in $G$. b. Argue that if there is an approximation algorithm that has a constant approximation ratio for finding a maximum-size clique, then there is a polynomial-time approximation scheme for the problem. (Omit!)","title":"35-2 Approximating the size of a maximum clique"},{"location":"Chap35/Problems/35-3/","text":"Suppose that we generalize the set-covering problem so that each set $S_i$ in the family $\\mathcal F$ has an associated weight $w_i$ and the weight of a cover $\\mathcal C$ is $\\sum_{S_i \\in \\mathcal C} w_i$. We wish to determine a minimum-weight cover. (Section 35.3 handles the case in which $w_i = 1$ for all $i$.) Show how to generalize the greedy set-covering heuristic in a natural manner to provide an approximate solution for any instance of the weighted set-covering problem. Show that your heuristic has an approximation ratio of $H(d)$, where $d$ is the maximum size of any set $S_i$. (Omit!)","title":"35-3 Weighted set-covering problem"},{"location":"Chap35/Problems/35-4/","text":"Recall that for an undirected graph $G$, a matching is a set of edges such that no two edges in the set are incident on the same vertex. In Section 26.3, we saw how to find a maximum matching in a bipartite graph. In this problem, we will look at matchings in undirected graphs in general (i.e., the graphs are not required to be bipartite). a. A maximal matching is a matching that is not a proper subset of any other matching. Show that a maximal matching need not be a maximum matching by exhibiting an undirected graph $G$ and a maximal matching $M$ in $G$ that is not a maximum matching. ($\\textit{Hint:}$ You can find such a graph with only four vertices.) b. Consider an undirected graph $G = (V, E)$. Give an $O(E)$-time greedy algorithm to find a maximal matching in $G$. In this problem, we shall concentrate on a polynomial-time approximation algorithm for maximum matching. Whereas the fastest known algorithm for maximum matching takes superlinear (but polynomial) time, the approximation algorithm here will run in linear time. You will show that the linear-time greedy algorithm for maximal matching in part (b) is a $2$-approximation algorithm for maximum matching. c. Show that the size of a maximum matching in $G$ is a lower bound on the size of any vertex cover for $G$. d. Consider a maximal matching $M$ in $G = (V, E)$. Let $$T = \\{v \\in V: \\text{ some edge in } M \\text{ is incident on } v\\}.$$ What can you say about the subgraph of $G$ induced by the vertices of $G$ that are not in $T$? e. Conclude from part (d) that $2|M|$ is the size of a vertex cover for $G$. f. Using parts (c) and (e), prove that the greedy algorithm in part (b) is a $2$-approximation algorithm for maximum matching. (Omit!)","title":"35-4 Maximum matching"},{"location":"Chap35/Problems/35-5/","text":"In the parallel-machine-scheduling problem , we are given $n$ jobs, $J_1, J_2, \\dots, J_n$, where each job $J_k$ has an associated nonnegative processing time of $p_k$. We are also given $m$ identical machines, $M_1, M_2, \\dots, M_m$. Any job can run on any machine. A schedule specifies, for each job $J_k$, the machine on which it runs and the time period during which it runs. Each job $J_k$ must run on some machine $M_i$ for $p_k$ consecutive time units, and during that time period no other job may run on $M_i$. Let $C_k$ denote the completion time of job $J_k$, that is, the time at which job $J_k$ completes processing. Given a schedule, we define $C_{\\max} = \\max_{1 \\le j \\le n} C_j$ to be the makespan of the schedule. The goal is to find a schedule whose makespan is minimum. For example, suppose that we have two machines $M_1$ and $M_2$ and that we have four jobs $J_1, J_2, J_3, J_4$, with $p_1 = 2$, $p_2 = 12$, $p_3 = 4$, and $p_4 = 5$. Then one possible schedule runs, on machine $M_1$, job $J_1$ followed by job $J_2$, and on machine $M_2$, it runs job $J_4$ followed by job $J_3$. For this schedule, $C_1 = 2$, $C_2 = 14$, $C_3 = 9$, $C_4 = 5$, and $C_{\\max} = 14$. An optimal schedule runs $J_2$ on machine $M_1$, and it runs jobs $J_1$, $J_3$, and $J_4$ on machine $M_2$. For this schedule, $C_1 = 2$, $C_2 = 12$, $C_3 = 6$, $C_4 = 11$, and $C_{\\max} = 12$. Given a parallel-machine-scheduling problem, we let $C_{\\max}^*$ denote the makespan of an optimal schedule. a. Show that the optimal makespan is at least as large as the greatest processing time, that is, $$C_{\\max}^* \\ge \\max_{1 \\le k \\le n} p_k.$$ b. Show that the optimal makespan is at least as large as the average machine load, that is, $$C_{\\max}^* \\ge \\frac 1 m \\sum_{1 \\le k \\le n} p_k.$$ Suppose that we use the following greedy algorithm for parallel machine scheduling: whenever a machine is idle, schedule any job that has not yet been scheduled. c. Write pseudocode to implement this greedy algorithm. What is the running time of your algorithm? d. For the schedule returned by the greedy algorithm, show that $$C_{\\max} \\le \\frac 1 m \\sum_{1 \\le k \\le n} p_k + \\max_{1 \\le k \\le n} p_k.$$ Conclude that this algorithm is a polynomial-time $2$-approximation algorithm. (Omit!)","title":"35-5 Parallel machine scheduling"},{"location":"Chap35/Problems/35-6/","text":"Let $G = (V, E)$ be an undirected graph with distinct edge weights $w(u, v)$ on each edge $(u, v) \\in E$. For each vertex $v \\in V$, let $\\max(v) = \\max_{(u, v) \\in E} \\{w(u, v)\\}$ be the maximum-weight edge incident on that vertex. Let $S_G = \\{\\max(v): v \\in V\\}$ be the set of maximum-weight edges incident on each vertex, and let $T_G$ be the maximum-weight spanning tree of $G$, that is, the spanning tree of maximum total weight. For any subset of edges $E' \\subseteq E$, define $w(E') = \\sum_{(u, v) \\in E'} w(u, v)$. a. Give an example of a graph with at least $4$ vertices for which $S_G = T_G$. b. Give an example of a graph with at least $4$ vertices for which $S_G \\ne T_G$. c. Prove that $S_G \\subseteq T_G$ for any graph $G$. d. Prove that $w(T_G) \\ge w(S_G) / 2$ for any graph $G$. e. Give an $O(V + E)$-time algorithm to compute a $2$-approximation to the maximum spanning tree. (Omit!)","title":"35-6 Approximating a maximum spanning tree"},{"location":"Chap35/Problems/35-7/","text":"Recall the knapsack problem from Section 16.2. There are $n$ items, where the $i$th item is worth $v_i$ dollars and weighs $w_i$ pounds. We are also given a knapsack that can hold at most $W$ pounds. Here, we add the further assumptions that each weight $w_i$ is at most $W$ and that the items are indexed in monotonically decreasing order of their values: $v_1 \\ge v_2 \\ge \\cdots \\ge v_n$. In the 0-1 knapsack problem, we wish to find a subset of the items whose total weight is at most $W$ and whose total value is maximum. The fractional knapsack problem is like the 0-1 knapsack problem, except that we are allowed to take a fraction of each item, rather than being restricted to taking either all or none of each item. If we take a fraction $x_i$ of item $i$, where $0 \\le x_i \\le 1$, we contribute $x_iw_i$ to the weight of the knapsack and receive value $x_iv_i$. Our goal is to develop a polynomial-time $2$-approximation algorithm for the 0-1 knapsack problem. In order to design a polynomial-time algorithm, we consider restricted instances of the 0-1 knapsack problem. Given an instance $I$ of the knapsack problem, we form restricted instances $I_j$, for $j = 1, 2, \\dots, n$, by removing items $1, 2, \\dots, j - 1$ and requiring the solution to include item $j$ (all of item $j$ in both the fractional and 0-1 knapsack problems). No items are removed in instance $I_1$. For instance $I_j$, let $P_j$ denote an optimal solution to the 0-1 problem and $Q_j$ denote an optimal solution to the fractional problem. a. Argue that an optimal solution to instance $I$ of the 0-1 knapsack problem is one of $\\{P_1, P_2, \\dots, P_n\\}$. b. Prove that we can find an optimal solution $Q_j$ to the fractional problem for instance $I_j$ by including item $j$ and then using the greedy algorithm in which at each step, we take as much as possible of the unchosen item in the set $\\{j + 1, j + 2, \\dots, n\\}$ with maximum value per pound $v_i / w_i$. c. Prove that we can always construct an optimal solution $Q_j$ to the fractional problem for instance $I_j$ that includes at most one item fractionally. That is, for all items except possibly one, we either include all of the item or none of the item in the knapsack. d. Given an optimal solution $Q_j$ to the fractional problem for instance $I_j$, form solution $R_j$ from $Q_j$ by deleting any fractional items from $Q_j$. Let $v(S)$ denote the total value of items taken in a solution $S$. Prove that $v(R_j) \\ge v(Q_j) / 2 \\ge v(P_j) / 2$. e. Give a polynomial-time algorithm that returns a maximum-value solution from the set $\\{R_1, R_2, \\dots, R_n\\}$, and prove that your algorithm is a polynomial-time $2$-approximation algorithm for the 0-1 knapsack problem. (Omit!)","title":"35-7 An approximation algorithm for the 0-1 knapsack problem"}]}